
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.0">
    
    
      
        <title>Mega-TTS 2 - Sapphire Lab</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.9f615399.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mega-tts-2" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Sapphire Lab" class="md-header__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sapphire Lab
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Mega-TTS 2
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Sapphire Lab" class="md-nav__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sapphire Lab
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TTS
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../../PDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PDE
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract: 摘要
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1introduction" class="md-nav__link">
    1.Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2related-works" class="md-nav__link">
    2.Related Works: 相关工作
  </a>
  
    <nav class="md-nav" aria-label="2.Related Works: 相关工作">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adaptive-tts" class="md-nav__link">
    Adaptive TTS: 自适应文本转语音
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-shot-tts" class="md-nav__link">
    Zero-Shot TTS: 零样本文本转语音
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prosody-transfer-for-speech-synthesis" class="md-nav__link">
    Prosody Transfer for Speech Synthesis: 语音合成的韵律迁移
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3methodology" class="md-nav__link">
    3.Methodology: 方法
  </a>
  
    <nav class="md-nav" aria-label="3.Methodology: 方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31decomposition-for-prosody-and-timbre" class="md-nav__link">
    3.1.Decomposition for Prosody and Timbre: 韵律和音色的分解
  </a>
  
    <nav class="md-nav" aria-label="3.1.Decomposition for Prosody and Timbre: 韵律和音色的分解">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-formulation" class="md-nav__link">
    Problem Formulation: 问题定义
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decomposition-via-corpus-partition" class="md-nav__link">
    Decomposition via Corpus Partition: 语料库分割
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32compressive-acoustic-autoencoder" class="md-nav__link">
    3.2.Compressive Acoustic Autoencoder: 压缩性声学自编码器
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33prosody-latent-language-model" class="md-nav__link">
    3.3.Prosody Latent Language Model: 韵律隐语言模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34prosody-interpolation" class="md-nav__link">
    3.4.Prosody Interpolation: 韵律插值
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4experiments" class="md-nav__link">
    4.Experiments
  </a>
  
    <nav class="md-nav" aria-label="4.Experiments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41experimental-setup" class="md-nav__link">
    4.1.Experimental Setup
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42results-of-zero-shot-speech-synthesis" class="md-nav__link">
    4.2.Results of Zero-Shot Speech Synthesis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43results-of-prosody-transfer" class="md-nav__link">
    4.3.Results of Prosody Transfer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44ablation-studies" class="md-nav__link">
    4.4.Ablation Studies
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5conclusions" class="md-nav__link">
    5.Conclusions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" class="md-nav__link">
    Appendix
  </a>
  
    <nav class="md-nav" aria-label="Appendix">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adetailed-experimental-settings" class="md-nav__link">
    A.Detailed Experimental Settings
  </a>
  
    <nav class="md-nav" aria-label="A.Detailed Experimental Settings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a1details-in-objective-evaluations" class="md-nav__link">
    A.1.Details in Objective Evaluations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a2details-in-subjective-evaluations" class="md-nav__link">
    A.2.Details in Subjective Evaluations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a3details-in-network-structure" class="md-nav__link">
    A.3.Details in Network Structure
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a4model-configuration" class="md-nav__link">
    A.4.MODEL CONFIGURATION
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a5error-bars-and-random-seeds" class="md-nav__link">
    A.5.ERROR BARS AND RANDOM SEEDS
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="mega-tts-2">Mega-TTS 2<a class="headerlink" href="#mega-tts-2" title="Permanent link">&para;</a></h1>
<details>
<summary>基本信息</summary>

- 标题: Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis
- 作者:
  - 01 [Ziyue Jiang](../../Authors/Ziyue_Jiang.md)
  - 02 [Jinglin Liu](../../Authors/Jinglin_Liu.md)
  - 03 [Yi Ren](../../Authors/Yi_Ren_(任意).md)
  - 04 [Jinzheng He](../../Authors/Jinzheng_He.md)
  - 05 [Zhenhui Ye](../../Authors/Zhenhui_Ye.md)
  - 06 [Shengpeng Ji](../../Authors/Shengpeng_Ji.md)
  - 07 [Qian Yang](../../Authors/Qian_Yang.md)
  - 08 [Chen Zhang](../../Authors/Chen_Zhang.md)
  - 09 [Pengfei Wei](../../Authors/Pengfei_Wei.md)
  - 10 [Chunfeng Wang](../../Authors/Chunfeng_Wang.md)
  - 11 [Xiang Yin](../../Authors/Xiang_Yin.md)
  - 12 [Zejun Ma](../../Authors/Zejun_Ma.md)
  - 13 [Zhou Zhao](../../Authors/Zhou_Zhao_(赵洲).md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2023.07.14 ArXiv v1 (Zero-Shot Text-to-Speech with Arbitrary Length Speech Prompts)
  - 预印时间: 2023.09.28 ArXiv v2
  - 预印时间: 2024.03.18 ArXiv v3 (Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis) 
  - 发表时间: 2024.01.16 ICLR2024
  - 预印时间: 2024.04.10 ArXiv v4
  - 更新笔记: 2024.06.17
- 发表:
  - [ICLR 2024](../../Publications/ICLR.md)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2307.07218)
  - [DOI]()
  - [Github]()
  - [Demo](https://boostprompt.github.io/boostprompt/)
  - [Scholar](https://scholar.google.com/scholar?cluster=16735322993503076322)
- 标签:
  - [语音合成](../../Tags/SpeechSynthesis.md)
  - [零样本](../../Tags/Zero-Shot.md)
  - [语言模型](../../Tags/LanguageModel.md)
- 页数: 21
- 引用: ?
- 被引: 14
- 数据:

</details>

<h2 id="abstract">Abstract: 摘要<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech prompts, which significantly reduces the data and computation requirements for voice cloning by skipping the fine-tuning process.
However, the prompting mechanisms of zero-shot TTS still face challenges in the following aspects: </p>
<ol>
<li>previous works of zero-shot TTS are typically trained with single-sentence prompts, which significantly restricts their performance when the data is relatively sufficient during the inference stage.</li>
<li>The prosodic information in prompts is highly coupled with timbre, making it untransferable to each other.</li>
</ol>
<p>This paper introduces <strong><em>Mega-TTS 2</em></strong>, a generic prompting mechanism for zero-shot TTS, to tackle the aforementioned challenges.
Specifically, we design a powerful acoustic autoencoder that separately encodes the prosody and timbre information into the compressed la tent space while providing high-quality reconstructions.
Then, we propose a multi-reference timbre encoder and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts.
We further leverage the probabilities derived from multiple P-LLM outputs to produce transferable and controllable prosody.
Experimental results demonstrate that <strong><em>Mega-TTS 2</em></strong> could not only synthesize identity-preserving speech with a short prompt of an unseen speaker from arbitrary sources but consistently outperform the fine-tuning method when the volume of data ranges from 10 seconds to 5 minutes.
Furthermore, our method enables to transfer various speaking styles to the target timbre in a fine-grained and controlled manner.
Audio samples can be found in https://boostprompt.github.io/boostprompt/.</p>
</blockquote>
<h2 id="1introduction">1.Introduction<a class="headerlink" href="#1introduction" title="Permanent link">&para;</a></h2>
<blockquote>
<p>In recent years, there has been remarkable progress in the development of text-to-speech (TTS) technology (Shen et al., 2018; Jia et al., 2018; Li et al., 2019; Kim et al., 2020; Ren et al., 2019; 2020; Kim et al., 2021; 2022a).
Among them, adaptive TTS systems (Chen et al., 2021; Min et al., 2021; Kim et al., 2022b) are capable of cloning personalized voices given a few minutes of speech data.
However, the performance of these systems relies heavily on the quality and quantity of the data utilized during the fine-tuning phases (Tan et al., 2021).
Insufficient data during the fine-tuning stages can lead to diminished audio naturalness or speech intelligibility (Kang et al., 2023).
Moreover, the computational demands also constrain its application for cloning everyone’s voice.</p>
<p>To reduce such a reliance, existing works leverage generative models to perform zero-shot TTS (Cooper et al., 2020a; Casanova et al., 2022; Huang et al., 2022a; Kang et al., 2023; Kharitonov et al., 2023; Wang et al., 2023; <a href="../Diffusion/2023.04.18_NaturalSpeech2.md">NaturalSpeech 2</a>; Matthew et al., 2023).
These powerful models can effectively synthesize speech given only a single speech prompt, eliminating the need for data preparation and the computational requirements for fine-tuning methods.
However, the prompting mechanisms of current solutions still face two primary challenges: </p>
<ul>
<li>Lack of multi-sentence prompting strategies.
Previous works of zero-shot TTS typically employ single-sentence speech prompts during training (Wang et al., 2023; <a href="../Diffusion/2023.04.18_NaturalSpeech2.md">NaturalSpeech 2</a>; Matthew et al., 2023).
In inference, the information in the single-sentence speech prompt is insufficient to guide the zero-shot TTS systems to imitate the voice variability of a natural person perfectly.1From another perspective, the performance of fine-tuning methods can be further improved by increasing the amount of data, while zero-shot TTS systems lack an appropriate strategy to extract useful information from multi-sentence speech prompts.</li>
<li>Lack of specialized prompting mechanism for prosodic information.
Current solutions for zero-shot TTS primarily concentrate on improving the similarity of timbre and prosody between the generated speech and the prompts.
However, they neglect to express various unseen prosodic styles in a controlled manner while also preserving the unique timbre of the given one-sentence prompt.
In order to control the prosodic styles, it is necessary to disentangle the prosody information from speech prompts.</li>
</ul>
<p>We address the above challenges by decomposing speech into content, timbre, and prosody.
Intuitively, representing speeches for numerous speakers requires a substantial number of codebook entries for timbre modeling (Defossez et al., 2022; Yang et al., 2023).
Through the decoupling of prosody information, a highly compact codebook for prosody modeling can be obtained, which enables our model to effectively handle extremely long prompts and have flexible control over prosodic styles.
Therefore, this work proposes <strong><em>Mega-TTS 2</em></strong>, a generic framework that boosts the prompting mechanisms for zero-shot TTS systems.
Specifically, we begin by designing an acoustic autoencoder that can effectively decompose speech into prosody and timbre representations and represent them in a compact latent space.
Then, we design a <strong>Multi-Reference Timbre Encoder (MRTE)</strong> and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts.
In addition to the multi-sentence prompting mechanism, we propose a prosody interpolation technique to control the generation process of prosody codes by utilizing prosody prompts from multiple speakers while maintaining the target speaker’s timbre.
By utilizing the probabilities derived from both the prosodic prompts of the target speaker and the auxiliary speaker, the prosodic styles of speech can be generated in a controlled manner.
Experiments on LibriSpeech test-clean (Panayotov et al., 2015) and ESD (Zhou et al., 2021) datasets show that <strong><em>Mega-TTS 2</em></strong> outperforms other state-of-the-art fine-tuning and zero-shot TTS models in terms of speaker similarity and speech naturalness.
Notably, when the length of the prompt is further extended, our method surpasses the fine-tuning baseline model in the objective and subjective evaluations.
The extensive studies on adaptive prosody transfer further highlight the superiority of our proposed prompting mechanisms.
The main contributions of this work are summarized as follows: 
- We design an acoustic autoencoder that separately compresses the prosody and timbre information into the latent space, which allows our model to process prompts of up to 300 seconds in length effectively.
- We propose a multi-reference timbre encoder and an auto-regressive prosody language model to extract fine-grained information from multiple reference speeches, which bridges the speaker similarity gap between zero-shot methods and fine-tuning methods.
- Experimental results also reveal that the performance of <strong><em>Mega-TTS 2</em></strong> surpasses the powerful fine-tuning baseline when we have 10 seconds to 5 minutes of data for each unseen speaker, indicating the superiority of our proposed prompting mechanisms.
- The proposed prosody interpolation technique ensures the controllability of prosody and is capable of transferring various speaking styles to the desired timbre.
For instance, we can transform a voice with a sad tone into a happier one with the auxiliary prosody prompt from another speaker.</p>
</blockquote>
<h2 id="2related-works">2.Related Works: 相关工作<a class="headerlink" href="#2related-works" title="Permanent link">&para;</a></h2>
<h3 id="adaptive-tts">Adaptive TTS: 自适应文本转语音<a class="headerlink" href="#adaptive-tts" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Adaptive TTS (Arik et al., 2018; Kons et al., 2019; Moss et al., 2020; Chien et al., 2021) focuses on synthesizing personalized voice for any user with few data.
During the adaptation process, a TTS model pre-trained on a multi-speaker speech dataset is typically fine-tuned with few adaptation data for the target voice (Tan et al., 2021).
Chen et al. (2018) design independent learned embeddings for each speaker, which requires few data at deployment time to adapt to new speakers rapidly.
AdaSpeech (Chen et al., 2021) proposes an acoustic-condition modeling method for high-quality and efficient customization of new voices.
There are also some works leveraging the meta-learning approach (Chen et al., 2018; Min et al., 2021; Huang et al., 2022b) and data augmentation (Cooper et al., 2020b; Yang &amp; He, 2020) for speaker adaptation.
However, although some works are data-efficient (Min et al., 2021; Huang et al., 2022b) and parameter-efficient (Chen et al., 2021), these systems still suffer from audio quality issues when data size is small, as well as computational cost issues due to hundreds of fine-tuning steps.</p>
</blockquote>
<h3 id="zero-shot-tts">Zero-Shot TTS: 零样本文本转语音<a class="headerlink" href="#zero-shot-tts" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Zero-shot adaptation (Jia et al., 2018; Arik et al., 2018; Cooper et al., 2020a; Casanova et al., 2021; Wu et al., 2022; Huang et al., 2022b;a; Casanova et al., 2022) aims to synthesize unseen voices with a speaker encoder that extracts speaker embeddings from the reference audio.
This scenario is highly attractive because it does not require any adaptation data or parameters (Kang et al., 2022).
The attention-based adaptation method (Choi et al., 2020; Zhou et al., 2022; Yin et al., 2022; Lin et al., 2021) utilizes attention mechanisms to extract fine-grained speech features from reference audios.
Among them, Attentron (Choi et al., 2020) proposes to extracts useful style information from arbitrary number of reference audios.
However, they do not separately model the timbre and prosody information, lacking controllability over timbre and prosody.
Most recently, some works (Kharitonov et al., 2023; Zhang et al., 2023) are proposed to use in-context learning methods (Dong et al., 2022) to efficiently extract speaker information from acoustic prompts and have achieved remarkable results in zero-shot TTS.
<a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> proposes the neural codec language model that exhibits strong in-context learning capability for zero-shot speech generation.
<a href="../Diffusion/2023.04.18_NaturalSpeech2.md">NaturalSpeech 2</a> introduces in-context learning to latent diffusion model (Rombach et al., 2022), which is achieved by partitioning a speech clip into the prompt and target regions.
VoiceBox (Matthew et al., 2023) solves a text-guided speech-infilling task with large-scale data to learn from context information.
However, these methods are trained with single-sentence prompts, lacking an appropriate strategy to extract fine-grained information from multi-sentence speech prompts.</p>
</blockquote>
<h3 id="prosody-transfer-for-speech-synthesis">Prosody Transfer for Speech Synthesis: 语音合成的韵律迁移<a class="headerlink" href="#prosody-transfer-for-speech-synthesis" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Prosody transfer (Lee &amp; Kim, 2019; Klimkov et al., 2019; Gururani et al., 2019; Pan &amp; He, 2021; Karlapati et al., 2022) aims to transfer the prosody from a reference utterance to the synthesized target speech, which is essential for producing natural and expressive speech in a controlled manner (Wagner &amp; Watson, 2010).
Skerry-Ryan et al. (2018) first integrate a prosody reference encoder into a TTS system based on Tacotron (Wang et al., 2017), which is capable of performing similar-text prosody transfer.
Recent works try to transfer prosody in different-text and different-speaker settings (Karlapati et al., 2020; Zaıdi et al., 2021) with the bottleneck of the prosody encoder.
Among them, Daft-Exprt (Zaıdi et al., 2021) uses a gradient reversal layer to penalize the prosody encoder if its output contains information about the speaker identity from the reference utterance, which enhances the target speaker fidelity for cross-speaker prosody transfer.
However, as pointed out by Sigurgeirsson &amp; King (2023), current solutions do not learn a transferable representation of prosody, but rather an utterance-level representation that is relatively dependent on both the reference speaker and reference text.</p>
</blockquote>
<h2 id="3methodology">3.Methodology: 方法<a class="headerlink" href="#3methodology" title="Permanent link">&para;</a></h2>
<blockquote>
<p>This section introduces <strong><em>Mega-TTS 2</em></strong>.
To begin with, we provide an intuitive illustration of how <strong><em>Mega-TTS 2</em></strong> decomposes the timbre and prosody information from speech.
Next, we provide detailed explanations of our prompting mechanisms and the two-stage training process of the proposed model.</p>
</blockquote>
<p>本节介绍 <strong><em>Mega-TTS 2</em></strong>.
首先我们提供 <strong><em>Mega-TTS 2</em></strong> 解构语音的音色和韵律信息的直观插图.
然后我们提供我们提示机制的详细解释和两阶段训练过程.</p>
<h3 id="31decomposition-for-prosody-and-timbre">3.1.Decomposition for Prosody and Timbre: 韵律和音色的分解<a class="headerlink" href="#31decomposition-for-prosody-and-timbre" title="Permanent link">&para;</a></h3>
<h4 id="problem-formulation">Problem Formulation: 问题定义<a class="headerlink" href="#problem-formulation" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Denote $H(X)$ as the Shannon entropy of $X$ and Denote $I(Y;X)$ as the mutual information. 
We assume that the mel-spectrogram $y$ can be reconstructed through the following generative process: $y=D(z_c,z_{pd},z_t,g)$, where $z_c$ and $z_t$ denote the fine-grained content and timbre hidden states.
$g$ denotes the global style information that contains timbre and prosody.
We assume that $z_{pd}=(z_p,z_d)$ contains the fine-grained prosodic style information of pitch and energy $z_p$ and duration $z_d$.
$z_d=Aligner(y)$ can be obtained by the external alignment tools (McAuliffe et al., 2017) and disentangled from $z_{pd}$.
Denote $D$ as the mel-spectrogram decoder.
Our goal is to construct an autoencoder-based model to disentangle speech components.</p>
</blockquote>
<h4 id="decomposition-via-corpus-partition">Decomposition via Corpus Partition: 语料库分割<a class="headerlink" href="#decomposition-via-corpus-partition" title="Permanent link">&para;</a></h4>
<p>Decomposition via Corpus Partition Denote Y = {y1, · · · , yn} as the speech corpus for a certain speaker S. In training, we partition Y into the target mel-spectrogram yt and the other mel-spectrograms ˜y. Here, we make an important assumption that the mutual information between yt and ˜y only contains timbre information H(zt) and global style information H(g) of yt, i.e.,
First, based on the assumption, zt and g can be extracted through Et(˜y), and there is no way for Et(˜y) to obtain the zp and zc. Second, if we only feed phoneme sequence to Ec, Ec can only pass all the content information zc. Third, since the information zc and zt are available now, the prosody encoder Ep will prioritize removing the fine-grained content and timbre information if it is forced to lose some information by information bottleneck B(·) (Qian et al., 2019). The bottleneck forces Ep(yt) to pass only the fine-grained prosodic style zp that other encoders cannot supply, hence achieving the decomposition. We provide a detailed explanation of how we ensure the validity of Equation 1 in Appendix A.8. After the decomposition, we describe the detailed designs of our prompting mechanisms in the following subsections.</p>
<h3 id="32compressive-acoustic-autoencoder">3.2.Compressive Acoustic Autoencoder: 压缩性声学自编码器<a class="headerlink" href="#32compressive-acoustic-autoencoder" title="Permanent link">&para;</a></h3>
<p>Note that to store timbre information for thousands of speakers, we need a large number of codebook entries. However, since the prosody and timbre have been decomposed, the prosodic information zp can be compressed into a highly compact codebook, and the timbre information zt can be extracted via a powerful speaker encoder. The decomposition strategy not only allows our model to accommodate extremely long prosody prompts but also enables our model to control the prosodic styles of generated speeches. As shown in Figure 1, we design the vector quantised (VQ) encoder as Ep, the multireference timbre encoder as Et, and the content encoder as Ec. Since Ep mainly captures the prosodic variance information, a GAN-based mel-spectrogram decoder D is adopted to model the highfrequency details in spectrograms, which ensures perceptually high-quality reconstructions. Overall, the first-stage training loss can be formulated as L = Lrec + LVQ + LAdv, where Lrec = ∥yt − ˆyt∥2 is the reconstruction loss, LVQ is the VQ codebook loss (Van Den Oord et al., 2017), and LAdv is the LSGAN-styled adversarial loss (Mao et al., 2017) whose objective is to minimize the distribution distance between the predicted mel-spectrograms and the ground truth mel-spectrograms. Among the proposed three encoders, the content encoder is composed of several feed-forward Transformer layers following common practice in non-autoregressive TTS systems (Ren et al., 2019). In the following paragraphs, we describe the details of the prosody and timbre encoders, respectively.</p>
<p>Vector Quantised Encoder The vector quantised encoder Ep consists of two convolution stacks and a vector quantization bottleneck. The first convolution stacks compress mel-spectrograms into hidden states by a factor of r in length, and the second stacks capture the correlation of features. After that, the vector quantization layer utilizes these hidden states to obtain prosody codes u = {u1, u2, ..., un} and hidden states zp. The information bottleneck B(·) of the VQ encoder is composed of the temporal compression and the vector quantization layer. The detailed instructions for ensuring an appropriate information bottleneck B(·) can be found in Appendix F.
Multi-Reference Timbre Encoder Our objective is to extract fine-grained timbre information from multi-sentence speech prompts. Since speakers can change their timbre by using different speaking techniques according to their speaking habits or desired semantic meanings (McAdams, 2013), the timbre encoder needs to extract fine-grained timbre information from multiple prompts that can represent the speakers’ habits. Here, we introduce a multi-reference timbre encoder (MRTE) to achieve this objective. First, we concatenate the reference mel-spectrograms ˜y that belong to the target speaker but are different from the target mel-spectrogram. The mel encoder then compresses the concatenated mel-spectrogram into acoustic hidden states zt by a factor of d in length. Subsequently, to extract semantically relevant timbre information from speech prompts, we introduce a timbre-tocontent attention module. This module takes zc as the query and zt as both the key and the value. Finally, we upsample the output of the timbre-to-content attention module to match the length of the target mel-spectrogram using the length regulator (Ren et al., 2019).</p>
<h3 id="33prosody-latent-language-model">3.3.Prosody Latent Language Model: 韵律隐语言模型<a class="headerlink" href="#33prosody-latent-language-model" title="Permanent link">&para;</a></h3>
<p>Unlike previous models that are trained with single-sentence prompts, our prosody latent language model (P-LLM) aims to capture the speaker’s prosodic patterns from multi-sentence prompts effectively. During the second-stage training process, we first extract the compressed prosody hidden states {zp1, zp2, · · · , zpn} and the content hidden states {zc1, zc2, · · · , zcn} from multiple speech clips {s1, s2, · · · , sn} of the target speaker using the proposed compressive acoustic autoencoder. We then concatenate them along the time axis to construct z′ p = Concat(zp1, zp2, · · · , zpn) and z′ c = Concat(zc1, zc2, · · · , zcn). In order to match the lengths of z′ c in the temporal dimension, we expand z′ c to the frame level with duration information zd and compress it r times with a max pooling layer. After that, we transform z′ c into the P-LLM, which predicts the prosody code in an auto-regressive manner:
p to prosody code u′ and then feed u′ and z′
p and z′
where θ is the parameters of P-LLM and L is the length of the concatenated prosody code u′. In training, we set batch size as 1 to increase the maximum number m of prosody codes in each batch as much as possible. If the total number of speech frames from a single speaker is less than m × r, we will include speech samples from other speakers in this batch and incorporate speaker-level attention masks into P-LLM. We do not specifically define the speech prompt; instead, we train the language model directly using the concatenated speech samples through the teacher-forcing technique with the cross-entropy loss. To avoid the transition area problems caused by directly concatenating the prompts, we assign the start token and end token to each sentence, which guides P-LLM to continue writing the current sentence and extract useful information from previous sentences. This training strategy enables the model to capture the useful prosody-level information contained in the multi-sentence prompts. Therefore, in the inference stage, users can flexibly improve the generation quality by extending the length of prompts by concatenating the reference speech clips. For duration modeling, we propose a phoneme-level auto-regressive duration model. This model enhances the duration modeling by leveraging the powerful in-context learning capabilities of auto-regressive models. The overall architecture of the auto-regressive duration model remains the same as P-LLM, but we use mean squared error (MSE) loss instead.</p>
<h3 id="34prosody-interpolation">3.4.Prosody Interpolation: 韵律插值<a class="headerlink" href="#34prosody-interpolation" title="Permanent link">&para;</a></h3>
<p>Here, we propose a prosody interpolation technique to control or replace the prosodic style of the target speaker in the discrete space while ensuring the quality of timbre reconstruction. We achieve this objective by interpolating the probabilities from multiple P-LLM outputs, which come from multiple speakers. For example, our target speaker has a relatively sad speaking tone, but we want to generate speeches that sound happier for him while preserving his timbre. The solution is to 1) extract prosody latent ua from speeches in a happy tone of other speakers and the sad prosody latent ub from the target speech prompt; 2) utilize two language models to separately decode the target prosody code ˆu with the prosodic prompt ua and ub. These language models share the same parameters. In every step t of the decoding process, the probability distributions of the two language models are interpolated with the weight γ, which can be formulated as follows:
where zcb and zca are the content information from speech clips sb and sa. ˆzc is the content information of the target sentence. With our prosody interpolation technique, users can freely control the prosodic style of the generated speech in the inference stage. Moreover, the proposed prosody interpolation algorithm utilizes the autoregressive probability distribution of the language model for prosody transfer. Compared with directly substituting the time-averaged prosody representation ub with ua (Karlapati et al., 2020; Zaıdi et al., 2021), the prosody latent language model is able to mix ua and ub in a soft and fine-grained manner in the autoregressive generation process.</p>
<h2 id="4experiments">4.Experiments<a class="headerlink" href="#4experiments" title="Permanent link">&para;</a></h2>
<h3 id="41experimental-setup">4.1.Experimental Setup<a class="headerlink" href="#41experimental-setup" title="Permanent link">&para;</a></h3>
<p>Training Datasets. We train <strong><em>Mega-TTS 2</em></strong> and all baselines on LibriLight (Kahn et al., 2020), which contains 60K hours of unlabelled speech derived from LibriVox audiobooks. The sample rate is 16KHz for all speech data. We transcribe the speech with the hybrid DNN-HMM ASR model pre-trained on 960 hours labeled LibriSpeech following VALL-E (Wang et al., 2023). We align the phoneme sequence with speech using the external alignment tool (McAuliffe et al., 2017).
Model Configuration. We provide model configuration in Appendix A.4 and detailed hyperparameter settings in Table 5.
Training and Inference. In the first training stage, we train the first-stage model on 4 NVIDIA A100 GPUs, with a batch size of 48 sentences on each GPU. In the second stage, we train the P-LLM and duration model on 8 NVIDIA A100 GPUs, with a batch size of 4,000 tokens on each GPU. It means that our model supports 4,000 × 8 frames of prompts theoretically. We use the Adam optimizer with β1 = 0.9, β2 = 0.999, ϵ = 10−9 and follow the same learning rate schedule in Vaswani et al. (2017). It takes 600k steps for the first stage model’s training and 300K steps for the second stage model’s training until convergence. The predicted mel-spectrograms are transformed into audio samples using pre-trained HiFi-GAN V1 (Kong et al., 2020).
Objective Metrics. For zero-shot TTS, we evaluate the word error rate (WER), speaker similarity (SIM), and average dynamic time warping (DTW) (M¨uller, 2007) distance of the pitch for the groundtruth speech and synthesized speech. In terms of the cosine speaker similarity, we use the WavLM model (Chen et al., 2022) fine-tuned for speaker verification2 to compute the cosine speaker similarity score between the ground-truth speech and the synthesized speech. The similarity score is in the range of [−1, 1], where a larger value indicates a higher similarity of input samples. We also evaluate the word error rate (WER) for cross-lingual TTS. We use the released HuBERT-Large model (Hsu et al., 2021) fine-tuned on the LibriSpeech 960h dataset to transcribe the generated speech into text. Then, the WER between the transcribed text and the original target text is measured. We use all samples in</p>
<p>the test set for the objective evaluation. For prosody transfer, we evaluate the WER, SIM, duration error (DE), and the moments (standard deviation (σ), skewness (γ) and kurtosis (κ)) (Andreeva et al., 2014; Niebuhr &amp; Skarnitzl, 2019) of the pitch distribution.
Subjective Metrics. We conduct the MOS (mean opinion score) evaluation on the test set to measure the audio naturalness via Amazon Mechanical Turk. We keep the text content and prompt speech consistent among different models to exclude other interference factors. We randomly choose 50 samples from the test set of each dataset for the subjective evaluation, and each audio is listened to by at least 20 testers. We analyze the MOS in two aspects: QMOS (Quality, clarity, naturalness, and high-frequency details) and SMOS (Speaker similarity in terms of timbre reconstruction and prosodic pattern). We also analyze the CMOS in terms of audio quality and speaker similarity. We tell the testers to focus on one corresponding aspect and ignore the other aspect when scoring.</p>
<h3 id="42results-of-zero-shot-speech-synthesis">4.2.Results of Zero-Shot Speech Synthesis<a class="headerlink" href="#42results-of-zero-shot-speech-synthesis" title="Permanent link">&para;</a></h3>
<p>In this subsection, we evaluate our model with various lengths of speech prompts and compare our model with zero-shot and fine-tuning baselines to demonstrate the effectiveness of the multi-sentence prompting mechanism. We randomly choose 20 speakers from the LibriSpeech test-clean set and randomly choose 400 seconds of speeches for each of them. We split the 400 seconds of speech into a 300-second prompt set and a 100-second target set. We keep the prompts consistent among different models to exclude other interference factors. We compare the zero-shot speech synthesis performance of <strong><em>Mega-TTS 2</em></strong> with two systems, including: 1) VALL-E (zero-shot) (Wang et al., 2023), a large-scale zero-shot TTS model using large language models to generate discrete speech codes. Since VALL-E has not been open-sourced yet, we carefully implement it for optimal performance; 2) Baseline (fine-tune), a model that incorporates the GAN used in our <strong><em>Mega-TTS 2</em></strong> to the FastSpeech 2 backbone (Ren et al., 2020). To make the baseline support adaptive scenarios, we use the powerful speaker encoder from Meta-StyleSpeech (Min et al., 2021) to extract timbre information. We carefully fine-tune the baseline system for 2,000 steps to reach an optimal balance between WER and SIM. Note that all of the systems in this experiment are pre-trained on the LibriLight dataset. We provide further explanation for the selection of the baseline systems in Appendix A.7.
Analysis As shown in Table 1, as the amount of adaptation data increases, the performance of MegaTTS 2 continues to improve. Although the performance of VALL-E improves as the data volume increases from 3 seconds to 10 seconds, the performance significantly drops in the 20-second setting due to the single-sentence prompting mechanisms in training. Moreover, since the compression rate of the Encodec model restricts the length of prompts, VALL-E fails to generate reasonable speeches with prompts longer than 20 seconds in our experiments. From another perspective, when we have 10 seconds or 60 seconds of speeches for each speaker, our <strong><em>Mega-TTS 2</em></strong> surpasses the fine-tuning baseline in terms of speech naturalness and speaker similarity. Additionally, when we have 300</p>
<p>seconds of speeches per speaker, <strong><em>Mega-TTS 2</em></strong> still outperforms the baseline system in terms of WER and achieves comparable performance with it in terms of speaker similarity. We also visualize the WER and SIM in the fine-tuning process and compare the baseline system with <strong><em>Mega-TTS 2</em></strong> in Figure 3. Our approach can enhance speaker similarity by utilizing more data like fine-tuning baseline, while maintaining a relatively low word error rate.</p>
<h3 id="43results-of-prosody-transfer">4.3.Results of Prosody Transfer<a class="headerlink" href="#43results-of-prosody-transfer" title="Permanent link">&para;</a></h3>
<blockquote>
<p>In this subsection, we evaluate the prosody transfer performance of our model by transferring the emotional styles from the ESD dataset (Zhou et al., 2021) to speakers in the LibriSpeech test-clean dataset. We randomly choose 20 speakers from the LibriSpeech test-clean set and choose 50 sentences for each of them. Then, we randomly select an emotional speech clip from the ESD dataset for each of the sentences in the LibriSpeech test-clean set and use the selected emotional speech as the prosodic reference. We keep the reference speeches consistent among different models to exclude other interference factors. We compare the prosody transfer performance of <strong><em>Mega-TTS 2</em></strong> with two systems, including: 
(1) CopyCat (Karlapati et al., 2020), a model that utilizes a reference encoder architecture capable of capturing temporal prosodic representations; 
(2) Daft-Exprt (Zaıdi et al., 2021), a model disentangles identity and prosodic information through an adversarial training strategy that enables accurate prosody transfer across speakers. </p>
<p>To make fair comparisons, we incorporate the techniques for prosody transfer from CopyCat and Daft-Exprt to the baseline system proposed in the previous subsection and scale up the model capacity to ensure that all models have a comparable number of parameters. All of the systems in this experiment are pre-trained on the LibriLight dataset.</p>
<p>Analysis Table 2 demonstrates that compared with CopyCat and Daft-Exprt, the moments (σ, γ, and κ) of the generated speeches of Megs-TTS are closer to the ground-truth audio and the DE is lower than other methods, demonstrating the effectiveness of the proposed prosody interpolation techniques. Besides, we observe that our method can efficiently preserve the original timbre and maintain a high audio quality. We also visualize the prosody distribution before and after the prosody transfer process and compare the baseline system with <strong><em>Mega-TTS 2</em></strong> in Figure 4.</p>
</blockquote>
<h3 id="44ablation-studies">4.4.Ablation Studies<a class="headerlink" href="#44ablation-studies" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Prosody and Timbre Prompts We evaluate different lengths of prompts for the MRTE and P-LLM separately. In Table 3, the SIM score and the speech quality increase with longer timbre prompts while the DTW distance almost remains unchanged. When we increase the length of prosody prompts, the DTW distance decreases while the speaker similarity remains at the same level. It can be seen that the proposed timbre and prosody prompting mechanisms boost the subjective speaker similarity in terms of timbre and prosody modeling separately.</p>
<p>VQ Encoder and MRTE We test the following four settings: 
(1) w/o MRTE, which removes the MRTE from our model and does not disentangle the prosody and timbre; 
(2) w/ VAE, which uses VAE to perform generative prosody modeling; 
(3) w/ VAE+LDM, which uses VAE and latent diffusion model (LDM) (Rombach et al., 2022) to perform generative prosody modeling. </p>
<p>The architecture and prompting mechanism of LDM is based on <a href="../Diffusion/2023.04.18_NaturalSpeech2.md">NaturalSpeech 2</a>. All baselines use 10 seconds of prompts. The results are shown in Table 4. For setting 1), it can be observed that the removal of MRTE significantly affects both the audio quality and speaker similarity. This is because the timbre information is absorbed by the VQ codebook and puts great pressure on the P-LLM, which demonstrates the effectiveness of decomposing timbre and prosody information. For setting 3), substituting the VQ encoder and P-LLM with VAE and LDM results in similar performance compared to Ours-10s. However, the performance of w/ VAE+LM is still much inferior to Ours-300s, indicating the superiority of the proposed multi-sentence prompting mechanism.</p>
</blockquote>
<h2 id="5conclusions">5.Conclusions<a class="headerlink" href="#5conclusions" title="Permanent link">&para;</a></h2>
<blockquote>
<p>In this paper, we present <strong><em>Mega-TTS 2</em></strong>, a framework that boosts the prompting mechanisms for zero shot TTS systems.
With the proposed multi-sentence prompting strategy, our approach outperforms the fine-tuning baseline when 10 seconds to 5 minutes of adaptation data is available for each speaker.
Furthermore, our method utilizes a prosody interpolation technique to successfully transfer various prosodic styles to the target speaker while preserving the target speaker’s timbre.
Experimental results demonstrate that our method exhibits superior performance in terms of audio naturalness and speaker similarity.
Due to space limitations, we include additional discussions in the appendix.</p>
</blockquote>
<h2 id="appendix">Appendix<a class="headerlink" href="#appendix" title="Permanent link">&para;</a></h2>
<h3 id="adetailed-experimental-settings">A.Detailed Experimental Settings<a class="headerlink" href="#adetailed-experimental-settings" title="Permanent link">&para;</a></h3>
<h4 id="a1details-in-objective-evaluations">A.1.Details in Objective Evaluations<a class="headerlink" href="#a1details-in-objective-evaluations" title="Permanent link">&para;</a></h4>
<p>Here, we provide details of the model used in objective evaluations.
Speaker Similarity Model To measure the speaker similarity, we use the WavLM (Chen et al., 2022) model fine-tuned for speaker verification from https://huggingface.co/microsoft/ wavlm-base-plus-sv to extract the speaker embedding. Then the cosine similarity between the synthesized speech’s speaker embedding and the ground-truth speech’s speaker embedding is calculated as the speaker similarity score. The WavLM model is pre-trained on 94,000 hours of speech data and fine-tuned on the VoxCeleb1 dataset using an X-Vector head with an Additive Margin Softmax loss, which achieves 0.84%, 0.928%, and 1.758% EER (Equal Error Rate) on the Vox1-O, Vox1-E, and Vox1-H trial lists.
ASR Model To measure the audio quality and speech intelligibility, we evaluate the word error rate (WER) metric. We use the fine-tuned HuBERT-Large model to transcribe the synthesized speech into text and calculate the WER between the transcribed text and the original target text. The HuBERTLarge model from https://huggingface.co/facebook/hubert-large-ls960-ft is fine-tuned on 960h of Librispeech and achieves 1.5%, 3.0%, 1.9%, and 3.3% WER on the dev-clean, dev-other, test-clean, and test-other set of Librispeech.</p>
<h4 id="a2details-in-subjective-evaluations">A.2.Details in Subjective Evaluations<a class="headerlink" href="#a2details-in-subjective-evaluations" title="Permanent link">&para;</a></h4>
<p>We perform the audio quality and speaker similarity evaluations on Amazon Mechanical Turk (MTurk). For each dataset, we randomly select 50 samples from the test set and use the TTS systems to generate the audio samples. Each audio has been listened to by at least 20 listeners. For MOS, each tester is asked to evaluate the subjective score of a sentence on a 1-5 Likert scale. For CMOS, listeners are asked to compare pairs of audio generated by systems A and B following Loizou (2011), indicating which of the two audio they prefer. For audio quality evaluation (QMOS and CMOS-Q), we tell listeners to “Please focus on the speech quality in terms of clarity, naturalness, and high-frequency details, and ignore other factors”. For speaker similarity evaluations (MOS-S), we tell listeners to “Please focus only on the similarity of the speaker to the reference one in terms of the timbre and prosodic patterns, and ignore the differences of content, grammar, audio quality, or other factors.”. We paid $15 to participants hourly and totally spent about $1200 on participant compensation. We tell the participants that the data will be used in scientific research.</p>
<h4 id="a3details-in-network-structure">A.3.Details in Network Structure<a class="headerlink" href="#a3details-in-network-structure" title="Permanent link">&para;</a></h4>
<p>MRTE As shown in Figure 5, the proposed MRTE is composed of two convolution stacks and a downsampling block. To reduce the computational requirements while maintaining the quality of timbre reconstruction, we downsample the timbre hidden states by a factor of d = 16 in length. In training, we randomly sample 2,000 frames from ˜y for training efficiency.
VQ Encoder The bottleneck of our VQ Encoder is composed of a max pooling layer with a stride of 8 and a vector quantised layer. In our experiments, we found that compressing the mel-spectrograms with a compression rate of r = 8 yields superior results compared to phoneme-level compression. We have tried different compression rates (2, 4, 8, 16, 32) and found that r = 8 reached an optimal balance between the reconstruction performance and compression. On the other hand, in the training process, we also found that the vanilla VQ-VAE suffers from codebook collapse (Takida et al., 2022), which means only a small portion of codebook vectors are optimized. It restricts the expressive capacity of the codebook and affects the convergence of the training process. To solve the codebook collapse issue, we adopt a dynamical initialization strategy based on CVQ-VAE (Zheng &amp; Vedaldi, 2023) during training, which ensures the code vectors that are less-used or unused to be modified more than frequently used ones.</p>
<h4 id="a4model-configuration">A.4.MODEL CONFIGURATION<a class="headerlink" href="#a4model-configuration" title="Permanent link">&para;</a></h4>
<p>Our <strong><em>Mega-TTS 2</em></strong> consists of three encoders, a prosody latent language model, a mel decoder, and a discriminator. The prosody encoder, timbre encoder, and decoder consist of 5 convolutional blocks with 512 hidden size and 5 kernel size. The content encoder is an 8-layer Transformer (Vaswani et al., 2017; Shen et al., 2023a) with 512 hidden size. The GAN discriminator follows the architecture of ML-GAN proposed in Chen et al. (2020). The P-LLM model is a decoder-only architecture that contains 12 Transformer layers with 1024 hidden size, which has 151M parameters. The duration predictor is an 8-layer decoder-only Transformer model with 512 hidden size. The codebook embedding size is 1024, and the hidden size of the codebook vector is 256. The compression rate r and d is set as 8 and 16, respectively. For prosody transfer experiments, γ is set as 0.8. We provide detailed hyper-parameter settings about the model configuration in Table 5.</p>
<h4 id="a5error-bars-and-random-seeds">A.5.ERROR BARS AND RANDOM SEEDS<a class="headerlink" href="#a5error-bars-and-random-seeds" title="Permanent link">&para;</a></h4>
<p>For the subjective evaluations, we report confidence intervals of the results of MOS tests. For the objective evaluations, we ran the experiments 10 times with 10 different random seeds ([1234, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]) and obtained the averaged results.</p>
<p>A.6 SAMPLING STRATEGY FOR P-LLM
In all of our experiments, we utilize the top-k sampling strategy for P-LLM, where k is set to 10. The sampling-based method, when used with an appropriate k, enhances the output diversity compared to greedy decoding.
A.7 ABOUT THE SELECTION OF BASELINES
VALL-E (Wang et al., 2023), <a href="../Diffusion/2023.04.18_NaturalSpeech2.md">NaturalSpeech 2</a> (Leng et al., 2023), and VoiceBox (Matthew et al., 2023) are the state-of-the-art zero-shot TTS models. In the experiments of zero-shot TTS, we have tried to carefully reproduce their works but failed to reproduce <a href="../Diffusion/2023.04.18_NaturalSpeech2.md">NaturalSpeech 2</a> and VoiceBox. Since all of them do not provide the pre-trained models and source code, we only compare <strong><em>Mega-TTS 2</em></strong> with VALL-E in our experiments.
A.8 DETAILED DECOMPOSITION STRATEGY
The prosody encoder Ep aims to capture fine-grained and local prosodic style zp. For local prosodic style zp, we assume that psd(·) is a perfect local prosody extractor, and we can obtain the following equation: I(psd(yt), psd(˜y)) = 0. The content information zc is also local and fine-grained like zp. On the other hand, the global prosodic information like the averaged volume and pitch can not be captured by Ec, intuitively. And since we have designed an information bottleneck B(·) for Ep, the global prosodic information will be prioritized by the timbre encoder Et and stored in H(zt). Now that both the local and global prosodic information is appropriately extracted, the validity of Equation 1 and our disentanglement strategy can be ensured.
B ABOUT SCALING UP DATASET SIZE
Scaling up dataset size is crucial for the practical application of zero-shot TTS. Therefore, we crawled 200K hours of audiobook recordings from YouTube and novelfm3. The crawled corpus contains both labelled and unlabelled speeches, and most of them do not have speaker information. To transcribe the unlabelled speech in the wild, we use a powerful ASR model called WhisperX (Bain et al., 2023). And to obtain the speaker information, we use a released automatic speaker diarization model called pyannote.audio4, which achieves DER=11.24% on the VoxConverse dataset and DER=14.09% on the AISHELL-4 dataset. In this experiment, we do not change the hyperparameter settings of our model. The results are shown in Table 6. It can be seen that increasing the dataset size can improve the speaker similarity of the generated speeches.
C ABOUT THE DEFINITION OF ADAPTIVE TTS
The concept of adaptive TTS encompasses many aspects like the adaption for different voices, languages, styles, and domains (Tan et al., 2021). It is also known as various terms in academia and industry, such as voice adaptation (Chen et al., 2018), voice cloning (Arik et al., 2018), custom voice (Chen et al., 2021), etc. In this paper, we primarily focus on adaptive TTS for different voices.</p>
<p>D VISUALIZATION OF ATTENTION MATRICES
To further verify the proposed P-LLM and multi-sentence prompting mechanism, we visualize the attention matrices averaged across all layers of P-LLM in Figure 6. In this experiment, we separately conduct short-sentence generation and long-sentence generation. For short-sentence generation, we randomly selected two sentences that are shorter than 3 seconds from speaker “908” in the LibriSpeech test-clean set and concatenated them together. The target texts for Figure 6 (a) and (b) are both about 15 words in length. For long-sentence generation, we randomly selected two sentences that are longer than 15 seconds from speaker “908” in the LibriSpeech test-clean set and concatenated them together. The target texts for Figure 6 (c) and (d) are both about 100 words in length. It can be seen that our P-LLM can capture both short-term and long-term information, demonstrating the effectiveness of the P-LLM’s training strategy and the multi-sentence prompting mechanism.
E LIMITATIONS AND ETHICS IMPACTS
In this section, we begin by discussing the limitations of the proposed method and outlining our strategies for addressing them in future research. Subsequently, we discuss the ethical impacts that might be brought by zero-shot TTS and our measures to address these concerns.
E.1 LIMITATIONS AND FUTURE WORK
Firstly, our model is trained on an English dataset and does not support multilingual TTS. We plan to address this problem by introducing more multilingual training data. Secondly, the speech quality can be improved by introducing more high-fidelity training data. Thirdly, a well-designed attention window may further enhance the in-context-learning capability of our P-LLM.
E.2 ETHICS IMPACTS
<strong><em>Mega-TTS 2</em></strong> improves the quality and efficiency of zero-shot speech synthesis, which makes it easier for people to synthesize personalized speeches. Under appropriate and legal usage, this technique could facilitate applications like movies, games, podcasts, and other services, making human life more convenient. However, zero-shot TTS may be misused in deepfake-related usages, such as spoofing voices. To handle this, potential solutions like building a corresponding deepfake detection model should be considered. We also plan to add watermarks to the synthesized speeches so that the public can easily tell whether the speeches are synthesized or not. Additionally, restrictions will be included in the license of our project to prevent the misuse of the model.
F DESCRIPTION OF INFORMATION BOTTLENECK
The settings of the information bottleneck B(·) are crucial for the performance of disentanglement of our proposed method. Intuitively, there are four crucial variables for ensuring an appropriate information bottleneck: the number of codebook embedding, the codebook channel size, the compression</p>
<p>rate r of the VQ Encoder, and the downsampling rate d of the MRTE. However, the search space of these hyperparameters is too large. Since the settings of r and d also influence the reconstruction quality, the burden of P-LLM, and the computational requirements, we first consider r and d, fix them, and find the best setting for the hyperparameters of the codebook.
The Compression Rate r We have conducted evaluations for different compression rates r of the VQ Encoder. In the experiments, we found that a lower compression rate would result in better reconstruction performance for the compressive acoustic autoencoder, but it would impose a heavier burden on P-LLM since the token sequence is longer. As shown in Table 9, although r = 2 achieves the highest objective similarity score, the subjective speech quality and similarity significantly decrease, which means the final quality of generation is affected. Therefore, we use r = 8 to reach an optimal balance between the reconstruction performance and compression.
The Downsampling Rate d We have conducted evaluations for different downsampling rates d of the MRTE. The results are shown in Figure 10. It can be seen that when the downsampling rate d of the MRTE is low, the mel-spectrogram sequence can provide more information to the timbre encoder, resulting in better reconstruction. However, a low downsampling ratio puts a significant computational burden on the attention operation in MRTE. To reduce the computational requirements while maintaining the quality of timbre reconstruction, we choose d = 16 for our <strong><em>Mega-TTS 2</em></strong>.
The Information Bottleneck with Different Amount of Data Intuitively, the performance of the information bottleneck might be very sensitive to the size of the dataset. Therefore, we conduct experiments analyzing the relationship between dataset size and the hyperparameters. The results are presented in Appendix G. Although the hyperparameters do not change across these experiments, we find that the model consistently performs well in scenarios with varying amounts of available data.
G SCALING WITH DIFFERENT SIZES OF TRAINING DATA
Here we evaluated the performance of our <strong><em>Mega-TTS 2</em></strong> scale with varying amounts of available data. In this experiment, all of the systems use 3 seconds of speech prompts. The results are shown in the following table. We can see that <strong><em>Mega-TTS 2</em></strong> performs well with different sizes of training data, while VALL-E fails to obtain satisfying results when the data is insufficient. We also scale our <strong><em>Mega-TTS 2</em></strong> with 200K hours of speeches and the results can be found in Appendix B.
H THE STRATEGY OF PROSODY MODELING
In this section, we conduct experiments to verify the performance of the phoneme-level, word-level, and stride-8-level prosody modeling. Stride-8 means that the stride of the pooling layer inside the VQ encoder is set to 8. It is worth noting that ProsoSpeech (Ren et al., 2022) utilizes word-level prosody modeling. Both of us use the auto-regressive Transformer for prosody modeling. However, ProsoSPeech aims to improve the naturalness of prosody modeling. Compared with it, our P-LLM aims at improving the similarity of speaker-relevant prosodic patterns, which extracts fine-grained prosodic information from latent prosodic prompts by leveraging the powerful in-context learning capability of LLM. The experimental results are shown in the following table. It can be seen that the stride-8-level prosody modeling achieves the best performance. Intuitively speaking, the phonemelevel prosody modeling provides finer-grained information for better reconstruction while word-level prosody modeling provides more semantic information. Both of these methods would be easily</p>
<p>afftected by the alignment accuracy of the speeches in training and inference stages. In order to enhance the stability and performance of the proposed model, we use stride-8-level prosody modeling.
I SPECIAL CASES FOR ASSUMPTION 1
In practical scenarios, there is a special case for Assumption 1: the timbre of a speaker may vary significantly over different time periods. To address this special case, we select ˜y randomly from regions near yt as much as possible, ensuring that the timbre information of ˜y is close to that of yt.
J DIFFERENT LENGTHS OF CONTEXT DURING TRAINING
Here we make ablation studies for different lengths of context during our model’s training process. We separately train P-LLM with different numbers of the contextual VQ code tokens and train our compressive acoustic autoencoder with different numbers of the contextual mel-spectrogram frames for MRTE. The results are shown in Figure 7. It can be seen that when we increase the length of context, the performance of the model during training significantly improves, demonstrating the effectiveness of our multi-reference training strategy.
K EXPLANATIONS ABOUT MORE CASES OF ROBUST SPEECH SYNTHESIS
In our <strong><em>Mega-TTS 2</em></strong>, we employ a language model only for prosody modeling, enabling our model to benefit from the advantages of in-context learning provided by the LLM model. This approach also helps to address the robustness issues (word skipping or repeating) associated with the autoregressive TTS model. Therefore, we make explanations about more cases of robust speech synthesis, to demonstrate our method’s necessity. In commercial scenarios, news reporting, and other formal scenarios, robustness is a crucial factor. Just a few repeating or skipping words can have significant negative impacts. These situations are better suited for models with duration models that ensure robustness, such as FastSpeech [4], Glow-TTS [5], and our <strong><em>Mega-TTS 2</em></strong>. However, for models like tacotron [6], word omissions or repetitions can significantly affect the listening experience. On
the other hand, in some scenarios, robustness is relatively less important. For example, occasional missing words or repetitions in dialogue scenes can also be natural.
L RESULTS WITH NOISY REFERENCE PROMPTS
To verify our model’s robustness against noisy reference prompts, we conduct experiments on LibriSpeech test-other set. The experimental setup for this experiment is consistent with the one described in Section 4.2. The results are shown in Table 11. It can be seen that <strong><em>Mega-TTS 2</em></strong> maintains excellent performance with noisy reference prompts.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
    
  </body>
</html>