
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.0">
    
    
      
        <title>VALL-T - Sapphire Lab</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.9f615399.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#vall-t" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Sapphire Lab" class="md-header__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sapphire Lab
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              VALL-T
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Sapphire Lab" class="md-nav__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sapphire Lab
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TTS
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../../PDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PDE
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract·摘要
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1introduction" class="md-nav__link">
    1.Introduction·引言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2related-work" class="md-nav__link">
    2.Related Work·相关工作
  </a>
  
    <nav class="md-nav" aria-label="2.Related Work·相关工作">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21decoder-only-zero-shot-tts-with-speech-promptstts" class="md-nav__link">
    2.1.Decoder-Only Zero-Shot TTS with Speech Prompts·使用语音提示的仅解码器的零样本TTS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22transducer" class="md-nav__link">
    2.2.Transducer·转录器
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3vall-t-decoder-only-generative-transducervall-t" class="md-nav__link">
    3.VALL-T: Decoder-Only Generative Transducer·VALL-T: 仅解码器的生成转录器
  </a>
  
    <nav class="md-nav" aria-label="3.VALL-T: Decoder-Only Generative Transducer·VALL-T: 仅解码器的生成转录器">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31training" class="md-nav__link">
    3.1.Training·训练
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32monotonic-auto-regressive-inference" class="md-nav__link">
    3.2.Monotonic Auto-Regressive Inference·单调自回归推理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33pseudo-prompt-transcription-for-untranscribed-speech-prompt" class="md-nav__link">
    3.3.Pseudo Prompt Transcription for Untranscribed Speech Prompt·未转录语音提示的伪提示转录
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34aligned-context-window-for-lengthy-speech-synthesis" class="md-nav__link">
    3.4.Aligned Context Window for Lengthy Speech Synthesis·长语音合成的上下文对齐窗口
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4experiments-and-results" class="md-nav__link">
    4.Experiments and Results·实验与结果
  </a>
  
    <nav class="md-nav" aria-label="4.Experiments and Results·实验与结果">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41setup" class="md-nav__link">
    4.1.Setup·设置
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42alignment-analysis" class="md-nav__link">
    4.2.Alignment Analysis·对齐分析
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43evaluation-on-zero-shot-ttstts" class="md-nav__link">
    4.3.Evaluation on Zero-Shot TTS·零样本TTS的评估
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44leveraging-untranscribed-speech-prompts" class="md-nav__link">
    4.4.Leveraging Untranscribed Speech Prompts·利用未转录语音提示
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#45evaluation-on-lengthy-speech-generation" class="md-nav__link">
    4.5.Evaluation on Lengthy Speech Generation·长语音生成的评估
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5conclusion" class="md-nav__link">
    5.Conclusion·结论
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="vall-t">VALL-T<a class="headerlink" href="#vall-t" title="Permanent link">&para;</a></h1>
<details>
<summary>基本信息</summary>

- 标题: VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech
- 作者:
  - 01 [Chenpeng Du](../../Authors/Chenpeng_Du.md)
  - 02 [Yiwei Guo](../../Authors/Yiwei_Guo.md)
  - 03 [Hankun Wang](../../Authors/Hankun_Wang.md)
  - 04 [Yifan Yang](../../Authors/Yifan_Yang.md)
  - 05 [Zhikang Niu](../../Authors/Zhikang_Niu.md)
  - 06 [Shuai Wang](../../Authors/Shuai_Wang_(王帅).md)
  - 07 [Hui Zhang](../../Authors/Hui_Zhang.md)
  - 08 [Xie Chen](../../Authors/Xie_Chen_(陈谐).md)
  - 09 [Kai Yu](../../Authors/Kai_Yu_(俞凯).md)
- 机构:
  - [上海交通大学](../../Institutions/SJTU_上海交通大学.md)
- 时间:
  - 预印时间: 2024.01.25 ArXiv v1
  - 预印时间: 2024.01.30 ArXiv v4
  - 更新笔记: 2024.06.06
- 发表:
  - 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2401.14321)
  - [DOI]()
  - [Github](https://github.com/cpdu/vallt)
  - [Demo](http://cpdu.github.io/vallt)
- 标签:
  - [语音合成](../../Tags/SpeechSynthesis.md)
  - [零样本](../../Tags/Zero-Shot.md)
- 页数: 13
- 引用: ?
- 被引: 1

</details>

<h2 id="abstract">Abstract·摘要<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Recent TTS models with decoder-only Transformer architecture, such as <a href="../../Models/Speech_LLM/2023.02.07_SPEAR-TTS.md">SPEAR-TTS (2023)</a> and <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt.
However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and repeating.
To address this limitation, we propose <strong><em>VALL-T</em></strong>, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer.
Consequently, <strong><em>VALL-T</em></strong> retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3% in the word error rate.
Furthermore, the controllability of alignment in <strong><em>VALL-T</em></strong> during decoding facilitates the use of untranscribed speech prompts, even in unknown languages.
It also enables the synthesis of lengthy speech by utilizing an aligned context window.
The audio samples are available at http://cpdu.github.io/vallt.</p>
</blockquote>
<h2 id="1introduction">1.Introduction·引言<a class="headerlink" href="#1introduction" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Text-to-speech (TTS) synthesis is a monotonic sequence-to-sequence task, maintaining a strict order between the input phoneme sequence and the output speech sequence.
Moreover, the output speech sequence is at frame-level and one phoneme may correspond to multiple frames of speech, so the output sequence is significantly longer than its corresponding input phoneme sequence.
Mainstream neural text-to-speech models, such as <a href="../../TTS2_Acoustic/2020.06.08_FastSpeech2/">FastSpeech2</a>, GradTTS (Popov et al., 2021) and VoiceFlow (Guo et al., 2024), integrate a duration prediction module.
Prior to training, the target duration is conventionally derived using the Viterbi forced alignment algorithm.
During training, this module is optimized by minimizing the mean square error (MSE) between predicted and target durations.
In the inference phase, the duration predictor module predicts the duration for each input phoneme, establishing the alignment between the input and output sequences accordingly.
The encoded input phoneme sequence is then expanded to the frame level based on the predicted duration and is subsequently passed to the speech decoder.
This mechanism enforces monotonic alignment constraints on the sequence-to-sequence process, ensuring robustness in the synthesis of speech.</p>
<p>Over the past two years, utilizing discrete speech tokens for speech generation is proposed in GSLM (Lakhotia et al., 2021) and VQTTS (Du et al., 2022), paving the way for integrating cutting-edge language modeling techniques into TTS systems.
Inspired by exceptional strides in natural language processing driven by decoder-only large Transformer models like GPT 3 (Brown et al., 2020) and the <a href="../../Models/LLM/2023.07.18_LLaMA2.md">LLaMA2 (2023)</a>, <a href="../../Models/_tmp/2023.05.12_TorToise-TTS.md">Tortoise-TTS (2023)</a>, <a href="../../Models/Speech_LLM/2023.02.07_SPEAR-TTS.md">SPEAR-TTS (2023)</a>, <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> and LauraGPT (Wang et al., 2023b) adopted the decoder-only architecture for TTS, achieving remarkable naturalness.
<a href="../../Models/Speech_LLM/2023.02.07_SPEAR-TTS.md">SPEAR-TTS (2023)</a> and <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> also have the ability to perform zero-shot speaker adaptation through Auto-Regressive (AR) continuation from a given speech prompt.
Furthermore, these decoder-only TTS models, unlike traditional neural TTS model, circumvent explicit duration modeling and the requirement for phoneme durations obtained prior to training.
This characteristic offers convenience and simplifies training process, especially when training on large scale datasets.
However, the implicit duration modeling within these systems lacks the monotonic alignment constraints, often leading to hallucination issues like mispronunciation, word skipping and repeating.</p>
<p>In fact, we do have a training scheme named Transducer (Graves, 2012) designed specifically for monotonic sequence-to-sequence task and has demonstrated success in automatic speech recognition (ASR) (He et al., 2019).
It adopts a modularized architecture, composed of an encoder, a prediction network and a joint network.
However, such modularized architecture of Transducer is specifically designed for ASR as a classification task, making it less suited for TTS as a generation task.
Further insights into this matter will be discussed in Chapter 3.</p>
<p>To achieve the best of both worlds, we propose <strong><em>VALL-T</em></strong>, a generative Transducer model that utilizes the decoder-only Transformer architecture.
Specifically, alongside the conventional absolute position embedding, we incorporate additional relative position embeddings into the input phoneme sequence.
Here, a relative position of 0 specifies the current phoneme under synthesis, allowing us to explicitly guide the monotonic generation process through shifting the relative positions from left to right.
To the best of our knowledge, this is the first work that implements Transducer with a decoder-only Transformer architecture.
<strong><em>VALL-T</em></strong> presents several advantages compared to previous TTS models:
- <strong><em>VALL-T</em></strong> introduces monotonic alignment constraints without altering the decoder-only architecture, leading to a better robustness against hallucination.
- <strong><em>VALL-T</em></strong> utilizes implicit duration modeling, removing the necessity for acquiring phoneme durations before training.
- The alignment controllability of <strong><em>VALL-T</em></strong> during decoding enables the utilization of untranscribed speech prompts, even in unknown languages.</p>
</blockquote>
<h2 id="2related-work">2.Related Work·相关工作<a class="headerlink" href="#2related-work" title="Permanent link">&para;</a></h2>
<h3 id="21decoder-only-zero-shot-tts-with-speech-promptstts">2.1.Decoder-Only Zero-Shot TTS with Speech Prompts·使用语音提示的仅解码器的零样本TTS<a class="headerlink" href="#21decoder-only-zero-shot-tts-with-speech-promptstts" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Zero-shot TTS refers to the ability to generate speech in the voice of an unseen speaker given only a short sample of that speaker’s speech.
Decoder-only TTS models, such as <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, are able to perform zero-shot speaker adaptation through auto-regressive continuation from the target speaker’s sample.
Therefore, the speech sample of the target speaker is also named speech prompt.</p>
<p>Specifically, in the training process, illustrated in Fig.01(a), the phoneme and speech sequences are concatenated along the time axis and fed into a decoder-only Transformer model.
It is assumed that the speaker’s voice remains constant within each training utterance.
In the inference phase, as shown in Fig.01(b), a speech prompt yp is required to determine the voice of the generated speech.
The phoneme transcription of the speech prompt xp and the speech prompt itself yp are positioned at the beginning of the input and output sequences respectively, followed by the input phonemes to be generated and their corresponding output speech tokens.
The process of auto-regressive continuation from the speech prompt is believed to preserve the speaker’s voice in the generated output.</p>
</blockquote>
<h3 id="22transducer">2.2.Transducer·转录器<a class="headerlink" href="#22transducer" title="Permanent link">&para;</a></h3>
<blockquote>
<p>The Transducer model (Graves, 2012), also known as RNN- T, is designed for monotonic sequence-to-sequence tasks and comprises three components: an encoder, a prediction network, and a joint network.
Here, the prediction network is an auto-regressive network, such as RNN and LSTM.
Transducer model also introduces a special output token called blank, denoted as \varnothing , which signifies the alignment boundary between output and input sequence.
We define Yas the vocabulary of output tokens and \bar{Y} = Y ∪ { \varnothing }as the extended vocabulary.
Also, we denote the lengths of the input sequence x and output sequence y as T and U and the size of the extended vocabulary \bar{Y} as ¯V .</p>
<p>In the training phase, as shown in Fig.02(a), the encoder and prediction network encode the two sequences x and y respectively, yielding encoded hidden sequences f and g.
Subsequently, we slice the hidden vectors ft and gu at position st and u respectively, then send them to the joint network to calculate the probability pt,u= Pr(\bar{y}t+u|ft, gu) for the next token prediction, where \bar{y}t+u∈ \bar{Y}.
We iterate over all possible sliced hidden vectors of the two sequences, from f0tofT −1and from g0togU, generating a matrix p of shapeT ×(U +1)whose entry at(t, u)is pt,u.
Each path \bar{y} from the bottom left corner to the top right corner represents an alignment between x and y, with a length ofT +U.
Fig.02(b) demonstrates an example of the alignment path where \bar{y} = [y1, y2,  \varnothing , y3,  \varnothing , y4, y5,  \varnothing , y6,  \varnothing ].
The training criterion of Transducer model is to maximize the probability ofPr(y|x), which is the summation of the probabilities of all possible alignment paths \bar{y}, that is where fti and gui are sliced hidden vectors at corresponding positions specified by the alignment path $\bar{y}$.
In practice, this probability can be effectively calculated with dynamic programming.
In the inference phase, the prediction network auto-regressively predicts the next token, conditioning on the sliced input hidden vectors that slide from f0tofT −1whenever the blank token $\varnothing$ emerges.
The Transducer model has demonstrated remarkable success in ASR.
However, its modularized architecture is not suitable enough for generation tasks.
Recently, some literatures have explored the application of Transducer to TTS (Chen et al., 2021; Kim et al., 2023), but they still rely on the typical modularized architecture and consequently result in limited performance.
Different from the previous works, we propose for the first time to implement Transducer with a decoder-only architecture that achieves better performance.</p>
</blockquote>
<h2 id="3vall-t-decoder-only-generative-transducervall-t">3.VALL-T: Decoder-Only Generative Transducer·VALL-T: 仅解码器的生成转录器<a class="headerlink" href="#3vall-t-decoder-only-generative-transducervall-t" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Current modularized Transducer model has demonstrated significant success in ASR.
Nevertheless, its suitability for generation tasks is limited.
Typically, the joint network is a small network, comprising only one or a few linear projection layers, and the prediction network is LSTM or Transformer blocks.
This architecture introduces a limitation wherein the input condition x is not incorporated into the generation process until it reaches the joint network.
Worse still, the joint network is too small to effectively integrate input conditions into the generation process.
Moreover, the modularized Transducer model utilizes slicing to denote specific positions.
Consequently, the joint network is unable to explicitly perceive the input context, further making difficulties in achieving satisfactory performance for conditional generation tasks.</p>
<p>To address the above issues, we propose <strong><em>VALL-T</em></strong> that integrates the encoder, the prediction network and the joint network into one single decoder-only Transformer architecture and leverages relative position embedding to denote the corresponding positions.
We discuss the training and inference details below.</p>
</blockquote>
<h3 id="31training">3.1.Training·训练<a class="headerlink" href="#31training" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We use a decoder-only architecture for <strong><em>VALL-T</em></strong>.
Similar to the approach in the previous work <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, we concatenate the input phoneme and output speech tokens along the time axis and present them to the model as a unified sequence.
Unlike traditional RNN and LSTM architectures, the Transformer lacks a specific time order for input tokens, relying instead on position embeddings to indicate their positions.
The position indices for the input sequence range from0to T − 1and are converted into position embeddings through a sinusoidal function (Vaswani et al., 2017).
Similarly, the output sequence adopts position indices from0toU, including an additional <code>&lt;sos&gt;</code> token at the beginning.
Following <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, we utilize a triangular attention mask for the output sequence, facilitating auto-regressive generation.
This mask ensures that each speech token attends to only previously generated tokens, maintaining a proper sequential order in the output.</p>
<p>Beyond the typical absolute position indices starting from 0, we introduce additional relative position indices in <strong><em>VALL-T</em></strong> for input tokens.
The relative position index0specifies the current phoneme under synthesis.
The phonemes to its left are assigned negative position indices starting from −1, while those to its right are assigned positive position indices starting from1.
These relative position indices are converted to relative position embeddings with a same sinusoidal function as the absolute position indices.
The resulting absolute and relative position embeddings are added to the input phoneme embeddings and subsequently presented to the decoder-only Transformer.
In adopting this approach, the model gains awareness of the phoneme presently undergoing synthesis, specifically the one assigned a relative position of0, and the phonemes serving as its preceding and subsequent contexts.</p>
<p>To eliminate the need for explicit duration modeling, we introduce a special output token called blank, which serves as a marker denoting the end of each phoneme’s generation.
Consequently, the output projection following the decoder-only Transformer projects the hidden sequence into a size of¯V.
The projected hidden sequence, with a length of U + 1, undergoes a Softmax function to yield a sequence representing the output distribution.
Illustrated in Fig.03, we iteratively assign relative position0to each of theT phonemes and subsequently stack every output sequence, each of lengthU + 1.
This stacking process results in a matrix p of shapeT × (U + 1).
The optimization of <strong><em>VALL-T</em></strong> utilizes the Transducer loss, calculated using this matrix and the ground-truth speech tokens, to maximize the probability of p(y|x) following Equation (1).</p>
</blockquote>
<h3 id="32monotonic-auto-regressive-inference">3.2.Monotonic Auto-Regressive Inference·单调自回归推理<a class="headerlink" href="#32monotonic-auto-regressive-inference" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Let us first consider the auto-regressive inference process without a speech prompt.
Initially, the relative position 0is designated to the first phoneme, starting the speech generation from the <code>&lt;sos&gt;</code> token.
The model then auto-regressively produces speech tokens based on the input phoneme tokens and previously generated speech tokens until the blank token \varnothing emerges.
The emergence of \varnothing denotes the completion of the first phoneme’s generation and triggers a shift in relative positions.
We iteratively conduct the above process until the appearance of \varnothing for the last phoneme, indicating the conclusion of the entire generation process for the input phoneme sequence.
Since the model is encouraged to generate speech tokens for the phoneme assigned relative position0by Transducer loss during training, the step-by-step shifting operation during decoding facilitates the monotonic generation process and consequently enhance the robustness against hallucination.</p>
<p>Next, we consider the integration of the speech prompt for zero-shot speaker adaptation.
Following the approach used in <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, the phoneme transcription of the speech prompt is placed at the start of the input sequence, while the speech prompt itself is positioned at the beginning of the output sequence.
The two sequences are followed by the input phonemes to be generated and their corresponding output speech tokens respectively.
Given that the speech prompt are provided, we assign the relative position 0 to the first phoneme right after the prompt transcription, as shown in Fig.03, and perform speech continuation.
Likewise, the relative positions undergo a shift each time \varnothing emerges, repeating until the generation for the final phoneme is completed.</p>
</blockquote>
<h3 id="33pseudo-prompt-transcription-for-untranscribed-speech-prompt">3.3.Pseudo Prompt Transcription for Untranscribed Speech Prompt·未转录语音提示的伪提示转录<a class="headerlink" href="#33pseudo-prompt-transcription-for-untranscribed-speech-prompt" title="Permanent link">&para;</a></h3>
<blockquote>
<p>In previous decoder-only TTS models, the alignment is learned implicitly with self-attentions.
These models have to discern which phoneme is currently being synthesized at each time step solely based on the self-attentions between the input tokens and the preceding output tokens.
Therefore, they rely on correct transcription of the speech prompt to get correct alignment and start the generation accordingly.
However, in practice, it is inconvenient to obtain transcribed speech prompt, so we hope to leverage speech prompt directly and eliminate the need of its transcription.</p>
<p>In <strong><em>VALL-T</em></strong>, it is evident that the alignment is controllable during inference, allowing us to manipulate the generation process by assigning position0to the phoneme we intend to synthesize without relying on a paired speech prompt and its transcription.
Accordingly, we can perform zero-shot adaptation with untranscribed speech prompts.
Specifically, given an untranscribed speech prompt, we use the phoneme sequence of a random utterance, referred to as pseudo prompt transcription, as its transcription and place it at the beginning of the input sequence.
Then the generation can start correctly by leveraging exactly the same algorithm as described in section 3.2.
The reason for using a pseudo prompt transcription rather than no prompt transcription lies in the presence of absolute position embeddings in the input sequence.
We need to avoid unseen alignment pattern in the view of absolute position embeddings.
Moreover, since there is no necessity for transcribing the speech prompt, the utilization of untranscribed speech prompts can be expanded to include prompts in unknown languages.
This enables cross-lingual zero-shot adaptive speech synthesis.</p>
</blockquote>
<h3 id="34aligned-context-window-for-lengthy-speech-synthesis">3.4.Aligned Context Window for Lengthy Speech Synthesis·长语音合成的上下文对齐窗口<a class="headerlink" href="#34aligned-context-window-for-lengthy-speech-synthesis" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Decoder-only Transformer models have very limited ability to generalize to unseen position embeddings.
That means if we are synthesizing lengthy speech that exceeds the maximum length encountered during training, the performance would be degraded.</p>
<p>Fortunately, in <strong><em>VALL-T</em></strong>, the alignment is available during inference, allowing us to employ aligned context window that constrains both the input and output sequence length simultaneously.
Specifically, at each decoding step, we retain only n phonemes that precede the current phoneme and m phonemes that follow it, creating a constrained sliding context window on input phonemes.
Also, we preserve only the speech tokens corresponding to the n preceding phonemes given the alignment and discard more distant history, forming a context window on the output sequence as well.
Hence, by leveraging aligned context window, <strong><em>VALL-T</em></strong> consistently maintains a limited context on both input and output sequence, allowing it to generate speech of any lengths.</p>
</blockquote>
<h2 id="4experiments-and-results">4.Experiments and Results·实验与结果<a class="headerlink" href="#4experiments-and-results" title="Permanent link">&para;</a></h2>
<h3 id="41setup">4.1.Setup·设置<a class="headerlink" href="#41setup" title="Permanent link">&para;</a></h3>
<blockquote>
<p>In our experiments, we leverage our <a href="../../Speech_Neural_Codec/2022.10.24_EnCodec/">Encodec (2022)</a> (Defossez et al., 2022) speech tokenizer whose frame shift is 20ms and the sampling rate of output waveforms is 16k.
It comprises 8 residual vector quantization (RVQ) indices for each frame.
To ensure a fair comparison between <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> and our proposed model <strong><em>VALL-T</em></strong>, we follow the approach introduced in <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> that predicts the sequence of the first RVQ index with the auto-regressive models and then predicts the remaining 7 RVQ indices conditioned on the first RVQ index with a separate non-auto-regressive (NAR) model.
Both the input and output sequences are encoded with BPE (Sennrich et al., 2016) algorithm to shorten sequence lengths and diminish GPU memory consumption.
<strong><em>VALL-T</em></strong> adopts an identical architecture to <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, containing 12 layers of Transformer blocks.
Each block comprises 12 attention heads and has a hidden dimension of 1024.</p>
<p>We use LibriTTS (Zen et al., 2019) dataset in our experiments, which is a multi-speaker transcribed English speech dataset.
Its training set consists of approximately 580 hours of speech data from 2,306 speakers.
We train our model for 40 epochs using a ScaledAdam (Yao et al., 2023) optimizer.
The learning rate scheduler is Eden (Yao et al., 2023) with a base learning rate of0.05, an epoch scheduling factor of 4 and a step scheduling factor of 5000.</p>
</blockquote>
<h3 id="42alignment-analysis">4.2.Alignment Analysis·对齐分析<a class="headerlink" href="#42alignment-analysis" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We first do alignment analysis to check if relative position embedding in <strong><em>VALL-T</em></strong> indicates the alignment as expected.
Given the speech y and its transcription x, we iterate over all relative positions and calculate the matrix p of output distributions in the shape ofT ×(U +1).
Then we calculate the forward variables, backward variables and posterior probabilities accordingly.
The concepts of forward variable, backward variables, and posterior probabilities were initially introduced in Hidden Markov Models (Young et al., 2002) and were also introduced in Transducer (Graves, 2012).
The definitions and calculation for these values are elaborated in Appendix A.</p>
<p>In Fig.04, we illustrate an example of the forward variable, backward variable, and posterior probability for <strong><em>VALL-T</em></strong>, with darker colors indicating lower values.
The values are plotted on a logarithmic scale.
In Fig.04(a) and 4(b), we can see a faint bright line on the diagonal of the two graphs.</p>
<p>Pixel-wise summing the values from Fig.04(a) and Fig.04(b) produces Fig.04(c), which represents the posterior probability.
The diagonal line becomes much clearer in this composite figure, indicating that <strong><em>VALL-T</em></strong> correctly models the alignment between the input and output sequences with relative position embeddings.
Accordingly, <strong><em>VALL-T</em></strong> is capable of forced alignment, where the most probable path from the bottom-left corner to the top-right corner in the posterior probability map serves as the alignment path.
The alignment path for this example is depicted in Fig.04(d).
Since ground-truth labels for alignment are unavailable, our alignment analysis here only focuses on qualitative aspects.</p>
</blockquote>
<h3 id="43evaluation-on-zero-shot-ttstts">4.3.Evaluation on Zero-Shot TTS·零样本TTS的评估<a class="headerlink" href="#43evaluation-on-zero-shot-ttstts" title="Permanent link">&para;</a></h3>
<blockquote>
<p>In this section, we conduct an evaluation of our models on zero-shot TTS task.
The task refers to synthesizing speech in the voices of unseen speakers given speech prompts and their corresponding transcriptions.
Our test set uses a same test set as in (Du et al., 2024), containing 500 utterances and involving 37 speakers from the LibriTTS test set.
Each speaker is assigned a specific speech prompt.
Before assessing the performance of our models, we conduct speech resynthesis using our <a href="../../Speech_Neural_Codec/2022.10.24_EnCodec/">Encodec (2022)</a> to evaluate the speech tokenizer.
We also do an experiment named “NAR resynthesis”.
In this experiment, we send the ground-truth first RVQ index to the NAR model for predicting the remaining 7 RVQ indices.
Then, we convert all the 8 RVQ indices to waveform using the <a href="../../Speech_Neural_Codec/2022.10.24_EnCodec/">Encodec (2022)</a> decoder.
The purpose of the NAR resynthesis experiment is to demonstrate the performance degradation introduced by the NAR model, so we can better analyze the results of the entire pipelines, where the AR models are the primary focus of our paper.</p>
<p>The baselines of this experiment include two models.
One is the popular decoder-only TTS model <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> and another is the recently proposed TTS model with a modularized Transducer architecture called “Transduce and Speak” (Kim et al., 2023).
The main evaluation metric in this paper is the word error rate (WER).
In our evaluation process, we first synthesize speech for the test set, and then perform speech recognition using a well-known ASR model, Whisper1(Radford et al., 2023).
The transcriptions obtained from the ASR model are then compared to the ground-truth input text to calculate the word error rate.
Tab.01 shows that <strong><em>VALL-T</em></strong> attains significant lower WER than baselines, which is a 28.3% relative reduction when compared to <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> and is only 0.41 higher than NAR resynthesis, suggesting the robustness of <strong><em>VALL-T</em></strong>.</p>
<p>Additionally, we present the mel-cepstral distortion (MCD) in the table, serving as a metric for quantifying the distance between the generated speech and the corresponding ground-truth recordings.
<strong><em>VALL-T</em></strong> also achieves the lowest MCD across all models.
Further evaluations extend to Mean Opinion Score (MOS) listening tests for naturalness and speaker similarity. 15 listeners were tasked with rating each utterance on a scale from 1 to 5, with higher scores indicating better naturalness and similarity.
Note that the speaker similarity is evaluated between the generated speech and the provided speech prompt, not the corresponding ground-truth speech.
This distinction arises from the variability in a speaker’s timbre across different utterances, and the goal is to emulate solely the timbre of the given prompt.
In the listening tests, <strong><em>VALL-T</em></strong> achieves a naturalness score comparable to <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, with a slightly better speaker similarity.
Finally, the evaluation extends to the calculation of Speaker Embedding Cosine Similarity (SECS), measured using a pretrained speaker verification model2.
This metric measures the speaker similarity by assessing the cosine similarity between the speaker embeddings of the generated speech and the provided speech prompt.
While <strong><em>VALL-T</em></strong> exhibits a marginally lower SECS value than <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, it still surpasses other models and does not detrimentally affect human perception according to the results of subjective listening tests on similarity.</p>
</blockquote>
<h3 id="44leveraging-untranscribed-speech-prompts">4.4.Leveraging Untranscribed Speech Prompts·利用未转录语音提示<a class="headerlink" href="#44leveraging-untranscribed-speech-prompts" title="Permanent link">&para;</a></h3>
<blockquote>
<p>The alignment controllability of <strong><em>VALL-T</em></strong> allow us to leverage untranscribed speech prompts for zero-shot TTS.
In this experiment, we still use a same test set as in the previous section, excluding the transcription of the speech prompts to simulate a scenario where prompt transcriptions are unavailable.
One utterance is randomly chosen from the LibriTTS test set, and its phoneme transcription serves as the pseudo prompt transcription for generating all utterances in the test set.
We compare the proposed approach with three baselines.
The first baseline is generating with <strong><em>VALL-T</em></strong> but do not use any prompt transcription.
The remaining two baselines use <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, one utilizing pseudo prompt transcriptions and the other using no prompt transcription.</p>
<p>The results are presented in Tab.02.
We find <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> consistently fails to perform continuation in the absence of the correct prompt transcription, regardless of whether pseudo prompt transcriptions are provided or not.
Although VALL- T exhibits improved robustness, it still fails in continuation tasks when no prompt transcription is used.
This failure is caused by the unseen alignment pattern in the view of absolute position embeddings.
When provided with pseudo prompt transcriptions, <strong><em>VALL-T</em></strong> successfully accomplishes the continuation from the speech prompt.
The WER is significantly lower than the three baselines and even lower than both the results obtained using real prompt transcription and using NAR resynthesis in Tab.01.
This improvement may be attributed to the reduced noise in the fixed pseudo prompt transcription compared to the diverse real prompt transcriptions.
This result further demonstrate the robustness of <strong><em>VALL-T</em></strong>.</p>
<p>Similarly, we observe a lower MCD compared with other baselines with the proposed approach.
We do not conduct listening tests on the three baselines since it makes no sense to assess the naturalness and similarity for entirely incorrect generated audio samples.
The naturalness of the proposed approach is almost the same as that observed when using real prompt transcriptions while its speaker similarity is slightly lower.
We can also observe that in SECS evaluation.</p>
<p>Next, we extend the utilization of untranscribed speech prompts to those spoken in unknown languages.
Specifically, we continue to use the same test set as in the previous experiments, but leverage speech prompts from 10 German and 10 Spanish speakers randomly selected from the Multilingual Librispeech dataset (Pratap et al., 2020), simulating the speech prompt in unknown languages.
Employing the same English pseudo prompt transcription as in the previous experiment for both <strong><em>VALL-T</em></strong> and the baseline <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, we generate continuations from the speech prompts in German and Spanish.
The results are posted in Tab.03.
<a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> continues to fail in the generation due to the unknown prompt transcription.
On the contrary, <strong><em>VALL-T</em></strong> still successfully performs the zero-shot TTS from the speech prompts in German and Spanish, achieving a WER of 4.22.
Note that the similarity MOS and SECS in this experiment cannot be directly compared with the corresponding results in Tab.01 and 2 since the speakers of the speech prompts differ.
We do not have corresponding ground-truth speech that speaks the utterances in the test set in the voice of German and Spanish speakers, so we also do not calculate the MCD in this experiment.</p>
</blockquote>
<h3 id="45evaluation-on-lengthy-speech-generation">4.5.Evaluation on Lengthy Speech Generation·长语音生成的评估<a class="headerlink" href="#45evaluation-on-lengthy-speech-generation" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We also evaluate our model on lengthy speech synthesis that exceeds the maximum length encountered during training.
Due to the limitation of GPU memory, the maximum duration of training utterances is approximately 15 seconds.
The test set for this experiment consists of 85 utterances, each formed by concatenating five utterances from the previous test set to simulate lengthy utterance.
The generated speech in this test set exceeds 20 seconds.
We use n = 50and m = 15 as the context window size.</p>
<p>Examining the results in Tab.04, we observe that <strong><em>VALL-T</em></strong> exhibits superior generalization to long speech compared to <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, attributed to its utilization of relative position embedding, even in the absence of an aligned context window.
In contrast, <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> often starts mumbling after generating approximately 20 seconds of speech and frequently terminates prematurely without completing the generation.
Upon applying the aligned context window, the WER of <strong><em>VALL-T</em></strong> further decreases and approaches the result of generating normal utterances.
Additionally, the gap in MOS scores for naturalness and speaker similarity between generated speech and ground-truth is also comparable to the result of synthesizing normal utterances.</p>
</blockquote>
<h2 id="5conclusion">5.Conclusion·结论<a class="headerlink" href="#5conclusion" title="Permanent link">&para;</a></h2>
<blockquote>
<p>In this research, we present <strong><em>VALL-T</em></strong>, a decoder-only generative Transducer model designed to improve the robustness and controllability of TTS models.
<strong><em>VALL-T</em></strong> incorporates monotonic alignment constraints into the decoder-only TTS framework, enabling implicit modeling of phoneme durations.
Therefore, this model eliminates the need for acquiring phoneme durations before training.
<strong><em>VALL-T</em></strong> supports forced alignment given input phonemes and the corresponding output speech by searching the best path on the posterior probability map.
This alignment is controllable during inference, facilitating zero-shot synthesis with untranscribed speech prompts even in unknown languages.
Additionally, <strong><em>VALL-T</em></strong> exhibits the capability of streaming generation, coupled with an aligned context window for synthesizing lengthy speech.
These features make <strong><em>VALL-T</em></strong> a powerful model for TTS applications.</p>
</blockquote>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
    
  </body>
</html>