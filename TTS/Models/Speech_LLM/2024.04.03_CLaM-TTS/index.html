
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.0">
    
    
      
        <title>CLaM-TTS - Sapphire Lab</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.9f615399.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#clam-tts" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Sapphire Lab" class="md-header__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sapphire Lab
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              CLaM-TTS
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Sapphire Lab" class="md-nav__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sapphire Lab
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TTS
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../../PDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PDE
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1. Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-related-work" class="md-nav__link">
    2. Related Work
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-preliminaries" class="md-nav__link">
    3. Preliminaries
  </a>
  
    <nav class="md-nav" aria-label="3. Preliminaries">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-residual-quantized-variational-autoencoder-rq-vae" class="md-nav__link">
    3.1. Residual Quantized Variational Autoencoder (RQ-VAE)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-mean-field-variational-inference" class="md-nav__link">
    3.2. Mean-Field Variational Inference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-method" class="md-nav__link">
    4. Method
  </a>
  
    <nav class="md-nav" aria-label="4. Method">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-mel-vae" class="md-nav__link">
    4.1. Mel-VAE
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-latent-language-modeling" class="md-nav__link">
    4.2. Latent Language Modeling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-model-architecture-and-inference" class="md-nav__link">
    4.3. Model Architecture and Inference
  </a>
  
    <nav class="md-nav" aria-label="4.3. Model Architecture and Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-architecture" class="md-nav__link">
    Model Architecture
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    Inference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-experimental-setup" class="md-nav__link">
    5. Experimental Setup
  </a>
  
    <nav class="md-nav" aria-label="5. Experimental Setup">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-dataset" class="md-nav__link">
    Training Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training" class="md-nav__link">
    Training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#baselines" class="md-nav__link">
    Baselines
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#metrics" class="md-nav__link">
    Metrics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tasks" class="md-nav__link">
    Tasks
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-experimental-results" class="md-nav__link">
    6. Experimental Results
  </a>
  
    <nav class="md-nav" aria-label="6. Experimental Results">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-english-only-evaluations" class="md-nav__link">
    6.1. English-Only Evaluations
  </a>
  
    <nav class="md-nav" aria-label="6.1. English-Only Evaluations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#evaluation-methodology" class="md-nav__link">
    Evaluation Methodology
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#analysis" class="md-nav__link">
    Analysis
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-multilingual-evaluations" class="md-nav__link">
    6.2. Multilingual Evaluations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-ablation-study" class="md-nav__link">
    6.3. Ablation Study
  </a>
  
    <nav class="md-nav" aria-label="6.3. Ablation Study">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#effectiveness-of-proposed-rvq" class="md-nav__link">
    Effectiveness of Proposed RVQ
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparision-of-pre-trained-lm-and-input-variants" class="md-nav__link">
    Comparision of Pre-trained LM and Input Variants
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-discussion" class="md-nav__link">
    7. Discussion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-conclusion" class="md-nav__link">
    8. Conclusion
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="clam-tts">CLaM-TTS<a class="headerlink" href="#clam-tts" title="Permanent link">&para;</a></h1>
<details>
<summary>基本信息</summary>

- 标题: CLaM-TTS: Improving Neural Codec Language Modeling for Zero-Shot Text-to-Speech
- 作者:
  - [Jaehyeon Kim](../../Authors/Jaehyeon_Kim.md)
  - [Keon Lee](../../Authors/Keon_Lee.md)
  - [Seungjun Chung](../../Authors/Seungjun_Chung.md)
  - [Jaewoong Cho](../../Authors/Jaewoong_Cho.md)
- 机构:
  - [KRAFTON](../../Institutions/KRAFTON.AI.md)
- 时间:
  - 预印时间: 2024.04.03 ArXiv v1
  - 更新笔记: 2024.06.06
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2404.02781)
  - [DOI]()
  - [Github]()
  - [Demo](https://clam-tts.github.io)
- 标签:
  - [语音合成](../../Tags/SpeechSynthesis.md)
  - [语言模型](../../Tags/LanguageModel.md)
  - [零样本](../../Tags/Zero-Shot.md)
- 页数: 24
- 引用: ?
- 被引: ?

</details>

<h2 id="abstract">Abstract<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h2>
<blockquote>
<p>With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot <term>Text-to-Speech (TTS)</term> synthesis. 
Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. 
To mitigate these issues, we present <strong>CLaM-TTS</strong> that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. 
Our experimental results demonstrate that <strong>CLaM-TTS</strong> is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. 
In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances. </p>
</blockquote>
<h2 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h2>
<blockquote>
<p><term>Large language models (LLMs)</term>, characterized by a considerable number of model parameters and trained on massive text data, have demonstrated remarkable zero-shot learning capabilities (Brown et al., 2020; Chung et al., 2022; Kaplan et al., 2020). 
While scaling paradigm affects not only the natural language processing domain but also other fields such as image generation (Ramesh et al., 2021; Saharia et al., 2022), image recognition (Radford et al., 2021), and speech recognition (Baevski et al., 2020b; Radford et al., 2023), significant challenges in their efficient training and inference simultaneously arise. 
In the realm of image processing, discretizing image representation (Razavi et al., 2019; Ramesh et al., 2021; Esser et al., 2021) has been shown to mitigate these issues by effectively reducing the input length to a manageable size. </p>
<p>Language modeling in the speech domain has become feasible with the emergence of neural audio codecs (Zeghidour et al., 2021; D´ efossez et al., 2023) that enable high-fidelity audio tokenization. 
For Text-to-Speech (TTS) synthesis, there have been several attempts to adopt the LLMs for zero-shot TTS, which namely synthesize the diverse speech of any human voice (Zhang et al., 2023; Wang et al., 2023; Kharitonov et al., 2023; Rubenstein et al., 2023). 
These attempts move away from the previous research direction to train models on curated high-quality recording datasets and produce human-like voices on benchmark datasets (Li et al., 2019; <a href="../../E2E/2021.06.11_VITS/">VITS (2021)</a>; Tan et al., 2024; <a href="../../Models/E2E/2021.12.04_YourTTS.md">YourTTS (2021)</a>). 
It is demonstrated that, by training LLMs on tens of thousands of hours of diverse audio data, zero-shot adaptation can be accomplished with just a few seconds of audio input. </p>
<p>Despite the significant advancements in TTS at scale, it still poses challenges to further scale up the models. 
Neural audio codecs typically generate multiple sequences of audio tokens. 
For instance, Encodec (D´ efossez et al., 2023) encodes a 5-second speech into 8 sequences of 375 audio tokens. 
Several work (Kharitonov et al., 2023; <a href="../2023.05.16_SoundStorm/">SoundStorm (2023)</a>) employ the semantic tokens from self-supervised speech representation learning (Chung et al., 2021) as an intermediary between text and audio tokens. 
Although semantic tokens compress information more concisely than audio tokens, a 5-second speech segment still demands 125 semantic tokens, presenting a hurdle even setting aside the further complexities of audio token modeling from them. </p>
<p>In this work, we aim to bring the capability of efficient training and inference of large-language models within the TTS domain. 
To this end, we propose an improved <strong>C</strong>odec <strong>La</strong>nguage <strong>M</strong>odel-based <strong>TTS</strong> (<strong>CLaM-TTS</strong>) system that encodes speech into multiple token sequences similar to existingmethods but in a more concise way. 
With <strong>CLaM-TTS</strong>, all multiple tokens at each time step in these sequences are generated through a single autoregressive step of a language model, eliminating the need for iterative generative modeling along the number of sequences. 
The core of our method lies in the probabilistic discrete representation learning, ensuring that all discrete latent codes participate in the training process, resulting in a high-quality autoencoder for speech. 
Furthermore, we provide a principled framework enabling a latent language model to efficiently generate a stack of tokens at once; the latent language model produces a continuous latent audio representation and converts it to a discrete representation with the proposed probabilistic quantization method. 
We scale up the training dataset to 100K hours. 
Our experimental findings indicate that <strong>CLaM-TTS</strong> either surpasses or is on par with leading zero-shot neural codec-based TTS models in aspects such as naturalness, intelligibility, speaker similarity, and inference speed. 
Furthermore, we investigate how the depth of pretraining in the language models and their methods of text tokenization influence TTS outcomes. 
Our generated samples are available on our <a href="https://clam-tts.github.io">demo page</a>.</p>
</blockquote>
<h2 id="2-related-work">2. Related Work<a class="headerlink" href="#2-related-work" title="Permanent link">&para;</a></h2>
<blockquote>
<p><strong>Neural audio codec</strong>
The neural discrete representation learning within a variational autoencoder(VAE) framework, called the vector-quantized VAE (VQ-VAE), has been proven effective in encoding raw-waveforms into discrete tokens (Baevski et al., 2020a), employed as a speech codec (Oord et al., 2017; Gˆ arbacea et al., 2019). 
Similar to VQ-VAE, the neural audio codec methods usually use a framework that jointly trains an encoder, a decoder, and a quantizer (Li et al.; Zeghidour et al., 2021; Jiang et al., 2022; Jayashankar et al., 2022; D´ efossez et al., 2023; Kumar et al., 2023; Wu et al., 2023). 
Zeghidour et al.(2021) pioneers using residual vector quantization (RVQ) (Gray, 1984; Vasuki &amp; Vanathi, 2006; Lee et al., 2022) in a neural audio codec model. 
It operates efficiently on clean and noisy speech and music, even at low bit-rates. 
EnCodec (D´ efossez et al., 2023) employs a similar model structure with improved training efficiency and stability to achieve a downsampling rate of 320 for input waveforms. 
Kumar et al.(2023) identify the issue of codebook under-utilization in EnCodec and improve the codebook utilization with the techniques introduced in Yu et al.(2021) resulting in state-of-the-art performance as a neural audio codec. </p>
<p>Building on these advancements, we focus more on the discrete representation learning of speech rather than general audio and optimize the compression level to be suitable for the TTS task. 
In other words, we compress mel-spectrograms rather than raw waveforms, delegating the task of converting mel-spectrograms back into raw waveforms to standard vocoders. </p>
<p><strong>Large-scale TTS</strong>
<a href="../../Models/Speech_LLM/2022.09.07_AudioLM.md">AudioLM (2022)</a> is a language model directly trained on audio tokens. 
In <a href="../../Models/Speech_LLM/2022.09.07_AudioLM.md">AudioLM (2022)</a>, semantic tokens are first generated. 
These tokens originate from self-supervised discrete speech representation methods (<a href="../../Models/Speech_Representaion/2021.06.14_HuBERT.md">HuBERT (2021)</a>, <a href="../../Models/Speech_Representaion/2021.08.07_W2V-BERT.md">W2V-BERT (2021)</a>) that have previously been utilized for speech resynthesis or generation without text (Lakhotia et al., 2021; Polyak et al., 2021; Nguyen et al., 2023). 
Following this, the model produces acoustic tokens of neural audio codes given the semantic tokens. 
Wang et al. propose the first neural codec language model, <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, for text-to-speech that utilizes a pre-trained neural audio codec, EnCodec (D´ efossez et al., 2023). 
In a different approach, following <a href="../../Models/Speech_LLM/2022.09.07_AudioLM.md">AudioLM (2022)</a>, text-to-speech has been realized by applying language modeling to generate the semantic tokens from text, as demonstrated by <a href="../../Models/Speech_LLM/2023.02.07_SPEAR-TTS.md">SPEAR-TTS (2023)</a>. 
A shared characteristic among these neural codec language models is their two-stage pipeline; they autoregressively generate coarse-grained audio tokens and decode them into fine-grained representations. 
Recent work in music generation hints at an efficient way to eliminate the second-stage modeling by interleaving audio tokens in a delayed pattern (Copet et al., 2023), but its application in TTS remains unexplored. </p>
<p>Given the complexities in modeling long audio sequences, several studies have incorporated phonemes and durations to alleviate the need for speech synthesizers to predict speech rates (Shen et al., 2023; Le et al., 2023; Jiang et al., 2023). 
Some work shows that non-autoregressive generative models, such as a diffusion model and flow-matching (Ho et al., 2020; Lipman et al., 2023), can produce diverse and natural-sounding speech with large-scale training. 
A hybrid method is utilized in another approach, employing non-autoregressive architecture except prosody modeling (Jiang et al., 2023). 
This method aligns with previous work that applies VQ-VAEs to capture fine-grained speech features so that the prosody is controllable by them (Sun et al., 2020; Ren et al., 2022). </p>
<p>To address the challenges associated with neural codec language models while not relying on the phoneme and its duration that requires significant domain expertise, we design a language model that generates from coarse to fine-grained tokens without needing a two-stage pipeline. 
Our approach is similar to recent work that utilizes pre-trained language models, Spectron (Nachmani et al., 2023) and <a href="../2023.05.16_SoundStorm/">SoundStorm (2023)</a>. 
While Spectron employs pre-trained transformer decoders to directly model the mel-spectrogram and then fine-tunes it, our method preserves the pre-trained text encoder and decodes speech tokens that are shorter than the mel-spectrogram using a latent transformer decoder. 
SoundStorm freezes a pre-trained text encoder similar to ours, but it generates semantic tokens and subsequently decodes acoustic tokens using an iterative generative model. </p>
</blockquote>
<h2 id="3-preliminaries">3. Preliminaries<a class="headerlink" href="#3-preliminaries" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Building upon the approach proposed by <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, we explore zero-shot TTS through the lens of neural codec language modeling task. 
We consider a setting that includes two types of data: (i) text data and (ii) mel-spectrogram representation of its corresponding speech data, denoted by $\pmb{x}$ and $\pmb{y}$, respectively. 
We model a sequence of $T$ discrete codes $\pmb{c}<em>{1:T}={\pmb{c}_1,\cdots,\pmb{c}_T}$ from latent representations $\pmb{z}</em>{1:T}$ of a mel-spectrogram $\pmb{y}$, using the framework of a <algo>Variational Autoencoder (VAE)</algo> with <algo>Residual Vector Quantization (RVQ)</algo>. 
Here, $\pmb{c_t}$ represents the $D$-depth of quantized, discrete codes. 
We interchangeably use $\pmb{c}<em>{t,1:D}$ with $\pmb{c}_t$. 
Subsequently, a neural language model $p</em>\theta(\pmb{c}<em>{1:T}|\pmb{x})$ is employed, aiming to predict $\pmb{c}</em>{1:T}$ from the text transcript $\pmb{x}$. 
During the inference phase, the language model generates $\pmb{c}_{1:T}$ for a given text $\pmb{x}$, which is subsequently transformed into speech through the <algo>VAE</algo> decoder and a pre-trained vocoder. </p>
</blockquote>
<h3 id="31-residual-quantized-variational-autoencoder-rq-vae">3.1. Residual Quantized Variational Autoencoder (RQ-VAE)<a class="headerlink" href="#31-residual-quantized-variational-autoencoder-rq-vae" title="Permanent link">&para;</a></h3>
<blockquote>
<p>An <a href="../_tmp/2022.03_RQ-VAE.md">RQ-VAE (2022)</a> is a neural network architecture representing data as discrete codes using residual vector quantization. 
It comprises of three components: 
(1) an encoder parameterized by $\phi$ that maps data $\pmb{y}$ into a sequence of latent representations $\pmb{z}<em>{1:T}$; 
(2) a residual vector quantizer $RQ</em>\psi(\cdot)$, converting the latent vector $\pmb{z}<em>t$ at each time $t$ into the discrete code representation $\pmb{c}</em>{t,1:D}=RQ_\psi(\pmb{z}<em>t)$, or the corresponding quantized embedding $\hat{\pmb{z}}_t$;
(3) a decoder parameterized by $\omega$ reconstructs the data $\hat{y}$ from a sequence of the quantized latent representations $\hat{z}</em>{1:T}$
Here $\pmb{c}<em>{1,1:D}$ represents the set ${c</em>{t,1},\cdots,c_{t,D}}$ with $D$ indicating the total depth of the quantizer.
The latent representation from the encoder is quantized through the multi-stage nearest-neighbor lookup over the codebook embeddings, of which the vocab size is $V$.
The process is defined as finding the optimal code from the codebook, which minimizes the residual error at each depth $d$:</p>
<p>$$
c_{t,d} = \arg\min_{c'\in{1,\cdots,V}}|\pmb{r}<em>{t,d-1}-e</em>{\psi}(c';d)|^2\
\pmb{r}<em>{t,d} =\pmb{r}</em>{t,d-1}-e_{\psi}(c_{t,d};d)
$$</p>
<p>for all $d\in [1,D]$,
where $\pmb{r}<em>{t,0}=\pmb{z}_t$ and $e</em>{\psi}(c;d)$ corresponds to the $c$-th embedding vector in the codebook at depth $d$.
The sum of embeddings $\sum_{d=1}^D e_{\psi}(c_{t,d};d)$ becomes the quantized latent representation $\hat{\pmb{z}}_t$, which is converted back to the input space through the decoder. 
The codebook embeddings are updated with the clustered latents by the exponential moving average updates (<a href="">VQ-VAE-2 (2019)</a>). </p>
</blockquote>
<h3 id="32-mean-field-variational-inference">3.2. Mean-Field Variational Inference<a class="headerlink" href="#32-mean-field-variational-inference" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Consider a latent variable model characterized by a joint distribution $p_\psi(\pmb{z}<em>t,\pmb{c}</em>{t,1:D})$ parameterized by $\psi$. 
Here, $\pmb{z}<em>t$ denotes an observed random variable, $\pmb{c}</em>{t,1:D}$ indicates a set of latent random variable ${c_{t,1},\cdots,c_{t,D}}$.
In this model, variational inference is a method to approximate the intractable distribution $p_\psi(\pmb{z}<em>t,\pmb{c}</em>{t,1:D})$ by solving an optimization problem with respect to parameters of approximate distribution $q(\pmb{c}<em>{t,1:D}|\pmb{z}_t)$.
We can derive a lower bound on the marginal log-likelihood $p</em>\psi(\pmb{z}_t)$ known as the <term>evidence lower bound (ELBO)</term> (Blei et al., 2017):
$$</p>
<p>$$</p>
<p>Mean-field variational inference (Koller &amp; Friedman, 2009; Blei et al., 2017) is a specific approach of variational inference that assumes the independence among the latent variables conditioned on the observed variable: $ $.
We can show that each optimal variationalposterior distribution $ $, which maximizes the ELBO, satisfies: 
$$</p>
<p>$$</p>
<p>where $ $ denotes the latent variables of all depth $ $ except $ $. 
An iterative coordinate ascentalgorithm based on Eq.2 can be used to update the distribution
 (Bishop &amp; Nasrabadi, 2006), andthe complexity of the algorithm mainly lies on the computation of the expectation over $ $.</p>
</blockquote>
<h2 id="4-method">4. Method<a class="headerlink" href="#4-method" title="Permanent link">&para;</a></h2>
<h3 id="41-mel-vae">4.1. Mel-VAE<a class="headerlink" href="#41-mel-vae" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We aim to develop a neural codec that can generate discrete speech codes within a short sequence length to make speech audios suitable for language model utilization. 
To achieve this, we employ a RQ-VAE that compresses mel-spectrograms of speech audios (see Fig. 
1a). 
We introduce a variational inference based method for learning residual codewords to address the codeword collapse issue found in conventional vector quantization methods (Kaiser et al., 2018; Roy et al., 2018; Zeghidour et al., 2021; Kumar et al., 2023). 
We illustrate Mel-VAE similar to RQ-VAE following most of notations from Sec.3.1. 
The encoder maps a mel-spectrogram $ $ into a sequence of latent representations 
 , and a residual vectorquantizer
, converting the latent vector 
 at each time 
 into the discrete code representation 
, or its corresponding quantized embedding 
. 
The decoder reconstructs themel-spectrogram
 from a sequence of quantized latent representations 
With the assumptions that
 is uniformly dis-tributed, mean-field variational inference yields the condition of such distribution as the following (see Eq.2):
where the latents follows a normal distribution:
However, the mutual interdependence of codes at every depth in the latter equation makes it difficult to solve it without an iterative approach. 
Instead of using an iterative coordinate update approach, we approximate
 pointwisely as 
 for all 
. 
The posterior then has a form: 
We independently optimize the codebook embeddings at each depth
, in a variational inferenceframework:
The other modules of Mel-VAE, including the encoder parameters
 and the decoder parameters 
are trained with commitment loss, reconstruction loss, and adversarial losses. </p>
<p>corresponds to coefficients of the reconstruction loss, commitment loss, and ad-versarial losses, respectively. 
For adversarial training, we adopt the multi-length discriminator (Chen et al., 2020) that distinguishes mel-spectrograms at different lengths and a modified multi-resolution spectrogram discriminator (Lee et al., 2023) that accepts mel-spectrogram rather than linear spectrogram. 
We combine the least squares GAN objective (Mao et al., 2017) and the
 feature matchingloss (Kumar et al., 2023) into a loss function denoted by</p>
</blockquote>
<h3 id="42-latent-language-modeling">4.2. Latent Language Modeling<a class="headerlink" href="#42-latent-language-modeling" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We propose a conditional speech code language model given text $\pmb{x}$ aimed at enhancing the efficiency of the model. 
This improvement stems from the insight that speech codes are generated through vector quantization. 
Our approach leverages this by predicting the vector itself, which can then be converted into multiple tokens at each layer via residual vector quantization. 
This is a departure from previous methods that predict speech code tokens sequentially. </p>
<p>Specifically, rather than directly predicting tokens from text, we consider a continuous latent representation $\pmb{z}_t$ of speech that can be converted into a speech code using residual vector quantization: 
$$
$$</p>
<p>where $ $ indicate $ $, and we employ $ $, the probabilistic quantizer distribution learnedtogether with the Mel-VAE model (see Sec.4.1), as a replacement of $ $. 
Here, wedefine the conditional distribution $ $ as a Gaussian mixture model: </p>
</blockquote>
<p>In this model, we can derive the following variational lower bound on the log-likelihood: 
)) + log 
for any 
. </p>
<blockquote>
<p>The derivation of the lower bound and the definition of 
 are providedin Appendix A. 
Here, we set
With the second loss
, which is associated with training a binary classifier to identify theend of speech (
), the total loss for training the latent language model is the sum of the twolosses above:
As shown in Fig. 
1, we implement an autoregressive latent model that yields three distinctive outputs:the mixture weights and the means of the Gaussian mixture distribution as well as the probability of concluding the generation. 
Specifically, it incorporates a transformer decoder followed three parallel modules, comprising (1) a prediction layer with softmax activation for
; (2) a predictionlayer for
; (3) a binary classifier layer for 
 prediction. 
Additionally, we use thepre-trained quantizer
 of Mel-VAE.</p>
</blockquote>
<h3 id="43-model-architecture-and-inference">4.3. Model Architecture and Inference<a class="headerlink" href="#43-model-architecture-and-inference" title="Permanent link">&para;</a></h3>
<h4 id="model-architecture">Model Architecture<a class="headerlink" href="#model-architecture" title="Permanent link">&para;</a></h4>
<blockquote>
<p>For the Mel-VAE, we adopt a causal 1d convolutional U-Net, a variant of themodel used in Ho et al. 
(2020). 
We remove the skip connections and attention layers and append 1-d ConvNeXt (Liu et al., 2022) blocks used in Siuzdak (2023) to the final layer of the decoder. 
We employ 32-stage residual vector quantization with a codebook size of 1,024 for each depth. 
For the text-to-code latent language model, we adopt a transformer-based encoder-decoder LM, especially a pre-trained ByT5-large (Xue et al., 2021a) similar to <a href="../2023.05.16_SoundStorm/">SoundStorm (2023)</a>. 
We keep the text encoder frozen. 
Please refer to Appendix C for more detailed model configuration. </p>
</blockquote>
<h4 id="inference">Inference<a class="headerlink" href="#inference" title="Permanent link">&para;</a></h4>
<blockquote>
<p>The text-to-code generation unfolds in three steps: 
1. at time step $t$, we randomly select $k$ from the distribution $p_\theta(k|\pmb{x},\pmb{c}<em>{&lt;t})$;
2. following this, randomly sample the latent vector $\pmb{z}_t$. 
Consequently, at time step $t$, the discrete code is obtained through the learned quantizer, $\pmb{c}_t=RQ</em>\psi(\pmb{z}_t)$; 
3. if the probability of EOS exceeeds 0.5, conclude the generation, or proceed to step, otherwise. 
Subsequently, the generated codes are decoded to mel-spectrograms using the decoder of Mel-VAE, then converted to raw-waveforms through an off-the-shelf pre-trained vocoder, BigVGAN (Lee et al., 2023). </p>
</blockquote>
<h2 id="5-experimental-setup">5. Experimental Setup<a class="headerlink" href="#5-experimental-setup" title="Permanent link">&para;</a></h2>
<h3 id="training-dataset">Training Dataset<a class="headerlink" href="#training-dataset" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We employ 100K hours of over 12K distinct speakers’ speech-transcript dataset spanning 11 languages: English, Korean, Chinese, Japanese, German, Dutch, French, Spanish, Italian, Portuguese, and Polish. 
We train two models: (1) CLaM-en: an English-only model on 55K hour English dataset and (2) CLaM-multi: a multilingual model trained on 11-language dataset. 
We provide details of dataset for each language in Appendix B.1, and data pre-processing in Appendix B.2 and B.3. </p>
</blockquote>
<h3 id="training">Training<a class="headerlink" href="#training" title="Permanent link">&para;</a></h3>
<blockquote>
<ol>
<li>Mel-VAE: We train the model on 4 NVIDIA A100 40GB GPUs for around 2Msteps. 
Each GPU processes a size one minibatch containing concatenated mel-spectrograms of several audios. 
We trim the trailing end to have it precisely 32,768 frames long. 
We use Adam optimizer (Kingma &amp; Ba, 2015) with a constant learning rate of 0.0002 throughout the training. </li>
<li>Text-to-code: We train only the decoder and use a learned codebook from Mel-VAE. 
The model is trained on 4 NVIDIA A100 40GB GPUs for around 4M steps with dynamic batching while keeping a maximum code size of 2,560. 
We use AdamW optimizer (Loshchilov &amp; Hutter, 2019), and the learning rate is fixed to 0.0002 throughout the training. 
Throughout all our experiments, during the model inference, we sample $k$ using top $p$ sampling (Holtzman et al., 2020) with 0.5 and $z$ is sampled with temperature (Kingma &amp; Dhariwal, 2018) of 2.6, which matches the empirical standard deviation in our validation dataset. </li>
</ol>
</blockquote>
<h3 id="baselines">Baselines<a class="headerlink" href="#baselines" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We compare the proposed model with the following baselines: 
(1) <a href="../../Models/E2E/2021.12.04_YourTTS.md">YourTTS (2021)</a>, a zero-shot TTS built on <a href="../../E2E/2021.06.11_VITS/">VITS (2021)</a> which is flow-based end-to-end TTS (representing <strong>Conventional TTS</strong>), 
(2) <a href="../2023.01.05_VALL-E/">Vall-E (2023)</a> and (3) <a href="../../Models/Speech_LLM/2023.02.07_SPEAR-TTS.md">SPEAR-TTS (2023)</a> (representing<strong>Neural Codec LM</strong>), 
(4) <a href="../../Models/Speech_LLM/2023.06.23_VoiceBox.md">VoiceBox (2023)</a>, a flow-matching-based TTS model trained on large-scale training data (representing <strong>Non-Autoregressive Model with Phoneme Input and Duration</strong>).</p>
</blockquote>
<h3 id="metrics">Metrics<a class="headerlink" href="#metrics" title="Permanent link">&para;</a></h3>
<blockquote>
<ol>
<li>Intelligibility and Robustness: 
We measure these attributes by <term>character error rate (CER)</term> and <term>word error rate (WER)</term> of the synthesized transcription from generated speech concerning the input text. 
We follow the procedure in <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>. 
In English-only Evaluation, we synthesize the transcription by using the <term>automatic speech recognition (ASR)</term> model, the CTC-based <a href="../../Models/Speech_Representaion/2021.06.14_HuBERT.md">HuBERT (2021)</a>-Large model pre-trained on <a href="">Libri-Light (2020)</a> and then fine-tuned on <a href="">LibriSpeech (2015)</a>. 
In the Multilingual Evaluation, we use OpenAI’s <a href="">Whisper (2023)</a> model. 
We adopt NVIDIA’s NeMo-text-processing (Zhang et al., 2021; Bakhturina et al., 2022) for text normalization; </li>
<li>Speak Similarity: 
We assess the speaker similarity of two separate speech audio clips by following the same procedure outlined in <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>. 
We employ WavLM-TDCNN (Chen et al., 2022) which outputs the embedding vector representing the speaker’s voice attribute. 
We measure the cosine similarity between the two embedding vectors to get a score in $[-1,1]$, where a higher score indicates a higher speaker similarity of the audios. 
We borrow the definition of SIM-o and SIM-r from Le et al. (2023). 
SIM-o measures the similarity between the generated and the original target speeches, while SIM-r measures the similarity concerning the target speech reconstructed from the original speech by MelVAE and the pre-trained vocoder; </li>
<li>Subjective Speech Quality: 
We measure the quality of the generated speech from human evaluations via three types of Mean Opinion Score (MOS) (Ribeiro et al., 2011): </li>
<li><term>Quality MOS (QMOS)</term> for an overall audio assessment, </li>
<li><term>Similarity MOS (SMOS)</term> to measure speaker similarity between the prompt and the generated speech,</li>
<li><term>Comparative MOS (CMOS)</term> to compare our model with available baselines. </li>
</ol>
<p>Detailed settings of subjective tests are described in Sec.B.5. </p>
</blockquote>
<h3 id="tasks">Tasks<a class="headerlink" href="#tasks" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We measure the performances of the proposed model under two different tasks:
1. continuations: Given a text and corresponding initial 3 seconds of the Ground Truth speech as a prompt, thetask is to seamlessly synthesize the subsequent portion of the speech, 
2. cross-sentence: The model is given a text, a 3-second speech segment, and its corresponding transcript (the transcript is different from the text). 
The task is to synthesize a speech reading the text in the style of the provided 3-second speech. </p>
<p>We include our samples across the tasks discussed above, covering speaker diversity, text prompting, and other aspects, on our demo page. </p>
</blockquote>
<h2 id="6-experimental-results">6. Experimental Results<a class="headerlink" href="#6-experimental-results" title="Permanent link">&para;</a></h2>
<h3 id="61-english-only-evaluations">6.1. English-Only Evaluations<a class="headerlink" href="#61-english-only-evaluations" title="Permanent link">&para;</a></h3>
<h4 id="evaluation-methodology">Evaluation Methodology<a class="headerlink" href="#evaluation-methodology" title="Permanent link">&para;</a></h4>
<blockquote>
<p>We evaluate performances of <strong>CLaM-en</strong> across continuation and cross-sentence tasks. 
Following the evaluation setting in <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, we employ a subset of the <a href="">LibriSpeech</a> test-clean dataset. 
This subset comprises speech clips ranging from 4 to 10 seconds, each with a corresponding transcript. 
Note that <a href="../../Models/E2E/2021.12.04_YourTTS.md">YourTTS (2021)</a> has official checkpoints, <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> has an unofficial checkpoint, and others do not have checkpoints. 
We use checkpoints of <a href="../../Models/E2E/2021.12.04_YourTTS.md">YourTTS (2021)</a> and <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a> for evaluations. 
We compare the other baselines with ours via the performances reported in their papers (<a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>; <a href="2023.02.07_SPEAR-TTS.md">SPEAR-TTS (2023)</a>; [VoiceBox (2023)](2023.06.23_Voice../../Models/Speech_LLM/2023.06.23_VoiceBox.md
Since <a href="2023.02.07_SPEAR-TTS.md">SPEAR-TTS (2023)</a> and [VoiceBox (2023)](2023.06.../../Models/Speech_LLM/2023.06.23_VoiceBox.mdvaluate using the same approach with <a href="../2023.01.05_VALL-E/">VALL-E (2023)</a>, they can be directly compared with our model as well. 
Details of evaluation are provided in Appendix B.4. </p>
</blockquote>
<h4 id="analysis">Analysis<a class="headerlink" href="#analysis" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Tab. 1 and 2 show the results of continuation and cross-sentence task, respectively. 
Ours offers great performances for all measures, ranking either first or second, except SIM-r in crosssentence task.</p>
<p>It is worth noting that <a href="../../Models/Speech_LLM/2023.06.23_VoiceBox.md">VoiceBox (2023)</a>, a phoneme-based duration model, shows better performances than ours. 
However, it requires both phoneme and duration for speech synthesis, whereas our model directly employs a pretrained language model. 
This allows for the seamless integration of LMs, which are trained across a broad spectrum of texts and tasks, enabling a plug-and-play methodology. 
Experimental results of training several T5 variants is shown in Appendix D.2, illustrating the trade-off between leveraging the inherent capacity of LMs and ensuring robustness. 
We also compare the end-to-end inference time for a 10-second utterance. 
Our method is faster than the generation speed of Vall-E reported in <a href="../../Models/Speech_LLM/2023.06.23_VoiceBox.md">VoiceBox (2023)</a>.
While ours is faster than <a href="../../Models/Speech_LLM/2023.06.23_VoiceBox.md">VoiceBox (2023)</a> with 64 decoding steps, <a href="../../Models/Speech_LLM/2023.06.23_VoiceBox.md">VoiceBox (2023)</a> can use fewer iterations of decoding steps. 
Tab. 
3 presents the subjective audio evaluations. 
CLaM-en significantly outperforms the baseline, <a href="../../Models/E2E/2021.12.04_YourTTS.md">YourTTS (2021)</a>, in quality and intelligibility, as indicated by QMOS. 
Our adherence to the prompt audio surpasses that of the baseline, as measured by the SMOS. 
The comparative scores (CMOS) highlight CLaM-en’s proximity to the Ground Truth regarding naturalness, clarity, and comprehensibility. 
Overall, CLaM-en’s generated speech naturalness, quality, intelligibility, and similarity exceed the baseline. </p>
</blockquote>
<h3 id="62-multilingual-evaluations">6.2. Multilingual Evaluations<a class="headerlink" href="#62-multilingual-evaluations" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We evaluate our model, CLaM-multi trained on the multilingual dataset. 
On the test set, we measure WER, CER, and SIM-o which are defined in Sec.5. 
Here, we only consider continuation task inthis experiment since we cannot get full alignmnets between audio and text for all languages and datasets. 
Tab.4 shows the partial results of the multilingual continuation task. 
We sample a hundred random samples from the test set of each dataset, ranging from 4 to 20 seconds, and average the scores of three trials. Refer to Tab. 10 for evaluation on other languages and datasets.</p>
</blockquote>
<h3 id="63-ablation-study">6.3. Ablation Study<a class="headerlink" href="#63-ablation-study" title="Permanent link">&para;</a></h3>
<h4 id="effectiveness-of-proposed-rvq">Effectiveness of Proposed RVQ<a class="headerlink" href="#effectiveness-of-proposed-rvq" title="Permanent link">&para;</a></h4>
<blockquote>
<p>To demonstrate the effect of the proposed RVQ on Mel-VAE,we conduct an ablation study by assessing speech reconstruction capability. 
We train two MelVAEs: one with the proposed RVQ and the other with the baseline RVQ (D´ efossez et al., 2023). 
We train both for 500k steps on the same training dataset described in Sec.5. 
The generated speech is compared to the Ground Truth speech using two metrics: Perceptual Evaluation of Speech Quality (PESQ) (Rix et al., 2001) and Virtual Speech Quality Objective Listener (ViSQOL) (Chinen et al., 2020) in speech mode. 
For evaluation, we randomly select 1,800 samples from the test set of each dataset, proportional to the size of each dataset, each being at least 3 seconds long. 
The scores of these samples are then averaged for comparison. 
Tab.5a shows that ours is more effective than the baseline RVQ. 
See Fig.2 to verify the superior codebook usage of our approach. 
We also compare the fully trained Mel-VAE with Encodec at 6K bitrates (D´ efossez et al., 2023), which is widely employed in neural codec language models (Wang et al., 2023; Zhang et al., 2023). 
Tab.5b confirms that ours outperforms Encodec in speech reconstruction performance across both measures. </p>
</blockquote>
<h4 id="comparision-of-pre-trained-lm-and-input-variants">Comparision of Pre-trained LM and Input Variants<a class="headerlink" href="#comparision-of-pre-trained-lm-and-input-variants" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Our language model is based on T5 (Raffelet al., 2020). 
We conduct an ablation studty to compare T5, its variants and a phoneme encoder of comparable size. 
The results indicate that ByT5 surpasses other T5 variants with the sole exception of the phoneme model. 
This suggests that: 1) the more the pretraining phase is leveraged, the greater the potential increase in TTS performance, and 2) in moderate-sized language modeling, phonemes remain an effective input representation. 
For the experimental results and a comprehensive analysis, refer to Appendix D.2. 
In addition to the ablation studies presented, we have conducted further experiments detailed in the appendix, which explore the effects of codeword emitting rate on speech codec quality and language modeling as well as the scale of training data on model efficacy. 
For comprehensive results and discussions, refer to Appendix D.3 and D.4. </p>
</blockquote>
<h2 id="7-discussion">7. Discussion<a class="headerlink" href="#7-discussion" title="Permanent link">&para;</a></h2>
<blockquote>
<p><strong>Choice of Codeword Rate</strong> 
Our approach enjoys a 10Hz codeword rate for efficient modeling.We set the codeword rate following the average phoneme rate in English speech (Roach, 2009) since phoneme is the minimum spoken unit. 
Nevertheless, we conjecture this may have to be adjusted depending on the language or speaker. 
A more compressed codeword rate, for example, 5Hz, might lead to more significant information loss than their efficiency. 
There exists an efficiencyperformance tradeoff for rates above 10Hz, which can be optimized as needed. </p>
<p><strong>Robustness</strong> 
We have noticed some words can be muddled, omitted, or repeated, which predom-inantly stems from autoregressive modeling. 
We will address it by employing non-autoregressive architecture or improving the attention mechanism in future work. </p>
<p><strong>Expressiveness</strong> 
100K hours of training data may not ensure a complete representation of all voicetypes, especially accentuated ones. 
Our datasets predominantly capture audiobook reading styles, leading to limited diversity in speaking styles. 
We believe that increasing the model and data size can significantly tackle the expressiveness challenges in zero-shot TTS. </p>
<p><strong>Instruction Prompting</strong> 
We suggest various ways to use the full knowledge of the language model.One can incorporate speaker metadata into each transcript to perform various intriguing tasks. 
Such tasks might include synthesizing speech or even conversations characterized by specific genders, voice ages, or accents. 
We leave the other tasks for future work. </p>
</blockquote>
<h2 id="8-conclusion">8. Conclusion<a class="headerlink" href="#8-conclusion" title="Permanent link">&para;</a></h2>
<blockquote>
<p>We introduce <strong>CLaM-TTS</strong>, which leverages mean-field variational inference based probabilistic residual vector quantization (1) achieving significant compression in token length, and (2) allowing a latent language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. 
We scale up the training dataset to 100K hours. 
We empirically show that <strong>CLaM-TTS</strong> is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. </p>
</blockquote>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
    
  </body>
</html>