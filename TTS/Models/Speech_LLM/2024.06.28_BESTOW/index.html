
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.0">
    
    
      
        <title>BESTOW - Sapphire Lab</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.9f615399.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#bestow" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Sapphire Lab" class="md-header__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sapphire Lab
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              BESTOW
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Sapphire Lab" class="md-nav__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sapphire Lab
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TTS
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../../PDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PDE
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract: 摘要
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1introduction" class="md-nav__link">
    1.Introduction: 引言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2related-works" class="md-nav__link">
    2.Related Works: 相关工作
  </a>
  
    <nav class="md-nav" aria-label="2.Related Works: 相关工作">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21speech-foundational-model" class="md-nav__link">
    2.1.Speech Foundational Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22speechllm" class="md-nav__link">
    2.2.SpeechLLM
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23streaming-speech-models" class="md-nav__link">
    2.3.Streaming Speech Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-streaming-speechllm-is-hard" class="md-nav__link">
    Why Streaming SpeechLLM is Hard: 流式语音大语言模型的困难
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3methodology" class="md-nav__link">
    3.Methodology: 方法
  </a>
  
    <nav class="md-nav" aria-label="3.Methodology: 方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31unified-offline-streaming-framework" class="md-nav__link">
    3.1.Unified Offline-Streaming Framework
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32introduce-speech-modality-with-cross-attention" class="md-nav__link">
    3.2.Introduce Speech Modality with Cross Attention
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33from-offline-to-streaming" class="md-nav__link">
    3.3.From Offline to Streaming
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34bestow-vs" class="md-nav__link">
    3.4.BESTOW v.s.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4experiments" class="md-nav__link">
    4.Experiments: 实验
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5conclusions" class="md-nav__link">
    5.Conclusions: 结论
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="bestow">BESTOW<a class="headerlink" href="#bestow" title="Permanent link">&para;</a></h1>
<details>
<summary>基本信息</summary>

- 标题: BESTOW: Efficient and Streamable Speech Language Model with the Best of Two Worlds in GPT and T5
- 作者:
  - 01 [Zhehuai Chen](../../Authors/Zhehuai_Chen.md)
  - 02 [He Huang](../../Authors/He_Huang.md)
  - 03 [Oleksii Hrinchuk](../../Authors/Oleksii_Hrinchuk.md)
  - 04 [Krishna C. Puvvada](../../Authors/Krishna_C._Puvvada.md)
  - 05 [Nithin Rao Koluguri](../../Authors/Nithin_Rao_Koluguri.md)
  - 06 [Piotr Żelasko](../../Authors/Piotr_Żelasko.md)
  - 07 [Jagadeesh Balam](../../Authors/Jagadeesh_Balam.md)
  - 08 [Boris Ginsburg](../../Authors/Boris_Ginsburg.md)
- 机构:
  - [NVIDIA](../../Institutions/Nvidia.md)
- 时间:
  - 预印时间: 2024.06.28 ArXiv v1
  - 更新笔记: 2024.07.01
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.19954)
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - [语言模型](../../Tags/LanguageModel.md)
- 页数: 9
- 引用: 56
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

<h2 id="abstract">Abstract: 摘要<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Incorporating speech understanding capabilities into pretrained large-language models has become a vital research direction (SpeechLLM). 
The previous architectures can be categorized as:
i) {\em GPT-style}, prepend speech prompts to the text prompts as a sequence of LLM inputs like a decoder-only model; 
ii) {\em T5-style}, introduce speech cross-attention to each layer of the pretrained LLMs. </p>
<p>We propose BESTOW architecture to bring {\em the BESt features from TwO Worlds} into a single model that is highly efficient and has strong multitask capabilities.
Moreover, there is no clear streaming solution for either style, especially considering the solution should generalize to speech multitask.
We reformulate streamable SpeechLLM as a read-write policy problem and unifies the offline and streaming research with BESTOW architecture. 
Hence we demonstrate the first open-source SpeechLLM solution that enables {\em Streaming} and {\em Multitask at scale} (beyond ASR) at the same time. 
This streamable solution achieves very strong performance on a wide range of speech tasks (ASR, AST, SQA, unseen DynamicSuperb).
It is end-to-end optimizable, with {\em lower training/inference cost}, and demonstrates LLM knowledge transferability to speech. </p>
</blockquote>
<h2 id="1introduction">1.Introduction: 引言<a class="headerlink" href="#1introduction" title="Permanent link">&para;</a></h2>
<blockquote>
<p>With the huge success of large language models (LLMs)~\cite{brown2020gpt,team2023gemini}, researchers start to explore the possibilities of extending the capabilities of LLMs with multi-modal understanding skills, and many works have been proposed to support image and audio understanding~\cite{alayrac2022flamingo,liu2024llava,zhang2023speechgpt,gong2023ltu-as,tang2023salmonn}.</p>
<p>This work focuses on leveraging speech encoder and LLM ({\em SpeechLLM}) to build a speech foundational model for many speech-and-audio to text applications (STT). 
One popular framework in the direction is Speech-LLaMA~\cite{wu2023decoder,fathullah2023prompting} and its extensions, which updates the input of LLM by prepending speech prompts to the text prompts while keeping the rest of LLM unchanged or LoRA finetuned.
This design shows good modularity which allows knowledge transfer from LLMs to speech and results in strong ASR and AST performance~\cite{wang2023slm,qwen2023}. 
The modular design also shows strong in-context learning ability \cite{chen2024salm}. 
\cite{tang2023salmonn} further introduces a bespoke Q-former module before prepending speech prompts to better bring speech, audio, and music features to LLM space.</p>
<p>Nevertheless, there are several potential drawbacks in this popular design:
i) Efficiency problem raised from the interaction between self-attention and the longer speech embeddings than text, which will be elaborated in Section~\ref{sec:efficiency}.
Workarounds like massive downsampling of speech embeddings usually come with information loss and cannot completely avoid. % the per-layer and per-step LLM computation increase from speech. 
ii) As the speech embeddings of the whole utterance are treated as prompt and always prepended beforehand, it disables many streaming applications in speech, e.g. streaming ASR and simultaneous speech translation (SST). </p>
<p>In this work, we propose an alternative {\em modular} and {\em multitask} SpeechLLM design that is both {\em streamable} and {\em efficient}. </p>
<p>Our main contributions are summarized as follows:</p>
<p>\item To the best of our knowledge, this is the first open SpeechLLM solution that enables {\em streaming} and {\em multitask at scale} (beyond ASR) at the same time.
Moreover, the solution is end-to-end optimizable and allows LLM knowledge transfer to speech.
\item Propose a different backbone architecture from the popular Speech-LLaMA variants that is based on cross-attention and read-write policy.
The novel backbone unifies the offline and streaming modes and achieves state-of-the-art on several large-scale and multitask speech-to-text benchmarks (ASR, AST, SQA, DynamicSuperb), with {\em lower training/inference cost}.</p>
</blockquote>
<h2 id="2related-works">2.Related Works: 相关工作<a class="headerlink" href="#2related-works" title="Permanent link">&para;</a></h2>
<h3 id="21speech-foundational-model">2.1.Speech Foundational Model<a class="headerlink" href="#21speech-foundational-model" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Motivated by the success of foundation models in NLP~\cite{brown2020gpt,team2023gemini}, recent speech foundational model research has been shifted towards developing universal pretrained models capable of handling multilingual speech and audio tasks.
Recent advances include but are not limited to: 
i) Large scale multilingual self-supervised learning and semi-supervised learning to leverage unlabled speech, e.g. XLSR~\cite{conneau2020unsupervised} and USM~\cite{zhang2023google}. 
ii) Large scale multitask supervised training, e.g. Whisper variants~\cite{radford2022robust,peng2024owsm} and SeamlessM4T~\cite{barrault2023seamless}. 
iii) More powerful and efficient speech encoder design, e.g. Conformer and variants~\cite{gulati2020conformer,rekesh2023fast}. 
iv) Multilingual multitask speech benchmarks, e.g. XTREME~\cite{conneau2022xtreme} and ML-SUPERB~\cite{shi2023ml}.</p>
</blockquote>
<h3 id="22speechllm">2.2.SpeechLLM<a class="headerlink" href="#22speechllm" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Recently, researchers started to look at combining pretrained speech models with large language models to form a new type of speech foundational model, {\em SpeechLLM}, which can be categorized through speech embedding types.
SpeechGPT~\cite{zhang2023speechgpt} and AudioPaLM~\cite{rubenstein2023audiopalm} quantize speech into discrete tokens, so speech tokens and text tokens can be combined into a single vocabulary. 
Nevertheless, this approach is limited by the quality and diversity of the speech quantization model especially for STT tasks.
Another more popular method is to feed speech into pretrained speech encoder, where the speech features are then projected into the word embedding space of the LLM~\cite{chen2024salm,kong2024audio,qwen2023,tang2023salmonn,gong2023ltu-as}.
Our work belongs to this category.</p>
<p>After extracting speech features, the previous architectures to provide speech information to LLMs can be further categorized into two branches: 
1. {\em GPT-style} models~\cite{chen2024salm,tang2023salmonn,qwen2023} directly concatenate the speech prompts with text prompts and use the combined sequence as input to the LLM like a decoder-only GPT model; 
2. Flamingo and its extension~\cite{kong2024audio,alayrac2022flamingo,radhakrishnan2023whispering} are another branch of works, where cross-modal cross-attention is added to each layer/block of the pretrained GPT-based LLM with shared text query. %, such that the text tokens can access speech information at every layer of the LLM. </p>
<p>The resultant architecture is similar to the {\em T5 architecture}~\cite{2020t5}. </p>
<p>Some recent works look at introducing cross-attention layers into the first branch of methods, but with different motivations and resultant designs from our work.
E.g. \cite{yu2023connecting,hussain2023m} introduces cross attention before the above concatenation to bridge the gap between the speech encoder and LLM, whose computation is the same or more.</p>
</blockquote>
<h3 id="23streaming-speech-models">2.3.Streaming Speech Models<a class="headerlink" href="#23streaming-speech-models" title="Permanent link">&para;</a></h3>
<blockquote>
<p>In streaming ASR, Efforts have been made to enable streaming into Transformer~\cite{zhang2020transformer,noroozi2023stateful}.
These methods either use limited context with offline models or train models in a streaming manner. 
The methods usually build on top of Transducer~\cite{graves2012sequence} which rarely benefits from pretrained LLM.</p>
<p>In simultaneous speech translation, researchers have been looking at fixed and adaptive read-write policies.
Wait-k~\cite{ma2020simulmt} and variants are the most longstanding fixed policy.
Typical adaptive policies try to learn the policy with a bespoke model from the training data, e.g.
EMMA~\cite{ma2023efficient} and ITST~\cite{zhang2022information}.</p>
</blockquote>
<hr />
<h3 id="why-streaming-speechllm-is-hard">Why Streaming SpeechLLM is Hard: 流式语音大语言模型的困难<a class="headerlink" href="#why-streaming-speechllm-is-hard" title="Permanent link">&para;</a></h3>
<blockquote>
<p>While making SpeechLLMs operate in real-time and proactively respond is important for human-machine interface, the topic is under-explored besides some very recent works. </p>
<p>As SpeechLLM usually starts offline text prediction after it accepts a complete speech segment as the speech prompt, these streaming works propose to update the prompt format to take interleaved block-wise speech features and text~\cite{seide2024speech,tsunoo2024decoderonly}. 
\cite{agostinelli2023simul,koshkin2024transllama} also belongs to this category except focusing on text machine translation with an ASR model as cascaded speech frontend.
There are several limitations in this line of approaches:
1. <strong>Prompt format mismatch</strong> between text pretrain and offline/streaming modes of SpeechLLM.
The updated interleaved format with injected {\scriptsize BLANK}\cite{seide2024speech} or {\scriptsize WAIT}\cite{koshkin2024transllama} tokens circumvents the LLM textual knowledge transfer to speech.
Previous research cannot show extra wins from leveraging pretrained LLMs and didn't demonstrate abilities beyond ASR~\cite{seide2024speech,tsunoo2024decoderonly}. 
2. <strong>Not end-to-end optimizable</strong> blocked by the introduced alignment stage.
This alignment preprocess design not only results in hardness in generalizing to multitask, e.g. AST, SQA, but also is bounded by the alignment errors in e.g. CTC \cite{seide2024speech} and translation \cite{koshkin2024transllama}, especially considering certain word can cross speech segments.
3. <strong>Higher inference cost</strong>, stems from either longer text prediction length in the new prompt format~\cite{seide2024speech} or requiring beam search~\cite{wang2023simultaneous}.</p>
<p>A new streamable SpeechLLM solution will be proposed which keeps the same LLM prompt format and unifies the learning framework of offline and streaming modes.
Several unique advantages that will be demonstrated: i) <strong>LLM knowledge transfer</strong> ii) <strong>multitask support</strong> iii) <strong>end-to-end optimizable</strong> iv) <strong>lower training/inference cost</strong>.
Lastly, we believe this is the first open-source streamable SpeechLLM solution. </p>
</blockquote>
<h2 id="3methodology">3.Methodology: 方法<a class="headerlink" href="#3methodology" title="Permanent link">&para;</a></h2>
<h3 id="31unified-offline-streaming-framework">3.1.Unified Offline-Streaming Framework<a class="headerlink" href="#31unified-offline-streaming-framework" title="Permanent link">&para;</a></h3>
<blockquote>
<p>The goal is to design a unified framework for offline and streaming SpeechLLM so as to maximize the LLM knowledge transfer from pretrain and instruction tuning.
Offline and streaming modes should share most of the architectures and end-to-end optimizable. %, different from previous works in Section~\ref{sec:prev_stream} </p>
<p>As Figure~\ref{fig:framework}, we propose to formulate the streaming problem of SpeechLLM as the read-write policy problem previously defined in simultaneous speech translation~\cite{ma2020simulmt}. % where offline SpeechLLM is generalized through offline policy.
At every LLM step, the model decides whether to wait for more speech incoming features (READ) or to predict a target word (WRITE).
The solution still keeps the prompt format of LLM unchanged and experiment results will show its benefit on LLM knowledge transfer. 
The prerequisite of this solution is to decouple READ and WRITE operations from LLM and make it modeled by a standalone module, the cross-attention feature extractor proposed in Section~\ref{sec:xattn}. 
Empirical result will show this module is on par with Speech-LLaMA architecture. % in apple-to-apple setting. % and performs the best among open SpeechLLM.
After that, streaming is straightforward and end-to-end optimizable, discussed in Section~\ref{sec:stream}.
Lastly we discuss additional efficiency benefit of this architecture in Section~\ref{sec:efficiency}.</p>
</blockquote>
<h3 id="32introduce-speech-modality-with-cross-attention">3.2.Introduce Speech Modality with Cross Attention<a class="headerlink" href="#32introduce-speech-modality-with-cross-attention" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We propose a new mechanism on how text prompts interact with speech prompts in the LLM input side, which will show unique advantages in streaming and efficiency. 
Different from majority~\cite{wu2023decoder,fathullah2023prompting,chen2024salm,tang2023salmonn} of previous works that simply concatenate speech prompts with text prompts as input to the LLM (Figure~\ref{fig:model}(a)), we inject a trainable transformer-like self-attention and cross-attention layers before feeding into LLMs to let text prompt attend to different regions of speech prompt to extract features that are more relevant to the current LLM step as illustrated in Figure~\ref{fig:model}(b).</p>
<p>With this new design, the speech features extracted from the speech encoder serve as the keys and values for the cross-attention mechanism, while the input text (text prompt and previous tokens) is firstly embedded by the LLMs input embedding matrix and later used as the queries. 
To make the query considering both the current step and the context, we further inject causal self-attention layers between the LLM input embeddings and the cross-attention layers.
In the ablation study section, we also consider an alternative design of RNNs for the same goal. 
To preserve the original textual knowledge, we include a residual connection that adds text prompts directly to the output and the combined speech and text embeddings serve as the final input to the pretrained LLM. 
The resultant design is essentially one cross-attention transformer layer proposed in \cite{vaswani2017attention} which can be repeated for $X$ times.
We empirically found in the ablation study section that $X=2$ is sufficient. % where all the LLM blocks can be frozen with LoRA~\cite{hu2021lora} finetuning. 
Besides, the residual design above ensures the model can fall back to the original textual LLM by learning to ignore the cross-attention outputs in non-speech steps.</p>
<p>The proposed model can be trained using next-token-prediction as in training other GPT models~\cite{brown2020gpt}. 
During inference, LLM can take text prompts and generate output tokens in a step-by-step fashion.
The only difference is that each predicted token is fed back to both the input of LLM and the cross-attention module above. % so that it also attends to the speech features like the input text prompt.</p>
</blockquote>
<h3 id="33from-offline-to-streaming">3.3.From Offline to Streaming<a class="headerlink" href="#33from-offline-to-streaming" title="Permanent link">&para;</a></h3>
<blockquote>
<p>With the above cross-attention speech feature extractor, the speech context length required to make prediction at each decoder step is independent with LLM backbone and completely decided by the cross-attention module with a read-write policy in Figure~\ref{fig:framework}. 
This characteristic enables the streaming design in simply two steps.</p>
<p>The first step is to design a read-write policy for handling the streaming speech input. 
Our framework converts streamable SpeechLLM to a similar read-write problem as simultaneous translation where previous research in fixed and adaptive policies can be reused.
We will take the most popular and longstanding fixed policy, {\em wait-k}~\cite{ma2020simulmt}, as an example in the following while integrating more adaptive policies~\cite{ma2023efficient} will be interesting future topics. 
We first decide a fixed pre-decision ratio of $L$ which represents a step size of $(L<em>P</em>10\tt{ms})$ where $P$ is the downsampling ratio of the speech encoder.
After taking text context prompt without cross-attention, LLM starts to predict the first subword unit by cross-attending to the first $(K*L)$ speech embedding steps.
After that, LLM predict one next subword for every $L$ incoming speech embeddings in a streaming fashion.</p>
<p>The second step is to make the speech encoder work in streaming mode.
This can be done by two approaches: i) keep the bidirectional encoder in the inference time, recompute all the available encoder states after getting each new speech block, and provide that to the above read-write policy (denoted as {\em BESTOW-S-bidi}) ii) retrain the model with a unidirectional encoder (denoted as {\em BESTOW-S-unid}). 
For the former, we introduce a fixed right context of 13 frames in the inference time to compensate the training/inference mismatch.
For the latter, we adapt the cache aware streaming model\cite{noroozi2023stateful} to update the FastConformer pre-encoder layers.
We utilize causal convolutions with the following left and right context windows in frames: [[70,13],[70,6],[70,1],[70,0]], which can be chosen from in the inference time.</p>
<p>We initialize the streamable SpeechLLM ({\em BESTOW-S}) from the offline SpeechLLM ({\em BESTOW}) and continue training on the same data.
To improve the generalization, we train BESTOW-S with a random $K$ range so that in inference, any $K$ in the range can be used to allow latency-quality tradeoff.</p>
</blockquote>
<h3 id="34bestow-vs">3.4.BESTOW v.s.<a class="headerlink" href="#34bestow-vs" title="Permanent link">&para;</a></h3>
<blockquote>
<p>GPT-Style &amp; T5-Style</p>
<p>Besides the streamable capability above, compared with GPT-style SpeechLLMs (or Speech-LLaMA) ~\cite{chen2024salm,tang2023salmonn,gong2023ltu-as}, the proposed architecture is also computationally more efficient.
Let $L_t$ and $L_a$ denotes the lengths of text tokens and speech features respectively, then computational complexity of GPT-style models is to the order of $(L_t+L_a)^2 = L_t^2+L_a^2 + 2L_aL_t$, due to the quadratic complexity for self-attention mechanism.
Meanwhile, with our cross-attention between text and speech prompts, we are able to reduce the complexity to $L_tL_a + L_t^2$.
Given that the length of text tokens is usually much shorter than that of speech features (in which case $L_a \gg L_t$), in theory we can enjoy a speed up of $L_a$ times, which means the longer the speech the greater the speedup. 
This speedup is crucial especially considering very wide and deep LMs where the computation from speech encoder and cross attention can be omitted from LLM forward and backward computation. 
To compensate for that, all previous research has to introduce significant downsampling on the speech features to reduce the feature length, which potentially incurs information loss.</p>
<p>Although the T5-style models like Flamingo~\cite{alayrac2022flamingo,kong2024audio} and Whisper-LLaMA~\cite{radhakrishnan2023whispering} also add cross-attention on pretrained GPT-based LLMs, the cross-attention is added to each layer of the LLM, which introduces a large number of parameters.
In contrast, we show with only two layers of cross-attention before feeding into LLMs, we can achieve state-of-the-art performance in ASR and AST. 
Moreover, the modality-specific design in the proposed method is only introduced to the input of LLMs, which allows isolating the LLM parameter updates to parameter-efficient-finetune methods like LoRA~\cite{hu2021lora} as demonstrated in the experiment.  </p>
</blockquote>
<h2 id="4experiments">4.Experiments: 实验<a class="headerlink" href="#4experiments" title="Permanent link">&para;</a></h2>
<h2 id="5conclusions">5.Conclusions: 结论<a class="headerlink" href="#5conclusions" title="Permanent link">&para;</a></h2>
<blockquote>
<p>In this work, we propose the first open SpeechLLM solution that enables {\em Streaming} and {\em Multitask at scale} (beyond ASR) at the same time. %Moreover, the solution is end-to-end optimizable and allows LLM knowledge transfer to speech.
The solution is based on a different backbone architecture from the popular Speech-LLaMA variants that is based on cross-attention and read-write policy.
The novel backbone unifies the offline and streaming modes and achieves state-of-the-art on several large-scale and multitask speech-to-text benchmarks, with {\em lower training/inference cost}.
We will release the code and checkpoints to promote next-generation SpeechLLM using this backbone design. </p>
</blockquote>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
    
  </body>
</html>