
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.0">
    
    
      
        <title>How Should We Extract Discrete Audio Tokens from Self-Supervised Models? 我们应该如何从自监督模型中提取离散音频标识符? - Sapphire Lab</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.9f615399.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#how-should-we-extract-discrete-audio-tokens-from-self-supervised-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Sapphire Lab" class="md-header__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sapphire Lab
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              How Should We Extract Discrete Audio Tokens from Self-Supervised Models? <br> 我们应该如何从自监督模型中提取离散音频标识符?
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Sapphire Lab" class="md-nav__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sapphire Lab
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TTS
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../../PDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PDE
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract: 摘要
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1introduction" class="md-nav__link">
    1.Introduction: 引言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2related-works" class="md-nav__link">
    2.Related Works: 相关工作
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3methodology" class="md-nav__link">
    3.Methodology: 方法
  </a>
  
    <nav class="md-nav" aria-label="3.Methodology: 方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31tokenizer" class="md-nav__link">
    3.1.Tokenizer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32informed-layer-selector" class="md-nav__link">
    3.2.Informed Layer Selector
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33acoustic-model" class="md-nav__link">
    3.3.Acoustic Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34scalable-vocoder" class="md-nav__link">
    3.4.Scalable Vocoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4experiments" class="md-nav__link">
    4.Experiments: 实验
  </a>
  
    <nav class="md-nav" aria-label="4.Experiments: 实验">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41discriminative-tasks" class="md-nav__link">
    4.1.Discriminative Tasks
  </a>
  
    <nav class="md-nav" aria-label="4.1.Discriminative Tasks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#automatic-speech-recognition-asr" class="md-nav__link">
    Automatic Speech Recognition (ASR)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speaker-identification-sid" class="md-nav__link">
    Speaker Identification (SID)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#emotion-recognition-er" class="md-nav__link">
    Emotion Recognition (ER)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42generative-tasks" class="md-nav__link">
    4.2.Generative Tasks
  </a>
  
    <nav class="md-nav" aria-label="4.2.Generative Tasks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#speech-enhancement-se" class="md-nav__link">
    Speech Enhancement (SE)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-to-speech-tts" class="md-nav__link">
    Text-to-Speech (TTS)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5results" class="md-nav__link">
    5.Results: 结果
  </a>
  
    <nav class="md-nav" aria-label="5.Results: 结果">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51scalable-vocoder" class="md-nav__link">
    5.1.Scalable Vocoder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52layer-analysis" class="md-nav__link">
    5.2.Layer Analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53effect-of-number-of-clusters" class="md-nav__link">
    5.3.Effect of Number of Clusters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54effect-of-embedding-initialization" class="md-nav__link">
    5.4.Effect of Embedding Initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#55out-of-distribution-generation" class="md-nav__link">
    5.5.Out-of-Distribution Generation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6conclusions" class="md-nav__link">
    6.Conclusions: 结论
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="how-should-we-extract-discrete-audio-tokens-from-self-supervised-models">How Should We Extract Discrete Audio Tokens from Self-Supervised Models? <br> 我们应该如何从自监督模型中提取离散音频标识符?<a class="headerlink" href="#how-should-we-extract-discrete-audio-tokens-from-self-supervised-models" title="Permanent link">&para;</a></h1>
<details>
<summary>基本信息</summary>

- 标题: How Should We Extract Discrete Audio Tokens from Self-Supervised Models?
- 作者:
  - 01 [Pooneh Mousavi](../../Authors/Pooneh_Mousavi.md)
  - 02 [Jarod Duret](../../Authors/Jarod_Duret.md)
  - 03 [Salah Zaiem](../../Authors/Salah_Zaiem.md)
  - 04 [Luca Della Libera](../../Authors/Luca_Della_Libera.md)
  - 05 [Artem Ploujnikov](../../Authors/Artem_Ploujnikov.md)
  - 06 [Cem Subakan](../../Authors/Cem_Subakan.md)
  - 07 [Mirco Ravanelli](../../Authors/Mirco_Ravanelli.md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.06.15 ArXiv v1
  - 更新笔记: 2024.07.04
- 发表:
  - [InterSpeech 2024](../../Publications/InterSpeech.md)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.10735)
  - [DOI]()
  - [Github](https://github.com/speechbrain/benchmarks/tree/DASB)
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - [SSL](../../Tags/Learning_Self-Supervised.md)
  - [开源](../../Tags/OpenSource.md)
- 页数: 5
- 引用: 45
- 被引: ?
- 数据:
  - [LJSpeech](../../Datasets/LJSpeech.md)
- 对比:
  - ?
- 复现:
  - ?

</details>

<h2 id="abstract">Abstract: 摘要<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. 
Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details.
Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). 
Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear.</p>
<p>This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks.
We propose a scalable solution to train a universal vocoder across multiple SSL layers.
Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.</p>
</blockquote>
<h2 id="1introduction">1.Introduction: 引言<a class="headerlink" href="#1introduction" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Learning effective, efficient, and robust representations is a core problem in modern audio and speech processing systems.
Over the past few years, continuous representations learned by large self-supervised models such as <a href="../Speech_Representaion/2020.06.20_Wav2Vec2.0.md">Wav2Vec2</a>, <a href="../Speech_Representaion/2021.10.26_WavLM.md">WavLM</a>, and <a href="../../Speech_Representaion/2021.06.14_HuBERT/">HuBERT</a> have achieved unprecedented performance.
A recent research trend consists of learning discrete audio representations instead of continuous ones, resulting in what is known as audio tokens.
These discrete tokens offer several potential advantages.
Firstly, they facilitate the development of audio language models (LMs) (<a href="../Speech_LLM/2022.09.07_AudioLM.md">AudioLM</a>; <a href="../Speech_LLM/2023.06.22_AudioPaLM.md">AudioPaLM</a>; <a href="../Speech_LLM/2023.10.07_LauraGPT.md">LauraGPT</a>; <a href="../Speech_LLM/2023.05.25_VioLA.md">VioLA</a>; <a href="../../Speech_LLM/2023.01.05_VALL-E/">VALL-E</a>; <a href="../Speech_LLM/2023.08.14_SpeechX.md">SpeechX</a>) and the creation of multi-modal large language models (<a href="../LLM/Gemini.md">Gemini</a>), which can emit audio, text, and visual tokens.
Additionally, their compression potential can contribute to efficient data transmission and storage.
Discrete tokens also enable us to address audio generation tasks such as speech enhancement and synthesis using classification methods, instead of relying on complex high-dimensional regression models.</p>
<p>Following the terminology from (<a href="../Speech_LLM/2022.09.07_AudioLM.md">AudioLM</a>; <a href="../Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md">SpeechTokenizer</a>), audio tokenization techniques can be broadly categorized into Compression-based (codecs) tokens and Semantic tokens.
Compression-based tokens (<a href="../../Speech_Neural_Codec/2021.07.07_SoundStream/">SoundStream</a>; defossez2022high, kumar2024high, <a href="../Speech_Neural_Codec/2023.05.04_HiFi-Codec.md">HiFi-Codec</a>) utilize encoder-decoder architectures coupled with <a href="../../Speech_Neural_Codec/2021.07.07_SoundStream/">Residual Vector Quantization (RVQ)</a>.
They are explicitly trained to accurately reconstruct the original audio, making them particularly suitable for audio generation tasks.
Semantic tokens (<a href="../_Full/Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md">Speech Resynthesis</a>; <a href="../_Full/Phonetic_Analysis_of_Self-Supervised_Representations_of_English_Speech.md">Phonetic Analysis</a>; <a href="../Speech_Representaion/2021.08.07_W2V-BERT.md">W2V-BERT</a>), on the other hand, are generated through clustering or quantization of the layers of Self-Supervised Learning (SSL) models (<a href="../Speech_Representaion/2020.06.20_Wav2Vec2.0.md">Wav2Vec2</a>; <a href="../Speech_Representaion/2021.10.26_WavLM.md">WavLM</a>; <a href="../../Speech_Representaion/2021.06.14_HuBERT/">HuBERT</a>).
Often, this involves selecting a layer from the pretrained SSL model and clustering its representations, typically with the k-means algorithm.
Semantic tokens primarily capture coarse information such as phonetic, semantics, and syntactic details.
Since they are not explicitly trained to achieve accurate waveform reconstruction, it is more natural to use them in discriminative tasks like Automatic Speech Recognition (ASR).
Recent research, however, has shown that semantic tokens can be effective for generative tasks as well (<a href="../_tmp/SELM.md">SELM</a>; <a href="../_Full/Towards_Universal_Speech_Discrete_Tokens__A_Case_Study_for_ASR_&amp;_TTS.md">Towards Universal Speech Discrete Tokens</a>).
Additionally, semantic tokens have been used in a hybrid tokenizer (<a href="../Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md">SpeechTokenizer</a>; <a href="../Speech_Neural_Codec/2023.09.14_FunCodec.md">FunCodec</a>).
This hybrid approach combines semantic and compression-based tokens, separating content information in the initial layer while preserving paralinguistic details in subsequent layers.
A similar strategy has been widely adopted in audio LLMs (<a href="../Speech_LLM/2022.09.07_AudioLM.md">AudioLM</a>; <a href="../Speech_LLM/2023.06.22_AudioPaLM.md">AudioPaLM</a>; <a href="../Speech_LLM/2023.10.07_LauraGPT.md">LauraGPT</a>).
Nevertheless, the most effective setting for extracting semantic tokens remains largely unclear.
Recent studies have primarily focused on ASR and Speech Translation (<a href="../_Full/Exploring_Speech_Recognition_Translation_&amp;_Understanding_with_Discrete_Speech_Units__A_Comparative_Study.md">Chang2023Exploring</a>; <a href="../_tmp/DUB.md">DUB</a>; <a href="../_Full/Exploration_of_Efficient_E2E_ASR_Using_Discretized_Input_from_SSL.md">Chang2023Exploration</a>), without considering a broader range of discriminative and generative tasks.</p>
<p>This paper addresses this gap by evaluating the effects of different heuristics required to derive semantic tokens for several discriminative and generative tasks, such as speech recognition, speaker recognition, emotion classification, speech enhancement, and text-to-speech.
We investigate various crucial aspects, including the impact of the number of clusters and the selection of the intermediate layer of the SSL model to discretize.
The latter factor turned out to be crucial and task-dependent, as early layers capture low-level information and higher layers encode content and semantic nuances.
Common strategies include using the middle layer (<a href="../_tmp/SELM.md">SELM</a>; <a href="../_Full/Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md">Speech Resynthesis</a>) or leveraging the last layer (<a href="../_Full/Exploration_of_Efficient_E2E_ASR_Using_Discretized_Input_from_SSL.md">Chang2023Exploration</a>).
Instead of relying on partial information only, we introduced a novel technique based on an informed layer selection mechanism.
We propose to cluster all layers and inject their information into the acoustic models using learnable attention weights.
This approach significantly boosts performance while also providing valuable insights into the importance of each layer.</p>
<p>Since there is no built-in decoder in semantic tokens, a vocoder model for converting the semantic tokens into audio must be trained (<a href="../../TTS3_Vocoder/2020.10.12_HiFi-GAN/">HiFi-GAN</a>, <a href="../TTS3_Vocoder/2018.10.31_WaveGlow.md">WaveGlow</a>).
Training such a vocoder is computationally demanding, making it highly impractical to train a separate vocoder for each layer or combination of layers.
To address this challenge, we propose a novel scalable vocoder capable of operating with various layer combinations at no additional cost.
This is achieved through a layer dropout training scheme, inspired by the bitrate scalability mechanism used in <a href="../../Speech_Neural_Codec/2021.07.07_SoundStream/">SoundStream</a>.
Interestingly, our results show that the scalable vocoder outperforms all vocoders trained on every specific layer.
Finally, for a comprehensive comparison, we provide experimental evidence using both in-domain and out-of-domain datasets for training k-means.
For reproducibility and to encourage further research, we release the code, built on the popular <a href="../_tmp/SpeechBrain.md">SpeechBrain</a> toolkit, and pretrained models publicly at https://github.com/speechbrain/benchmarks/tree/DASB.</p>
</blockquote>
<h2 id="2related-works">2.Related Works: 相关工作<a class="headerlink" href="#2related-works" title="Permanent link">&para;</a></h2>
<p>None</p>
<h2 id="3methodology">3.Methodology: 方法<a class="headerlink" href="#3methodology" title="Permanent link">&para;</a></h2>
<blockquote>
<p>The proposed architecture, illustrated in Figure.01, consists of four components:
- Tokenizer
- Informed Layer Selector
- Acoustic Model
- Scalable Vocoder</p>
<p>The following subsections will describe each module.</p>
</blockquote>
<p><img alt="" src="../Images/2024.06.15.Fig.01.png" /></p>
<blockquote>
<p>Figure.01: The proposed method for audio token extraction from SSL models: 
(A) k-means discretizes the continuous representations of each layer, 
(B) an attention mechanism merges the discrete layer representations, 
(C) the mixed representations train acoustic models for discriminative and generative tasks, 
(D) our scalable vocoder generates waveforms (if needed).</p>
</blockquote>
<h3 id="31tokenizer">3.1.Tokenizer<a class="headerlink" href="#31tokenizer" title="Permanent link">&para;</a></h3>
<blockquote>
<p>For quantization, we cluster five layers taken from two pretrained SSL models using the k-means algorithm independently for each layer.
We consider two widely-used models: WavLM-large (<a href="huggingface.co/microsoft/wavlm-large">Checkpoints</a>) and HuBERT-large (<a href="huggingface.co/facebook/hubert-large-ll60k">Checkpoints</a>), both having 24 layers.
We choose two layers from the lower part (3, 7) to capture fine-grained information, the middle layer (12), and two layers from the higher part (18, 23) for encoding content and meaning.
This selection is based on observation from prior research (<a href="../Speech_Representaion/2021.10.26_WavLM.md">WavLM</a>; <a href="../../Evaluations/SUPERB.md">SUPERB</a>) which studied the contribution patterns of different layers across various tasks.
As a result, this set of discrete hierarchical tokens captures rich information from the original audio signal.
Each of the K clusters is assigned a unique index.
Additionally, we store the continuous coordinates of each centroid for studying the effect of initializing input embeddings in downstream acoustic models.
The outcome of this tokenization process is a tensor $\mathbf{d}$ of shape $B \times T \times n_l$, where B represents the batch size, T is the sequence length, and $n_l$ is the number of discretized layers.</p>
</blockquote>
<h3 id="32informed-layer-selector">3.2.Informed Layer Selector<a class="headerlink" href="#32informed-layer-selector" title="Permanent link">&para;</a></h3>
<blockquote>
<p>As evident from the SSL literature (<a href="../_Full/Speech_Self-Supervised_Representations_Benchmarking__Are_We_Doing_it_Right?.md">Zaiem2023Speech</a>; <a href="../../Evaluations/SUPERB.md">SUPERB</a>; <a href="Fine-Tuning_Strategies_for_Faster_Inference_Using_Speech_Self-Supervised_Models__A_Comparative_Study.md">Zaiem2023FineTuning</a>), the choice of the layer within the SSL model significantly influences the performance of the downstream task of interest.
This decision is equally critical for semantic tokens.
Unlike prior methods that rely on heuristic layer selection (<a href="../_tmp/SELM.md">SELM</a>; <a href="../_Full/Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md">Speech Resynthesis</a>; <a href="../_Full/Exploration_of_Efficient_E2E_ASR_Using_Discretized_Input_from_SSL.md">Chang2023Exploration</a>), we integrate the information from our hierarchical multi-layer audio tokens with an attention mechanism.
The attention mechanisms comprise a straightforward multi-layer perceptron (MLP) fed by the embeddings of the audio tokens from each layer.
The MLP generates a score for each selected layer, that is normalized by a softmax function as shown in the following equations:</p>
</blockquote>
<p>$$
    z_{l,t} = f\big(emb(d_{l,t})\big) \
$$</p>
<p>$$
    a_{l,t} = \frac{\exp(z_{l,t})}{\sum_{k=1}^{nl} \exp(z_{k,t})}, \quad h_t = \sum_l a_{l, t} z_{l, t},
$$</p>
<blockquote>
<p>where, $z_{l,t}$ represents the score assigned to layer $l$ at time $t$ by the MLP function $f$.
The variable $emb$ refers to the lookup table that assigns embeddings to discrete tokens in $d_l$.
The variable $a_{l,t}$ denotes the attention assigned to layer $l$ at time $t$, and lastly $h_t$ is the representation that is fed to the downstream MLP model.
Note that we learn different layer combinations at each time-step, making this mechanism particularly effective.</p>
<p>This simple yet effective approach offers several advantages.
Firstly, it enhances flexibility by reducing reliance on heuristic layer selection.
The model can now dynamically capture information from different layers for each task.
Additionally, as shown in <a href="#5results-结果">Results</a>, this mechanism yields performance improvements when compared with models utilizing information from a single SSL layer.
Lastly, the informed layer selections enhance interpretability, enabling us to analyze the learned weights and understand the relative importance of each layer for each downstream task.</p>
</blockquote>
<h3 id="33acoustic-model">3.3.Acoustic Model<a class="headerlink" href="#33acoustic-model" title="Permanent link">&para;</a></h3>
<blockquote>
<p>The mixed representations are fed to a neural model trained to address various downstream tasks.
We train the attention mechanism, embeddings, and the acoustic models jointly.
While previous studies (<a href="../_Full/Exploring_Speech_Recognition_Translation_&amp;_Understanding_with_Discrete_Speech_Units__A_Comparative_Study.md">Chang2023Exploring</a>, <a href="../_tmp/DUB.md">DUB</a>, <a href="../_Full/Exploration_of_Efficient_E2E_ASR_Using_Discretized_Input_from_SSL.md">Chang2023Exploration</a>) have primarily focused on a few discriminative tasks, we aim to provide evidence across a diverse range of speech applications, considering both discriminative and generative tasks.
We consider ASR, speaker identification, and emotion recognition as discriminative tasks.
For generative tasks, we focus on text-to-speech and speech enhancement.
The details for each task are reported in <a href="#4experiments-实验">Experiments</a>.</p>
</blockquote>
<h3 id="34scalable-vocoder">3.4.Scalable Vocoder<a class="headerlink" href="#34scalable-vocoder" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Although SSL models such as Wav2vec2, HuBERT, and WavLM are not designed for accurate waveform reconstruction, we can potentially adapt them for generative tasks by training a vocoder on top of their representations.
The dominant approach involves training a separate vocoder for each possible layer combination.
However, this approach is impractical and computationally demanding since each downstream task may require a different set of layers.
In this work, we propose a universal and scalable vocoder capable of accommodating various layer combinations.
To train such a model, we modify <a href="../Speech_Neural_Codec/2023.05.04_HiFi-Codec.md">HiFi-GAN</a> to accept a variable number of multi-layer discrete tokens as input.
We introduce a layer dropout mechanism, similar to structured <a href="../_Basis/Dropout.md">dropout</a>.
For each input example, we randomly sample $k$ layers from the range $[1, n_l]$, as shown in the following equations:</p>
</blockquote>
<p>$$
    \mathbf{d}_S \sim \text{Sample}(\mathbf{d}, k) , \quad \mathbf{o} = V(\mathbf{d}_S),\
$$</p>
<blockquote>
<p>where `Sample($\cdot$)' randomly selects ( k ) layers from the discrete representations ( \mathbf{d} ), and ( V ) represents the vocoder function that outputs the waveform ( \mathbf{o} ).
Layers are combined with an attention mechanism that assigns weights to different layers and ensures that the dimensionality of the embeddings remains consistent regardless of the number of layers. 
The model is trained to decode audio by considering all possible combinations of layers.
During inference, the desired combination of layers can be selected.
In addition to its flexibility, this vocoder has demonstrated superior performance compared to vocoders trained on single layers, as we will show in <a href="#5results-结果">Results</a>.</p>
</blockquote>
<h2 id="4experiments">4.Experiments: 实验<a class="headerlink" href="#4experiments" title="Permanent link">&para;</a></h2>
<blockquote>
<p>The tasks in our experiments are divided into two groups: Discriminative tasks involving transcription and classification, and generative tasks producing audio.
For the downstream architecture choices and training procedures, we follow the best-performing approaches for classic continuous self-supervised representations (<a href="../_Full/Speech_Self-Supervised_Representations_Benchmarking__Are_We_Doing_it_Right?.md">Zaiem2023Speech</a>).
We employ 1000 centroids across all tasks, except for ASR and emotion recognition, where we adopt 2000 centroids based on insights from prior research on ASR with discrete representations (<a href="../_Full/Exploration_of_Efficient_E2E_ASR_Using_Discretized_Input_from_SSL.md">Chang2023Exploration</a>).
The effect of this selection is probed in <a href="#53effect-of-number-of-clusters">Effect of Number of Clusters</a>.</p>
</blockquote>
<h3 id="41discriminative-tasks">4.1.Discriminative Tasks<a class="headerlink" href="#41discriminative-tasks" title="Permanent link">&para;</a></h3>
<h4 id="automatic-speech-recognition-asr">Automatic Speech Recognition (ASR)<a class="headerlink" href="#automatic-speech-recognition-asr" title="Permanent link">&para;</a></h4>
<blockquote>
<p>We consider two CTC-based speech recognition tasks.
The first one is English ASR using Librispeech train-clean-100 for training and test-clean, test-other for testing.
The second one uses French data coming from the <a href="../../Datasets/CommonVoice.md">CommonVoice (CV)</a> 16.1 Corpus.
We select $100$ hours for training, keeping the original validation and test sets.
We use two layers of BiLSTM as a downstream head.
The evaluation metric is the Word Error Rate (WER).</p>
</blockquote>
<h4 id="speaker-identification-sid">Speaker Identification (SID)<a class="headerlink" href="#speaker-identification-sid" title="Permanent link">&para;</a></h4>
<blockquote>
<p>We employ an <a href="../_tmp/ECAPA-TDNN.md">ECAPA-TDNN</a> model to determine the speaker identity of each utterance.
The widely used <a href="../../Datasets/VoxCeleb.md">VoxCeleb</a> is adopted, and the evaluation metric is accuracy (ACC).</p>
</blockquote>
<h4 id="emotion-recognition-er">Emotion Recognition (ER)<a class="headerlink" href="#emotion-recognition-er" title="Permanent link">&para;</a></h4>
<blockquote>
<p>We use ECAPA-TDNN for emotion recognition on the <a href="../../Datasets/IEMOCAP.md">IEMOCAP</a> dataset.
The task consists of predicting one of the four considered classes: happy, sad, angry, and neutral.
The evaluation metric is accuracy (ACC).</p>
</blockquote>
<h3 id="42generative-tasks">4.2.Generative Tasks<a class="headerlink" href="#42generative-tasks" title="Permanent link">&para;</a></h3>
<h4 id="speech-enhancement-se">Speech Enhancement (SE)<a class="headerlink" href="#speech-enhancement-se" title="Permanent link">&para;</a></h4>
<blockquote>
<p>We utilize a non-autoregressive <a href="../_Transformer/2017.06.12_Transformer.md">Transformer</a> encoder, which consists of 6 layers, 4 attention heads, a model dimension of 256, and a feed-forward layer dimension of 2048.
Input tokens are extracted from the noisy signal, and target tokens from the clean one.
Training is conducted end-to-end using cross-entropy loss. 
Noisy samples are generated by mixing clean samples from <a href="../../Datasets/LJSpeech.md">LJSpeech</a> with noise from <a href="../_tmp/WHAM!.md">WHAM!</a>.
The signal-to-noise ratios (SNRs) are uniformly distributed between 0 and 5 dB.
Due to the misalignment of the vocoder's output with the target at the sample level, metrics like Si-SNR can be degraded.
Therefore, we use the <a href="../../Evaluations/DNSMOS.md">deep noise suppression mean opinion score (DNSMOS)</a> for the speech quality metric, following a previous study (<a href="../_tmp/SELM.md">SELM</a>).
Intelligibility is evaluated through the <a href="../../Evaluations/dWER.md">differential word error rate (dWER)</a>, which measures the WER between the transcribed enhanced signal and the transcribed target signal.
Transcriptions are obtained using the small version of <a href="../Speech_LLM/Whisper.md">Whisper</a>.</p>
</blockquote>
<h4 id="text-to-speech-tts">Text-to-Speech (TTS)<a class="headerlink" href="#text-to-speech-tts" title="Permanent link">&para;</a></h4>
<blockquote>
<p>We train an end-to-end autoregressive <a href="../_Transformer/2017.06.12_Transformer.md">Transformer</a> with 6 layers in the encoder, 12 layers in the decoder, 4 attention heads, a model dimension of 512, and a feed-forward layer in 2048.
To facilitate convergence, we employ <a href="../_tmp/Guided_Attention.md">guided attention</a>.
The model takes text embeddings as its input and generates the audio tokens for each considered layer.
We utilize a shared transformer decoder, where each tokenizer head has its own learned embedding, and there is a distinct final linear layer for each token.
We train all models on the <a href="../../Datasets/LJSpeech.md">LJSpeech</a> dataset.
For assessing speech quality, we use <a href="../../Evaluations/UTMOS.md">UTMOS</a> to estimate human quality ratings.
To evaluate fidelity to the text, we assess generated samples using the WER computed with the small version of <a href="../Speech_LLM/Whisper.md">Whisper</a>.</p>
</blockquote>
<h2 id="5results">5.Results: 结果<a class="headerlink" href="#5results" title="Permanent link">&para;</a></h2>
<h3 id="51scalable-vocoder">5.1.Scalable Vocoder<a class="headerlink" href="#51scalable-vocoder" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Our results cover findings from two distinct setups: 1) a scalable vocoder trained across five layers, and 2) a vocoder trained on a single layer.
In both setups, the tokenizers and the vocoders are trained with LJSpeech (in-domain condition).
In the first scenario, models are trained with HuBERT discrete tokens and WavLM discrete tokens, each with the number of clusters set to 1000.
To further explore the influence of k-means cluster size on speech quality, we introduce an additional model with the number of clusters set to 2000.
In the second setup, we focus on models trained specifically on a single layer (3 or 23) using HuBERT discrete tokens and in-domain tokenizer.
This experiment aims to compare the performance of the scalable vocoder against the vocoder trained on a single layer.</p>
<p>The results are summarized in Figure.02, WavLM combined with an in-domain tokenizer achieves higher UTMOS and lower dWER scores across all setups.
About the impact of the number of clusters, our experiment shows that setting $k$ to 2000 degrades the quality of synthesized speech.
Finally, both models trained on a single layer are outperformed on both evaluation metrics by the one trained on five layers, confirming the benefits of the scalable approach.
Lastly, we explore an out-of-domain scenario where the tokenizers are trained on LibriSpeech and the vocoders are on LJSpeech.
As shown in Table.02 (last column), we do not observe any significant performance degradation when using the scalable vocoder in an out-of-domain condition.</p>
</blockquote>
<h3 id="52layer-analysis">5.2.Layer Analysis<a class="headerlink" href="#52layer-analysis" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Figure.03 depicts the average weights assigned to different layers in the WavLM model across various downstream tasks on the test dataset. In both TTS and the scalable vocoder, lower levels get greater importance as they prioritize effective reconstruction.
Conversely, for ASR, the upper layers become more crucial in capturing the semantic aspects of spoken utterances.
In the case of ER and SID, the third layer receives the highest weight.
Our findings align with the observed pattern in continuous representations (<a href="../_Full/2023.08.28_Speech_Self-Supervised_Representations_Benchmarking__A_Case_for_Larger_Probing_Heads.md">"Speech Self-Supervised Representations Benchmarking"</a>).
For SE, all layers are equally weighted, indicating the necessity of all hierarchical levels to achieve optimal audio quality while preserving the semantic content of the input. </p>
</blockquote>
<h3 id="53effect-of-number-of-clusters">5.3.Effect of Number of Clusters<a class="headerlink" href="#53effect-of-number-of-clusters" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We train k-means models with both 1000 and 2000 centroids and examine the impact of the number of clusters across different tasks, as illustrated in Table.01. 
In Generative tasks, TTS and SE, no significant differences are observed between models trained with 1000 and 2000 clusters.
However, for ASR in both English and French, as well as ER, models with a higher number of clusters outperform those with fewer clusters.
In the case of SID, the model trained with 1000 clusters exhibits comparable accuracy to the model with 2000 centroids.
As expected, the ideal number of clusters is task-dependent.
For multi-modal LLMs where a single set of tokens is desired to solve multiple tasks, we recommend a cluster count between 1000 and 2000.</p>
</blockquote>
<h3 id="54effect-of-embedding-initialization">5.4.Effect of Embedding Initialization<a class="headerlink" href="#54effect-of-embedding-initialization" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We study various configurations for initializing the embedding layers of audio tokens (Table.01).
Three options are considered:
- Random initialization of the embedding layers,
- Initialization of the embedding layer with the corresponding centroid's embedding, while freezing the layer, and 
- Initialization of the embedding layer with the corresponding centroid's embedding, without freezing the layer.</p>
<p>Across all tasks, there is no advantage observed in initializing the embedding with pretrained centroid embeddings, and random initialization consistently outperforms it in all scenarios.
However, discriminative tasks show greater benefits from random initialization, while generative tasks exhibit comparable performance across all three settings.
This observation eliminates the need for having the same embedding size as the SSL models, allowing the choice of a smaller and more efficient embeddings. %In all tasks, except for ER, achieving better or comparable performance is possible when fine-tuning the pretrained embeddings rather than freezing them. </p>
</blockquote>
<h3 id="55out-of-distribution-generation">5.5.Out-of-Distribution Generation<a class="headerlink" href="#55out-of-distribution-generation" title="Permanent link">&para;</a></h3>
<blockquote>
<p>To evaluate the robustness of discrete representations under distribution shifts, we train tokenizers on both in-domain and out-of-domain datasets (Table.02).
In discriminative tasks, k-means models are trained using the same dataset employed for training acoustic models.
In the out-of-domain scenarios, k-means models are trained on train-clean-100, train-clean-360, and train-other-500.
For generative tasks, k-means models are trained on LJSpeech for in-domain evaluation and LibriSpeech-960h for OOD evaluation, while both the acoustic model and vocoder are trained on LJSpeech.
For all discriminative tasks, the in-domain tokenizer outperforms its OOD counterpart.
Interestingly, in all generative tasks, training the model using the OOD tokenizer does not adversely affect performance and, in some instances, even improves the results.
We speculate that this trend may arise because generative tasks primarily depend on tokens capturing low-level information, which tends to be more ``universal" and transferable across different domains.</p>
</blockquote>
<h2 id="6conclusions">6.Conclusions: 结论<a class="headerlink" href="#6conclusions" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Discrete semantic tokens, derived from the quantization of SSL models, play an important role, providing "pseudo-text" valuable for training text-free speech language models and multimodal LLMs.
We explore the optimal configuration of semantic tokens across discriminative and generative tasks.
We introduce a novel technique involving an informed layer selection mechanism, utilizing learnable attention weights to integrate information from different SSL layers.
This approach significantly enhances the performance and interpretability of the model.
Furthermore, we propose a scalable solution for training a universal vocoder across multiple SSL layers, demonstrating its superiority over vocoders trained on specific layers.
As future work, we plan to explore more diverse tasks and quantization methods, and the development of a multi-speaker vocoder.</p>
</blockquote>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
    
  </body>
</html>