
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.0">
    
    
      
        <title>StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models - Sapphire Lab</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.9f615399.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#styletts-2-towards-human-level-text-to-speech-through-style-diffusion-and-adversarial-training-with-large-speech-language-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Sapphire Lab" class="md-header__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sapphire Lab
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Sapphire Lab" class="md-nav__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sapphire Lab
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TTS
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../../PDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PDE
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract·摘要
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1introduction" class="md-nav__link">
    1.Introduction·引言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2related-work" class="md-nav__link">
    2.Related Work·相关工作
  </a>
  
    <nav class="md-nav" aria-label="2.Related Work·相关工作">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diffusion-models-for-speech-synthesis" class="md-nav__link">
    Diffusion Models for Speech Synthesis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-to-speech-with-large-speech-language-models" class="md-nav__link">
    Text-to-Speech with Large Speech Language Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#human-level-text-to-speech" class="md-nav__link">
    Human-Level Text-to-Speech
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3method" class="md-nav__link">
    3.Method·方法
  </a>
  
    <nav class="md-nav" aria-label="3.Method·方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31styletts-overview" class="md-nav__link">
    3.1.StyleTTS Overview
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32styletts2" class="md-nav__link">
    3.2.StyleTTS2
  </a>
  
    <nav class="md-nav" aria-label="3.2.StyleTTS2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#321end-to-end-training" class="md-nav__link">
    3.2.1.End-to-End Training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#322style-diffusion" class="md-nav__link">
    3.2.2.Style Diffusion
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#323slm-discriminators" class="md-nav__link">
    3.2.3.SLM Discriminators
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#324differentiable-duration-modeling" class="md-nav__link">
    3.2.4.Differentiable Duration Modeling
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4experiments" class="md-nav__link">
    4.Experiments
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5conclusions" class="md-nav__link">
    5.Conclusions
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="styletts-2-towards-human-level-text-to-speech-through-style-diffusion-and-adversarial-training-with-large-speech-language-models">StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models<a class="headerlink" href="#styletts-2-towards-human-level-text-to-speech-through-style-diffusion-and-adversarial-training-with-large-speech-language-models" title="Permanent link">&para;</a></h1>
<h2 id="abstract">Abstract·摘要<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h2>
<blockquote>
<p>In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis.
StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models.
Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness.
StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers.
Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation.
This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs.
The audio demos and source code are available at https://styletts2.github.io/. </p>
</blockquote>
<h2 id="1introduction">1.Introduction·引言<a class="headerlink" href="#1introduction" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Text-to-speech (TTS) synthesis has seen significant advancements in recent years, with numerous applications such as virtual assistants, audiobooks, and voice-over narration benefiting from increasingly natural and expressive synthetic speech [1,2].
Some previous works have made significant progress towards human-level performance [3,4,5].
However, the quest for robust and accessible human-level TTS synthesis remains an ongoing challenge because there is still room for improvement in terms of diverse and expressive speech [5,6], robustness for out-of-distribution (OOD) texts [7], and the requirements of massive datasets for high-performing zero-shot TTS systems [8].</p>
<p>In this paper, we introduce StyleTTS 2, an innovative TTS model that builds upon the style-based generative model StyleTTS [6] to present the next step towards human-level TTS systems.
We model speech styles as a latent random variable and sample them with a probabilistic diffusion model, allowing the model to efficiently synthesize highly realistic speech without the need for reference audio.
Since it only needs to sample a style vector instead of the entire speech as a latent variable, StyleTTS 2 is faster than other diffusion TTS models while still benefiting from the diverse speech synthesis enabled by diffusion models.
One of the key contributions of StyleTTS 2 is the use of large pre-trained speech language models (SLMs) like Wav2Vec 2.0 [9], HuBERT [10], and WavLM [11] as discriminators, in conjunction with a novel differentiable duration modeling approach.
This end-to-end (E2E) training setup leverages SLM representations to enhance the naturalness of the synthesized speech, transferring knowledge from large SLMs for speech generation tasks. </p>
<p>Our evaluations suggest that speech generated by StyleTTS 2 surpasses human recordings as judged by native English speakers on the benchmark LJSpeech [12] dataset with statistically significant comparative mean opinion scores (CMOS) of+0.28 (p &lt; 0.05).
Additionally, StyleTTS 2 advances the state-of-the-art by achieving a CMOS of+1.07 (p ≪ 0.01)compared to NaturalSpeech [5].
Furthermore, it attains human-level performance on the multispeaker VCTK dataset [13] in terms of naturalness (CMOS =−0.02,p ≫ 0.05) and similarity (CMOS =+0.30,p &lt; 0.1) to the reference speaker.
When trained on a large number of speakers like the LibriTTS dataset [14], StyleTTS 2 demonstrates potential for speaker adaptation.
It surpasses previous publicly available models in this task and outperforms Vall-E [8] in naturalness.
Moreover, it achieves slightly worse similarity to the target speaker with only a 3-second reference speech, despite using around 250 times less data compared to Vall-E, making it a data-efficient alternative for large pre-training in the zero-shot speaker adaptation task.
As the first model to achieve human-level performance on publicly available single and multispeaker datasets, StyleTTS 2 sets a new benchmark for TTS synthesis, highlighting the potential of style diffusion and adversarial training with SLMs for human-level TTS synthesis. </p>
</blockquote>
<h2 id="2related-work">2.Related Work·相关工作<a class="headerlink" href="#2related-work" title="Permanent link">&para;</a></h2>
<h3 id="diffusion-models-for-speech-synthesis">Diffusion Models for Speech Synthesis<a class="headerlink" href="#diffusion-models-for-speech-synthesis" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Diffusion models have gained traction in speech synthesis due to their potential for diverse speech sampling and fine-grained speech control [15].
They have been applied to mel-based text-to-speech [16,17,18,19,20], mel-to-waveform vocoder [21,22, 23,24,25,26], and end-to-end speech generation [27,28,29].
However, their efficiency is limited compared to non-iterative methods, like GAN-based models [30,31,32], due to the need to iteratively sample mel-spectrograms, waveforms, or other latent representations proportional to the target speech duration [15].
Furthermore, recent works suggest that state-of-the-art GAN-based models still perform better than diffusion models in speech synthesis [26,33].
To address these limitations, we introduce style diffusion, where a fixed-length style vector is sampled by a diffusion model conditioned on the input text.
This approach significantly improves model speed and enables end-to-end training.
Notably, StyleTTS 2 synthesizes speech using GAN-based models, with only the style vector dictating the diversity of speech sampled.
This unique combination allows StyleTTS 2 to achieve high-quality synthesis with fast inference speed while maintaining the benefits of diverse speech generation, further advancing the capabilities of diffusion models in speech synthesis. </p>
</blockquote>
<h3 id="text-to-speech-with-large-speech-language-models">Text-to-Speech with Large Speech Language Models<a class="headerlink" href="#text-to-speech-with-large-speech-language-models" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Recent advancements have proven the effectiveness of large-scale self-supervised speech language models (SLMs) in enhancing text-to-speech (TTS) quality [34,35,36,37] and speaker adaptation [8,38,29,39].
These works typically convert text input into either continuous or quantized representations derived from pre-trained SLMs for speech reconstruction.
However, SLM features are not directly optimized for speech synthesis, while tuning SLMs as a neural codec [34,35,8,29] involves two-stage training.
In contrast, our model benefits from the knowledge of large SLMs via adversarial training using SLM features without latent space mapping, thus directly learning a latent space optimized for speech synthesis like other end-to-end (E2E) TTS models.
This innovative approach signifies a new direction in TTS with SLMs. </p>
</blockquote>
<h3 id="human-level-text-to-speech">Human-Level Text-to-Speech<a class="headerlink" href="#human-level-text-to-speech" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Several recent works have advanced towards human-level TTS [3,4,5] using techniques like BERT pre-training [4,40,7] and E2E training [32,5] with differentiable duration modeling [41,42].
VITS [3] demonstrates MOS comparable to human recordings on the LJSpeech and VCTK datasets, while PnG-BERT [4] obtains human-level results on a proprietary dataset.
NaturalSpeech [5], in particular, achieves both MOS and CMOS on LJSpeech statistically indistinguishable from human recordings.
However, we find that there is still room for improvement in speech quality beyond these state-of-the-art models, as we attain higher performance and set a new standard for human-level TTS synthesis.
Furthermore, recent work shows the necessity for disclosing the details of evaluation procedures for TTS research [43].
Our evaluation procedures are detailed in Appendix E, which can be used for reproducible future research toward human-level TTS. </p>
</blockquote>
<h2 id="3method">3.Method·方法<a class="headerlink" href="#3method" title="Permanent link">&para;</a></h2>
<h3 id="31styletts-overview">3.1.StyleTTS Overview<a class="headerlink" href="#31styletts-overview" title="Permanent link">&para;</a></h3>
<blockquote>
<p>StyleTTS [6] is a non-autoregressive TTS framework using a style encoder to derive a style vector from reference audio, enabling natural and expressive speech generation.
The style vector is incorporated into the decoder and duration and prosody predictors using adaptive instance normalization (AdaIN) [44], allowing the model to generate speech with varying duration, prosody, and emotions.
The model comprises eight modules, organized into three categories: (1) a speech generation system (acoustic modules) with a text encoder, style encoder, and speech decoder; (2) a TTS prediction system with duration and prosody predictors; and (3) a utility system for training, including a discriminator, text aligner, and pitch extractor.
It undergoes a two-stage training process: the first stage trains the acoustic modules for mel-spectrogram reconstruction, and the second trains TTS prediction modules using the fixed acoustic modules trained in the first stage.</p>
<p>In the first stage, the text encoder $T$ encodes input phonemes $t$ into phoneme representations $h_{text}= T(t)$, while the text aligner $A$ extracts speech-phoneme alignment $a_{algn}= A(x, t)$ from input speech $x$ and phonemes $t$ to produce aligned phoneme representations $h_{algn}= h_{text}\cdot a_{algn}$ via dot product.
Concurrently, the style encoder $E$ obtains the style vector $s = E(x)$, and the pitch extractor $F$ extracts the pitch curve $p_x= F(x)$ along with its energy $n_x= |x|$.
Lastly, the speech decoderG reconstructs $\hat{x}=G(h_{algn}, s, p_x, n_x)$, which is trained to match input $x$ using a $L_1$ reconstruction loss $L_{mel}$ and adversarial objectives $L_{adv}$, $L_{fm}$ with a discriminator $D$.
Transferable monotonic aligner (TMA) objectives are also applied to learn optimal alignments (see Appendix G for details).</p>
<p>In the second stage, all components except the discriminatorDare fixed, with only the duration and prosody predictors being trained.
The duration predictor $S$ predicts the phoneme duration with $d_{pred}= S(h_{text}, s)$, whereas the prosody predictor $P$ predicts pitch and energy as $\hat{p}<em>{x}, \hat{n}</em>{x}= P(h_{text}, s)$.
The predicted duration is trained to match the ground truth duration $d_{gt}$ derived from the summed monotonic version of the alignment $a_{algn}$ along the time axis with an $L_1$ loss $L_{dur}$.
The predicted pitch $\hat{p}<em>x$ and energy $\hat{n}_x$ are trained to match the ground truth pitch $p_x$ and energy $n_x$ derived from pitch extractor $F$ with $L_1$ loss $L</em>{f0}$ and $L_n$.
During inference, $d_{pred}$ is used to upsample $h_{text}$ through $a_{pred}$, the predicted alignment, obtained by repeating the value 1 for $d_{pred[i]}$ times at $l_{i-1}$, where $l_i$ is the end position of the $i$-th phoneme $t_i$ calculated by summing $d_{pred}[k]$ for $k \in {1, \cdots, i}$, and $d_{pred}[i]$ are the predicted duration of $t_i$.
The mel-spectrogram is synthesized by $x_{pred}= G(h_{text}\cdot a_{pred}, E(\tilde{x}), \hat{p}<em>{\tilde{x}}, \hat{n}</em>{\tilde{x}}$) with $\tilde{x}$ an arbitrary reference audio that influences the style of $x_{pred}$, which is then converted into a waveform using a pre-trained vocoder.</p>
<p>Despite its state-of-the-art performance in synthesizing diverse and controllable speech, StyleTTS has several drawbacks, such as a two-stage training process with an additional vocoding stage that degrades sample quality, limited expressiveness due to deterministic generation, and reliance on reference speech hindering real-time applications. </p>
</blockquote>
<h3 id="32styletts2">3.2.StyleTTS2<a class="headerlink" href="#32styletts2" title="Permanent link">&para;</a></h3>
<blockquote>
<p>StyleTTS 2 improves upon the StyleTTS framework, resulting in a more expressive text-to-speech (TTS) synthesis model with human-level quality and improved out-of-distribution performance.
We introduce an end-to-end (E2E) training process that jointly optimizes all components, along with direct waveform synthesis and adversarial training with large speech language models (SLMs) enabled by our innovative differentiable duration modeling.
The speech style is modeled as a latent variable sampled through diffusion models, allowing diverse speech generation without reference audio.
We outline these important changes in the following sections with an overview in Figure 1. </p>
</blockquote>
<h4 id="321end-to-end-training">3.2.1.End-to-End Training<a class="headerlink" href="#321end-to-end-training" title="Permanent link">&para;</a></h4>
<blockquote>
<p>E2E training optimizes all TTS system components for inference without relying on any fixed components like pre-trained vocoders that convert mel-spectrograms into waveforms [3,32].
To achieve this, we modify the decoderGto directly generate the waveform from the style vector, aligned phoneme representations, and pitch and energy curves.
We remove the last projection layer for mel-spectrograms of the decoder and append a waveform decoder after it.
We propose two types of decoders: HiFiGAN-based and iSTFTNet-based.
The first is based on HiFi-GAN [30], which directly generates the waveform.
In contrast, the iSTFTNet-based decoder [45] produces magnitude and phase, which are converted into waveforms using inverse short-time Fourier transform for faster training and inference.
We employ the snake activation function [46], proven effective for waveform generation in [31].
An AdaIN module [44] is added after each activation function to model the style dependence of the speech, similar to the original StyleTTS decoder.
We replace the mel-discriminator in [6] with the multi-period discriminator (MPD) [30] and multi-resolution discriminator (MRD) [47] along with the LSGAN loss functions [48] for decoder training, and incorporate the truncated point-wise relativistic loss function [49] to enhance sound quality (see Appendix F and G for details).</p>
<p>We found that well-trained acoustic modules, especially the style encoder, can accelerate the training process for TTS prediction modules.
Therefore, before jointly optimizing all components, we first pre-train the acoustic modules along with the pitch extractor and text aligner via $L_{mel}$, $L_{adv}$, $L_{fm}$ and TMA objectives for $N$ epochs where $N$ depends on the size of the training set, in the same way as the first training stage of [6].
However, we note that this pre-training is not an absolute necessity: despite being slower, starting joint training directly from scratch also leads to model convergence.</p>
<p>After acoustic module pre-training, we jointly optimize $L_{mel}$, $L_{adv}$, $L_{fm}$, $L_{dur}$, $L_{f0}$ and $L_n$, where $L_{mel}$ is modified to match the mel-spectrograms of waveforms reconstructed from predicted pitch $\hat{p}<em>x$ and energy $\hat{n}_x$ (Fig 1a).
During joint training, stability issues emerge from diverging gradients, as the style encoder must encode both acoustic and prosodic information.
To address this inconsistency, we introduce a prosodic style encoder $E_p$ alongside the original acoustic style encoder $E_a$, previously denoted as $E$ in section 3.1.
Instead of using $s_a= E_a(x)$, predictors $S$ and $P$ take $s_p= E_p(x)$ as the input style vector.
The style diffusion model generates the augmented style vector $s = [sp, sa]$.
This modification effectively improves sample quality (see section 5.3).
To further decouple the acoustic modules and predictors, we replace the phoneme representations $h</em>{text}$ from $T$, now referred to as acoustic text encoder, with $h_{bert}$ from another text encoder $B$ based on BERT transformers, denoted as prosodic text encoder.
Specifically, we employ a phoneme-level BERT [7] pre-trained on extensive corpora of Wikipedia articles as the prosodic text encoder.
This approach has been shown to enhance the naturalness of StyleTTS in the second stage [7], similar to our proposed usage here. </p>
<p>With differentiable upsampling and fast style diffusion, we can generate speech samples during training in a fully differentiable manner, just as during inference.
These samples are used to optimize $L_{slm}$ (eq. 5) during joint training to update the parameters of all components for inference (Fig 1b). </p>
</blockquote>
<h4 id="322style-diffusion">3.2.2.Style Diffusion<a class="headerlink" href="#322style-diffusion" title="Permanent link">&para;</a></h4>
<blockquote>
<p>In StyleTTS 2, we model the speech $x$ as a conditional distribution $p(x|t) =\int p(x|t, s)p(s|t) ds$ through a latent variable $s$ that follows the distribution $p(s|t)$.
We call this variable the generalized speech style, representing any characteristic in speech beyond phonetic content $t$, including but not limited to prosody, lexical stress, formant transitions, and speaking rate [6].
We sample $s$ by EDM [50] that follows the combined probability flow [51] and time-varying Langevin dynamics [52]: </p>
<p>Unlike [50] that uses 2nd-order Heun, we solve eq. 4 with the ancestral DPM-2 solver [54] for fast and diverse sampling as we demand speed more than accuracy.
On the other hand, we use the same scheduler as in [50] withσmin= 0.0001, σmax= 3andρ = 9.
This combination allows us to sample a style vector for high-quality speech synthesis with only three steps, equivalent to running a 9-layer transformer model, minimally impacting the inference speed (see Appendix B for more discussions).
Vconditions ontthroughhbertconcatenated with the noisy inputE(x) + σξ, andσis conditioned via sinusoidal positional embeddings [53].
In the multispeaker setting, we modelp(s|t, c)by K(s; t, c, σ)with an additional speaker embeddingc = E(xref)wherexrefis the reference audio of the target speaker.
The speaker embedding c is injected into V by adaptive layer normalization [6]. </p>
</blockquote>
<h4 id="323slm-discriminators">3.2.3.SLM Discriminators<a class="headerlink" href="#323slm-discriminators" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Speech language models (SLMs) encode valuable information ranging from acoustic to semantic aspects [55], and SLM representations are shown to mimic human perception for evaluating synthesized speech quality [45].
We uniquely transfer knowledge from SLM encoders to generative tasks via adversarial training by employing a 12-layer WavLM [11] pre-trained on 94k hours of data1as the discriminator.
As the number of parameters of WavLM is greater than StyleTTS 2, to avoid discriminator overpowering, we fix the pre-trained WavLM modelWand append a convolutional neural network (CNN)Cas the discriminative head.
We denote the SLM discriminatorDSLM= C ◦ W.
The input audios are downsampled to 16 kHz before being fed intoDSLMto match that of WavLM.
Cpools featureshSLM= W(x)from all layers with a linear map from13 × 768to 256 channels.
We train the generator components (T, B, G, S, P, V , denoted as G) and DSLMto optimize: </p>
</blockquote>
<h4 id="324differentiable-duration-modeling">3.2.4.Differentiable Duration Modeling<a class="headerlink" href="#324differentiable-duration-modeling" title="Permanent link">&para;</a></h4>
<blockquote>
<p>The duration predictor produces phoneme durationsdpred, but the upsampling method described in section 3.1 to obtainapredis not differentiable, blocking gradient flow for E2E training.
NaturalSpeech [5] employs an attention-based upsampler [42] for human-level TTS.
However, we find this approach unstable during adversarial training because we train our model using differentiable upsampling with only the adversarial objective described in eq. 5 and without extra loss terms due to the length mismatch caused by deviations ofdpredfromdgt.
Although this mismatch can be mitigated with soft dynamic time warping as used in [42,5], we find this approach both computationally expensive and unstable with mel-reconstruction and adversarial objectives.
To achieve human-level performance with adversarial training, a non-parametric upsampling method is preferred for stable training. </p>
</blockquote>
<h2 id="4experiments">4.Experiments<a class="headerlink" href="#4experiments" title="Permanent link">&para;</a></h2>
<h2 id="5conclusions">5.Conclusions<a class="headerlink" href="#5conclusions" title="Permanent link">&para;</a></h2>
<blockquote>
<p>In this study, we present StyleTTS 2, a novel text-to-speech (TTS) model with human-level performance via style diffusion and speech language model discriminators.
In particular, it exceeds the ground truth on LJSpeech and performs on par with it on the VCTK dataset.
StyleTTS 2 also shows potential for zero-shot speaker adaption, with remarkable performance even on limited training data compared to large-scale models like Vall-E.
With our innovative style diffusion method, StyleTTS 2 generates expressive and diverse speech of superior quality while ensuring fast inference time.
While StyleTTS 2 excels in several areas, our results indicate room for improvement in handling large-scale datasets such as LibriTTS, which contain thousands of or more speakers, acoustic environments, accents, and other various aspects of speaking styles.
The speaker similarity in the aforementioned zero-shot adaptation speaker task could also benefit from further improvements.
However, zero-shot speaker adaptation has the potential for misuse and deception by mimicking the voices of individuals as a potential source of misinformation or disinformation.
This could lead to harmful, deceptive interactions such as theft, fraud, harassment, or impersonations of public figures that may influence political processes or trust in institutions.
In order to manage the potential for harm, we will require users of our model to adhere to a code of conduct that will be clearly displayed as conditions for using the publicly available code and models.
In particular, we will require users to inform those listening to samples synthesized by StyleTTS 2 that they are listening to synthesized speech or to obtain informed consent regarding the use of samples synthesized by StyleTTS 2 in experiments.
Users will also be required to use reference speakers who have given consent to have their voice adapted, either directly or by license.
Finally, we will make the source code publicly available for further research in speaker fraud and impersonation detection.
In addition, while human evaluators have favored StyleTTS 2 over ground truth with statistical significance on the LJSpeech dataset, this preference may be context-dependent.
Original audio segments from larger contexts like audiobooks could inherently differ in naturalness when isolated, potentially skewing the evaluations in favor of synthesized speech.
Additionally, the inherent variability in human speech, which is context-independent, might lead to lower ratings when compared to the more uniform output from StyleTTS 2.
Future research should aim to improve evaluation methods to address these limitations and develop more natural and human-like speech synthesis models with longer context dependencies. </p>
</blockquote>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
    
  </body>
</html>