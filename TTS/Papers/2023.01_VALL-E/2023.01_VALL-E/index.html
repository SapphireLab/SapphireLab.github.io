
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.0">
    
    
      
        <title>VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers - Sapphire Lab</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.9f615399.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#vall-e-neural-codec-language-models-are-zero-shot-text-to-speech-synthesizers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Sapphire Lab" class="md-header__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sapphire Lab
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Sapphire Lab" class="md-nav__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sapphire Lab
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TTS
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../../PDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PDE
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1introduction" class="md-nav__link">
    1.Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2related-work" class="md-nav__link">
    2.Related Work
  </a>
  
    <nav class="md-nav" aria-label="2.Related Work">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21zero-shot-tts" class="md-nav__link">
    2.1.Zero-Shot TTS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22spoken-generative-pre-trained-models" class="md-nav__link">
    2.2.Spoken Generative Pre-Trained Models
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3background-speech-quantization" class="md-nav__link">
    3.Background: Speech Quantization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4vall-e" class="md-nav__link">
    4.VALL-E
  </a>
  
    <nav class="md-nav" aria-label="4.VALL-E">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41problem-formulation-regarding-tts-as-conditional-codec-language-modeling" class="md-nav__link">
    4.1.Problem Formulation: Regarding TTS as Conditional Codec Language Modeling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42training-conditional-codec-language-modeling" class="md-nav__link">
    4.2.Training: Conditional Codec Language Modeling
  </a>
  
    <nav class="md-nav" aria-label="4.2.Training: Conditional Codec Language Modeling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#421autoregressive-codec-language-modeling" class="md-nav__link">
    4.2.1.Autoregressive Codec Language Modeling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#422non-autoregressive-codec-language-modeling" class="md-nav__link">
    4.2.2.Non-Autoregressive Codec Language Modeling
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43inference-in-context-learning-via-prompting" class="md-nav__link">
    4.3.Inference: In-Context Learning via Prompting
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5experiment" class="md-nav__link">
    5.Experiment
  </a>
  
    <nav class="md-nav" aria-label="5.Experiment">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51experiment-setup" class="md-nav__link">
    5.1.Experiment Setup
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52librispeech-evaluation" class="md-nav__link">
    5.2.LibriSpeech Evaluation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53vctk-evaluation" class="md-nav__link">
    5.3.VCTK Evaluation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54qualitative-analysis" class="md-nav__link">
    5.4.Qualitative Analysis
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6conclusion-limitations-future-work" class="md-nav__link">
    6.Conclusion, Limitations, Future Work
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="vall-e-neural-codec-language-models-are-zero-shot-text-to-speech-synthesizers">VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers<a class="headerlink" href="#vall-e-neural-codec-language-models-are-zero-shot-text-to-speech-synthesizers" title="Permanent link">&para;</a></h1>
<p><a href="../../Authors/%E7%8E%8B%E7%A8%8B%E4%B8%80_%28Chengyi_Wang%29/">王程一</a> <a href="../../Authors/%E9%99%88%E4%B8%89%E5%85%83_%28Sanyuan_Chen%29/">陈三元</a>, <a href="../../Authors/%E5%90%B4%E4%BF%A3_%28Yu_Wu%29/">吴俣</a>, Ziqiang Zhang, <a href="../../Authors/%E5%91%A8%E9%BE%99_%28Long_Zhou%29/">周龙</a>, <a href="../../Authors/%E5%88%98%E6%A0%91%E6%9D%B0_%28Shujie_Liu%29/">刘树杰</a>, <a href="../../Authors/Zhuo_Chen/">Zhuo Chen</a>, <a href="../../Authors/Yanqing_Liu/">Yanqing Liu</a>, <a href="../../Authors/Huaming_Wang/">Huaming Wang</a>, <a href="../../Authors/Jinyu_Li/">Jinyu Li</a>, <a href="../../Authors/Lei_He/">Lei He</a>, <a href="../../Authors/Sheng_Zhao/">Sheng Zhao</a>, <a href="../../Authors/%E9%9F%A6%E7%A6%8F%E5%A6%82_%28Furu_Wei%29/">韦福如</a></p>
<h2 id="abstract">Abstract<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h2>
<blockquote>
<p>We introduce a language modeling approach for <u><em>text-to-speech synthesis (TTS)</em></u>.
Specifically, we train a neural codec language model (called <u><strong>VALL-E</strong></u>) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work.
During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems.
<u><strong>VALL-E</strong></u> emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt.
Experiment results show that <u><strong>VALL-E</strong></u> significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity.
In addition, we find that <u><strong>VALL-E</strong></u> could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis.
See https://aka.ms/valle for demos of our work.</p>
</blockquote>
<p>我们介绍了一种用于语音合成的语言建模方法.
具体来说, 我们使用从现成的神经音频编解码器模型中导出的离散编码训练了一个神经编解码器语言模型 (称为 VALL-E), 并将 TTS 视为一个条件语言建模任务, 而不是像以前的工作那样视为连续信号回归任务.
在预训练阶段, 我们将 TTS 训练数据扩展到 60K 小时的英语语音, 这比现有系统大数百倍.
VALL-E 展现出上下文学习能力, 并可用于使用仅 3 秒的未见说话者的输入录音作为声学提示来合成高质量的个性化语音.
实验结果表明 VALL-E 在语音自然度和说话者相似度方面显著优于最先进的零次语言合成系统.
此外, 我们发现 VALL-E 可以在合成中保留声学提示中的说话者的情感和声学环境
请访问 https://aka.ms/valle 查看本项工作的示例.</p>
<h2 id="1introduction">1.Introduction<a class="headerlink" href="#1introduction" title="Permanent link">&para;</a></h2>
<blockquote>
<p>The last decade has yielded dramatic breakthroughs in speech synthesis through the development of neural networks and end-to-end modeling.
Currently, cascaded <u><em>text-to-speech (TTS)</em></u> systems (<a href="2017.12_Tacotron2.md">Tacotron2 (2017)</a>, <a href="2019.05_FastSpeech.md">FastSpeech (2019)</a>, <a href="2018.09_Transformer_TTS.md">Transformer TTS (2018)</a>) usually leverage a pipeline with an acoustic model and a vocoder using mel spectrograms as the intermediate representations.
While advanced TTS systems can synthesize high-quality speech from single or multiple speakers (<a href="2022.07_DelightfulTTS2.md">DelightfulTTS2 (2022)</a>, <a href="2021.06_VITS.md">VITS (2021)</a>), it still requires high-quality clean data from the recording studio.
Large-scale data crawled from the Internet cannot meet the requirement, and always lead to performance degradation.
Because the training data is relatively small, current TTS systems still suffer from poor generalization.
Speaker similarity and speech naturalness decline dramatically for unseen speakers in the zero-shot scenario.</p>
<p>To tackle the zero-shot TTS problem, existing work leverages speaker adaptation [Chen et al., 2019, Wang et al., 2020] and speaker encoding [Arik et al., 2018, <a href="2021.12_YourTTS.md">YourTTS (2021)</a>] methods, requiring additional fine-tuning, complex pre-designed features, or heavy structure engineering.</p>
<p>Instead of designing a complex and specific network for this problem, the ultimate solution is to train a model with large and diverse data as much as possible, motivated by success in the field of text synthesis [Brown et al., 2020, Chowdhery et al., 2022].
Recent years have witnessed notable performance improvement for data increase in the text language model, from 16GB of uncompressed text [Devlin et al., 2019], to 160GB [Liu et al., 2019], to 570GB [Brown et al., 2020], and finally, around 1TB [Chowdhery et al., 2022].
Transferring this success to the field of speech synthesis, we introduce <u><strong>VALL-E</strong></u>, the first language model-based TTS framework leveraging large, diverse, and multi-speaker speech data.</p>
</blockquote>
<p><img alt="" src="../2023.01_VALL-E_FIG01.png" /></p>
<blockquote>
<p>As shown in <a href="">Fig.01</a>, to synthesize personalized speech (e.g., zero-shot TTS), <u><strong>VALL-E</strong></u> generates the corresponding acoustic tokens conditioned on the acoustic tokens of the 3-second enrolled recording and the phoneme prompt, which constrain the speaker and content information respectively.
Finally, the generated acoustic tokens are used to synthesize the final waveform with the corresponding neural codec decoder [Défossez et al., 2022].
The discrete acoustic tokens derived from an audio codec model enable us to treat TTS as conditional codec language modeling and advanced prompting-based large-model techniques (as in GPTs [Brown et al., 2020])can be leveraged for the TTS tasks.
The acoustic tokens also allow us to generate diverse synthesized results in TTS by using different sampling strategies during inference.</p>
<p>We train <u><strong>VALL-E</strong></u> with LibriLight [Kahn et al., 2020], a corpus consisting of 60K hours of English speech with over 7000 unique speakers.
The original data is audio-only, so we employ a speech recognition model to generate the transcriptions.
Compared to previous TTS training datasets, such as LibriTTS [Zen et al., 2019], our data contain more noisy speech and inaccurate transcriptions but provide diverse speakers and prosodies.
We believe the proposed approach is robust to the noise and generalize well by leveraging large data.
It is worth noting that existing TTS systems are always trained with dozens of hours of single-speaker data or hundreds of hours of multi-speaker data, which is over hundreds of times smaller than <u><strong>VALL-E</strong></u>.
<a href="">Tab.01</a> summarizes the innovation of <u><strong>VALL-E</strong></u>, a language model approach for TTS, using audio codec codes as intermediate representations, leveraging large and diverse data, leading to strong in-context learning capabilities.</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align: center;">Table 1</th>
<th style="text-align: center;">Current Systems</th>
<th style="text-align: center;">VALL-E</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Intermediate Representation</td>
<td style="text-align: center;">Mel Spectrogram</td>
<td style="text-align: center;">Audio Codec Code</td>
</tr>
<tr>
<td style="text-align: center;">Objective Function</td>
<td style="text-align: center;">Continuous Signal Regression</td>
<td style="text-align: center;">Language Model</td>
</tr>
<tr>
<td style="text-align: center;">Training Data</td>
<td style="text-align: center;">≤600 Hours</td>
<td style="text-align: center;">60K Hours</td>
</tr>
<tr>
<td style="text-align: center;">In-Context Language</td>
<td style="text-align: center;">×</td>
<td style="text-align: center;">√</td>
</tr>
</tbody>
</table>
<blockquote>
<p>We evaluate <u><strong>VALL-E</strong></u> on LibriSpeech [Panayotov et al., 2015] and VCTK [Veaux et al., 2016]datasets, where all test speakers are unseen in the training corpus.
<u><strong>VALL-E</strong></u> significantly outperforms the state-of-the-art zero-shot TTS system (<a href="2021.12_YourTTS.md">YourTTS (2021)</a>) in terms of speech naturalness and speaker similarity, with +0.12 comparative mean option score (CMOS) and +0.93 similarity mean option score (SMOS) improvement on LibriSpeech.
<u><strong>VALL-E</strong></u> also beats the baseline on VCTK with+0.11 SMOS and +0.23 CMOS improvements.
It even achieves a +0.04 CMOS score against ground truth, showing the synthesized speech of unseen speakers is as natural as human recordings on VCTK.
Moreover, the qualitative analysis shows that <u><strong>VALL-E</strong></u> is able to synthesize diverse outputs with the same text and target speaker, which could benefit pseudo-data creation for the speech recognition task.
We also find that <u><strong>VALL-E</strong></u> could keep the acoustic environment (e.g., reverberation) and emotion (e.g. anger) of the acoustic prompt.</p>
<p>In summary, we make the following contributions.
- We propose <u><strong>VALL-E</strong></u>, the first TTS framework with strong in-context learning capabilities as GPT-3, which treats TTS as a language model task with audio codec codes as an intermediate representation to replace the traditional mel spectrogram.
It has in-context learning capability and enables prompt-based approaches for zero-shot TTS, which does not require additional structure engineering, pre-designed acoustic features, and fine-tuning as in previous work.
- We build a generalized TTS system in the speaker dimension by leveraging a huge amount of semi-supervised data, suggesting that simple scaling up semi-supervised data has been underestimated for TTS.
- <u><strong>VALL-E</strong></u> is able to provide diverse outputs with the same input text and keep the acoustic environment and speaker’s emotion of the acoustic prompt.
- We verify that <u><strong>VALL-E</strong></u> synthesizes natural speech with high speaker similarity by prompt-ing in the zero-shot scenario.
Evaluation results show that <u><strong>VALL-E</strong></u> significantly outperforms the state-of-the-art zero-shot TTS system on LibriSpeech and VCTK.</p>
<p>We encourage the reader to listen to our samples on the demo page https://aka.ms/valle.</p>
</blockquote>
<h2 id="2related-work">2.Related Work<a class="headerlink" href="#2related-work" title="Permanent link">&para;</a></h2>
<h3 id="21zero-shot-tts">2.1.Zero-Shot TTS<a class="headerlink" href="#21zero-shot-tts" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Current TTS methods can be categorized into cascaded and end-to-end methods.
Cascaded TTS systems (<a href="2017.12_Tacotron2.md">Tacotron2 (2017)</a>, <a href="2019.05_FastSpeech.md">FastSpeech (2019)</a>, <a href="2018.09_Transformer_TTS.md">Transformer TTS (2018)</a>) usually leverage a pipeline with an acoustic model and a vocoder using mel spectrograms as the intermediate representations.
To tackle the drawbacks of the vocoder, end-to-end TTS models (<a href="2021.06_VITS.md">VITS (2021)</a>, <a href="2022.07_DelightfulTTS2.md">DelightfulTTS2 (2022)</a>) are proposed to jointly optimize the acoustic model and vocoder.
In real scenarios, it is highly desirable to customize a TTS system to an arbitrary voice with rare enrolled recordings.
Therefore, there is growing interest in the zero-shot multi-speaker TTS techniques, and most of work is done in the context of cascaded TTS systems.
As the pioneers, Arik et al.2018 proposes speaker adaptation and speaker encoding approaches.
In the line of speaker adaptation, the following work [Chen et al., 2019, Wang et al., 2020, Chen et al., 2021] tries to improve the adaptation efficiency with less target speaker data and speaker-specific parameters.
Huang et al.[2022] applies meta-learning on speaker adaptation, which only requires 5-shot to build a well-performed system.
In parallel, speaker encoding-based methods achieved great progress in recent years.
A speaker encoding based system contains a speaker encoder and a TTS component, where the speaker encoder could be pre-trained on the speaker verification task [Jia et al., 2018].
In Jia et al.[2018] and Arik et al.[2018], the experiments show that the model is able to generate high-quality outputs with 3 seconds enrolled recordings for in-domain speakers.
To improve the quality of unseen speakers, advanced speaker embedding models [Cai et al., 2018] can be employed, but it is still undesirable according to Tan et al.[2021].
Another way is to design advanced but complex speaker encoder [Wu et al., 2022].Diffusion model based TTS [Popov et al., 2021, Kim et al., 2022] is also extended to zero-shot TTS [Kang et al., 2022] and achieved good results.
Compared to previous work [<a href="2019.05_FastSpeech.md">FastSpeech (2019)</a>, Du et al.,2022], our work follows the line of cascaded TTS but first uses audio codec code as intermediate representations.
It is the first one that has strong in-context learning capabilities as GPT-3, which does not require fine-tuning, pre-designed features, or a complex speaker encoder.</p>
</blockquote>
<h3 id="22spoken-generative-pre-trained-models">2.2.Spoken Generative Pre-Trained Models<a class="headerlink" href="#22spoken-generative-pre-trained-models" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Self-supervised learning is widely investigated in the field of speech understanding [<a href="2020.06_Wav2Vec2.0.md">Wav2Vec2.0 (2020)</a>, <a href="2021.06_HuBERT.md">HuBERT (2021)</a>, Chen et al., 2022] and speech-to-speech generation [Lakhotia et al., 2021, <a href="2022.09_AudioLM.md">AudioLM (2022)</a>].
In the context of speech-to-speech generation, a hot topic is how to synthesize speech in a textless setting.
GSLM [Lakhotia et al.,2021] proposes to synthesize speech based on <a href="2021.06_HuBERT.md">HuBERT (2021)</a> codes, and Polyak et al.[2021] improves the performance by combining HuBERT codes with codes of VQVAE and a speaker encoder.
<a href="2022.09_AudioLM.md">AudioLM (2022)</a> follows a similar way but use audio codecs [Zeghidour et al.,2022] to synthesize speech, together with semantic codes.
It should be noted that AudioLM is able to synthesize speech based on audio codecs without training an additional vocoder such as <a href="2020.10_HiFi-GAN.md">HifiGAN (2020)</a>.
AudioLM is a speech-to-speech model, whereas <u><strong>VALL-E</strong></u> is a TTS model, so we can explicitly control the content in speech synthesis.
Another direction is to apply pre-training to the neural TTS.
Chung et al.[2018] pre-trains speech decoder in TTS through autoregressive mel-spectrogram prediction.
In Ao et al.[2022], the authors propose a unified-modal encoder-decoder framework SpeechT5, which can leverage unlabeled speech and text data to pre-train all components of TTS model.
Tjandra et al.[2019] quantizes unlabeled speech into discrete tokens by a VQVAE model [van den Oord et al., 2017], and train a model with the token-to-speech sequence.
They demonstrate that the pre-trained model only requires a small amount of real data for fine-tuning.
Bai et al.[2022] proposes mask and reconstruction on mel spectrogram and showing better performance on speech editing and synthesis.
Previous TTS pre-training work leverages less than 1K hours of data, whereas <u><strong>VALL-E</strong></u> is pre-trained with 60K hours of data.
Furthermore, <u><strong>VALL-E</strong></u> is the first to use audio codec codes as intermediate representations, and emerge in-context learning capability in zero-shot TTS.</p>
</blockquote>
<h2 id="3background-speech-quantization">3.Background: Speech Quantization<a class="headerlink" href="#3background-speech-quantization" title="Permanent link">&para;</a></h2>
<blockquote>
<p>Since audio is typically stored as a sequence of 16-bit integer values, a generative model is required to output $2^{16}=65,536$ probabilities per timestep to synthesize the raw audio.
In addition, the audio sample rate exceeding ten thousand leads to an extraordinarily long sequence length, making it more intractable for raw audio synthesis.
To this end, speech quantization is required to compress integer values and sequence length. 
$\mu$-law transformation can quantize each timestep to 256 values and reconstruct high-quality raw audio.
It is widely used in speech generative models, such as WaveNet [van den Oord et al., 2016], but the inference speed is still slow since the sequence length is not reduced.
Recently, vector quantization is widely applied in self-supervised speech models for feature extraction, such as vq-wav2vec [Baevski et al., 2020a] and <a href="2021.06_HuBERT.md">HuBERT (2021)</a>.
The following work [Lakhotia et al., 2021, Du et al., 2022] shows the codes from self-supervised models can also reconstruct content, and the inference speed is faster than WaveNet.
However, the speaker identity has been discarded and the reconstruction quality is low <a href="2022.09_AudioLM.md">AudioLM (2022)</a>.
<a href="2022.09_AudioLM.md">AudioLM (2022)</a> trains speech-to-speech language models on both k-means tokens from a self-supervised model and acoustic tokens from a neural codec model, leading to high-quality speech-to-speech generation.</p>
<p>In this paper, we follow <a href="2022.09_AudioLM.md">AudioLM (2022)</a> to leverage neural codec models to represent speech in discrete tokens.
To compress audio for network transmission, codec models are able to encode waveform into discrete acoustic codes and reconstruct high-quality waveform even if the speaker is unseen in training.
Compared to traditional audio codec approaches, the neural-based codec is significantly better at low bitrates, and we believe the quantized tokens contain sufficient information about the speaker and recording conditions.
Compared to other quantization methods,the audio codec shows the following advantages: 
1. It contains abundant speaker information and acoustic information, which could maintain speaker identity in reconstruction compared to <a href="2021.06_HuBERT.md">HuBERT (2021)</a> codes.
2. There is an off-the-shelf codec decoder to convert discrete tokens into a waveform, without the additional efforts on vocoder training like VQ-based methods that operated on spectrum [Du et al., 2022].
3. It could reduce the length of time steps for efficiency to address the problem in $\mu$-law transformation [van den Oord et al., 2016].</p>
<p>We adopt a pre-trained neural audio codec model, EnCodec [Défossez et al., 2022], as our tokenizer.
EnCodec is a convolutional encoder-decoder model, whose input and output are both 24 kHz audio across variable bitrates.
The encoder produces embeddings at 75 Hz for input waveforms at 24 kHz,which is a 320-fold reduction in the sampling rate.
Each embedding is modeled by a <u><em>residual vector quantization (RVQ)</em></u>, in which we choose eight hierarchy quantizers with 1024 entries each as shown in <a href="">Fig.02</a>.</p>
</blockquote>
<p><img alt="" src="../2023.01_VALL-E_FIG02.png" /></p>
<blockquote>
<p>This configuration corresponds to EnCodec at 6K bitrates for 24 kHz audio reconstruction.
In this setting, given a 10-second waveform, the discrete representation is a matrix with750 × 8entries, where 750 =24,000×10/320 is the downsampled time step and 8 is the number of quantizers.
It is fine to choose other bitrate settings.
A larger bitrate corresponds to more quantizers and better reconstruction quality.
For example, if we choose EnCodecc at 12K bitrates, there are 16 quantizers are needed and the 10-second waveform corresponds to a matrix with 750×16 entries.
With the discrete codes from all quantizers, the convolutional decoder of EnCodec generates real-valued embeddings and reconstructs the waveform at 24 kHz.</p>
</blockquote>
<h2 id="4vall-e">4.VALL-E<a class="headerlink" href="#4vall-e" title="Permanent link">&para;</a></h2>
<h3 id="41problem-formulation-regarding-tts-as-conditional-codec-language-modeling">4.1.Problem Formulation: Regarding TTS as Conditional Codec Language Modeling<a class="headerlink" href="#41problem-formulation-regarding-tts-as-conditional-codec-language-modeling" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Given a dataset $\mathcal{D}={\mathbf{x}<em>i, \mathbf{y}_i}$, where $\mathbf{y}$ is an audio sample and $\mathbf{x} = {x_0,x_1, \cdots x_L}$ is its corresponding phoneme transcription, we use a pre-trained neural codec model to encode each audio sample into discrete acoustic codes, denoted as $\text{Encodec}(\mathbf{y}) = C^{T\times 8}$, where $C$ represents the two-dimensional acoustic code matrix, and $T$ is the downsampled utterance length.
The row vector of each acoustic code matrix $c</em>{t,:}$ represents the eight codes for frametand the column vector of each acoustic code matrix $c_{:,j}$ represents the code sequence from the $j$-th codebook, where $j \in {1,\cdots 8}$.
After quantization, the neural codec decoder is able to reconstruct the waveform, denoted as $\text{Decodec}(C)\approx\hat{\mathbf{y}}$.</p>
<p>Zero-shot TTS requires the model to synthesize high-quality speech for unseen speakers.
In this work, we regard zero-shot TTS as a conditional codec language modeling task.
We train a neural language model to generate an acoustic code matrix $C$ conditioned on a phoneme sequence $\mathbf{x}$ and an acoustic prompt matri $\tilde{C}^{T'\times 8}$ with the optimization objective of $\max p(C|\mathbf{x},\tilde{C})$.
Here, $\tilde{C}$ is obtained by the same neural codec with an enrolled recording as the input.
We expect the neural language model learns to extract the content and speaker information from the phoneme sequence and the acoustic prompt, respectively.
During inference, given a phoneme sequence and a 3-second enrolled recording of the unseen speaker, the acoustic code matrix with corresponding content and speaker’s voice is firstly estimated by the trained language model.
Then the neural codec decoder synthesizes the high-quality speech.</p>
</blockquote>
<h3 id="42training-conditional-codec-language-modeling">4.2.Training: Conditional Codec Language Modeling<a class="headerlink" href="#42training-conditional-codec-language-modeling" title="Permanent link">&para;</a></h3>
<blockquote>
<p>The neural speech codec model allows us to operate on discrete audio representations.
Due to residual quantization in the neural codec model, the tokens have a hierarchical structure: tokens from previous quantizers recover acoustic properties like speaker identity, while the consecutive quantizers learn fine acoustic details.
Each quantizer is trained to model the residual from the previous quantizers.
Motivated by this, we design two conditional language models in a hierarchical manner.</p>
<p>For the discrete tokens from the first quantizer $c_{:,1}$, we train an <u><em>autoregressive (AR)</em></u> decoder-only language model.
It is conditioned on the phoneme sequencexand the acoustic prompt $\tilde{C}<em>{:,1}$, formulated as 
$$
  p(c</em>{:,1}|\mathbf{x}, \tilde{C}<em>{:,1}; \theta</em>{AR}) =\prod_{t=0}^T p(c_{t,1}|c_{&lt;t,1},\tilde{c}<em>{:,1}, \mathbf{x}; \theta</em>{AR}) \tag{1}
$$</p>
<p>Since <u><strong>VALL-E</strong></u> is a decoder-only LM, the concatenation of $\tilde{c}<em>{:,1}$ and $c</em>{:,1}$ is a whole sequence, and we do not distinguish them or insert a specific token in training.
Only $c_{:,1}$ is predicted while the prefix $\tilde{c}_{:,1}$ is given during inference.</p>
<p>For the discrete tokens from the second to the last quantizers, $c_{:,j}\in[2,8]$, we train a <u><em>non-autoregressive (NAR)</em></u> language model.
Since the tokens can not access each other in a NAR manner, to constrain the speaker identity, the acoustic prompt matrix $\tilde{C}$ is used as an acoustic prompt.
Thus, the model is conditioned on the phoneme sequencex, the acoustic prompt $\tilde{C}$ and the predicted acoustic tokens belong to the previous codebooks $C_{:,&lt;j}$:
$$
  p(C_{:,2:8}|\mathbf{x},\tilde{C};\theta_{NAR})=\prod_{j=2}^{8}p(c_{:,j}|C_{:,&lt;j},\mathbf{x},\tilde{C};\theta_{NAR}) \tag{2}
$$</p>
<p>The combination of the AR model and the NAR model provides a good trade-off between speech quality and inference speed.
On the one hand, the rate of the generated speech should be consistent with the enrolled recording, and it is hard to train a length predictor for different speakers since their speaking speed may be very diverse.
In this case, the AR model is a more natural choice with its flexibility for acoustic sequence length prediction.
On the other hand, for the consecutive stages, as the number of output slots follows the sequence length of the first stage, NAR can reduce the time complexity from $\mathcal{O}(T)$ to $\mathcal{O}(1)$.
Overall, the prediction of C can be modeled as:
$$
  p(C|\mathbf{x},\tilde{C};\theta)=p(c_{:,1}|\tilde{C}<em>{:,1}, \mathbf{X}; \theta</em>{AR}) \prod_{j=2}^{8}p(c_{:,j}|c_{:,&lt;j},\mathbf{x},\tilde{C};\theta_{NAR}) \tag{3}
$$</p>
</blockquote>
<h4 id="421autoregressive-codec-language-modeling">4.2.1.Autoregressive Codec Language Modeling<a class="headerlink" href="#421autoregressive-codec-language-modeling" title="Permanent link">&para;</a></h4>
<blockquote>
<p>The autoregressive language model generates the tokens from the first quantizer.
It comprises a phoneme embedding $W_x$, an acoustic embedding $W_a$, a transformer decoder, and a prediction layer.
In order to generate speech with specific content, we use the phoneme sequence as the phoneme prompt of the language model.
Thus, the model input is the concatenation of $\mathbf{x}$ and $\mathbf{c}<em>{:,1}$, and two special <code>&lt;EOS&gt;</code> tokens are appended after each of them.
We compute sinuous position embedding separately for prompt and input tokens.
For the causal transformer model, each tokenct,1can attend to $(\mathbf{x}, c</em>{\leq t,1})$ as illustrated in the left part of <a href="">Fig.03</a>.</p>
</blockquote>
<p><img alt="" src="../2023.01_VALL-E_FIG03.png" /></p>
<blockquote>
<p>The model is optimized to maximize the probability of the next token in the first codebook.
We share the parameters of the output projection layer with the parameters of the acoustic embedding $W_a$.</p>
<p>In the AR model, we do not explicitly extract an audio clip as the prompt in training.
The training process is pure casual language model training.
In this way, any prefix sequence $c_{&lt;t,1}$ is treated as a prompt for the latter part of the sequence $c_{\geq t,1}$.
During inference, given an enrolled recording, we should concatenate the phoneme sequence of the enrolled recording and the phoneme sequence for synthesis together.
Meanwhile, the acoustic token sequence of the enrolled recording is used as the prefix in AR decoding, as formulated in <a href="">Eq.01</a>.
We will study the superiority of this setting in the experiment.</p>
</blockquote>
<h4 id="422non-autoregressive-codec-language-modeling">4.2.2.Non-Autoregressive Codec Language Modeling<a class="headerlink" href="#422non-autoregressive-codec-language-modeling" title="Permanent link">&para;</a></h4>
<blockquote>
<p>When we obtain the first quantizer codes by the AR model, we employ a non-autoregressive (NAR) model to generate codes of the other seven quantizers.
The NAR model has a similar architecture to the AR model, except that it contains eight separate acoustic embedding layers.
In each training step, we randomly sample a training stage $i\in [2, 8]$.
The model is trained to maximize the acoustic tokens from the $i$-th quantizer codebook.
The acoustic tokens from stage $1$ to stage $i−1$ are embedded and summed up as model input:
$$
\begin{align}e_{c_{t,j}}&amp;=W_a^j\odot c_{t,j}\tag{4}\\mathbf{e_{c_t}}&amp;=\sum_{j=1}^{i-1}e_{c_t,j}\tag{5}\end{align}
$$</p>
<p>where $\odot$ indicates index selection.</p>
<p>The phoneme sequence is also regarded as the prompt of the language model.
Besides, to clone the unique voice of the given speaker, we also use the acoustic tokens from the enrolled speech as the acoustic prompt.
Specifically, we first tokenize the enrolled speech with the neural codec model as $\tilde{C}^{T\times 8}$.
The embedded representations from all of the eight codebooks are summed up as the acoustic prompt $e_{\tilde{c}<em>t}=\sum</em>{j=1}^8 e_{\tilde{c}<em>{t,j}}$.
To predict the acoustic tokens from thei-th codebook, the transformer input is the concatenation of $(\mathbf{e}</em>{\mathbf{x}}, \mathbf{e}<em>{\tilde{c}}, \mathbf{e}</em>{c_{:,&lt;i}})$.
The positional embeddings are also computed separately for prompts and the acoustic sequence.
The current stage $i$ is injected into the network with Adaptive Layer Normalization [Xu et al., 2019] operator, i.e., $\text{AdaLN}(h, i) = a_i\text{LayerNorm}(h) + b_i$, where $h$ is the intermediate activations, $a_i$ and $b_i$ are obtained from a linear projection of the stage embedding.
Unlike AR, the NAR model allows each token to attend to all the input tokens in the self-attention layer.
We also share the parameters of the acoustic embedding layer and the output prediction layer, which means the weights of thej-th prediction layer are the same as the $(j + 1)$-th acoustic embedding layer.</p>
</blockquote>
<h3 id="43inference-in-context-learning-via-prompting">4.3.Inference: In-Context Learning via Prompting<a class="headerlink" href="#43inference-in-context-learning-via-prompting" title="Permanent link">&para;</a></h3>
<blockquote>
<p>In-context learning is a surprising ability of the text-based language model, which is able to predict labels for unseen inputs without additional parameter updates.
For TTS, if the model can synthesize high-quality speech for unseen speakers without fine-tuning, the model is believed to have in-context learning capability.
However, the in-context learning capability of existing TTS systems is not strong,because they either require additional fine-tuning or degrade dramatically for unseen speakers.</p>
<p>For language models, prompting is necessary to enable in-context learning in the zero-shot scenario.
We design prompts and inference as follows.
We first convert the text into a phoneme sequence and encode the enrolled recording into an acoustic matrix, forming the phoneme prompt and acoustic prompt.
Both prompts are used in the AR and NAR models.
For the AR model, we use sampling-based decoding conditioned on the prompts since we observe that beam search may lead the LM into an infinity loop.
Furthermore, the sampling-based method could significantly increase the diversity of the output.
For the NAR model, we use greedy decoding to choose the token with the highest probability.
Finally, we use the neural codec decoder to generate the waveform conditioned on the eight code sequences.</p>
<p>The acoustic prompt may or may not semantically relate to the speech to be synthesized, resulting in two cases:
- <u><strong>VALL-E</strong></u>: 
Our main interest is to generate given content for unseen speakers.
The model is given a text sentence, a segment of enrolled speech, and its corresponding transcription.
We prepend the transcription phoneme of the enrolled speech to the phoneme sequence of the given sentence as the phoneme prompt, and use the first layer acoustic token of the enrolled speech˜c:,1as an acoustic prefix.
With the phoneme prompt and the acoustic prefix, <u><strong>VALL-E</strong></u> generates the acoustic tokens for the given text cloning the voice of this speaker.
- <u><strong>VALL-E-continual</strong></u>: 
In this setting, we use the whole transcription and the first 3 seconds of the utterance as the phoneme and acoustic prompts respectively, and ask the model to generate the continuations.
The inference process is the same as setting <u><strong>VALL-E</strong></u>, except that the enrolled speech and the generated speech are semantically continuous.</p>
</blockquote>
<h2 id="5experiment">5.Experiment<a class="headerlink" href="#5experiment" title="Permanent link">&para;</a></h2>
<h3 id="51experiment-setup">5.1.Experiment Setup<a class="headerlink" href="#51experiment-setup" title="Permanent link">&para;</a></h3>
<h3 id="52librispeech-evaluation">5.2.LibriSpeech Evaluation<a class="headerlink" href="#52librispeech-evaluation" title="Permanent link">&para;</a></h3>
<h3 id="53vctk-evaluation">5.3.VCTK Evaluation<a class="headerlink" href="#53vctk-evaluation" title="Permanent link">&para;</a></h3>
<h3 id="54qualitative-analysis">5.4.Qualitative Analysis<a class="headerlink" href="#54qualitative-analysis" title="Permanent link">&para;</a></h3>
<h2 id="6conclusion-limitations-future-work">6.Conclusion, Limitations, Future Work<a class="headerlink" href="#6conclusion-limitations-future-work" title="Permanent link">&para;</a></h2>
<blockquote>
<p>We introduced <u><strong>VALL-E</strong></u>, a language model approach for TTS with audio codec codes as intermediate representations. 
We pre-train <u><strong>VALL-E</strong></u> with 60K hours of speech data, and show the in-context learning capability in zero-shot scenarios. 
We achieve new state-of-the-art zero-shot TTS results on LibriSpeech and VCTK. 
Furthermore, <u><strong>VALL-E</strong></u> could keep the acoustic environment and speaker’s emotion in synthesis, and provide diverse outputs in different sampling-based decoding processes.</p>
<p>Despite making significant progress, <u><strong>VALL-E</strong></u> still suffers from several issues.</p>
<p><strong>Synthesis robustness</strong>
We observe that some words may be unclear, missed, or duplicated in speech synthesis. 
It is mainly because the phoneme-to-acoustic language part is an autoregressive model, in which disordered attention alignments exist and no constraints to solving the issue. 
The phenomenon is also observed in vanilla Transformer-based TTS, which was addressed by applying non-autoregressive models or modifying the attention mechanism in modeling. 
In the future, we would like to leverage these techniques to solve the issue.</p>
<p><strong>Data coverage</strong>
Even if we use 60K hours of data for training, it still cannot cover everyone’s voice,especially accent speakers. 
The worse result on VCTK than LibriSpeech also implies insufficient coverage of accent speakers. Moreover, the diversity of speaking styles is not enough, as LibriLight is an audiobook dataset, in which most utterances are in reading style. 
In the future, we will further scale up the training data to improve the model performance across prosody, speaking style, and speaker similarity perspectives. 
We believe the zero-shot TTS task could be almost solved through our approach with model and data scale-up.</p>
<p><strong>Model Structure</strong>
Now, we use two models to predict codes of different quantizers. 
A promising direction is to predict them with a large universal model. 
Another interesting direction is using full NAR models to speed up model inference in the framework.</p>
<p><strong>Broader impacts</strong>
Since <u><strong>VALL-E</strong></u> could synthesize speech that maintains speaker identity, it may carry potential risks in misuse of the model, such as spoofing voice identification or impersonating a specific speaker. 
To mitigate such risks, it is possible to build a detection model to discriminate whether an audio clip was synthesized by <u><strong>VALL-E</strong></u>. 
We will also put Microsoft AI Principles∗into practice when further developing the models.</p>
</blockquote>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
    
  </body>
</html>