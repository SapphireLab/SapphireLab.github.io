
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.4.0">
    
    
      
        <title>MM-TTS: A Unified Framework for Multimodal, Prompt-Induced Emotional Text-to-Speech Synthesis - Sapphire Lab</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.9f615399.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mm-tts-a-unified-framework-for-multimodal-prompt-induced-emotional-text-to-speech-synthesis" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Sapphire Lab" class="md-header__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Sapphire Lab
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              MM-TTS: A Unified Framework for Multimodal, Prompt-Induced Emotional Text-to-Speech Synthesis
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Sapphire Lab" class="md-nav__button md-logo" aria-label="Sapphire Lab" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Sapphire Lab
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TTS
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../../../../PDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PDE
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract·摘要
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1introduction" class="md-nav__link">
    1.Introduction·引言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2related-work" class="md-nav__link">
    2.Related Work·相关工作
  </a>
  
    <nav class="md-nav" aria-label="2.Related Work·相关工作">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21text-to-speech-synthesis" class="md-nav__link">
    2.1.Text-to-Speech Synthesis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22emotional-text-to-speech-synthesis" class="md-nav__link">
    2.2.Emotional Text-to-Speech Synthesis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23multi-modal-representation-learning" class="md-nav__link">
    2.3.Multi-Modal Representation Learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3mm-tts-framework" class="md-nav__link">
    3.MM-TTS Framework
  </a>
  
    <nav class="md-nav" aria-label="3.MM-TTS Framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31emotion-prompt-alignment-module" class="md-nav__link">
    3.1.Emotion Prompt Alignment Module
  </a>
  
    <nav class="md-nav" aria-label="3.1.Emotion Prompt Alignment Module">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#311multimodal-encoders" class="md-nav__link">
    3.1.1.Multimodal Encoders.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#312multimodal-emotional-alignment" class="md-nav__link">
    3.1.2.Multimodal Emotional Alignment.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32emotion-embedding-induced-tts" class="md-nav__link">
    3.2.Emotion Embedding-induced TTS
  </a>
  
    <nav class="md-nav" aria-label="3.2.Emotion Embedding-induced TTS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#321prompt-anchoring-multimodal-fusion" class="md-nav__link">
    3.2.1.Prompt-Anchoring Multimodal Fusion.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#322unified-emotional-tts-framework" class="md-nav__link">
    3.2.2Unified Emotional-TTS Framework.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#323model-library" class="md-nav__link">
    3.2.3.Model Library.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4experiments" class="md-nav__link">
    4.Experiments
  </a>
  
    <nav class="md-nav" aria-label="4.Experiments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41experimental-setting" class="md-nav__link">
    4.1.Experimental Setting
  </a>
  
    <nav class="md-nav" aria-label="4.1.Experimental Setting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#411dataset" class="md-nav__link">
    4.1.1.Dataset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#412evaluation-metric" class="md-nav__link">
    4.1.2.Evaluation Metric
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42implementation-details" class="md-nav__link">
    4.2.Implementation Details
  </a>
  
    <nav class="md-nav" aria-label="4.2.Implementation Details">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#421data-preprocessing" class="md-nav__link">
    4.2.1.Data Preprocessing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#422model-training" class="md-nav__link">
    4.2.2.Model Training
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43comparison-with-the-state-of-the-art" class="md-nav__link">
    4.3.Comparison with the State-of-the-Art
  </a>
  
    <nav class="md-nav" aria-label="4.3.Comparison with the State-of-the-Art">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#431word-error-rate-and-character-error-rate-wer-and-cer" class="md-nav__link">
    4.3.1.Word Error Rate and Character Error Rate (WER and CER).
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#432emotion-similarity-mean-opinion-score-mos" class="md-nav__link">
    4.3.2.Emotion Similarity Mean Opinion Score (MOS).
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#433speech-naturalness-and-speaker-similarity-mos" class="md-nav__link">
    4.3.3.Speech Naturalness and Speaker Similarity MOS.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44ablation-study" class="md-nav__link">
    4.4.Ablation Study
  </a>
  
    <nav class="md-nav" aria-label="4.4.Ablation Study">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#441effectiveness-of-multimodal-fusion" class="md-nav__link">
    4.4.1.Effectiveness of Multimodal Fusion.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#442confusion-matrices-analysis" class="md-nav__link">
    4.4.2.Confusion Matrices Analysis.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#443impact-on-synthesized-speech-quality" class="md-nav__link">
    4.4.3.Impact on Synthesized Speech Quality.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5conclusions" class="md-nav__link">
    5.Conclusions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6supplementary-materials" class="md-nav__link">
    6.Supplementary Materials
  </a>
  
    <nav class="md-nav" aria-label="6.Supplementary Materials">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61implementation-details-of-emotion-embedding-induced-tts" class="md-nav__link">
    6.1.Implementation Details of Emotion Embedding-Induced TTS
  </a>
  
    <nav class="md-nav" aria-label="6.1.Implementation Details of Emotion Embedding-Induced TTS">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#611variants-vits" class="md-nav__link">
    6.1.1.Variants VITS.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#612variant-fastspeech2" class="md-nav__link">
    6.1.2.Variant FastSpeech2.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#613variation-tacotron2" class="md-nav__link">
    6.1.3.Variation Tacotron2.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62more-demonstrations" class="md-nav__link">
    6.2.More Demonstrations
  </a>
  
    <nav class="md-nav" aria-label="6.2.More Demonstrations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#621voiceover-scenario" class="md-nav__link">
    6.2.1.Voiceover Scenario.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#622emotional-text-to-speech-synthesis" class="md-nav__link">
    6.2.2.Emotional Text-to-Speech Synthesis.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#623zero-shot-emotional-speech" class="md-nav__link">
    6.2.3.Zero-Shot Emotional Speech.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="mm-tts-a-unified-framework-for-multimodal-prompt-induced-emotional-text-to-speech-synthesis">MM-TTS: A Unified Framework for Multimodal, Prompt-Induced Emotional Text-to-Speech Synthesis<a class="headerlink" href="#mm-tts-a-unified-framework-for-multimodal-prompt-induced-emotional-text-to-speech-synthesis" title="Permanent link">&para;</a></h1>
<h2 id="abstract">Abstract·摘要<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h2>
<blockquote>
<p><strong>Emotional Text-to-Speech (E-TTS)</strong> synthesis has gained significant attention in recent years due to its potential to enhance human-computer interaction.
However, current <strong>E-TTS</strong> approaches often struggle to capture the complexity of human emotions, primarily relying on oversimplified emotional labels or single-modality inputs.
To address these limitations, we propose the <strong><em>Multimodal Emotional Text-to-Speech System (MM-TTS)</em></strong>, a unified framework that leverages emotional cues from multiple modalities to generate highly expressive and emotionally resonant speech.
<strong><em>MM-TTS</em></strong> consists of two key components: 
(1) the <strong><em>Emotion Prompt Alignment Module (EP-Align)</em></strong>, which employs contrastive learning to align emotional features across text, audio, and visual modalities, ensuring a coherent fusion of multimodal information; 
(2) the <strong><em>Emotion Embedding-Induced TTS (EMI-TTS)</em></strong>, which integrates the aligned emotional embeddings with state-of-the-art <strong>TTS</strong> models to synthesize speech that accurately reflects the intended emotions.</p>
<p>Extensive evaluations across diverse datasets demonstrate the superior performance of <strong><em>MM-TTS</em></strong> compared to traditional <strong>E-TTS</strong> models.
Objective metrics, including Word Error Rate (WER) and Character Error Rate (CER), show significant improvements on ESD dataset, with <strong><em>MM-TTS</em></strong> achieving scores of 7.35% and 3.07%, respectively.
Subjective assessments further validate that <strong><em>MM-TTS</em></strong> generates speech with emotional fidelity and naturalness comparable to human speech.
Our code and pre-trained models are publicly available at https://anonymous.4open.science/r/MMTTS-D214.</p>
</blockquote>
<h2 id="1introduction">1.Introduction·引言<a class="headerlink" href="#1introduction" title="Permanent link">&para;</a></h2>
<blockquote>
<p><strong>Emotional text-to-speech (E-TTS)</strong> technology has emerged as a transformative force in artificial intelligence and multimedia, significantly enhancing the richness of human-computer interaction [40].
By imbuing synthetic voices with emotional depth, <strong>E-TTS</strong> transcends mere speech replication, bringing virtual agents and digital characters to life.
The incorporation of emotional expressiveness captivates users and fosters deeper emotional connections, catering to the fundamental human need for empathetic communication [5,49].
This heightened level of engagement has the potential to revolutionize industries from entertainment and education to healthcare and customer service.</p>
<p>Despite substantial advancements in <strong>text-to-speech (TTS)</strong> technologies resulting in naturalistic, high-quality speech, the primary focus has been on linguistic accuracy rather than capturing the inherent emotional nuances [21,31,36–38,45].
Contemporary <strong>E-TTS</strong> approaches have sought to address this gap by integrating emotion labels or utilizing emotional reference speech [5,6,11,23,26,46,49].
However, these methods often struggle to fully capture the intricacies of human emotions due to their reliance on oversimplified representations or single-modality inputs.
To truly unlock the potential of <strong>E-TTS</strong> and create emotionally resonant synthetic speech, a more comprehensive approach leveraging rich emotional information across multiple modalities is crucial.</p>
<p>Consider a voiceover scenario where one aims to generate emotionally consistent speech for a given dialogue, potentially with access to visual cues such as facial expressions, silent video clips, and reference audio samples, as shown in Fig.01.
Traditional <strong>E-TTS</strong> approaches relying on unimodal information have inherent limitations.
Facial expressions are dynamic, and their emotional interpretation can vary significantly depending on context.
Similarly, using reference emotional audio alone fails to capture the nuanced interplay of emotions across scenarios.
Moreover, in real-world applications, the availability of different modalities may vary.
Leveraging multimodal cues enables a more comprehensive understanding of the underlying emotional state, leading to the generation of speech with more intricate and nuanced emotional expressions tailored to the specific context.
Such multimodal integration enables a more flexible approach to emotional speech synthesis, catering to the intricate demands of human-computer interaction and content creation (e.g.Fig.01).</p>
<p>In response, we present the <strong><em>Multimodal Emotional Text-to-Speech System (MM-TTS)</em></strong>, a groundbreaking framework designed to elevate the expressiveness of synthesized speech by incorporating multimodal cues encompassing text, audio, and visual information (Sec.3).
<strong><em>MM-TTS</em></strong> comprises two critical components: the <strong><em>Emotion Prompt Alignment Module (EP-Align)</em></strong> and the <strong><em>Emotion Embedding-Induced TTS (EMI-TTS)</em></strong>.
<strong><em>EP-Align</em></strong> enables the seamless fusion of multimodal information by aligning emotional data across modalities through a cue anchoring mechanism (Sec. 3.1).
<strong><em>EMI-TTS</em></strong> leverages these aligned emotional embeddings to generate expressive and emotionally resonant speech suitable for a wide range of applications (Sec.3.2).
The integration of <strong><em>EP-Align</em></strong> and <strong><em>EMI-TTS</em></strong> within <strong><em>MM-TTS</em></strong> leads to emotionally intelligent speech synthesis.</p>
<p>To validate the effectiveness of <strong><em>MM-TTS</em></strong>, we conduct extensive testing across diverse datasets, benchmarking its performance against traditional models in crucial speech quality metrics.
By investigating the impact of <strong><em>MM-TTS</em></strong> on speech content accuracy with varied TTS framework, we find that it consistently improves performance on ESD dataset with <strong>Word Error Rate (WER)</strong> and <strong>Character Error Rate (CER)</strong> scores of 7.35% and 3.07%, respectively (Sec.4.3.1).
Subjective evaluations further corroborate these objective measures, confirming that <strong><em>MM-TTS</em></strong> matches the emotional fidelity and naturalness of human speech.
The combination of strong objective performance and subjective approval positions <strong><em>MM-TTS</em></strong> at the forefront of Emotional TTS technologies, highlighting its potential to revolutionize human-computer interactions through emotional speech synthesis.
The key contributions are threefold:</p>
<ul>
<li>We introduce the <strong><em>Emotion Prompt Alignment Module (EP-Align)</em></strong>, a novel approach employing contrastive learning to synchronize emotional features across modalities.
By effectively aligning emotional features and filtering out complex noise in multimodal content, <strong><em>EP-Align</em></strong> addresses the challenge of distribution discrepancies, serving as a critical component in enabling high-quality emotional speech generation.</li>
<li>Building upon <strong><em>EP-Align</em></strong>, we develop the <strong><em>Emotion EmbeddingInduced TTS (EMI-TTS)</em></strong> framework, which seamlessly integrates advanced TTS models with the aligned emotional embeddings to synthesize speech that authentically reflects intended emotions.
The incorporation of these embeddings enhances the naturalness and credibility of the generated audio, resulting in a more engaging and immersive user experience.</li>
<li>We conduct extensive evaluations across a wide spectrum of emotional categories and scenarios to validate the adaptability and superior processing capabilities of <strong><em>MM-TTS</em></strong>.
The results confirm that <strong><em>MM-TTS</em></strong> consistently maintains emotional consistency while preserving individual speaker characteristics, highlighting its transformative potential in revolutionizing human-computer interactions by providing a more engaging experience.</li>
</ul>
</blockquote>
<h2 id="2related-work">2.Related Work·相关工作<a class="headerlink" href="#2related-work" title="Permanent link">&para;</a></h2>
<h3 id="21text-to-speech-synthesis">2.1.Text-to-Speech Synthesis<a class="headerlink" href="#21text-to-speech-synthesis" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Text-to-speech (TTS) technology, known as speech synthesis, aims to generate human-like speech from text.
Recent neural network-based end-to-end TTS models, such as those developed in [21,31,36–38,45], have markedly improved speech quality over traditional methods like concatenation synthesis [17] and statistical parametric synthesis [41,48].
Typically, these models involve converting text or phonemes into linguistic features using embeddings, expanding these features to meet the acoustic model’s requirements via a length regulator, and then converting the acoustic features to waveforms using a vocoder.
Different models employ various approaches for the acoustic model.
For example, the Tacotron series uses RNN-based models [38,39,45], VITS utilizes flow-based models [8,21], and FastSpeech features Transformer-based models [36,37,44].
While these state-of-the-art TTS models have made some progress in generating natural and high-quality speech, they have primarily focused on linguistic accuracy and clarity, often overlooking the emotional dimensions conveyed through speech.</p>
</blockquote>
<h3 id="22emotional-text-to-speech-synthesis">2.2.Emotional Text-to-Speech Synthesis<a class="headerlink" href="#22emotional-text-to-speech-synthesis" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Emotional Text-to-Speech (ETTS) focuses on enhancing synthesized speech with emotional expressiveness [40].
Researchers have developed methods to integrate emotions into TTS systems. 
(1) One technique uses emotion labels as additional conditioning data in TTS models.
For example, EmoSpeech [6] modifies the FastSpeech2 framework by adding speaker and emotion labels via Conditional Layer Normalization.
Similar strategies in works like [5,23,49] employ Tacotron-based models where emotion labels are embedded into hidden features as conditions.
EmoDiff [11] uses a diffusion model [16] with soft labels for emotions, replacing the traditional one-hot vectors. 
(2) Another approach leverages referenced speech capturing the desired emotional state.
Li et al. [26] introduce a method incorporating dual emotion classifiers and a style loss to align the generated speech’s emotional content with the reference mel-spectrum.
Moreover, [46] explores techniques for transferring the emotional style from reference speech to synthesized speech. 
(3) Some studies utilize textual descriptions of emotions as conditioning data, with [47] and [12] demonstrating effective conveyance of target emotions through guided textual descriptions in speech synthesis.
Despite advancements, previous ETTS approaches often face limitations like reliance on a single modality or simplistic emotional representations.
Tab.01 outlines these differences across existing models, focusing on supported emotion categories, conditioning data types, and zero-shot learning capabilities.
Addressing these limitations, our proposed framework, <strong><em>MM-TTS</em></strong>, employs multimodal representation learning to integrate diverse emotion-related data, enabling the generation of emotionally rich speech.</p>
</blockquote>
<h3 id="23multi-modal-representation-learning">2.3.Multi-Modal Representation Learning<a class="headerlink" href="#23multi-modal-representation-learning" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Multi-modal representation learning, a pivotal area in modern research, aims to find a shared latent space that effectively integrates diverse modalities such as text, images, videos, and audio [10,13,22,27,28,34]. 
This integration facilitates a deeper understanding of the emotional content underlying these modalities.
Multi-modal encoders extract contextually relevant features from each modality, capturing their unique characteristics.
These features are then aligned using supervised or unsupervised methods, creating connections that aid in cross-modal tasks [3,4,7].
Particularly, key advancements include the Contrastive Language-Image Pretraining (CLIP) model [34], which establishes a joint embedding for images and text, and the AudioCLIP model [13], which extends this to include audio.
Furthermore, Koepke et al. [22] have explored combining audio and visual data to enable applications like sound localization and audio-visual event detection.
Our framework, MMTTS, builds on these developments by using multi-modal representation learning to improve the emotional expressiveness of synthesized speech.
By extracting and aligning feature representations from text, images, videos, and audio with emotion tags through contrastive learning, <strong><em>MM-TTS</em></strong> captures comprehensive emotion-related information.
This enriched understanding allows <strong><em>MM-TTS</em></strong> to align explicit and implicit emotional cues, generating nuanced and emotionally rich synthesized speech.</p>
</blockquote>
<h2 id="3mm-tts-framework">3.MM-TTS Framework<a class="headerlink" href="#3mm-tts-framework" title="Permanent link">&para;</a></h2>
<blockquote>
<p>The primary goal of <strong><em>MM-TTS</em></strong> is to leverage emotions extracted from multiple modalities to generate emotional speech for different individuals in a data-efficient manner.
Fig.02 illustrates the framework comprising two main components: (1) Emotion Prompt Alignment Module (EP-Align) and (2) Emotion Embedding-induced TTS (EMI-TTS).</p>
<p>(1) Emotion Prompt Alignment Module (EP-Align) plays a crucial role in aligning the emotional representations derived from various modalities, such as visual data, audio segments, and textual descriptions.
It takes a multimodal emotional data tuple $Tup^{emo}=<v,a,s,p>$ as input, where $v$ represents visual data (image or video), $a$ is an audio segment, $s$ denotes an emotional text description, and $p$ is an emotional prompt label.
<strong><em>EP-Align</em></strong> processes these inputs using a set of multimodal encoders $\mathcal{E} = {\mathcal{E}^{vis}, \mathcal{E}^{audio}, \mathcal{E}^{tex}, \mathcal{E}^{prop}}$, which extract emotion features and generate a unified emotion embedding.
This emotion embedding is then passed to the <strong><em>EMI-TTS</em></strong> component.</p>
<p>(2) Emotion Embedding-induced TTS (EMI-TTS) component takes the aligned emotion embedding, along with the input text Tex and a pre-trained TTS model from the model library $\mathcal{M}$, to generate emotional speech.
The emotion embedding provides the necessary emotional context, while the input text and the TTS model determine the content and the overall style of the generated speech.</p>
<p>Formally, the audio with emotion can be generated by <strong><em>MM-TTS</em></strong> $\Phi$ as follows:</p>
</blockquote>
<p>$$
\tag{1}
$$</p>
<blockquote>
<p>The <strong><em>MM-TTS</em></strong> framework aims to address the challenges associated with generating emotional speech by leveraging the complementary information provided by multiple modalities.
By aligning the emotional representations from different sources and integrating them into the TTS process, <strong><em>MM-TTS</em></strong> enables the synthesis of speech that accurately conveys the desired emotions while maintaining the speaker’s characteristics.
In the following, we will delve into the details of the Emotion Prompt Alignment Module (EP-Align) and the Emotion Embedding-induced TTS (EMI-TTS) component.</p>
</blockquote>
<h3 id="31emotion-prompt-alignment-module">3.1.Emotion Prompt Alignment Module<a class="headerlink" href="#31emotion-prompt-alignment-module" title="Permanent link">&para;</a></h3>
<blockquote>
<p>Inspired by Contrastive Language-Image Pre-training (CLIP) [34], <strong><em>EP-Align</em></strong> employs a set of multimodal encoders to extract emotion features from various modalities.
These encoders are denoted as $\mathcal{E} = {\mathcal{E}^{vis}, \mathcal{E}^{audio}, \mathcal{E}^{tex}, \mathcal{E}^{prop}}$, corresponding to vision, audio, text, and emotion prompts, respectively.
The extracted features are then aligned into a unified emotion embedding space.</p>
</blockquote>
<h4 id="311multimodal-encoders">3.1.1.Multimodal Encoders.<a class="headerlink" href="#311multimodal-encoders" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Each modality is processed by a dedicated encoder to extract relevant emotion features.
Given a multimodal emotional data tuple $Tup^{emo}=<v,a,s,p>$, the encoders generate feature representations as follows:</p>
</blockquote>
<p>$$
\tag{2}
$$</p>
<blockquote>
<p>where $f^{vis}, f^{audio}, f^{tex}, f^{prop}$ represent the feature representations for visual, audio, textual, and emotional prompts, respectively.
Each feature representation has a dimensionality of𝐾, which is set to 512 in this work.</p>
<p>The architectures of the text encoder $\mathcal{E}^{tex}$, prompt encoder $\mathcal{E}^{prop}$, and visual encoder $\mathcal{E}^{vis}$ are similar to those in CLIP [34].
The audio encoder $\mathcal{E}^{audio}$ follows the design from [1] for effective audio encoding.
During training, the text and image encoders are initialized with pre-trained CLIP weights, while the audio encoder is initialized with a pre-trained model from [14].
To handle video inputs, the visual encoder $\mathcal{E}^{vis}$ processes a sequence of frames, extracts a feature bank, and performs average pooling in the temporal dimension to obtain the final representation.
For audio inputs, an additional linear projection layer is employed to map the audio features to the same dimension as the text and image features.</p>
</blockquote>
<h4 id="312multimodal-emotional-alignment">3.1.2.Multimodal Emotional Alignment.<a class="headerlink" href="#312multimodal-emotional-alignment" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Aligning the emotion representations from different modalities is a critical step in <strong><em>MM-TTS</em></strong>.
We propose a prompt-anchoring scheme that leverages prompt-based bridging to facilitate the alignment of multimodal representations.
First, we construct vision-prompt (vis-pro) and audio-prompt (aud-pro) embedding spaces by projecting the vision, audio, and text features using learned projection matrices:</p>
</blockquote>
<p>$$
\tag{3}
$$</p>
<blockquote>
<p>where $W^{vis−pro}$, $W^{aud−pro}$, $W^{tex−pro}\in\mathbb{R}^{K\times K}$ are the projection matrices, and $u^{vis}$, $u^{audio}$, and $u^{tex}$ }are the projected embeddings in the vis-pro, aud-pro, and tex-pro spaces, respectively.</p>
<p>The prompt embedding $u^{prop}$ is obtained by multiplying the corresponding projection matrix based on the alignment modality:</p>
</blockquote>
<p>$$
\tag{4}
$$</p>
<blockquote>
<p>We use $u^{exp}$ to denote the explicit emotion embedding obtained from the emotion prompt and $u^{imp}$ to represent the implicit emotion embeddings from vision, audio, or text.
The cosine similarity between the explicit and implicit embeddings is computed as:</p>
</blockquote>
<p>$$
\tag{5}
$$</p>
<blockquote>
<p>where $t$ is a learned temperature parameter, and $\sigma$ denotes the normalization operation.</p>
<p>The multimodal alignment loss $L_{align}$ is defined as the symmetric cross-entropy loss between the explicit and implicit embeddings:</p>
</blockquote>
<p>$$
\tag{6}
$$</p>
<blockquote>
<p>During inference, <strong><em>EP-Align</em></strong> selects the emotion prompt embedding $u^{prop}$ with the highest similarity score to the implicit embeddings as the aligned emotion representation $u^{emo}$.
This aligned emotion representation captures the emotional content conveyed by the multimodal inputs and serves as a unified representation for the subsequent TTS process.
By leveraging the prompt-anchoring scheme and the multimodal encoders, <strong><em>EP-Align</em></strong> effectively aligns the emotion representations from different modalities into a shared embedding space.
This alignment enables the TTS model to generate emotional speech that is consistent with the input emotional context, regardless of the source modality.</p>
</blockquote>
<h3 id="32emotion-embedding-induced-tts">3.2.Emotion Embedding-induced TTS<a class="headerlink" href="#32emotion-embedding-induced-tts" title="Permanent link">&para;</a></h3>
<blockquote>
<p>With the aligned emotion embedding $u^{emo}$ obtained from the previous <strong><em>EP-Align</em></strong> module, the Emotion Embedding-induced TTS (EM-TTS) component generates emotional speech by integrating the emotion representation into the TTS process.</p>
</blockquote>
<h4 id="321prompt-anchoring-multimodal-fusion">3.2.1.Prompt-Anchoring Multimodal Fusion.<a class="headerlink" href="#321prompt-anchoring-multimodal-fusion" title="Permanent link">&para;</a></h4>
<blockquote>
<p>To mitigate the bias among the multimodal emotion embedding spaces, we employ prompt-anchoring to fuse the aligned emotion embedding $u^{emo}$ with the input text representation.
Specifically, we use the implicit emotion embedding $u^{imp}$ to retrieve the most similar prompt embedding $u^{prop}$ based on cosine similarity.
The retrieved prompt embedding serves as an anchor to bridge the implicit and explicit emotion representations, enabling a more coherent integration of the emotion embedding into the TTS model.</p>
</blockquote>
<h4 id="322unified-emotional-tts-framework">3.2.2Unified Emotional-TTS Framework.<a class="headerlink" href="#322unified-emotional-tts-framework" title="Permanent link">&para;</a></h4>
<blockquote>
<p><strong><em>EMI-TTS</em></strong> provides a unified framework that integrates various TTS models, such as Tacotron2[38], VITS [21], and FastSpeech2 [36].
These models typically consist of a text encoder, a length regulator, an acoustic model, and a vocoder.
The text encoder converts the input text Tex into a sequence of linguistic features $h_{lg}$ .
The length regulator adjusts the duration of the linguistic features to match the desired speech length.
The acoustic model generates acoustic features $h_{ac}$ conditioned on the linguistic features and the emotion and speaker embeddings.
Finally, the vocoder converts the acoustic features into a speech waveform.
In <strong><em>EMI-TTS</em></strong>, the emotion embedding $u^{emo}$ and speaker embedding $u_{spk}$ are concatenated and integrated into the TTS model as additional conditioning inputs.
The emotion embedding provides the necessary emotional context, while the speaker embedding captures the speaker’s characteristics.
This allows the TTS model to generate speech that reflects both the desired emotion and the target speaker’s style.</p>
<p>Formally, the <strong><em>EMI-TTS</em></strong> process can be represented as follows:</p>
</blockquote>
<p>$$
\tag{7}
$$</p>
<blockquote>
<p>where $h_{lg}^{emo}$ represents the linguistic features conditioned on the emotion and speaker embeddings, and $Audio^{emo}$ is the generated emotional speech.
During training, <strong><em>EMI-TTS</em></strong> optimizes the same loss functions as the base TTS models without requiring additional loss terms.
The model is trained on paired data consisting of input text, emotion features, and speaker information.
The vocoder used in <strong><em>EMI-TTS</em></strong> depends on the specific TTS model: VITS uses its original decoder, while Tacotron2 and FastSpeech2 employ a WaveNet[42] vocoder.</p>
</blockquote>
<h4 id="323model-library">3.2.3.Model Library.<a class="headerlink" href="#323model-library" title="Permanent link">&para;</a></h4>
<blockquote>
<p><strong><em>EMI-TTS</em></strong> incorporates several state-of-the-art TTS models, including Tacotron2, VITS, and FastSpeech2.
By providing a diverse set of TTS models, <strong><em>EMI-TTS</em></strong> offers flexibility in generating emotional speech across different scenarios and requirements.</p>
<p>(1) Tacotron2: Building upon the traditional Tacotron model[38], this version integrates WaveNet technology[42] for improved sound generation.
The Character Encoder transforms text into hidden states $h_{lg}$ , enhanced with emotion embeddings $u^{emo}$ to create emotionally enriched states $h_{lg}^{emo}$:</p>
</blockquote>
<p>$$
\tag{8}
$$</p>
<blockquote>
<p>This enhancement adjusts the Location Sensitive Attention to a more nuanced Location &amp; Emotion Sensitive Attention, allowing the model to better capture emotional nuances in speech synthesis.</p>
<p>(2) VITS: This model innovatively combines variational inference with normalizing flows and adversarial training, as outlined in VITS[21].
It includes a unique setup comprising a Text Encoder, Length Regulator, and an Emotion-condition Flow, among other components.
It utilizes an Emotion-condition Flow to adjust phoneme sequences encoded by the Text Encoder into emotion-laden hidden states $h_{lg}^{emo}$</p>
</blockquote>
<p>$$
\tag{9}
$$</p>
<blockquote>
<p>where $d$ is the duration predicted for each unit.
This architecture allows for a more dynamic and emotionally responsive speech output during inference.</p>
<p>(3) FastSpeech2: Leveraging a Transformer architecture, this model significantly enhances the synthesis speed and quality of speech as outlined in FastSpeech2[36].
It incorporates a Mel-spectrogram Decoder that uses Conditional Cross-Attention to embed emotional nuances effectively into the synthesized speech:</p>
</blockquote>
<p>$$
\tag{10}
$$</p>
<blockquote>
<p>where $\tilde{h}<em>{lg} = h</em>{lg} + u^{emo}$ represents the integration of the text input hidden states and emotion embeddings.
This single-step computation efficiently handles the extension of hidden states, their emotional modulation, and the application of cross-attention, resulting in seamless and expressive speech synthesis.</p>
<p>By employing prompt-anchoring multimodal fusion and a unified emotional-TTS framework, <strong><em>EMI-TTS</em></strong> generates emotional speech that accurately reflects the desired emotions while preserving the target speaker’s characteristics.
The choice of the TTS model can be based on the specific requirements of the application, such as synthesis speed, voice quality, and emotional expressiveness.
More details are in the supplementary material.</p>
</blockquote>
<h2 id="4experiments">4.Experiments<a class="headerlink" href="#4experiments" title="Permanent link">&para;</a></h2>
<h3 id="41experimental-setting">4.1.Experimental Setting<a class="headerlink" href="#41experimental-setting" title="Permanent link">&para;</a></h3>
<h4 id="411dataset">4.1.1.Dataset<a class="headerlink" href="#411dataset" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Our <strong><em>MM-TTS</em></strong> framework has been evaluated on three different datasets:
- <strong>Multimodal EmotionLines Dataset (MELD)</strong> [32] was utilized to evaluate the Emo-Alignment module’s performance in extracting and aligning emotions from both explicit and implicit multimodal cues.
The MELD dataset contains 13,708 utterances from the TV series Friends, covering emotions such as Anger, Disgust, Sadness, Joy, Neutral, Surprise, and Fear.
It is divided into training (9,989 utterances), validation (1,109 utterances), and test sets (2,610 utterances) to ensure a comprehensive evaluation.
- <strong>Emotion Speech Dataset (ESD)</strong> [50] was used to test the model’s capability in vocal emotion expression.
The ESD includes 17,500 utterances by 10 speakers, categorized into five emotions: neutral, happy, angry, sad, and surprised.
The dataset was split into training (14,000 utterances), validation (1,750 utterances), and test subsets (1,750 utterances) for structured evaluation.
- <strong>Real-world Expression Database (RAF-DB)</strong> [25] was employed to evaluate the model’s ability to recognize complex compound emotions from visual data.
The RAF-DB is divided into a basic emotion dataset with 15,339 images in seven categories and a compound emotion dataset of 3,954 images in 11 classes.</p>
<p>Training and test splits for the basic dataset are 12,271 and 3,068 images, respectively, while the compound dataset is divided into 3,162 images for training and 792 for testing.</p>
</blockquote>
<h4 id="412evaluation-metric">4.1.2.Evaluation Metric<a class="headerlink" href="#412evaluation-metric" title="Permanent link">&para;</a></h4>
<blockquote>
<p>To evaluate <strong><em>MM-TTS</em></strong> framework, we use both objective metrics and subjective human evaluations.
- <strong>Objective Metrics</strong> focus on the model’s ability to accurately extract and classify emotion features from multimodal inputs and to assess the quality of synthesized speech. 
(1) <strong>Multi-Modal Emotion Alignment Accuracy</strong>: We evaluate the ability of <strong><em>MM-TTS</em></strong> to extract emotion features by comparing implicit emotion embeddings from each modality with explicit emotion descriptions.
The accuracy is calculated based on how often the predicted emotion class matches the ground truth label, determined by the highest similarity score among potential emotion prompts. 
(2) <strong>Speech Quality Metrics (WER and CER)</strong>: The quality of generated speech is objectively measured using the Whisper [35] ASR model.
We calculate the Word Error Rate (WER) and Character Error Rate (CER) on ESD dataset by comparing the transcribed synthesized speech to the actual ground truth text, aiming to investigate the impact of our <strong><em>MM-TTS</em></strong> on speech content accuracy.
- <strong>Subjective Human Evaluations</strong> are conducted using the Mean Opinion Score (MOS) to evaluate emotional expressiveness, naturalness of speech, and speaker similarity of the synthesized audio.
(1) <strong>Emotion Similarity MOS</strong>: Raters evaluate how well the synthesized speech conveys the intended emotions.
Each sample’s emotion alignment is scored on a scale from 1 to 5, with higher scores indicating a closer match to the target emotion. 
(2) <strong>Speech Naturalness MOS</strong>: This metric assesses the naturalness of the speech output.
Raters listen to the synthesized utterances and score them based on how naturally they mimic human speech.
(3) <strong>Speaker Similarity MOS</strong>: Raters assess how accurately the synthesized speech matches the voice characteristics of the target speaker.
Samples are scored based on their similarity to reference utterances, focusing on voice timbre, pitch, and style.</p>
</blockquote>
<h3 id="42implementation-details">4.2.Implementation Details<a class="headerlink" href="#42implementation-details" title="Permanent link">&para;</a></h3>
<h4 id="421data-preprocessing">4.2.1.Data Preprocessing<a class="headerlink" href="#421data-preprocessing" title="Permanent link">&para;</a></h4>
<blockquote>
<p>We convert emotion labels into short description sentences to serve as emotion prompts for the models.
For text, we employ two strategies: for the flow-based model, we use the International Phonetic Alphabet (IPA) sequences similar to Glow-TTS [19], utilizing phonemizer software [2] for conversion.
For Transformer-based models, text sequences are transformed into phoneme sequences with duration information using the Montreal Forced Aligner (MFA) [30].
Audio inputs undergo the Short-time Fourier transform (STFT) to extract linear spectrograms [21], and are also converted into mel-spectrograms as per the methods in Tacotron2 [38].
Images are resized and normalized to a tensor format $V\in\mathbb{R}^{h\times w\times c}$, with the dimensions $h$ and $w$ both set to 244, and $c$ representing the three color channels of RGB.
Video preprocessing involves segmenting videos into frames, uniformly sampling 8 frames across the sequence, and converting these frames into tensors similar to image processing.</p>
</blockquote>
<h4 id="422model-training">4.2.2.Model Training<a class="headerlink" href="#422model-training" title="Permanent link">&para;</a></h4>
<blockquote>
<p>The training process is segmented into two main phases to manage resource utilization effectively.
Initially, the Emotion Prompt Alignment Module (EP-Align) is trained using multi-modal pairs from all datasets.
Subsequently, the prompts generated by the trained <strong><em>EP-Align</em></strong> are utilized with audio-text pairs to train the Emotion Embedding-induced TTS (EMI-TTS).
For the EPAlign module, we employ contrastive learning to fine-tune encoders from various modalities.
We use RoBERTa [29] and InstructERC[24] for text, VGGish [15] and wav2vec2 [1] for audio, and ResNet[18] and ViT [9,34] for images and videos.
The training of <strong><em>EP-Align</em></strong> was conducted over 100 epochs on 4 NVIDIA A100 GPUs.
For <strong><em>EMI-TTS</em></strong>, the flow-based models underwent 200,000 training steps until convergence, while Transformer-based models were trained for 40,000 steps and Recurrent-based models completed 250,000 steps.</p>
<p>More details are in the supplementary material.</p>
</blockquote>
<h3 id="43comparison-with-the-state-of-the-art">4.3.Comparison with the State-of-the-Art<a class="headerlink" href="#43comparison-with-the-state-of-the-art" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We compare our <strong><em>MM-TTS</em></strong> model to existing state-of-the-art TTS models across several metrics to demonstrate its effectiveness in synthesizing emotional and accurate speech.</p>
</blockquote>
<h4 id="431word-error-rate-and-character-error-rate-wer-and-cer">4.3.1.Word Error Rate and Character Error Rate (WER and CER).<a class="headerlink" href="#431word-error-rate-and-character-error-rate-wer-and-cer" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Tab.02 presents a comparison of WER and CER for different TTS models on ESD to illustrate the performance improvement achieved by the <strong><em>MM-TTS</em></strong> framework.
The <strong><em>MM-TTS</em></strong> implementation with the FastSpeech structure achieves the lowest WER at 7.35% and CER at 3.07%, indicating an effective approximation to the ground truth audio quality.
This performance surpasses other implementations such as VITS, Tacotron, and the EmoSpeech [6] models.
This improvement is due to the multimodal emotion alignment within <strong><em>MM-TTS</em></strong>, which improves the extraction and representation of emotion features, allowing for better model convergence and superior generalization capabilities in speech synthesis.
This methodological refinement results in more natural and accurate emotional speech output, establishing <strong><em>MM-TTS</em></strong> as a significant advance over traditional label-based emotion encoding methods.</p>
</blockquote>
<h4 id="432emotion-similarity-mean-opinion-score-mos">4.3.2.Emotion Similarity Mean Opinion Score (MOS).<a class="headerlink" href="#432emotion-similarity-mean-opinion-score-mos" title="Permanent link">&para;</a></h4>
<blockquote>
<p>The subjective evaluations presented in Tab.03 illustrate the effectiveness of the <strong><em>MM-TTS</em></strong> models in generating emotionally congruent speech.
The <strong><em>MM-TTS</em></strong> (FastSpeech) variant notably scored an average MOS of 4.37, closely matching the ground truth (MOS of 4.57) and surpassing other models.
This performance highlights the <strong><em>MM-TTS</em></strong> framework’s capability to effectively capture and render nuanced emotional expressions, with scores across various emotions showing consistent improvement over other TTS models.
This indicates a robust alignment of emotional tones in speech synthesis, positioning <strong><em>MM-TTS</em></strong> (FastSpeech) as a leading solution in emotional TTS technologies.</p>
</blockquote>
<h4 id="433speech-naturalness-and-speaker-similarity-mos">4.3.3.Speech Naturalness and Speaker Similarity MOS.<a class="headerlink" href="#433speech-naturalness-and-speaker-similarity-mos" title="Permanent link">&para;</a></h4>
<blockquote>
<p>The MMTTS models exhibit strong performance in speech naturalness and speaker similarity, as demonstrated in Tables 4 and 5.
These evaluations confirm the <strong><em>MM-TTS</em></strong>’s capability to maintain high standards of voice quality and speaker-specific traits across different emotional expressions.
Specifically, <strong><em>MM-TTS</em></strong> (FastSpeech) consistently scores near or above ground truth levels, underscoring its effectiveness in producing natural-sounding and speaker-consistent emotional speech, crucial for enhancing user engagement in personalized TTS applications.
These findings not only demonstrate the technical excellence of <strong><em>MM-TTS</em></strong> but also highlight its practical effectiveness in real-world scenarios where emotional variance and speaker identity are crucial for user interaction and satisfaction.</p>
</blockquote>
<h3 id="44ablation-study">4.4.Ablation Study<a class="headerlink" href="#44ablation-study" title="Permanent link">&para;</a></h3>
<blockquote>
<p>We perform ablation studies to quantify the impact of <strong><em>EP-Align</em></strong> on emotion classification accuracy and synthesized speech quality across different modalities.
Particularly, we assess the impact of EPAlign using the MELD dataset for multi-modal emotion recognition and the RAF-DB dataset for compound emotion classification.
The effectiveness of <strong><em>EP-Align</em></strong> is measured in terms of emotion recognition accuracy (weighted F1 scores) and speech synthesis quality (WER, CER, and MOS), respectively.</p>
</blockquote>
<h4 id="441effectiveness-of-multimodal-fusion">4.4.1.Effectiveness of Multimodal Fusion.<a class="headerlink" href="#441effectiveness-of-multimodal-fusion" title="Permanent link">&para;</a></h4>
<blockquote>
<p>To evaluate the impact of the Emotion Prompt Alignment Module (EP-Align) on improving emotion recognition, we conducted a series of tests using the MELD dataset, which is known for its rich multimodal input.
Tab.06 clearly demonstrates that <strong><em>EP-Align</em></strong> significantly improves emotion recognition accuracy.
Notably, the configuration using combined modalities, which synthesizes text, audio, and video inputs, shows the most marked improvement, with a F1 score increase from 0.68 to 0.75.
This highlights <strong><em>EP-Align</em></strong>’s effectiveness in multimodal fusion, proving crucial in environments where emotional cues are inherently dispersed across different channels.
By seamlessly blending these inputs, <strong><em>EP-Align</em></strong> enhances the coherence and precision of emotion classification systems.
Such advancements are particularly beneficial for developing more sophisticated, context-aware applications in sectors like interactive voice response systems and affective computing.</p>
</blockquote>
<h4 id="442confusion-matrices-analysis">4.4.2.Confusion Matrices Analysis.<a class="headerlink" href="#442confusion-matrices-analysis" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Confusion matrices are instrumental in evaluating classification models by illustrating the accuracy across different classes and highlighting potential areas of misclassification.
Figures 3 and 4 present the confusion matrices for the MELD and RAF-DB datasets, respectively, following the integration of the Emotion Prompt Alignment Module (EP-Align).
These matrices provide insights into <strong><em>EP-Align</em></strong>’s performance in aligning multimodal inputs to predict complex emotional states accurately.
Specifically, Fig.03 depicts the classification results for the MELD dataset, which includes a range of basic and complex emotions.
This visualization helps identify which emotions <strong><em>EP-Align</em></strong> accurately recognizes and where it tends to confuse one emotion for another.
For instance, a common misclassification might be the confusion between ’joy’ and ’surprise’, which often share similar expressive features.
By examining these overlaps, we can better understand the nuances of <strong><em>EP-Align</em></strong>’s performance, including its strengths in distinguishing closely related emotional states and its limitations.</p>
<p>Similarly, the confusion matrix for the RAF-DB dataset, shown in Fig.04, illustrates the classification results for compound emotions, which are inherently more complex due to the blending of multiple emotional states.
This matrix is particularly useful for assessing <strong><em>EP-Align</em></strong>’s effectiveness in resolving the intricacies of compound emotions.
For example, the misclassifications between ’happiness’ combined with ’surprise’ versus ’happiness’ combined with ’sadness’ indicate the challenges in distinguishing between subtle emotional blends.
Insights from this analysis guide further improvements in <strong><em>EP-Align</em></strong>’s algorithm to enhance its sensitivity to subtle emotional distinctions.</p>
</blockquote>
<h4 id="443impact-on-synthesized-speech-quality">4.4.3.Impact on Synthesized Speech Quality.<a class="headerlink" href="#443impact-on-synthesized-speech-quality" title="Permanent link">&para;</a></h4>
<blockquote>
<p>To empirically verify the impact of <strong><em>EP-Align</em></strong> on synthesized speech quality, we conducted a detailed analysis comparing the performance of speech synthesis systems with and without <strong><em>EP-Align</em></strong>.
As Tab.07 shows, integrating <strong><em>EP-Align</em></strong> results in notable improvements across all evaluated metrics.
Specifically, the reduction in WER from 8.50% to 7.35% and in CER from 3.90% to 3.07% suggest that <strong><em>EP-Align</em></strong> significantly enhances the phonetic precision of the synthesized speech.
Furthermore, the increase in MOS from 4.12 to 4.37 highlights an improvement in the overall subjective listening experience of the synthesized speech.
These improvements suggest that <strong><em>EP-Align</em></strong> effectively adjusts the alignment of phonetic elements, which is crucial for producing clear and accurate speech.
This capability not only enhances the intelligibility of the speech output but also its naturalness, thus contributing positively to user experience.</p>
</blockquote>
<h2 id="5conclusions">5.Conclusions<a class="headerlink" href="#5conclusions" title="Permanent link">&para;</a></h2>
<blockquote>
<p>In this work, we introduce <strong><em>MM-TTS</em></strong>, a multimodal framework that revolutionizes emotional speech synthesis by harnessing textual, auditory, and visual information.
The <strong><em>EP-Align</em></strong> module ensures seamless emotional feature alignment across modalities through contrastive learning, while <strong><em>EMI-TTS</em></strong> elegantly incorporates these features into cutting-edge TTS models.
The resulting emotionally rich speech closely mirrors human emotional expression, as demonstrated by our rigorous evaluations.
<strong><em>MM-TTS</em></strong> surpasses traditional <strong>E-TTS</strong> models in both objective and subjective metrics, showcasing its ability to generate natural and emotionally resonant speech.
By open-sourcing our code and models, we aim to drive further innovation and contribute to the progress of emotionally intelligent speech synthesis.
<strong><em>MM-TTS</em></strong> sets a new standard for emotional speech synthesis, paving the way for more empathetic and engaging human-computer interactions across diverse applications.
In future work, we plan to integrate additional modalities like gesture and facial expression to further enhance emotional expressiveness.</p>
<p>Furthermore, we aim to apply our framework to low-resource languages and accented speech, expanding the reach of emotionally intelligent speech synthesis.</p>
</blockquote>
<h2 id="6supplementary-materials">6.Supplementary Materials<a class="headerlink" href="#6supplementary-materials" title="Permanent link">&para;</a></h2>
<blockquote>
<p>This section complements the comprehensive discussions presented in our paper on the Multi-Modal Text-to-Speech (MM-TTS) framework.
The purpose of this content is to provide additional technical details, empirical evidence, and demonstrative examples that support the innovative approaches we have developed for emotion embedding-induced speech synthesis.
Within this supplementary material, readers will find extensive evaluations, interactive demonstrations, and comparative analyses that illustrate the effectiveness and versatility of the <strong><em>MM-TTS</em></strong> framework across various application scenarios.
The additional demonstrations are organized into three primary demos:</p>
<p>(1) Voiceover Scenario Demonstrations: This demo presents visual and auditory examples that showcase the <strong><em>MM-TTS</em></strong> framework’s ability to synthesize emotionally resonant voice from multimodal inputs.
The demonstrations highlight the framework’s capacity to effectively integrate and interpret emotional cues from visual (facial expressions), auditory (tone of voice), and textual (script context) data sources to generate context-appropriate speech outputs.</p>
<p>(2) Emotional Text-to-Speech Synthesis Comparisons: In this demo, we provide a series of comparative demonstrations that evaluate the emotional speech outputs generated by our <strong><em>EMI-TTS</em></strong> system against those produced by traditional emotional TTS approaches.
These comparisons aim to showcase the enhanced expressiveness and naturalness of speech synthesized through our framework, emphasizing the improvements achieved over existing technologies.</p>
<p>(3) Zero-Shot Emotional Speech Synthesis: This demo focuses on demonstrating the zero-shot capabilities of our framework by presenting synthesized speech outputs that reflect complex and compound emotions not explicitly encountered during the training process.
These examples underscore the adaptability and creative potential of the <strong><em>MM-TTS</em></strong> framework, highlighting its ability to enable personalized and dynamic speech generation applications.</p>
<p>Note that each demo is supplemented with relevant figures, audio samples, and detailed annotations to facilitate a comprehensive understanding of the methodologies employed and the results obtained.
The provided materials are intended to assist researchers and practitioners in exploring the full capabilities of the <strong><em>MM-TTS</em></strong> framework and its potential applications in real-world scenarios.</p>
</blockquote>
<h3 id="61implementation-details-of-emotion-embedding-induced-tts">6.1.Implementation Details of Emotion Embedding-Induced TTS<a class="headerlink" href="#61implementation-details-of-emotion-embedding-induced-tts" title="Permanent link">&para;</a></h3>
<blockquote>
<p>This section presents implementation details of the emotion embedding-induced text-to-speech (EMI-TTS) component, mentioned in Section 3.2 of the original paper, as shown in Fig.02, which aims to introduce emotional expression into text-to-speech (TTS) synthesis.</p>
<p>The <strong><em>EMI-TTS</em></strong> component is implemented across three TTS architectures: Variant VITS, Variant FastSpeech2, and Variant Tacotron2.</p>
<p>The following subsections describe the integration of emotion embedding into each architecture, outlining key enhancements to support expressive speech synthesis.</p>
</blockquote>
<h4 id="611variants-vits">6.1.1.Variants VITS.<a class="headerlink" href="#611variants-vits" title="Permanent link">&para;</a></h4>
<blockquote>
<p>Variant VITS models incorporate text-to-speech (TTS) architecture and emotion-aware processing technology.
These elements are integrated to achieve speech synthesis with emotional content.
The model includes an effectively conditioned flow, a structure designed to incorporate affective context into the speech synthesis process.
The model is inspired by the architecture of WaveGlow [33] and Glow-TTS [20], using affine coupling layers containing Emotional WaveNet residual blocks.
These blocks capture and express emotional nuances in synthesized speech.</p>
<p>The Variant VITS model, as shown on the left of the Emotion embedding-induced TTS (EMI-TTS) part of Fig.02, uses global conditioning techniques [43] to integrate emotional information into the speech synthesis pipeline.
These techniques insert emotion embeddings, represented as $u^{emo}$, into components such as the Spectrogram Encoder and Emotional WaveNet (EWN) in Emotion-condition FLow.
The integration process is controlled by the following operations:</p>
</blockquote>
<p>$$
$$</p>
<blockquote>
<p>where $AX()$ represents the affine Xform operator in the emotional conditional flow, $h$ represents the initial hidden feature, and $h'$ is the updated hidden feature after single-flow layer transformation.</p>
<p>This approach propagates emotional information throughout the network, allowing the synthesized speech to reflect the intended emotional state.</p>
<p>The Variant VITS model also includes a linear layer that converts emotion embeddings before adding them to the length modifier $h_{lg}$ and the emotion vocoder $h_{ac}$ .
This transformation infuses emotion into these components, as shown in the following equation:</p>
</blockquote>
<p>$$
$$</p>
<blockquote>
<p>where this process modifies the pitch, prosodic aspects, duration, and sound texture of the synthesized speech to match the desired emotional characteristics.
The full implementation of the Variant VITS model is publicly available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/VITS/model/models.py.</p>
</blockquote>
<h4 id="612variant-fastspeech2">6.1.2.Variant FastSpeech2.<a class="headerlink" href="#612variant-fastspeech2" title="Permanent link">&para;</a></h4>
<blockquote>
<p>The FastSpeech2 architecture is improved by the Variant FastSpeech2 model, as shown in the middle of the Emotion embedding-induced TTS (EMI-TTS) part of Fig.02, resulting in advancements in expressive speech synthesis.
This model incorporates a Conditional Cross-Attention mechanism, inspired by the EmoSpeech approach [6], to manage the interplay between speaker identity and emotional tone.
The integration of these elements aims to enhance emotional modulation in the synthesized speech.</p>
<p>The architecture employs a technique that combines the aligned emotion embedding $u^{emo}$ with the speaker embedding.
These embeddings are concatenated to form a unified conditioning feature, denoted as $c$.
The dimension of the conditioning feature matches the dimension $d$ of the model’s hidden states, with $h_{lg}$ used for textual processing and $h_{ac}$ for acoustic features.
The embeddings are processed using learnable matricesW𝑞,W𝑘, andW $v$ , which compute queries, keys, and values, respectively, as shown in the following equations:</p>
</blockquote>
<p>$$
$$</p>
<blockquote>
<p>where the attention mechanism applies a softmax function to the scaled dot product of $Q$ and $K^{\mathsf{T}}$.
The scaling is performed by dividing the dot product by the square root of the dimension $d$, as shown in the following equation:</p>
</blockquote>
<p>$$
$$</p>
<blockquote>
<p>where this operation reweights the contributions of the emotion and speaker embeddings within each attention layer, allowing their integration into the hidden states and features.
The impact of this integration can be observed in components such as the Duration Predictor and the Mel-spectrogram Decoder.</p>
<p>Particularly, the attentional mechanism in the Variant FastSpeech2 model aims to modulate speech outputs to reflect the intended emotional states.
The model strives to balance naturalness and clarity in the synthesized speech, which is relevant for applications in voice-assisted technologies and interactive systems where emotional resonance is important.
The implementation details of the Variant FastSpeech2 model, including configuration files and source code, are made publicly available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/FastSpeech2/.</p>
</blockquote>
<h4 id="613variation-tacotron2">6.1.3.Variation Tacotron2.<a class="headerlink" href="#613variation-tacotron2" title="Permanent link">&para;</a></h4>
<blockquote>
<p>The Tacotron2 framework has evolved into the Variant Tacotron2 model, as shown in the right of the Emotion embedding-induced TTS (EMI-TTS) part of Fig.02, which incorporates emotional expressions into its speech synthesis process.
The model integrates aligned emotion embeddings $u^{emo}$ by concatenating them with character-encoded hidden states $h_{lg}$ and speaker embeddings $u_{spk}$ .
This approach allows emotional context to be taken into account throughout the synthesis process, thereby affecting the prosody and intonation of the speech:</p>
</blockquote>
<p>$$
$$</p>
<blockquote>
<p>where this adjustment enhances the traditional position-sensitive attention mechanism and transforms it into a position &amp; emotion-sensitive attention system.
New attention mechanisms address the spatial and emotional components of speech synthesis, producing output that maintains natural intonation and is consistent with the desired emotional tone.</p>
<p>The process involves the following steps:
(1) Concatenation of embeddings: Aligned emotion embeddings $u^{emo}$ concatenated with hidden states derived from character encoders and speaker embeddings.
This composite feature vector is used as input to the attention mechanism.
(2) Enhanced attention mechanism: The attention mechanism has been updated to incorporate emotional cues and adjust how it processes text and sound information.
This modification enables the model to capture a wider range of emotional nuances.
(3) Integrated into synthesis: Adjustments to the attention mechanism directly impact the speech synthesis pipeline, affecting elements such as phoneme duration and intonation patterns to align with the intended emotional context.</p>
<p>These enhancements are designed to improve emotional speech synthesis, enabling TTS systems to generate output that reflects a wider range of emotions.
Integrating emotional cues into the synthesis process can facilitate applications where emotional expression is crucial.
Implementation details, including source code and configuration files, are available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/Tacotron2/.</p>
</blockquote>
<h3 id="62more-demonstrations">6.2.More Demonstrations<a class="headerlink" href="#62more-demonstrations" title="Permanent link">&para;</a></h3>
<blockquote>
<p>To further demonstrate the capabilities and potential applications of the proposed <strong><em>MM-TTS</em></strong> framework, we have developed an interactive demo available at https://anonymous.4open.science/api/repo/MMTTS-D214/file/demo/index.html?v.
These demos showcase various use cases, emotional speech samples, comparisons with other models, and the framework’s zero-shot abilities.</p>
</blockquote>
<h4 id="621voiceover-scenario">6.2.1.Voiceover Scenario.<a class="headerlink" href="#621voiceover-scenario" title="Permanent link">&para;</a></h4>
<blockquote>
<p>In the Voiceover Scenario module, as shown in Fig.05, we illustrate how the Emotion Prompt Alignment Module (EPAlign) can extract emotional cues from multimodal scenarios, enabling the Emotion Embedding-Induced TTS (EMI-TTS) to generate contextually appropriate emotional speech.
This capability is particularly valuable in applications that require voiceovers or narrations to seamlessly align with the emotional tone and context of multimedia content.</p>
<p>This Fig.05 presents a dialogue scene from a movie clip, with the left four columns representing multimodal references for emotion extraction: the origin video without sound, the origin face image, the origin speech audio, and the origin text transcript.
The fifth column displays the inferred aligned emotion representation $u^{emo}$’s class, where $u^{emo}$ is obtained through the EPAlign module by aligning the emotional cues from these multimodal inputs into a shared embedding space.
The final two columns showcase the emotional speech generated by <strong><em>EMI-TTS</em></strong> for two different speakers, effectively conveying the identified emotion while preserving the respective speaker characteristics.</p>
<p>By leveraging the complementary emotional information present across various modalities, such as visual cues (character expressions and scene visuals), auditory cues (speech waveforms), and textual cues (dialogue transcripts), EPAlign can disentangle and align the intricate emotional nuances exhibited in the multimedia content.</p>
<p>This aligned emotion representation is then seamlessly integrated into the <strong><em>EMI-TTS</em></strong> component, enabling the generation of emotional speech that accurately captures and reflects the intended affective tone within the given context.</p>
<p>The Voiceover Scenario module exemplifies the power of the <strong><em>MM-TTS</em></strong> framework in generating emotionally resonant voiceovers and narrations for multimedia applications.
By effectively aligning and fusing emotional cues from multiple modalities, <strong><em>MM-TTS</em></strong> can produce voiceovers that not only convey the desired emotional expressions but also maintain consistency with the overall emotional context of the multimedia content, thereby enhancing the immersive experience for end-users.</p>
</blockquote>
<h4 id="622emotional-text-to-speech-synthesis">6.2.2.Emotional Text-to-Speech Synthesis.<a class="headerlink" href="#622emotional-text-to-speech-synthesis" title="Permanent link">&para;</a></h4>
<blockquote>
<p>The Emotional Text-toSpeech Synthesis demo, as shown in Fig.06, presents randomly selected emotional speech samples generated by <strong><em>EMI-TTS</em></strong>, along with comparative samples from other models [6,23].
This comparison highlights <strong><em>EMI-TTS</em></strong>’s ability to provide a unified framework that integrates various TTS architectures, such as Tacotron2 [38], VITS[21], and FastSpeech2 [36].
By leveraging the EPAlign and <strong><em>EMI-TTS</em></strong> components, <strong><em>MM-TTS</em></strong> demonstrates enhanced emotional speech generation capabilities, outperforming traditional approaches.</p>
<p>For this demo, we randomly selected 20 text samples with associated emotion labels, comprising two sentences from each of 10 different speakers.
These text samples were then used to generate emotional speech using three variants of <strong><em>MM-TTS</em></strong> (corresponding to the integrated TTS architectures), as well as VITS (label), EmotionalTTS [23], and EmoSpeech [6] for a comprehensive evaluation.</p>
<p>By presenting this diverse set of emotional speech samples, we aim to demonstrate the effectiveness of the proposed <strong><em>MM-TTS</em></strong> framework in capturing and conveying a wide range of emotional expressions across various TTS architectures.
The comparison not only showcases the naturalness and expressiveness of the generated speech but also highlights the ability of <strong><em>MM-TTS</em></strong> to outperform traditional approaches in terms of emotional speech generation capabilities.</p>
<p>Moreover, this demo underscores the versatility and modularity of the <strong><em>MM-TTS</em></strong> framework, as it seamlessly integrates multiple TTS architectures while leveraging the EPAlign and <strong><em>EMI-TTS</em></strong> components to enhance emotion representation and synthesis.
This flexibility enables researchers and practitioners to leverage the strengths of different TTS architectures while benefiting from the improved emotional speech generation capabilities offered by the <strong><em>MM-TTS</em></strong> framework.</p>
</blockquote>
<h4 id="623zero-shot-emotional-speech">6.2.3.Zero-Shot Emotional Speech.<a class="headerlink" href="#623zero-shot-emotional-speech" title="Permanent link">&para;</a></h4>
<blockquote>
<p>The zero-shot emotional speech demo, as shown in Fig.07, showcases the zero-shot generalization capabilities of <strong><em>MM-TTS</em></strong> by generating emotional speech guided by complex, compound emotions.
By aligning emotional prompts within the shared emotion space using EPAlign, <strong><em>EMI-TTS</em></strong> can synthesize emotional speech for emotion categories unseen during training.
This ability opens up new avenues for creative expression and personalization in emotional speech synthesis, enabling users to craft tailored emotional expressions beyond those encountered in the training data.</p>
<p>Through this interactive demo, we aim to provide researchers and practitioners with a comprehensive understanding of <strong><em>MM-TTS</em></strong>’s potential applications, showcasing its ability to generate contextually appropriate, high-quality emotional speech across various scenarios.
By addressing the challenges of multimodal emotion disentanglement and alignment, <strong><em>MM-TTS</em></strong> represents a significant step forward in emotional speech synthesis, with far-reaching implications for human-computer interaction, multimedia content creation, and beyond.</p>
</blockquote>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
    
  </body>
</html>