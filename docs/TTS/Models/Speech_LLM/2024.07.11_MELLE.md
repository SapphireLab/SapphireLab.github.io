# MELLE

<details>
<summary>基本信息</summary>

- 标题: Autoregressive Speech Synthesis without Vector Quantization
- 作者:
  - 01 [Lingwei Meng](../../Authors/Lingwei_Meng.md)
  - 02 [Long Zhou](../../Authors/Long_Zhou_(周龙).md)
  - 03 [Shujie Liu](../../Authors/Shujie_Liu_(刘树杰).md)
  - 04 [Sanyuan Chen](../../Authors/Sanyuan_Chen_(陈三元).md)
  - 05 [Bing Han](../../Authors/Bing_Han.md)
  - 06 [Shujie Hu](../../Authors/Shujie_Hu.md)
  - 07 [Yanqing Liu](../../Authors/Yanqing_Liu.md)
  - 08 [Jinyu Li](../../Authors/Jinyu_Li_(李劲宇).md)
  - 09 [Sheng Zhao](../../Authors/Sheng_Zhao_(赵胜).md)
  - 10 [Xixin Wu](../../Authors/Xixin_Wu.md)
  - 11 [Helen Meng](../../Authors/Helen_Meng_(蒙美玲).md)
  - 12 [Furu Wei](../../Authors/Furu_Wei_(韦福如).md)
- 机构:
  - [香港中文大学](../../Institutions/CUHK_香港中文大学.md)
  - [Microsoft](../../Institutions/Microsoft.md)
- 时间:
  - 预印时间: 2024.07.11 ArXiv v1
  - 更新笔记: 2024.07.12
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2407.08551)
  <!-- - [DOI]() -->
  <!-- - [Github]() -->
  - [Demo](https://aka.ms/melle)
  <!-- - [Scholar](https://scholar.google.com/scholar?cluster=) -->
- 标签:
  - [自回归](../../Tags/Autoregressive.md)
- 页数: 17
- 引用: ?
- 被引: ?
- 数据:
  - 训练 [Libriheavy](../../Datasets/2023.09.15_Libriheavy.md) MELLE
  - 训练 [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) MELLE-limited
- 对比:
  - [ELLA-V](../../Models/Speech_LLM/2024.01.14_ELLA-V.md)
  - [VALL-E R](../../Models/Speech_LLM/2024.06.12_VALL-E_R.md)
  - [RALL-E](../../Models/Speech_LLM/2024.04.04_RALL-E.md)
  - [CLaM-TTS](../../Models/Speech_LLM/2024.04.03_CLaM-TTS.md)
  - [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md)
  - [VALL-E 2](../../Models/Speech_LLM/2024.06.08_VALL-E2.md)
  - [Voicebox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md)
- 复现:
  - ?

</details>

## Abstract: 摘要

> We present ***MELLE***, a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS).
> ***MELLE*** autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms.
> Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens. (ii) we have incorporated variational inference into ***MELLE*** to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness.
> Experiments demonstrate that, compared to the two-stage codec language models VALL-E and its variants, the single-stage ***MELLE*** mitigates robustness issues by avoiding the inherent flaws of sampling discrete codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm.
> See https://aka.ms/melle for demos of our work.

## 1.Introduction: 引言

> The objective of next-token prediction, which involves predicting the next discrete token based on the previous tokens as condition, is foundational to the recent progress observed in autoregressive large language models (LLMs).
> Recently, the success of LLMs in natural language processing (NLP) tasks has encouraged the exploration of autoregressive language modeling approaches in audio synthesis fields ([AudioLM](../../Models/Speech_LLM/2022.09.07_AudioLM.md); [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md)). 
> Neural codec language models, exemplified by [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) and [VALL-E X](../../Models/Speech_LLM/2023.03.07_VALL-E_X.md), reveal the potential of such principle in the zero-shot text-to-speech (TTS) task via leveraging large-scale multi-lingual multi-speaker multi-domain training corpus. 
> Unlike traditional TTS systems that rely heavily on complex and multi-step pipelines, they utilize a decoder-only approach to predict neural codec codes, which are discrete tokens encoded from continuous waveforms leveraging neural codec models ([SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md); [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md)). 
>
> Although achieving impressive naturalness and diversity in synthesized audios, VALL-E and its variants are plagued by following several drawbacks.
> First, neural codec codes, originally designed for audio compression, exhibit lower fidelity compared to the well-established mel-spectrogram \citep{puvvada2024discrete}.
> This phenomenon is also observed in the field of graphics, where the reconstruction quality of vector-quantized tokenizers typically lags behind that of their continuous-valued counterparts \citep{rombach2022high,huang2023not,kaiming2024autoregressive}.
> Second, the codec language model VALL-E suffers from robustness issues stemming from its random sampling strategy, which is inherited from text language model for selecting discrete tokens.
> This issue is more pronounced with acoustic tokens as opposed to textual ones due to the greater similarity among consecutive codec codes, which can lead to continuous stretches of silence or persistent noise ([VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md); [ELLA-V](../../Models/Speech_LLM/2024.01.14_ELLA-V.md)).
> Third, neural codec language models typically necessitate a complicated two-pass decoding process, involving an autoregressive (AR) model for generating coarse primary audio tokens, followed by a non-autoregressive (NAR) model to iteratively predict the rest multiple codebook codes for refinement. 
> This multi-step procedure compromises inference efficiency, leading to increased computational demands and doubled storage requirements.
> 
> To address the limitations associated with discrete tokens based codec language models, we are rethinking the potential of continuous speech representations and aim to determine whether continuous-valued tokens can supplant discrete-valued tokens within the paradigm of autoregressive speech synthesis models.
> The successful implementation of the autoregressive model without discrete vector quantization faces two key challenges.
> - **How to set training objective for continuous representation?**
> The continuous space significantly differs from that of vector-quantized tokens, for which autoregressive language models typically adopt a next-token prediction objective, with cross-entropy loss to measure the discrepancy between the predicted distribution and the ground truth. 
> - **How to enable sampling mechanism in continuous space?**
> The sampling strategy is a critical component in both text generation and speech synthesis systems, as it introduces diversity into the output and enhances the generalization ability.
> However, continuous-valued tokens based models can not employ top-p random sampling method used in discrete codec language models.
>
> In this work, we propose a continuous mel-spectrogram\footnote{We leave the exploration of other continuous representations, such as VAE latent hidden states, for future research endeavors.} based autoregressive language model (called ***MELLE***) for text-to-speech synthesis, as illustrated in Figure \ref{fig:overview}. ***MELLE*** is a robust single-pass zero-shot TTS model which autoregressively predicts mel-spectrogram frames based on previous mel-spectrogram and text tokens, thereby avoiding the inherent flaws associated with sampling discrete codec codes. 
> The mel-spectrogram is then converted into waveform utilizing an off-the-shelf vocoder. 
> In response to the aforementioned challenges, we first substitute cross-entropy loss with regression loss and introduce a spectrogram flux loss to promote variation of predicted mel-spectrograms and eliminate repetition issues.
> Second, we design a latent sampling module, derived from variational inference, functions as a sequence sampling strategy thereby enhancing the diversity of the generated audio samples. 
> As an option, by adjusting the reduction factor, ***MELLE*** can predict multiple frames at one step and accelerate inference, thereby further alleviating the robustness issues associated with long-sequence modeling and maintaining satisfactory performance. 
>
> We conducted evaluations of our ***MELLE*** on both the large-scale 50K-hour [Libriheavy](../../Datasets/2023.09.15_Libriheavy.md) training dataset and the relatively small 960-hour [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) training dataset. 
> Following recent works, we use LibriSpeech test-clean set for zero-shot TTS evaluation.
> Experimental results demonstrate that the proposed ***MELLE*** is on par with [VALL-E 2](../../Models/Speech_LLM/2024.06.08_VALL-E2.md) in objective metrics, and surpasses VALL-E 2 in subjective metrics.
> It also outperforms previous neural codec language models, including VALL-E and its other variants, achieving superior performance across multiple metrics that reflect naturalness, robustness, similarity, and inference efficiency. 
>
> Specifically, ***MELLE*** surpasses the ground truth audios in WER (1.47\% vs. 1.61\%), achieving a 47.9\% relative reduction in WER compared to VALL-E and an 8.1\% reduction compared to VALL-E 2 on the continuation inference task for zero-shot TTS. 
> For subjective evaluations, ***MELLE*** is more favorably received by human listeners than previous models, achieving comparable performance to the original ground truth in terms of MOS (4.20 vs. 4.29) and CMOS (-0.032 for ours vs. ground truth), and an even higher SMOS (4.40 vs. 3.94) than the ground-truth speech.

## 2.Related Works: 相关工作

### Traditional TTS

> Traditional speech synthesis methods can be categorized concatenative systems, parametric systems, and end-to-end neural systems.
> Concatenative TTS systems deconstruct original audio waves into smaller segments and then reassembles them using algorithms like Viterbi, followed by signal processing techniques, to create new audio waves.
> Parametric TTS systems convert speech waves into spectrograms, utilizing acoustic parameters such as fundamental frequency and duration to synthesize new audio outputs.
> With the rapid development of neural networks, end-to-end neural TTS systems are proposed to simplify previous speech synthesis pipeline via a single neural network, eliminating the need for the production of these linguistic and acoustic features ([Tacotron](../../Models/TTS2_Acoustic/2017.03.29_Tacotron.md); [TransformerTTS](../../Models/TTS2_Acoustic/2018.09.19_Transformer_TTS.md); [FastSpeech](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md)).
>
> The advanced end-to-end neural TTS models, such as [Tacotron](../../Models/TTS2_Acoustic/2017.03.29_Tacotron.md), [TransformerTTS](../../Models/TTS2_Acoustic/2018.09.19_Transformer_TTS.md), and [FastSpeech](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md), usually generate mel-spectrograms directly from texts, then synthesize the audio results from the mel-spectrogram by a vocoder such as [WaveNet](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md).
> TransformerTTS employs Transformer-based encoder-decoder network as the main framework to replace RNN structures in Tacotron.
> FastSpeech further improve the speech quality and decoding efficiency using the non-autoregressive generation model with a duration module.
> These model are trained on small-scale, clean, single-speaker or few-speaker dataset, such as LJSpeech and LibriTTS.
> Our ***MELLE*** leverages the well-established mel-spectrogram as the intermediate representation, however, it differs significantly in two key aspects: 
> (1) we adopt decoder-only network as foundational structure with improved methods, such as variational inference and spectrogram flux loss, 
> (2) ***MELLE*** is capable of zero-shot TTS via training on large-scale speech-text paired data, like [Libriheavy](../../Datasets/2023.09.15_Libriheavy.md).

### Zero-Shot TTS

> Motivated by the zero-shot and in-context learning capabilities of large language models (LLMs) on natural language processing (NLP) tasks, various research works are proposed to address zero-shot TTS through a language modeling approach.
> [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) first regards TTS tasks as a conditional language task, which utilizes neural codec codes as intermediate representation instead of mel-spectrogram, then uses a codec decoder to recover the waveform from predicted codec codes.
> VALL-E employs two-stage modeling method, with an autoregressive model for generating coarse audio tokens, followed by a non-autoregressive model to iteratively predict multi-codebook codes for refinement.
> [VALL-E X](../../Models/Speech_LLM/2023.03.07_VALL-E_X.md) extends VALL-E into multi-lingual scenario to support zero-shot cross-lingual speech synthesis and speech-to-speech translation.
> [Mega-TTS](../../Models/Speech_LLM/2023.06.06_Mega-TTS.md) proposes to disentangle the multiple attributes in speech, such as content, timbre, prosody, and phase attributes, then model each of them according to their intrinsic properties with a language modeling approach.
> [ELLA-V](../../Models/Speech_LLM/2024.01.14_ELLA-V.md), [RALL-E](../../Models/Speech_LLM/2024.04.04_RALL-E.md), and [VALL-E R](../../Models/Speech_LLM/2024.06.12_VALL-E_R.md) aims to improve robustness and stability of VALL-E via additional fine-grained speech-text alignment information. 
> [BASE TTS](../../Models/Speech_LLM/2024.02.12_BASE-TTS.md) employs discrete tokens derived from [WavLM](../../Models/Speech_Representaion/2021.10.26_WavLM.md) and scales the codec language model to larger parameters and training data.
> [Seed-TTS](../../Models/Speech_LLM/Seed-TTS.md) replaces NAR model in VALL-E with a diffusion model, which generates continuous speech representations according generated speech tokens from AR stage. 
> In parallel to our work, [VALL-E 2](../../Models/Speech_LLM/2024.06.08_VALL-E2.md) shares the same architecture as VALL-E but employs a repetition-aware sampling strategy that promotes more deliberate sampling choices.
>
> Other studies have investigated fully non-autoregressive approaches to increase the speed of model inference.
> For instance, [SoundStorm](../../Models/Speech_LLM/2023.05.16_SoundStorm.md) adapts a parallel, non-autoregressive, confidence-based decoding scheme for the generation of codec codes.
> [StyleTTS2](../../Models/Speech_LLM/2023.06.13_StyleTTS2.md) and [NaturalSpeech 3](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md) use diffusion model to achieve better TTS synthesis.
> [Voicebox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md) and [Audiobox](../../Models/Speech_LLM/2023.12.25_Audiobox.md) employ non-autoregressive flow-matching based models for transcript-guided speech generation.
> Recently, [E2 TTS](../../Models/_tmp/2024.06.26_E2_TTS.md) presents a fully non-autoregressive TTS systems consisting of flow-matching-based mel-spectrogram generator trained with the audio infilling task, and a vocoder.
> Different previous works, ***MELLE*** leverages continuous-valued tokens based autoregressive language modeling approach with variational inference for text-to-speech synthesis. 

## 3.Methodology: 方法

### 3.1.Problem Formulation: Mel-Spectrogram Language modeling

> This study regards zero-shot TTS as an autoregressive mel-spectrogram language modeling task.
> Instead of predicting highly-compressed neural codec codes like VALL-E and its variants, ~***MELLE*** directly predicts continuous mel-spectrograms which are then converted to waveforms with an off-the-shelf vocoder.
> Taking well-established mel-spectrogram as the training target, it strives to achieve higher fidelity and naturalness.
>
> Given an audio sample with byte-pair-encoded (BPE) text content $\bm{x} = [x_0, x_1, \ldots, x_{L-1}]$, ***MELLE*** is optimized to predict the mel-spectrogram $\bm{y} = [\bm{y}_0, \bm{y}_1, \ldots, \bm{y}_{T-1}]$ extracted from the corresponding audio.
> Specifically, at each autoregressive step, ***MELLE*** is expected to predict the next mel-spectrogram frame $\bm{y}_{t}$ conditioned on the text prompt $\bm{x}$ and the previous generated mel-spectrograms $\bm{y}_{<t}$, which is equivalent to maximizing the following distribution:
>
> $$
> p(\bm{y} \mid \bm{x}; \theta) = \prod_{t=0}^{T-1} p(\bm{y}_t \mid \bm{y}_{<t}, \bm{x}; \theta)\tag{01}
> $$
>
> where $\bm{y}_{<t}$ denotes $[\bm{y}_0, \bm{y}_1,..., \bm{y}_{t-1}]$ and $\theta$ represents the parameters of ***MELLE***. 
>
> Inspired by previous neural TTS models ([TransformerTTS](../../Models/TTS2_Acoustic/2018.09.19_Transformer_TTS.md)), we can also set a reduction factor $r$ (e.g., 2 or 4) to control the number of mel-spectrogram frames predicted at each decoding step, providing a balance between computational efficiency and the generation of high-quality speech.
> Formally, the original mel-spectrogram sequences $\bm{y}$ will be partitioned into $\bm{y}^r = [\bm{y}_{0:r}, \bm{y}_{r:2r}, ..., \bm{y}_{(T-r):T}]$ with reduction factor $r$, and the likelihood function can be expressed as follows:
>
> $$
> p(\bm{y} \mid \bm{x}; \theta) = \prod_{t=0}^{T/r-1} p(\bm{y}_{t\cdot r:(t+1)\cdot r} \mid \bm{y}_{<t\cdot r}, \bm{x}; \theta)\tag{02}
> $$
>
> During inference, ***MELLE*** executes zero-shot TTS tasks via prompting like VALL-E.
> Given the text content $\bm{x}$ for synthesis, text transcription $\tilde{\bm{x}}$ and mel-spectrogram $\tilde{\bm{y}}$ of speech prompt, the model is designed to generate the target mel-spectrogram $\bm{y}$ of the corresponding content while preserving the characteristics of the original speaker in prompt, with maximum likelihood probability as $\argmax_{\bm{y}} p(\bm{y}_{t\cdot r:(t+1)\cdot r} \mid [\tilde{\bm{x}}; \bm{x}; \tilde{\bm{y}}; \bm{y}_{<t\cdot r}]; \theta)$ at each step, where $[;]$ means concatenation operation and it backs to standard mode if $r=1$.

### 3.2.Architecture

> As illustrated in Figure \ref{fig:overview}, ***MELLE*** comprises the following main components: pre-nets that respectively convert text into sub-word tokens and extract mel-spectrograms from speech, before projecting them to the model dimension; an autoregressive (AR) Transformer decoder that serves as the language model; a latent sampling module that samples latent embedding from a predicted Gaussian distribution, and then projects it back to the spectrogram space; a stop prediction layer that determines the end of the speech and a convolutional post-net for spectrogram refinement, similar to the methods described in ([Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md); [TransformerTTS](../../Models/TTS2_Acoustic/2018.09.19_Transformer_TTS.md)).
> Finally, a vocoder is used to recover the speech from generated mel-spectrogram. 
>
> Unlike neural codec language models that iteratively predict multi-layer codec codes, we do not require an additional non-autoregressive (NAR) model thanks to the completeness of the mel-spectrogram.
> This simplification significantly improve computational and storage efficiency.
> Moreover, by adjusting the reduction factor, ***MELLE*** can generate multiple mel-spectrogram frames at one step, further enhancing efficiency while still maintaining superior performance.

#### 3.2.1.Autoregressive Language Model

> We employ a unidirectional Transformer decoder as the language model (LM) to autoregressively generates acoustic continuous features based on the textual input and acoustic prompts.
> Specifically, input text tokens $\bm{x}$, with an appended <EOS> token, are first converted into embeddings by the text embedding layer based on their indices. 
> Simultaneously, we employ a multi-layer perceptron, named pre-net, to project the mel-spectrogram $\bm{y}$ to the language model dimension.
> The LM, consisting of blocks of multi-head attention and feed-forward layers, takes the concatenation of text and acoustic embeddings as input to model the dependency between semantic and acoustic information.
> The output of the LM $\bm{e}_t$ at time step $t$ is subsequently processed by the following modules of ***MELLE*** to synthesize the next-frame output, which is detailed in the following section.

#### 3.2.2.Latent Sampling Module

> Sampling strategy is an critical part in TTS systems, as it not only introduces diversity in the output, but also enhances the generalization ability of the model. 
> For example, Tacotron-like models ([Tacotron](../../Models/TTS2_Acoustic/2017.03.29_Tacotron.md); [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md)) enable dropout in their pre-net during inference to introduce variation;
> Neural codec language models ([VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md); [VALL-E X](../../Models/Speech_LLM/2023.03.07_VALL-E_X.md)) adopt the top-p random sampling to avoid the collapse outputs leading by greedy search; Diffusion-based ([NaturalSpeech2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md); [NaturalSpeech3](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md)) and flow-matching-based methods ([Voicebox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md)) restore speech representations from the sampling of a simpler distribution. 
>
> In this study, inspired by variational autoencoders (VAEs) \citep{kingma2014vae, blei2017vi_review}, we integrate a novel latent sampling module within ***MELLE***, aimed at enhancing both expressive diversity and robustness, as shown in Figure \ref{fig:modules} (left).
> Based on $\bm{e}_t$, the output of the LM, this module predicts a distribution, from which a latent embedding $\bm{z}_t$ is sampled. %This latent embedding is then mapped to the mel-spectrogram space.
>
> Specifically, we assume that $\bm{z}_t$ follows a multivariate Gaussian distribution where each dimension is independent.
> As depicted in Figure \ref{fig:modules}, a linear layer ($\mathbf{W} [\cdot] + \mathbf{b}$) predicts a mean vector $\boldsymbol{\mu}_t$ and a log-magnitude variance vector $\log\bm{\sigma}_t^2$ of the multivariate Gaussian distribution based on $\bm{e}_t$.
> Leveraging the reparameterization technique, a $\bm{z}_t$ is sampled as
$$
\begin{aligned}
&\bm{z}_t = \bm{\mu}_t + \bm{\sigma}_t \odot \bm{\epsilon} \\ \text{where}\quad\bm{\epsilon} &\sim \mathcal{N}(0, \bm{I}), \quad  [\bm{\mu}_t, \log \bm{\sigma}_t^2] = \mathbf{W} \bm{e}_t + \mathbf{b}
\end{aligned}
$$

> Therefore, the probability density function can be defined as
> $$
> \begin{aligned}
> &p_\theta(\bm{z}_t \mid \bm{e}_t) = \mathcal{N}(\bm{z}_t \mid \bm{\mu}_t, \mathrm{diag}(\bm{\sigma}_t^2)) 
> \end{aligned}
> $$
>
> Note that the latent sampling module is differentiable with the reparameterization technique.
> Next, the latent variable $\bm{z}_t$ is passed through a multi-layer perceptron (MLP) with residual connections, mapping it to the mel-spectrogram space as $\bm{y}'_{t}$, where $t=0, 1, ..., T-1$.

#### 3.2.3.Stop Prediction Layer and Post-Net

> Since ***MELLE*** directly predicts continuous mel-spectrograms rather than discrete tokens, it cannot generate an <EOS> token to indicate the end of generation.
> Instead, we use a linear layer as a binary classifier, taking $\bm{e}_t$ to determine if the generation should conclude, as depicted in Figure \ref{fig:modules} (mid).
> Following previous neural TTS models ([Tacotron](../../Models/TTS2_Acoustic/2017.03.29_Tacotron.md); [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md)), we employ multiple convolutional blocks as the post-net to produce a residual that is added to $\bm{y}'=\{\bm{y}'_{0}, \bm{y}'_{1}, ..., \bm{y}'_{T-1}\}$, resulting in the refined mel-spectrogram $\bm{y}''=\{\bm{y}''_{0}, \bm{y}''_{1}, ..., \bm{y}''_{T-1}\}$, as shown in Figure \ref{fig:modules} (right). 
> During training, the model is trained using teacher-forcing; while during inference, post-net processes $\bm{y}'$ after the AR generation concludes.

### 3.3.Training Objective

> The training process of ***MELLE*** is efficient and straightforward, due to the absence of \mbox{VALL-E}'s complex hierarchical structure.
> As illustrated in Figure \ref{fig:overview}, during training, a single end-to-end autoregressive model is optimized with the teacher-forcing manner using four loss functions: (1) a regression loss; (2) a Kullback-Leibler (KL) divergence loss; (3) a novel spectrogram flux loss; and (4) a binary cross entropy (BCE) loss for stop prediction.
> They work collaboratively to enhance overall performance:

$$
\begin{aligned}
\mathcal{L} = \mathcal{L}_{\text{reg}} + \lambda \mathcal{L}_{\text{KL}} + \beta\mathcal{L}_{\text{flux}} + \gamma \mathcal{L}_{\text{stop}}
\end{aligned}
$$

#### Regression Loss

> The regression loss is a fundamental component of the training objective, ensuring the accurate prediction of mel-spectrogram frames, like conventional TTS ([TransformerTTS](../../Models/TTS2_Acoustic/2018.09.19_Transformer_TTS.md)).
> The regression loss, \(\mathcal{L}_{\text{reg}}\), is composed of a combination of L1 and L2 losses, applied to both intermediate prediction $\bm{y}'$ and final prediction $\bm{y}''$ of the mel-spectrogram.
> It is defined as follows:

$$
\begin{aligned}
\mathcal{L}_{\text{reg}}(\bm{y}, \bm{y}', \bm{y}'')
   &= \mathcal{L}_{\text{L1}}(\bm{y}, \bm{y}') + \mathcal{L}_{\text{L2}}(\bm{y}, \bm{y}') + \mathcal{L}_{\text{L1}}(\bm{y}, \bm{y}'') + \mathcal{L}_{\text{L2}}(\bm{y}, \bm{y}'') \notag \\
   &= \|\bm{y} - \bm{y}'\|_1 + \|\bm{y} - \bm{y}'\|_2^2 + \|\bm{y} - \bm{y}''\|_1 + \|\bm{y} - \bm{y}''\|_2^2
\end{aligned}
$$

> where $\bm{y}$ represents the ground-truth mel-spectrogram target.

#### KL Divergence Loss

> We introduce a Kullback-Leibler (KL) divergence loss based on the concept of variational inference \citep{kingma2014vae, blei2017vi_review}, to enhance the diversity and stability of ***MELLE***.
> The KL divergence measures the difference between the predicted latent distribution \( p_\theta(\bm{z}_t \mid \bm{e}_t) \) and a simpler distribution \( p(\bm{z}_t) \).
> Unlike \cite{kingma2014vae}, which selects \( p(\bm{z}_t) \) as a standard normal distribution, we let \(\bm{z}_t\) possess the same dimensionality as the mel-spectrogram and define \( p(\bm{z}_t) \) as \(\mathcal{N}(\bm{y}_t, \bm{I})\). 
> This can be seen as a shortcut on the optimization path thus accelerates the model's learning.
> Combining equation (\ref{eq:gaussian}), the KL divergence loss among $T$ time steps can be analytically computed as

$$
\begin{aligned}
\mathcal{L}_\text{KL}(\bm{y}, \bm{z}) &= \sum_{t=0}^{T-1} D_{\text{KL}}(p_\theta(\bm{z}_t \mid \bm{e}_t) \parallel p(\bm{z}_t)) \notag\\
&=\frac{1}{2} \sum_{t=0}^{T-1} \sum_{i=1}^d \left( \bm{\sigma}_t^2[i] + (\bm{\mu}_t[i]-\bm{y}_t[i])^2 - 1 - \log\bm{\sigma}_t^2[i] \right) 
\end{aligned}
$$

> where \( d \) is the dimensionality of the feature space.
> The detailed derivation is provided in Appendix \ref{appendix:derivation}.
> By integrating the KL divergence into the loss function, ***MELLE*** achieves a balance between reconstruction quality and latent space regularization, ultimately contributing to enhanced expressive diversity and robustness of the generated mel-spectrograms. 

#### The Spectrogram Flux Loss

> To encourage the dynamic variation of the generated mel-spectrogram frames, we propose a novel spectrogram flux loss as a regularization term.
> This loss function penalizes low variability between consecutive frames, thereby promoting changes and preventing the generation of overly static mel-spectrograms:

$$
\begin{aligned}
\mathcal{L}_{\text{flux}}(\bm{y}, \bm{\mu}) = - \sum_{t=1}^{T-1} \|\bm{\mu}_t - \bm{y}_{t-1}\|_1
\end{aligned}
$$

> where the L1 norm is employed to measure the difference between the predicted Gaussian mean vector $\bm{\mu}_t$ and the previous ground truth frame $\bm{y}_{t-1}$.
> By summing the negative value of the differences, the loss function rewards variations between frames, and the model is encouraged to reduces the generation of overly static frames that leading to repetition or endless silence in synthesized audio. 
>
> Additionally, by penalizing flat predictions, the model is incentivized to produce more diverse and dynamic frame sequences, thereby preventing monotonic and unnatural speech.

#### Stop Prediction Loss

> We use a linear layer to project the output of the LM to a logit and calculate the BCE loss, $\mathcal{L}_{\text{stop}}$, for stop prediction, like [TransformerTTS](../../Models/TTS2_Acoustic/2018.09.19_Transformer_TTS.md) and [SpeechT5](../../Models/_tmp/SpeechT5.md). 
> During this calculation, positive and negative frames are extremely imbalanced, as each utterance has only one positive frame indicating "stop." To address this, we impose a larger weight (100) for the positive frames in the BCE loss.

### 3.4.Inference: In-Context Learning for Mel-Spectrogram

> During inference, we perform zero-shot text-to-speech by autoregressively predicting the mel-spectrogram. 
> Given the text content $\bm{x}$ for synthesis, and a piece of speech prompt (with text transcription $\tilde{\bm{x}}$ and mel-spectrogram $\tilde{\bm{y}}$), at each time step \( t \), ***MELLE*** generates the next-frame mel-spectrogram \( \bm{y}_t' \) from a latent embedding \( \bm{z}_t \), which is sampled from a distribution conditioned on the concatenation of $\tilde{\bm{x}}$, $\bm{x}$, $\tilde{\bm{y}}$, and $\bm{y}_{<t}$.
> After the AR generation process concludes, the coarse mel-spectrogram \( \bm{y}' \) passes through the post-net to obtain the refined spectrogram \( \bm{y}'' \), which is then converted to speech audio using an off-the-shelf vocoder.
> If the reduction factor $r$ is set, the input and predicted mel-spectrograms will be grouped by that factor. 
>
> In contrast to neural codec language models (e.g., VALL-E) that rely on multi-stage iterative predictions across multi-layer codec codes and require the manual configuration of sampling parameters, ***MELLE*** accomplishes speech synthesis in a single forward pass and automatically samples from learned distributions that are unique to each input. 
> This automated approach ensures adaptive and consistent sampling, reduces human effort, and makes the method domain-independent.
> With the strong in-context learning capability from LLM, ***MELLE*** is capable of generating high-fidelity, natural-sounding speech for unseen speakers without fine-tuning. 

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this study, we propose a continuous acoustic representation-based language modeling approach for zero-shot text-to-speech synthesis tasks, thereby eliminating the use of discrete vector quantization.
> By exploring the potential of mel-spectrograms within the paradigm of language modeling, the proposed ***MELLE*** directly predicts mel-spectrograms conditioned on text content and speech prompt.
> This approach eliminates the need for the two-pass training and inference procedures typical of neural codec language model VALL-E, and can further accelerate decoding by setting the reduction factor. 
> With the aid of latent sampling and spectrogram flux loss, ***MELLE*** is capable of producing more diverse and robust predictions, attaining results comparable to human performance in subjective evaluations.

