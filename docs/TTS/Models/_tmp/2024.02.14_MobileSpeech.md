# MobileSpeech

<details>
<summary>基本信息</summary>

- 标题: MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech
- 作者:
  - 01 [Shengpeng Ji](../../Authors/Shengpeng_Ji.md)
  - 02 [Ziyue Jiang](../../Authors/Ziyue_Jiang.md)
  - 03 [Hanting Wang](../../Authors/Hanting_Wang.md)
  - 04 [Jialong Zuo](../../Authors/Jialong_Zuo.md)
  - 05 [Zhou Zhao](../../Authors/Zhou_Zhao_(赵洲).md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.02.14 ArXiv v1
  - 更新笔记: 2024.06.06
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2402.09378)
  - [DOI]()
  - [Github]()
  - [Demo](https://mobilespeech.github.io)
- 标签:
  - [语音合成](../../Tags/SpeechSynthesis.md)
  - [零样本](../../Tags/Zero-Shot.md)
  - [移动设备](../../Tags/MobileDevice.md)
- 页数: 13
- 引用: ?
- 被引: 1

</details>

## A.摘要
> Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts.
> However, all previous work has been developed for cloud-based systems.Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness.There-fore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorpo-rates hierarchical information from the speech codec and weight mechanisms across differ-ent codec layers during the generation process.
> Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at https://mobilespeech.github.io/ .

## 1.引言

> In recent years, remarkable progress has been made in the development of text-to-speech (TTS) technology ([FastSpeech (2019)](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md); [VITS (2021)](../../Models/E2E/2021.06.11_VITS.md); Huang et al., 2022a; Li et al., 2023). With advancements in this field, current state-of-the-art TTS systems have demonstrated the ability to generate high-quality voices of unseen speakers while maintaining con-sistency in tone and rhythm (Chen et al., 2021; [YourTTS (2021)](../../Models/E2E/2021.12.04_YourTTS.md)). However, these systems are often trained on datasets of only a few hundred hours and often require fine-tuning with dozens of sentences during the inference stage, which signifi-cantly limits the generative capabilities of the models and is time-consuming and not user-friendly.

> With the recent development of large-scale lan-guage models (Brown et al., 2020; Touvron et al.,2023) and the increase in training data for speech synthesis from hundreds of hours (Zen et al., 2019)to tens of thousands of hours (Kahn et al., 2020), current TTS models exhibit powerful zero-shot capabilities, enabling the cloning of unseen speak-ers’ voices in just a few seconds ([VALL-E (2023)](2023.01.05_VALL-E.md); [VALL-E X (2023)](../../Models/Speech_LLM/2023.03.07_VALL-E_X.md); [NaturalSpeech2 (2023)](../Diffusion/2023.04.18_NaturalSpeech2.md); [VoiceBox (2023)](2023.06.23_VoiceBox.md);Jiang et al., 2023b; [SPEAR-TTS (2023)](2023.02.07_SPEAR-TTS.md); [SoundStorm (2023)](../../Models/Speech_LLM/2023.05.16_SoundStorm.md); Huang et al., 2023; Yang et al., 2023b). However, these models primarily focus on in-context abilities and fail to address issues related to inference speed and model deployment parameters.
> Additionally, all existing zero-shot TTS models are cloud-based and lack a mobile-based deployment approach. MobileSpeech is the first zero-shot TTS synthesis system that can be deployed on mobile de-vices. Firstly we think deployable mobile zero-shot TTS models aim to achieve the following goals:
> - Fast: through our experiments, we have ob-served that the real-time factor (RTF) on a single mobile device often exceeds 8-10 times that of an A100 RTF for the same text. There-fore, the inference speed of the zero-shot TTS model must be significantly faster.
> - Lightweight: to enable deployment on mo-bile or edge devices, the model size should be small, and the runtime memory footprint should be minimal.
> - High similarity and diversity: zero-shot TTS system should be able to clone timbre and prosody with just a few seconds of prompts,yielding diverse speech even when provided with identical text.
> - High quality: to enhance the naturalness of synthesized speech, the model should pay attention to details such as frequency bins be-tween adjacent harmonics and possess power-ful duration modeling capabilities.
> - Robustness: a highly robust TTS system is essential. A zero-shot TTS system should minimize occurrences of missing or repeated words.

> To achieve these objectives, we have made our best efforts to reproduce several state-of-the-art generative models, which will be elaborated in the subsequent section on related work. We found cur-rent work is not applicable to mobile devices. Ul-timately, based on the Natspeech framework ([FastSpeech (2019)](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md), [FastSpeech2 (2020)](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md)) and the Mask generation module([MaskGIT (2022)](); [SoundStorm (2023)](../../Models/Speech_LLM/2023.05.16_SoundStorm.md)), we addi-tionally design the SMD module and the speaker prompt component, taking into account the discrete codec architecture of speech. 

> In summary, MobileSpeech contributes as follows:
> - MobileSpeech is the first zero-shot TTS syn-thesis system that can be deployed on mobile devices. MobileSpeech achieves a good bal-ance according to the above five evaluation metrics.
> - In MobileSpeech, we additionally design the SMD module based on the hierarchical token structure of discrete speech codecs and design the Speaker Prompt module to maintain high similarity, high quality, low latency.
> - Training MobileSpeech-english for 580 hours achieved SOTA inference speed and comparable audio quality. Training MobileSpeech-chinese for 40,000 hours resulted in SOTA performance in all aspects.
> - MobileSpeech has been successfully deployed on mobile phones and is expected to be used by several hundred thousand users.

## 2.相关工作

### 2.1.Zero-Shot TTS 

> Zero-shot speech synthesis refers to the ability to synthesize the voice of an unseen speaker based solely on a few seconds of audio prompt, also known as voice cloning. Due to its impressive performance, it has gained significant attention re-cently. Previous approaches in the field of voice cloning can be classified into speaker adaptation (Wang et al., 2018; Chen et al., 2021; Huang et al.,2022b) and speaker encoding methods ([YourTTS (2021)](../../Models/E2E/2021.12.04_YourTTS.md); Arik et al., 2018; Kang et al., 2022).These models are often trained on smaller datasets and require a substantial amount of data for fine-tuning or employ highly complex encoders. These limitations greatly restrict the audio quality and further usage.

> In recent months, with the advancement of generative large-scale models, a plethora of outstand-ing works have emerged. [VALL-E (2023)](2023.01.05_VALL-E.md) leverages discrete codec representations and combines autoregressive and non-autoregressive models in a cascaded manner, preserving the pow-erful contextual capabilities of language models. It can clone the voice of a target speaker with just a 3-second audio prompt. [VALL-E X (2023)](../../Models/Speech_LLM/2023.03.07_VALL-E_X.md) extends zero-shot TTS to multiple languages based on the cascaded structure of VALL-E. [NaturalSpeech2 (2023)](../Diffusion/2023.04.18_NaturalSpeech2.md) employs continu-ous vectors instead of discrete neural codec tokens and introduces in-context learning to a latent dif-fusion model. [SPEAR-TTS (2023)](2023.02.07_SPEAR-TTS.md) and Make-a-Voice (Huang et al., 2023) utilize semantic tokens to reduce the gap between text and acoustic features. [VoiceBox (2023)](2023.06.23_VoiceBox.md) is a non-autoregressive flow-matching model trained to infill speech, given audio context and text. Mega-TTS (Jiang et al., 2023b,a) , on the other hand, utilizes traditional mel-spectrograms, decoupling timbre and prosody and further modeling the prosody us-ing an autoregressive approach.

> However, none of the aforementioned works take into account the model’s speed and lightweight na-ture. It is well-known that autoregressive models often require large memory footprints and have slower iteration speeds, and diffusion models often require hundreds of sampling steps. Some work in-troduce a semantic token cascade structure, which will introduce more time overhead. However, com-pared to codecs, certain approaches based on mel-spectrogram exhibit lower generation diversity and audio quality (Siuzdak, 2023). MobileSpeech, on the other hand, is the first zero-shot TTS system that can be deployed on mobile devices, address-ing the fast speed,lightweight, high quality require-ments.

### 2.2.生成模型

> Generative models have been extensively utilized in various domains by employing techniques such as language models ([AudioLM (2023)](2022.09.07_AudioLM.md); Kreuk et al., 2022; Ji et al., 2023; [VALL-E (20232023.01.05_VALL-E.md.md); [VALL-E X (2023)](../../Models/Speech_LLM/2023.03.07_VALL-E_X.md)), Variational Autoencoders (VAE) (Ren et al., 2021; Lee et al., 2022), Generative Adver-sarial Networks (GAN) ([VITS (2021)](../../Models/E2E/2021.06.11_VITS.md); Kong et al., 2020), Normalizing Flow ([Glow-TTS (2020)](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md); Miao et al., 2020), Mask Autoencoders ([MaskGIT (2022)]())and diffusion models (Huang et al.,2022a). Considering the challenge of balancing perceptual quality and inference speed in zero-shot TTS, as well as the recent success of parallel de-coding in text (Ghazvininejad et al., 2019), image ([MaskGIT (2022)]()), audio ([SoundStorm](../../Models/Speech_LLM/2023.05.16_SoundStorm.md)),and video (Villegas et al., 2022) generation tasks,MobileSpeech has adopted masked parallel gen-eration as the audio synthesis approach. Further-more, MobileSpeech has additionally designed the Speech Codec Mask Decoder (SMD) module and Speaker prompt based on the unique characteristics of acoustic tokens and the specific requirements of the zero-shot TTS task, aiming to address the afore-mentioned trade-off. In Appendix A, we present the detail of discrete acoustic codec and in Ap-pendix B we present the distinctions between mask-based generative models and other generative mod-els in modeling discrete codecs.

## 3.方法

## 4.实验

## 5.结论

> In this paper, we propose MobileSpeech, a Fast and Lightweight Framework for Mobile zero-shot Text-to-Speech. MobileSpeech achieves a unique com-bination of audio naturalness and similarity perfor-mance, comparable to other zero-shot TTS systems on English corpora, while significantly enhancing inference speed through its distinctive SMD and Speaker modules. MobileSpeech stands as the first real-time zero-shot TTS system deployable at the edge, demonstrating state-of-the-art performance on Chinese corpora through extensive training on large-scale Chinese language data. We not only validate the significance of each module in Mo-bileSpeech through ablation experiments but also analyze the effects of different configurations and iterations to adapt to various deployment scenarios in mobile environments.

## 6.局限性与未来工作

> In this section, we will analyze the limitations of MobileSpeech and discuss potential future work.

> Failure cases 
> During our experiments in real-world scenarios (Figure 2), we observed occasional instances of stuttering and unstable pitch in Mo-bileSpeech. This could be attributed to noisy input prompts or the possibility that the mask mechanism excessively focuses on local positions. In future work, we aim to enhance the model’s noise robust-ness and design a more robust mask mechanism that considers a broader context.

> New Tasks
> The current zero-hhot TTS task it-self has significant potential for expansion. Most existing models primarily focus on similarity in timbre. However, future research can extend zero-shot TTS to include similarity in rhythm, emotion,and language. It could even involve using multiple audio segments to control different parts or combin-ing with text style control, representing meaningful new tasks.

> Broader impacts
> Since MobileSpeech could synthesize speech that maintains speaker identity,it may carry potential risks in misuse of the model,such as spoofing voice identification or impersonat-ing a specific speaker. To mitigate such risks, it is possible to build a detection model to discriminate whether an audio clip was synthesized by Mobile-Speech.
