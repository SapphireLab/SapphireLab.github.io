---
标题: "MM-TTS: A Unified Framework for Multimodal, Prompt-Induced Emotional Text-to-Speech Synthesis"
作者:
  - Xiang Li
  - Zhi-Qi Cheng
  - Jun-Yan He
  - Xiaojiang Peng
  - Alexander G. Hauptmann
机构: 
代码: https://anonymous.4open.science/r/MMTTS-D214 ArXiv: https://arxiv.org/abs/2404.18398
提出时间: 2024-04-29
出版社: 
发表期刊: 
发表时间: 
引文数量: 50
被引次数: 0 
tags:  
DOI:  
aliases:
  - MM-TTS 
ArXiv最新版本: "1" 
ArXiv最新时间: 2024-04-29 
PageNum:  13
Demo:
---
# MM-TTS: A Unified Framework for Multimodal, Prompt-Induced Emotional Text-to-Speech Synthesis

## Abstract·摘要

> Emotional Text-to-Speech (E-TTS) synthesis has gained significant attention in recent years due to its potential to enhance human-computer interaction.
> However, current E-TTS approaches often struggle to capture the complexity of human emotions, primarily relying on oversimplified emotional labels or single-modality inputs.
> To address these limitations, we propose the Multimodal Emotional Text-to-Speech System (MM-TTS), a unified framework that leverages emotional cues from multiple modalities to generate highly expressive and emotionally resonant speech.
> MM-TTS consists of two key components: 
> (1) the Emotion Prompt Alignment Module (EP-Align), which employs contrastive learning to align emotional features across text, audio, and visual modalities, ensuring a coherent fusion of multimodal information; 
> (2) the Emotion Embedding-Induced TTS (EMI-TTS), which integrates the aligned emotional embeddings with state-of-the-art TTS models to synthesize speech that accurately reflects the intended emotions.

> Extensive evaluations across diverse datasets demonstrate the superior performance of MM-TTS compared to traditional E-TTS models.
> Objective metrics, including Word Error Rate (WER) and Character Error Rate (CER), show significant improvements on ESD dataset, with MM-TTS achieving scores of 7.35% and 3.07%, respectively.
> Subjective assessments further validate that MM-TTS generates speech with emotional fidelity and naturalness comparable to human speech.
> Our code and pre-trained models are publicly available at https://anonymous.4open.science/r/MMTTS-D214.

## 1.Introduction·引言

> Emotional text-to-speech (E-TTS) technology has emerged as a transformative force in artificial intelligence and multimedia, significantly enhancing the richness of human-computer interaction [40].
> By imbuing synthetic voices with emotional depth, E-TTS transcends mere speech replication, bringing virtual agents and digital characters to life.
> The incorporation of emotional expressiveness captivates users and fosters deeper emotional connections, catering to the fundamental human need for empathetic communication [5,49].
> This heightened level of engagement has the potential to revolutionize industries from entertainment and education to healthcare and customer service.

> Despite substantial advancements in text-to-speech (TTS) technologies resulting in naturalistic, high-quality speech, the primary focus has been on linguistic accuracy rather than capturing the inherent emotional nuances [21,31,36–38,45].
> Contemporary E-TTS approaches have sought to address this gap by integrating emotion labels or utilizing emotional reference speech [5,6,11,23,26,46,49].
> However, these methods often struggle to fully capture the intricacies of human emotions due to their reliance on oversimplified representations or single-modality inputs.
> To truly unlock the potential of E-TTS and create emotionally resonant synthetic speech, a more comprehensive approach leveraging rich emotional information across multiple modalities is crucial.

> Consider a voiceover scenario where one aims to generate emotionally consistent speech for a given dialogue, potentially with access to visual cues such as facial expressions, silent video clips, and reference audio samples, as shown in Figure 1.
> Traditional E-TTS approaches relying on unimodal information have inherent limitations.
> Facial expressions are dynamic, and their emotional interpretation can vary significantly depending on context.
> Similarly, using reference emotional audio alone fails to capture the nuanced interplay of emotions across scenarios.
> Moreover, in real-world applications, the availability of different modalities may vary.
> Leveraging multimodal cues enables a more comprehensive understanding of the underlying emotional state, leading to the generation of speech with more intricate and nuanced emotional expressions tailored to the specific context.
> Such multimodal integration enables a more flexible approach to emotional speech synthesis, catering to the intricate demands of human-computer interaction and content creation (e.g.Figure 1).

> In response, we present the Multimodal Emotional Text-to-Speech System (MM-TTS), a groundbreaking framework designed to elevate the expressiveness of synthesized speech by incorporating multimodal cues encompassing text, audio, and visual information (Sec.3).
> MM-TTS comprises two critical components: the Emotion Prompt Alignment Module (EP-Align) and the Emotion Embedding-Induced TTS (EMI-TTS).
> EP-Align enables the seamless fusion of multimodal information by aligning emotional data across modalities through a cue anchoring mechanism (Sec. 3.1).
> EMI-TTS leverages these aligned emotional embeddings to generate expressive and emotionally resonant speech suitable for a wide range of applications (Sec.3.2).
> The integration of EP-Align and EMI-TTS within MM-TTS leads to emotionally intelligent speech synthesis.

> To validate the effectiveness of MM-TTS, we conduct extensive testing across diverse datasets, benchmarking its performance against traditional models in crucial speech quality metrics.
> By investigating the impact of MM-TTS on speech content accuracy with varied TTS framework, we find that it consistently improves performance on ESD dataset with Word Error Rate (WER) and Character Error Rate (CER) scores of 7.35% and 3.07%, respectively (Sec.4.3.1).
> Subjective evaluations further corroborate these objective measures, confirming that MM-TTS matches the emotional fidelity and naturalness of human speech.
> The combination of strong objective performance and subjective approval positions MM-TTS at the forefront of Emotional TTS technologies, highlighting its potential to revolutionize human-computer interactions through emotional speech synthesis.
> The key contributions are threefold:

> - We introduce the Emotion Prompt Alignment Module (EP-Align), a novel approach employing contrastive learning to synchronize emotional features across modalities.
> By effectively aligning emotional features and filtering out complex noise in multimodal content, EP-Align addresses the challenge of distribution discrepancies, serving as a critical component in enabling high-quality emotional speech generation.
> - Building upon EP-Align, we develop the Emotion EmbeddingInduced TTS (EMI-TTS) framework, which seamlessly integrates advanced TTS models with the aligned emotional embeddings to synthesize speech that authentically reflects intended emotions.
> The incorporation of these embeddings enhances the naturalness and credibility of the generated audio, resulting in a more engaging and immersive user experience.
> - We conduct extensive evaluations across a wide spectrum of emotional categories and scenarios to validate the adaptability and superior processing capabilities of MM-TTS.
> The results confirm that MM-TTS consistently maintains emotional consistency while preserving individual speaker characteristics, highlighting its transformative potential in revolutionizing human-computer interactions by providing a more engaging experience.

## 2.Related Work·相关工作

### 2.1.Text-to-Speech Synthesis

> Text-to-speech (TTS) technology, known as speech synthesis, aims to generate human-like speech from text.
> Recent neural network-based end-to-end TTS models, such as those developed in [21,31,36–38,45], have markedly improved speech quality over traditional methods like concatenation synthesis [17] and statistical parametric synthesis [41,48].
> Typically, these models involve converting text or phonemes into linguistic features using embeddings, expanding these features to meet the acoustic model’s requirements via a length regulator, and then converting the acoustic features to waveforms using a vocoder.
> Different models employ various approaches for the acoustic model.
> For example, the Tacotron series uses RNN-based models [38,39,45], VITS utilizes flow-based models [8,21], and FastSpeech features Transformer-based models [36,37,44].
> While these state-of-the-art TTS models have made some progress in generating natural and high-quality speech, they have primarily focused on linguistic accuracy and clarity, often overlooking the emotional dimensions conveyed through speech.

### 2.2.Emotional Text-to-Speech Synthesis

> Emotional Text-to-Speech (ETTS) focuses on enhancing synthesized speech with emotional expressiveness [40].
> Researchers have developed methods to integrate emotions into TTS systems. 
> (1) One technique uses emotion labels as additional conditioning data in TTS models.
> For example, EmoSpeech [6] modifies the FastSpeech2 framework by adding speaker and emotion labels via Conditional Layer Normalization.
> Similar strategies in works like [5,23,49] employ Tacotron-based models where emotion labels are embedded into hidden features as conditions.
> EmoDiff [11] uses a diffusion model [16] with soft labels for emotions, replacing the traditional one-hot vectors. 
> (2) Another approach leverages referenced speech capturing the desired emotional state.
> Li et al. [26] introduce a method incorporating dual emotion classifiers and a style loss to align the generated speech’s emotional content with the reference mel-spectrum.
> Moreover, [46] explores techniques for transferring the emotional style from reference speech to synthesized speech. 
> (3) Some studies utilize textual descriptions of emotions as conditioning data, with [47] and [12] demonstrating effective conveyance of target emotions through guided textual descriptions in speech synthesis.
> Despite advancements, previous ETTS approaches often face limitations like reliance on a single modality or simplistic emotional representations.
> Table 1 outlines these differences across existing models, focusing on supported emotion categories, conditioning data types, and zero-shot learning capabilities.
> Addressing these limitations, our proposed framework, MM-TTS, employs multimodal representation learning to integrate diverse emotion-related data, enabling the generation of emotionally rich speech.

### 2.3.Multi-Modal Representation Learning

> Multi-modal representation learning, a pivotal area in modern research, aims to find a shared latent space that effectively integrates diverse modalities such as text, images, videos, and audio [10,13,22,27,28,34]. 
> This integration facilitates a deeper understanding of the emotional content underlying these modalities.
> Multi-modal encoders extract contextually relevant features from each modality, capturing their unique characteristics.
> These features are then aligned using supervised or unsupervised methods, creating connections that aid in cross-modal tasks [3,4,7].
> Particularly, key advancements include the Contrastive Language-Image Pretraining (CLIP) model [34], which establishes a joint embedding for images and text, and the AudioCLIP model [13], which extends this to include audio.
> Furthermore, Koepke et al. [22] have explored combining audio and visual data to enable applications like sound localization and audio-visual event detection.
> Our framework, MMTTS, builds on these developments by using multi-modal representation learning to improve the emotional expressiveness of synthesized speech.
> By extracting and aligning feature representations from text, images, videos, and audio with emotion tags through contrastive learning, MM-TTS captures comprehensive emotion-related information.
> This enriched understanding allows MM-TTS to align explicit and implicit emotional cues, generating nuanced and emotionally rich synthesized speech.

## 3.MM-TTS Framework

> The primary goal of MM-TTS is to leverage emotions extracted from multiple modalities to generate emotional speech for different individuals in a data-efficient manner.
> Figure 2 illustrates the framework comprising two main components: (1) Emotion Prompt Alignment Module (EP-Align) and (2) Emotion Embedding-induced TTS (EMI-TTS).

> (1) Emotion Prompt Alignment Module (EP-Align) plays a crucial role in aligning the emotional representations derived from various modalities, such as visual data, audio segments, and textual descriptions.
> It takes a multimodal emotional data tuple $Tup^{emo}=<v,a,s,p>$ as input, where $v$ represents visual data (image or video), $a$ is an audio segment, $s$ denotes an emotional text description, and $p$ is an emotional prompt label.
> EP-Align processes these inputs using a set of multimodal encoders $\mathcal{E} = \{\mathcal{E}^{vis}, \mathcal{E}^{audio}, \mathcal{E}^{tex}, \mathcal{E}^{prop}\}$, which extract emotion features and generate a unified emotion embedding.
> This emotion embedding is then passed to the EMI-TTS component.

> (2) Emotion Embedding-induced TTS (EMI-TTS) component takes the aligned emotion embedding, along with the input text Tex and a pre-trained TTS model from the model library $\mathcal{M}$, to generate emotional speech.
> The emotion embedding provides the necessary emotional context, while the input text and the TTS model determine the content and the overall style of the generated speech.

> Formally, the audio with emotion can be generated by MM-TTS $\Phi$ as follows:

$$
\tag{1}
$$

> The MM-TTS framework aims to address the challenges associated with generating emotional speech by leveraging the complementary information provided by multiple modalities.
> By aligning the emotional representations from different sources and integrating them into the TTS process, MM-TTS enables the synthesis of speech that accurately conveys the desired emotions while maintaining the speaker’s characteristics.
> In the following, we will delve into the details of the Emotion Prompt Alignment Module (EP-Align) and the Emotion Embedding-induced TTS (EMI-TTS) component.

### 3.1.Emotion Prompt Alignment Module

> Inspired by Contrastive Language-Image Pre-training (CLIP) [34], EP-Align employs a set of multimodal encoders to extract emotion features from various modalities.
> These encoders are denoted as $\mathcal{E} = \{\mathcal{E}^{vis}, \mathcal{E}^{audio}, \mathcal{E}^{tex}, \mathcal{E}^{prop}\}$, corresponding to vision, audio, text, and emotion prompts, respectively.
> The extracted features are then aligned into a unified emotion embedding space.

#### 3.1.1.Multimodal Encoders. 

> Each modality is processed by a dedicated encoder to extract relevant emotion features.
> Given a multimodal emotional data tuple $Tup^{emo}=<v,a,s,p>$, the encoders generate feature representations as follows:

$$
\tag{2}
$$

> where $f^{vis}, f^{audio}, f^{tex}, f^{prop}$ represent the feature representations for visual, audio, textual, and emotional prompts, respectively.
> Each feature representation has a dimensionality of𝐾, which is set to 512 in this work.

> The architectures of the text encoder $\mathcal{E}^{tex}$, prompt encoder $\mathcal{E}^{prop}$, and visual encoder $\mathcal{E}^{vis}$ are similar to those in CLIP [34].
> The audio encoder $\mathcal{E}^{audio}$ follows the design from [1] for effective audio encoding.
> During training, the text and image encoders are initialized with pre-trained CLIP weights, while the audio encoder is initialized with a pre-trained model from [14].
> To handle video inputs, the visual encoder $\mathcal{E}^{vis}$ processes a sequence of frames, extracts a feature bank, and performs average pooling in the temporal dimension to obtain the final representation.
> For audio inputs, an additional linear projection layer is employed to map the audio features to the same dimension as the text and image features.

#### 3.1.2.Multimodal Emotional Alignment. 

> Aligning the emotion representations from different modalities is a critical step in MM-TTS.
> We propose a prompt-anchoring scheme that leverages prompt-based bridging to facilitate the alignment of multimodal representations.
> First, we construct vision-prompt (vis-pro) and audio-prompt
(aud-pro) embedding spaces by projecting the vision, audio, and text features using learned projection matrices:

$$
\tag{3}
$$

> where $W^{vis−pro}$, $W^{aud−pro}$, $W^{tex−pro}\in\mathbb{R}^{K\times K}$ are the projection matrices, and $u^{vis}$, $u^{audio}$, and $u^{tex}$ }are the projected embeddings in the vis-pro, aud-pro, and tex-pro spaces, respectively.

> The prompt embedding $u^{prop}$ is obtained by multiplying the corresponding projection matrix based on the alignment modality:

$$
\tag{4}
$$

> We use $u^{exp}$ to denote the explicit emotion embedding obtained from the emotion prompt and $u^{imp}$ to represent the implicit emotion embeddings from vision, audio, or text.
> The cosine similarity between the explicit and implicit embeddings is computed as:

$$
\tag{5}
$$

> where $t$ is a learned temperature parameter, and $\sigma$ denotes the normalization operation.

> The multimodal alignment loss $L_{align}$ is defined as the symmetric cross-entropy loss between the explicit and implicit embeddings:

$$
\tag{6}
$$

> During inference, EP-Align selects the emotion prompt embedding $u^{prop}$ with the highest similarity score to the implicit embeddings as the aligned emotion representation $u^{emo}$.
> This aligned emotion representation captures the emotional content conveyed by the multimodal inputs and serves as a unified representation for the subsequent TTS process.
> By leveraging the prompt-anchoring scheme and the multimodal encoders, EP-Align effectively aligns the emotion representations from different modalities into a shared embedding space.
> This alignment enables the TTS model to generate emotional speech that is consistent with the input emotional context, regardless of the source modality.

### 3.2.Emotion Embedding-induced TTS

> With the aligned emotion embedding $u^{emo}$ obtained from the previous EP-Align module, the Emotion Embedding-induced TTS (EM-TTS) component generates emotional speech by integrating the emotion representation into the TTS process.

#### 3.2.1.Prompt-Anchoring Multimodal Fusion. 

> To mitigate the bias among the multimodal emotion embedding spaces, we employ prompt-anchoring to fuse the aligned emotion embedding $u^{emo}$ with the input text representation.
> Specifically, we use the implicit emotion embedding $u^{imp}$ to retrieve the most similar prompt embedding $u^{prop}$ based on cosine similarity.
> The retrieved prompt embedding serves as an anchor to bridge the implicit and explicit emotion representations, enabling a more coherent integration of the emotion embedding into the TTS model.

#### 3.2.2Unified Emotional-TTS Framework. 

> EMI-TTS provides a unified framework that integrates various TTS models, such as Tacotron2[38], VITS [21], and FastSpeech2 [36].
> These models typically consist of a text encoder, a length regulator, an acoustic model, and a vocoder.
> The text encoder converts the input text Tex into a sequence of linguistic features $h_{lg}$ .
> The length regulator adjusts the duration of the linguistic features to match the desired speech length.
> The acoustic model generates acoustic features $h_{ac}$ conditioned on the linguistic features and the emotion and speaker embeddings.
> Finally, the vocoder converts the acoustic features into a speech waveform.
> In EMI-TTS, the emotion embedding $u^{emo}$ and speaker embedding $u_{spk}$ are concatenated and integrated into the TTS model as additional conditioning inputs.
> The emotion embedding provides the necessary emotional context, while the speaker embedding captures the speaker’s characteristics.
> This allows the TTS model to generate speech that reflects both the desired emotion and the target speaker’s style.

> Formally, the EMI-TTS process can be represented as follows:

$$
\tag{7}
$$

> where $h_{lg}^{emo}$ represents the linguistic features conditioned on the emotion and speaker embeddings, and $Audio^{emo}$ is the generated emotional speech.
> During training, EMI-TTS optimizes the same loss functions as the base TTS models without requiring additional loss terms.
> The model is trained on paired data consisting of input text, emotion features, and speaker information.
> The vocoder used in EMI-TTS depends on the specific TTS model: VITS uses its original decoder, while Tacotron2 and FastSpeech2 employ a WaveNet[42] vocoder.

#### 3.2.3.Model Library.

> EMI-TTS incorporates several state-of-the-art TTS models, including Tacotron2, VITS, and FastSpeech2.
> By providing a diverse set of TTS models, EMI-TTS offers flexibility in generating emotional speech across different scenarios and requirements.

> (1) Tacotron2: Building upon the traditional Tacotron model[38], this version integrates WaveNet technology[42] for improved sound generation.
> The Character Encoder transforms text into hidden states $h_{lg}$ , enhanced with emotion embeddings $u^{emo}$ to create emotionally enriched states $h_{lg}^{emo}$:

$$
\tag{8}
$$

> This enhancement adjusts the Location Sensitive Attention to a more nuanced Location & Emotion Sensitive Attention, allowing the model to better capture emotional nuances in speech synthesis.

> (2) VITS: This model innovatively combines variational inference with normalizing flows and adversarial training, as outlined in VITS[21].
> It includes a unique setup comprising a Text Encoder, Length Regulator, and an Emotion-condition Flow, among other components.
> It utilizes an Emotion-condition Flow to adjust phoneme sequences encoded by the Text Encoder into emotion-laden hidden states $h_{lg}^{emo}$

$$
\tag{9}
$$

> where $d$ is the duration predicted for each unit.
> This architecture allows for a more dynamic and emotionally responsive speech output during inference.

> (3) FastSpeech2: Leveraging a Transformer architecture, this model significantly enhances the synthesis speed and quality of speech as outlined in FastSpeech2[36].
> It incorporates a Mel-spectrogram Decoder that uses Conditional Cross-Attention to embed emotional nuances effectively into the synthesized speech:

$$
\tag{10}
$$

> where $\tilde{h}_{lg} = h_{lg} + u^{emo}$ represents the integration of the text input hidden states and emotion embeddings.
> This single-step computation efficiently handles the extension of hidden states, their emotional modulation, and the application of cross-attention, resulting in seamless and expressive speech synthesis.

> By employing prompt-anchoring multimodal fusion and a unified emotional-TTS framework, EMI-TTS generates emotional speech that accurately reflects the desired emotions while preserving the target speaker’s characteristics.
> The choice of the TTS model can be based on the specific requirements of the application, such as synthesis speed, voice quality, and emotional expressiveness.
> More details are in the supplementary material.

## 4.Experiments

### 4.1.Experimental Setting

#### 4.1.1.Dataset

> Our MM-TTS framework has been evaluated on three different datasets:
> - **Multimodal EmotionLines Dataset (MELD)** [32] was utilized to evaluate the Emo-Alignment module’s performance in extracting and aligning emotions from both explicit and implicit multimodal cues.
> The MELD dataset contains 13,708 utterances from the TV series Friends, covering emotions such as Anger, Disgust, Sadness, Joy, Neutral, Surprise, and Fear.
> It is divided into training (9,989 utterances), validation (1,109 utterances), and test sets (2,610 utterances) to ensure a comprehensive evaluation.
> - **Emotion Speech Dataset (ESD)** [50] was used to test the model’s capability in vocal emotion expression.
> The ESD includes 17,500 utterances by 10 speakers, categorized into five emotions: neutral, happy, angry, sad, and surprised.
> The dataset was split into training (14,000 utterances), validation (1,750 utterances), and test subsets (1,750 utterances) for structured evaluation.
> - **Real-world Expression Database (RAF-DB)** [25] was employed to evaluate the model’s ability to recognize complex compound emotions from visual data.
> The RAF-DB is divided into a basic emotion dataset with 15,339 images in seven categories and a compound emotion dataset of 3,954 images in 11 classes.

> Training and test splits for the basic dataset are 12,271 and 3,068 images, respectively, while the compound dataset is divided into 3,162 images for training and 792 for testing.

#### 4.1.2.Evaluation Metric

> To evaluate MM-TTS framework, we use both objective metrics and subjective human evaluations.
> - **Objective Metrics** focus on the model’s ability to accurately extract and classify emotion features from multimodal inputs and to assess the quality of synthesized speech. 
> (1) **Multi-Modal Emotion Alignment Accuracy**: We evaluate the ability of MM-TTS to extract emotion features by comparing implicit emotion embeddings from each modality with explicit emotion descriptions.
> The accuracy is calculated based on how often the predicted emotion class matches the ground truth label, determined by the highest similarity score among potential emotion prompts. 
> (2) **Speech Quality Metrics (WER and CER)**: The quality of generated speech is objectively measured using the Whisper [35] ASR model.
> We calculate the Word Error Rate (WER) and Character Error Rate (CER) on ESD dataset by comparing the transcribed synthesized speech to the actual ground truth text, aiming to investigate the impact of our MM-TTS on speech content accuracy.
> - **Subjective Human Evaluations** are conducted using the Mean Opinion Score (MOS) to evaluate emotional expressiveness, naturalness of speech, and speaker similarity of the synthesized audio.
> (1) **Emotion Similarity MOS**: Raters evaluate how well the synthesized speech conveys the intended emotions.
> Each sample’s emotion alignment is scored on a scale from 1 to 5, with higher scores indicating a closer match to the target emotion. 
> (2) **Speech Naturalness MOS**: This metric assesses the naturalness of the speech output.
> Raters listen to the synthesized utterances and score them based on how naturally they mimic human speech.
> (3) **Speaker Similarity MOS**: Raters assess how accurately the synthesized speech matches the voice characteristics of the target speaker.
> Samples are scored based on their similarity to reference utterances, focusing on voice timbre, pitch, and style.

### 4.2.Implementation Details

#### 4.2.1.Data Preprocessing

> We convert emotion labels into short description sentences to serve as emotion prompts for the models.
> For text, we employ two strategies: for the flow-based model, we use the International Phonetic Alphabet (IPA) sequences similar to Glow-TTS [19], utilizing phonemizer software [2] for conversion.
> For Transformer-based models, text sequences are transformed into phoneme sequences with duration information using the Montreal Forced Aligner (MFA) [30].
> Audio inputs undergo the Short-time Fourier transform (STFT) to extract linear spectrograms [21], and are also converted into mel-spectrograms as per the methods in Tacotron2 [38].
> Images are resized and normalized to a tensor format $V\in\mathbb{R}^{h\times w\times c}$, with the dimensions $h$ and $w$ both set to 244, and $c$ representing the three color channels of RGB.
> Video preprocessing involves segmenting videos into frames, uniformly sampling 8 frames across the sequence, and converting these frames into tensors similar to image processing.

#### 4.2.2.Model Training

> The training process is segmented into two main phases to manage resource utilization effectively.
> Initially, the Emotion Prompt Alignment Module (EP-Align) is trained using multi-modal pairs from all datasets.
> Subsequently, the prompts generated by the trained EP-Align are utilized with audio-text pairs to train the Emotion Embedding-induced TTS (EMI-TTS).
> For the EPAlign module, we employ contrastive learning to fine-tune encoders from various modalities.
> We use RoBERTa [29] and InstructERC[24] for text, VGGish [15] and wav2vec2 [1] for audio, and ResNet[18] and ViT [9,34] for images and videos.
> The training of EP-Align was conducted over 100 epochs on 4 NVIDIA A100 GPUs.
> For EMI-TTS, the flow-based models underwent 200,000 training steps until convergence, while Transformer-based models were trained for 40,000 steps and Recurrent-based models completed 250,000 steps.

> More details are in the supplementary material.

### 4.3.Comparison with the State-of-the-Art

> We compare our MM-TTS model to existing state-of-the-art TTS models across several metrics to demonstrate its effectiveness in synthesizing emotional and accurate speech.

#### 4.3.1.Word Error Rate and Character Error Rate (WER and CER).

> Table 2 presents a comparison of WER and CER for different TTS models on ESD to illustrate the performance improvement achieved by the MM-TTS framework.
> The MM-TTS implementation with the FastSpeech structure achieves the lowest WER at 7.35% and CER at 3.07%, indicating an effective approximation to the ground truth audio quality.
> This performance surpasses other implementations such as VITS, Tacotron, and the EmoSpeech [6] models.
> This improvement is due to the multimodal emotion alignment within MM-TTS, which improves the extraction and representation of emotion features, allowing for better model convergence and superior generalization capabilities in speech synthesis.
> This methodological refinement results in more natural and accurate emotional speech output, establishing MM-TTS as a significant advance over traditional label-based emotion encoding methods.

#### 4.3.2.Emotion Similarity Mean Opinion Score (MOS).

> The subjective evaluations presented in Table 3 illustrate the effectiveness of the MM-TTS models in generating emotionally congruent speech.
> The MM-TTS (FastSpeech) variant notably scored an average MOS of 4.37, closely matching the ground truth (MOS of 4.57) and surpassing other models.
> This performance highlights the MM-TTS framework’s capability to effectively capture and render nuanced emotional expressions, with scores across various emotions showing consistent improvement over other TTS models.
> This indicates a robust alignment of emotional tones in speech synthesis, positioning MM-TTS (FastSpeech) as a leading solution in emotional TTS technologies.

#### 4.3.3.Speech Naturalness and Speaker Similarity MOS.

> The MMTTS models exhibit strong performance in speech naturalness and speaker similarity, as demonstrated in Tables 4 and 5.
> These evaluations confirm the MM-TTS’s capability to maintain high standards of voice quality and speaker-specific traits across different emotional expressions.
> Specifically, MM-TTS (FastSpeech) consistently scores near or above ground truth levels, underscoring its effectiveness in producing natural-sounding and speaker-consistent emotional speech, crucial for enhancing user engagement in personalized TTS applications.
> These findings not only demonstrate the technical excellence of MM-TTS but also highlight its practical effectiveness in real-world scenarios where emotional variance and speaker identity are crucial for user interaction and satisfaction.

### 4.4.Ablation Study

> We perform ablation studies to quantify the impact of EP-Align on emotion classification accuracy and synthesized speech quality across different modalities.
> Particularly, we assess the impact of EPAlign using the MELD dataset for multi-modal emotion recognition and the RAF-DB dataset for compound emotion classification.
> The effectiveness of EP-Align is measured in terms of emotion recognition accuracy (weighted F1 scores) and speech synthesis quality (WER, CER, and MOS), respectively.

#### 4.4.1.Effectiveness of Multimodal Fusion.

> To evaluate the impact of the Emotion Prompt Alignment Module (EP-Align) on improving emotion recognition, we conducted a series of tests using the MELD dataset, which is known for its rich multimodal input.
> Table 6 clearly demonstrates that EP-Align significantly improves emotion recognition accuracy.
> Notably, the configuration using combined modalities, which synthesizes text, audio, and video inputs, shows the most marked improvement, with a F1 score increase from 0.68 to 0.75.
> This highlights EP-Align’s effectiveness in multimodal fusion, proving crucial in environments where emotional cues are inherently dispersed across different channels.
> By seamlessly blending these inputs, EP-Align enhances the coherence and precision of emotion classification systems.
> Such advancements are particularly beneficial for developing more sophisticated, context-aware applications in sectors like interactive voice response systems and affective computing.

#### 4.4.2.Confusion Matrices Analysis.

> Confusion matrices are instrumental in evaluating classification models by illustrating the accuracy across different classes and highlighting potential areas of misclassification.
> Figures 3 and 4 present the confusion matrices for the MELD and RAF-DB datasets, respectively, following the integration of the Emotion Prompt Alignment Module (EP-Align).
> These matrices provide insights into EP-Align’s performance in aligning multimodal inputs to predict complex emotional states accurately.
> Specifically, Figure 3 depicts the classification results for the MELD dataset, which includes a range of basic and complex emotions.
> This visualization helps identify which emotions EP-Align accurately recognizes and where it tends to confuse one emotion for another.
> For instance, a common misclassification might be the confusion between ’joy’ and ’surprise’, which often share similar expressive features.
> By examining these overlaps, we can better understand the nuances of EP-Align’s performance, including its strengths in distinguishing closely related emotional states and its limitations.

> Similarly, the confusion matrix for the RAF-DB dataset, shown in Figure 4, illustrates the classification results for compound emotions, which are inherently more complex due to the blending of multiple emotional states.
> This matrix is particularly useful for assessing EP-Align’s effectiveness in resolving the intricacies of compound emotions.
> For example, the misclassifications between ’happiness’ combined with ’surprise’ versus ’happiness’ combined with ’sadness’ indicate the challenges in distinguishing between subtle emotional blends.
> Insights from this analysis guide further improvements in EP-Align’s algorithm to enhance its sensitivity to subtle emotional distinctions.

#### 4.4.3.Impact on Synthesized Speech Quality.

> To empirically verify the impact of EP-Align on synthesized speech quality, we conducted a detailed analysis comparing the performance of speech synthesis systems with and without EP-Align.
> As Table 7 shows, integrating EP-Align results in notable improvements across all evaluated metrics.
> Specifically, the reduction in WER from 8.50% to 7.35% and in CER from 3.90% to 3.07% suggest that EP-Align significantly enhances the phonetic precision of the synthesized speech.
> Furthermore, the increase in MOS from 4.12 to 4.37 highlights an improvement in the overall subjective listening experience of the synthesized speech.
> These improvements suggest that EP-Align effectively adjusts the alignment of phonetic elements, which is crucial for producing clear and accurate speech.
> This capability not only enhances the intelligibility of the speech output but also its naturalness, thus contributing positively to user experience.

## 5.Conclusions

> In this work, we introduce MM-TTS, a multimodal framework that revolutionizes emotional speech synthesis by harnessing textual, auditory, and visual information.
> The EP-Align module ensures seamless emotional feature alignment across modalities through contrastive learning, while EMI-TTS elegantly incorporates these features into cutting-edge TTS models.
> The resulting emotionally rich speech closely mirrors human emotional expression, as demonstrated by our rigorous evaluations.
> MM-TTS surpasses traditional E-TTS models in both objective and subjective metrics, showcasing its ability to generate natural and emotionally resonant speech.
> By open-sourcing our code and models, we aim to drive further innovation and contribute to the progress of emotionally intelligent speech synthesis.
> MM-TTS sets a new standard for emotional speech synthesis, paving the way for more empathetic and engaging human-computer interactions across diverse applications.
> In future work, we plan to integrate additional modalities like gesture and facial expression to further enhance emotional expressiveness.

> Furthermore, we aim to apply our framework to low-resource languages and accented speech, expanding the reach of emotionally intelligent speech synthesis.

## 6.Supplementary Materials

> This section complements the comprehensive discussions presented in our paper on the Multi-Modal Text-to-Speech (MM-TTS) framework.
> The purpose of this content is to provide additional technical details, empirical evidence, and demonstrative examples that support the innovative approaches we have developed for emotion embedding-induced speech synthesis.
> Within this supplementary material, readers will find extensive evaluations, interactive demonstrations, and comparative analyses that illustrate the effectiveness and versatility of the MM-TTS framework across various application scenarios.
> The additional demonstrations are organized into three primary demos:

> (1) Voiceover Scenario Demonstrations: This demo presents visual and auditory examples that showcase the MM-TTS framework’s ability to synthesize emotionally resonant voice from multimodal inputs.
> The demonstrations highlight the framework’s capacity to effectively integrate and interpret emotional cues from visual (facial expressions), auditory (tone of voice), and textual (script context) data sources to generate context-appropriate speech outputs.

> (2) Emotional Text-to-Speech Synthesis Comparisons: In this demo, we provide a series of comparative demonstrations that evaluate the emotional speech outputs generated by our EMI-TTS system against those produced by traditional emotional TTS approaches.
> These comparisons aim to showcase the enhanced expressiveness and naturalness of speech synthesized through our framework, emphasizing the improvements achieved over existing technologies.

> (3) Zero-Shot Emotional Speech Synthesis: This demo focuses on demonstrating the zero-shot capabilities of our framework by presenting synthesized speech outputs that reflect complex and compound emotions not explicitly encountered during the training process.
> These examples underscore the adaptability and creative potential of the MM-TTS framework, highlighting its ability to enable personalized and dynamic speech generation applications.

> Note that each demo is supplemented with relevant figures, audio samples, and detailed annotations to facilitate a comprehensive understanding of the methodologies employed and the results obtained.
> The provided materials are intended to assist researchers and practitioners in exploring the full capabilities of the MM-TTS framework and its potential applications in real-world scenarios.

### 6.1.Implementation Details of Emotion Embedding-Induced TTS

> This section presents implementation details of the emotion embedding-induced text-to-speech (EMI-TTS) component, mentioned in Section 3.2 of the original paper, as shown in Figure 2, which aims to introduce emotional expression into text-to-speech (TTS) synthesis.

> The EMI-TTS component is implemented across three TTS architectures: Variant VITS, Variant FastSpeech2, and Variant Tacotron2.

> The following subsections describe the integration of emotion embedding into each architecture, outlining key enhancements to support expressive speech synthesis.

#### 6.1.1.Variants VITS.
> Variant VITS models incorporate text-to-speech (TTS) architecture and emotion-aware processing technology.
> These elements are integrated to achieve speech synthesis with emotional content.
> The model includes an effectively conditioned flow, a structure designed to incorporate affective context into the speech synthesis process.
> The model is inspired by the architecture of WaveGlow [33] and Glow-TTS [20], using affine coupling layers containing Emotional WaveNet residual blocks.
> These blocks capture and express emotional nuances in synthesized speech.

> The Variant VITS model, as shown on the left of the Emotion embedding-induced TTS (EMI-TTS) part of Figure 2, uses global conditioning techniques [43] to integrate emotional information into the speech synthesis pipeline.
> These techniques insert emotion embeddings, represented as$u^{emo}$, into components such as the Spectrogram Encoder and Emotional WaveNet (EWN) in Emotion-condition FLow.
> The integration process is controlled by the following operations:

$$
$$

> where $AX()$ represents the affine Xform operator in the emotional conditional flow, $h$ represents the initial hidden feature, and $h'$ is the updated hidden feature after single-flow layer transformation.

> This approach propagates emotional information throughout the network, allowing the synthesized speech to reflect the intended emotional state.

> The Variant VITS model also includes a linear layer that converts emotion embeddings before adding them to the length modifier $h_{lg}$ and the emotion vocoder $h_{ac}$ .
> This transformation infuses emotion into these components, as shown in the following equation:

$$
$$

> where this process modifies the pitch, prosodic aspects, duration, and sound texture of the synthesized speech to match the desired emotional characteristics.
> The full implementation of the Variant VITS model is publicly available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/VITS/model/models.py.

#### 6.1.2.Variant FastSpeech2.

> The FastSpeech2 architecture is improved by the Variant FastSpeech2 model, as shown in the middle of the Emotion embedding-induced TTS (EMI-TTS) part of Figure 2, resulting in advancements in expressive speech synthesis.
> This model incorporates a Conditional Cross-Attention mechanism, inspired by the EmoSpeech approach [6], to manage the interplay between speaker identity and emotional tone.
> The integration of these elements aims to enhance emotional modulation in the synthesized speech.

> The architecture employs a technique that combines the aligned emotion embedding $u^{emo}$ with the speaker embedding.
> These embeddings are concatenated to form a unified conditioning feature, denoted as $c$.
> The dimension of the conditioning feature matches the dimension $d$ of the model’s hidden states, with $h_{lg}$ used for textual processing and $h_{ac}$ for acoustic features.
> The embeddings are processed using learnable matricesW𝑞,W𝑘, andW $v$ , which compute queries, keys, and values, respectively, as shown in the following equations:

$$
$$

> where the attention mechanism applies a softmax function to the scaled dot product of $Q$ and $K^{\mathsf{T}}$.
> The scaling is performed by dividing the dot product by the square root of the dimension $d$, as shown in the following equation:

$$
$$

> where this operation reweights the contributions of the emotion and speaker embeddings within each attention layer, allowing their integration into the hidden states and features.
> The impact of this integration can be observed in components such as the Duration Predictor and the Mel-spectrogram Decoder.

> Particularly, the attentional mechanism in the Variant FastSpeech2 model aims to modulate speech outputs to reflect the intended emotional states.
> The model strives to balance naturalness and clarity in the synthesized speech, which is relevant for applications in voice-assisted technologies and interactive systems where emotional resonance is important.
> The implementation details of the Variant FastSpeech2 model, including configuration files and source code, are made publicly available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/FastSpeech2/.

#### 6.1.3.Variation Tacotron2.

> The Tacotron2 framework has evolved into the Variant Tacotron2 model, as shown in the right of the Emotion embedding-induced TTS (EMI-TTS) part of Figure 2, which incorporates emotional expressions into its speech synthesis process.
> The model integrates aligned emotion embeddings$u^{emo}$by concatenating them with character-encoded hidden states $h_{lg}$ and speaker embeddings $u_{spk}$ .
> This approach allows emotional context to be taken into account throughout the synthesis process, thereby affecting the prosody and intonation of the speech:

$$
$$

> where this adjustment enhances the traditional position-sensitive attention mechanism and transforms it into a position & emotion-sensitive attention system.
> New attention mechanisms address the spatial and emotional components of speech synthesis, producing output that maintains natural intonation and is consistent with the desired emotional tone.

> The process involves the following steps:
> (1) Concatenation of embeddings: Aligned emotion embeddings$u^{emo}$concatenated with hidden states derived from character encoders and speaker embeddings.
> This composite feature vector is used as input to the attention mechanism.
> (2) Enhanced attention mechanism: The attention mechanism has been updated to incorporate emotional cues and adjust how it processes text and sound information.
> This modification enables the model to capture a wider range of emotional nuances.
> (3) Integrated into synthesis: Adjustments to the attention mechanism directly impact the speech synthesis pipeline, affecting elements such as phoneme duration and intonation patterns to align with the intended emotional context.

> These enhancements are designed to improve emotional speech synthesis, enabling TTS systems to generate output that reflects a wider range of emotions.
> Integrating emotional cues into the synthesis process can facilitate applications where emotional expression is crucial.
> Implementation details, including source code and configuration files, are available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/Tacotron2/.

### 6.2.More Demonstrations 

> To further demonstrate the capabilities and potential applications of the proposed MM-TTS framework, we have developed an interactive demo available at https://anonymous.4open.science/api/repo/MMTTS-D214/file/demo/index.html?v.
> These demos showcase various use cases, emotional speech samples, comparisons with other models, and the framework’s zero-shot abilities.

#### 6.2.1.Voiceover Scenario.

> In the Voiceover Scenario module, as shown in Figure 5, we illustrate how the Emotion Prompt Alignment Module (EPAlign) can extract emotional cues from multimodal scenarios, enabling the Emotion Embedding-Induced TTS (EMI-TTS) to generate contextually appropriate emotional speech.
> This capability is particularly valuable in applications that require voiceovers or narrations to seamlessly align with the emotional tone and context of multimedia content.

> This figure presents a dialogue scene from a movie clip, with the left four columns representing multimodal references for emotion extraction: the origin video without sound, the origin face image, the origin speech audio, and the origin text transcript.
> The fifth column displays the inferred aligned emotion representation $u^{emo}$’s class, where$u^{emo}$is obtained through the EPAlign module by aligning the emotional cues from these multimodal inputs into a shared embedding space.
> The final two columns showcase the emotional speech generated by EMI-TTS for two different speakers, effectively conveying the identified emotion while preserving the respective speaker characteristics.

> By leveraging the complementary emotional information present across various modalities, such as visual cues (character expressions and scene visuals), auditory cues (speech waveforms), and textual cues (dialogue transcripts), EPAlign can disentangle and align the intricate emotional nuances exhibited in the multimedia content.

> This aligned emotion representation is then seamlessly integrated into the EMI-TTS component, enabling the generation of emotional speech that accurately captures and reflects the intended affective tone within the given context.

> The Voiceover Scenario module exemplifies the power of the MM-TTS framework in generating emotionally resonant voiceovers and narrations for multimedia applications.
> By effectively aligning and fusing emotional cues from multiple modalities, MM-TTS can produce voiceovers that not only convey the desired emotional expressions but also maintain consistency with the overall emotional context of the multimedia content, thereby enhancing the immersive experience for end-users.

#### 6.2.2.Emotional Text-to-Speech Synthesis.
> The Emotional Text-toSpeech Synthesis demo, as shown in Figure 6, presents randomly selected emotional speech samples generated by EMI-TTS, along with comparative samples from other models [6,23].
> This comparison highlights EMI-TTS’s ability to provide a unified framework that integrates various TTS architectures, such as Tacotron2 [38], VITS[21], and FastSpeech2 [36].
> By leveraging the EPAlign and EMI-TTS components, MM-TTS demonstrates enhanced emotional speech generation capabilities, outperforming traditional approaches.

> For this demo, we randomly selected 20 text samples with associated emotion labels, comprising two sentences from each of 10 different speakers.
> These text samples were then used to generate emotional speech using three variants of MM-TTS (corresponding to the integrated TTS architectures), as well as VITS (label), EmotionalTTS [23], and EmoSpeech [6] for a comprehensive evaluation.

> By presenting this diverse set of emotional speech samples, we aim to demonstrate the effectiveness of the proposed MM-TTS framework in capturing and conveying a wide range of emotional expressions across various TTS architectures.
> The comparison not only showcases the naturalness and expressiveness of the generated speech but also highlights the ability of MM-TTS to outperform traditional approaches in terms of emotional speech generation capabilities.

> Moreover, this demo underscores the versatility and modularity of the MM-TTS framework, as it seamlessly integrates multiple TTS architectures while leveraging the EPAlign and EMI-TTS components to enhance emotion representation and synthesis.
> This flexibility enables researchers and practitioners to leverage the strengths of different TTS architectures while benefiting from the improved emotional speech generation capabilities offered by the MM-TTS framework.

#### 6.2.3.Zero-Shot Emotional Speech.

> The zero-shot emotional speech demo, as shown in Figure 7, showcases the zero-shot generalization capabilities of MM-TTS by generating emotional speech guided by complex, compound emotions.
> By aligning emotional prompts within the shared emotion space using EPAlign, EMI-TTS can synthesize emotional speech for emotion categories unseen during training.
> This ability opens up new avenues for creative expression and personalization in emotional speech synthesis, enabling users to craft tailored emotional expressions beyond those encountered in the training data.

> Through this interactive demo, we aim to provide researchers and practitioners with a comprehensive understanding of MM-TTS’s potential applications, showcasing its ability to generate contextually appropriate, high-quality emotional speech across various scenarios.
> By addressing the challenges of multimodal emotion disentanglement and alignment, MM-TTS represents a significant step forward in emotional speech synthesis, with far-reaching implications for human-computer interaction, multimedia content creation, and beyond.

