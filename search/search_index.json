{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Sapphire Lab!","text":""},{"location":"TTS/","title":"Text-to-Speech (TTS) Synthesis","text":""},{"location":"TTS/#_1","title":"\u6a21\u578b\u5217\u8868","text":""},{"location":"TTS/#vocoder","title":"\u58f0\u7801\u5668 Vocoder","text":""},{"location":"TTS/#acoustic-model","title":"\u58f0\u5b66\u6a21\u578b Acoustic Model","text":""},{"location":"TTS/#end-to-end-model","title":"\u7aef\u5230\u7aef\u6a21\u578b End-to-End Model","text":""},{"location":"TTS/#large-language-model-based-model","title":"\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b Large Language Model-Based Model","text":"<ul> <li>2023.01 VALL-E</li> <li>2024.04 RALL-E</li> </ul>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/","title":"2024.04.23 FlashSpeech","text":"<p>@import \"../../style.less\"</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#flashspeech-efficient-zero-shot-speech-synthesis","title":"FlashSpeech: Efficient Zero-Shot Speech Synthesis","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: FlashSpeech: Efficient Zero-Shot Speech Synthesis - \u4f5c\u8005:   - [Zhen Ye](../../Authors/Zhen_Ye.md)   - [\u741a\u6cfd\u8c26](../../Authors/Zeqian_Ju_(\u741a\u6cfd\u8c26).md)   - [Haohe Liu](../../Authors/Haohe_Liu.md)   - [\u8c2d\u65ed](../../Authors/Xu_Tan_(\u8c2d\u65ed).md)   - [Jianyi Chen](../../Authors/Jianyi_Chen.md)   - [Yiwen Lu](../../Authors/Yiwen_Lu.md)   - [Peiwen Sun](../../Authors/Peiwen_Sun.md)   - [Jiahao Pan](../../Authors/Jiahao_Pan.md)   - [Weizhen Bian](../../Authors/Weizhen_Bian.md)   - [Shulin He](../../Authors/Shulin_He.md)   - [\u67f3\u5d0e\u5cf0](../../Authors/Qifeng_Liu_(\u67f3\u5d0e\u5cf0).md)   - [\u90ed\u6bc5\u53ef](../../Authors/Yike_Guo_(\u90ed\u6bc5\u53ef).md)   - [\u96ea\u5dcd](../../Authors/Wei_Xue_(\u96ea\u5dcd).md) - \u673a\u6784:   - [\u9999\u6e2f\u79d1\u6280\u5927\u5b66](../../Institutions/HKUST_\u9999\u6e2f\u79d1\u6280\u5927\u5b66.md)   - [\u5fae\u8f6f](../../Institutions/Microsoft.md)   - [\u8428\u91cc\u5927\u5b66](../../Institutions/University_of_Surrey.md)   - [\u5185\u8499\u53e4\u5927\u5b66](../../Institutions/IMU_\u5185\u8499\u53e4\u5927\u5b66.md)   - [\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66](../../Institutions/USTC_\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66.md)   - [\u65b0\u52a0\u5761\u56fd\u7acb\u5927\u5b66](../../Institutions/National_University_of_Singapore.md) - \u65f6\u95f4:   - 2024.04.23 ArXiv v1   - 2024.04.24 ArXiv v2   - 2024.04.25 ArXiv v3 - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2404.14700)   - [Demo](https://flashspeech.github.io/) - \u6807\u7b7e:   - \u8bed\u97f3\u5408\u6210   - \u96f6\u6837\u672c - \u9875\u6570: 17 - \u5f15\u7528: ? - \u88ab\u5f15: 0"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"\u539f\u6587  &gt; Recent progress in large-scale zero-shot speech synthesis has been significantly advanced by language models and diffusion models. &gt; However, the generation process of both methods is slow and computationally intensive. &gt; Efficient speech synthesis using a lower computing budget to achieve quality on par with previous work remains a significant challenge. &gt; In this paper, we present FlashSpeech, a large-scale zero-shot speech synthesis system with approximately 5\\% of the inference time compared with previous work. &gt; FlashSpeech is built on the latent consistency model and applies a novel adversarial consistency training approach that can train from scratch without the need for a pre-trained diffusion model as the teacher. &gt; Furthermore, a new prosody generator module enhances the diversity of prosody, making the rhythm of the speech sound more natural. &gt; The generation processes of FlashSpeech can be achieved efficiently with one or two sampling steps while maintaining high audio quality and high similarity to the audio prompt for zero-shot speech generation. &gt; Our experimental results demonstrate the superior performance of FlashSpeech. &gt; Notably, FlashSpeech can be about 20 times faster than other zero-shot speech synthesis systems while maintaining comparable performance in terms of voice quality and similarity. &gt; Furthermore, FlashSpeech demonstrates its versatility by efficiently performing tasks like voice conversion, speech editing, and diverse speech sampling. &gt; Audio samples can be found in\u00a0[this https URL](https://flashspeech.github.io/).   <p>\u8fd1\u671f, \u5927\u578b\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u5728\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7684\u63a8\u52a8\u4e0b\u53d6\u5f97\u663e\u8457\u8fdb\u5c55. \u7136\u800c, \u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u751f\u6210\u8fc7\u7a0b\u7f13\u6162\u4e14\u8ba1\u7b97\u5bc6\u96c6. \u5728\u8f83\u4f4e\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u548c\u4e4b\u524d\u5de5\u4f5c\u76f8\u5ab2\u7f8e\u7684\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218. \u672c\u5de5\u4f5c\u4ecb\u7ecd\u4e86 FlashSpeech, \u4e00\u4e2a\u4ec5\u9700\u5148\u524d\u5de5\u4f5c\u63a8\u7406\u65f6\u95f4 5% \u7684\u5927\u578b\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7cfb\u7edf. FlashSpeech \u5efa\u7acb\u5728\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\u4e4b\u4e0a, \u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u4e00\u81f4\u6027\u8bad\u7ec3\u65b9\u6cd5, \u80fd\u591f\u4ece\u96f6\u8bad\u7ec3\u800c\u65e0\u9700\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08. \u6b64\u5916, \u4e00\u4e2a\u65b0\u7684\u97f5\u5f8b\u751f\u6210\u5668\u6a21\u5757\u589e\u5f3a\u4e86\u97f5\u5f8b\u591a\u6837\u6027, \u4f7f\u5f97\u8bed\u97f3\u7684\u8282\u594f\u542c\u8d77\u6765\u66f4\u81ea\u7136. FlashSpeech \u7684\u751f\u6210\u8fc7\u7a0b\u80fd\u591f\u901a\u8fc7\u4e00\u4e24\u4e2a\u91c7\u6837\u6b65\u9aa4\u9ad8\u6548\u5b9e\u73b0, \u540c\u65f6\u4fdd\u6301\u9ad8\u97f3\u8d28\u4e0e\u96f6\u6837\u672c\u8bed\u97f3\u751f\u6210\u65f6\u53c2\u8003\u97f3\u9891\u7684\u9ad8\u76f8\u4f3c\u6027. \u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bf4\u660e\u4e86 FlashSpeech \u7684\u4f18\u8d8a\u6027. \u503c\u5f97\u6ce8\u610f\u7684\u662f, FlashSpeech \u76f8\u6bd4\u5176\u4ed6\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u5feb 20 \u500d, \u4f46\u5728\u8bed\u97f3\u8d28\u91cf\u548c\u76f8\u4f3c\u5ea6\u65b9\u9762\u4fdd\u6301\u4e86\u4e0e\u4e4b\u524d\u5de5\u4f5c\u76f8\u5f53\u7684\u6027\u80fd. \u6b64\u5916, FlashSpeech \u7684\u591a\u6837\u6027\u4f7f\u5176\u80fd\u591f\u9ad8\u6548\u5b8c\u6210\u8bf8\u5982\u8bed\u97f3\u8f6c\u6362\u3001\u8bed\u97f3\u7f16\u8f91\u548c\u591a\u6837\u5316\u8bed\u97f3\u91c7\u6837\u7b49\u4efb\u52a1. \u97f3\u9891\u793a\u4f8b\u53ef\u4ee5\u5728\u6b64\u94fe\u63a5\u4e2d\u627e\u5230.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"\u539f\u6587  &gt; In recent years, the landscape of speech synthesis has been transformed by the advent of large-scale generative models. &gt; Consequently, the latest research efforts have achieved notable advancements in zero-shot speech synthesis systems by significantly increasing the size of both datasets and models. &gt; Zero-shot speech synthesis, such as Text-to-Speech (TTS), Voice Conversion (VC) and Editing, aims to generate speech that incorporates unseen speaker characteristics from a reference audio segment during inference, without the need for additional training. &gt; Current advanced zero-shot speech synthesis systems typically leverage Language Models (LMs) for in-context speech generation on the large-scale dataset. &gt; &gt; However, the generation process of these methods needs a long-time iteration. &gt; For example, [VALL-E (2023)](../Speech_LLM/2023.01.05_VALL-E.md) builds on the language model to predict 75 audio token sequences for a 1-second speech, in its first-stage AutoRegressive (AR) token sequence generation. &gt; When using a Non-AutoRegressive (NAR) [Latent Diffusion Model](../_Basis/2021.12.20_LDM.md) based framework, [NaturalSpeech2 (2023)](2023.04.18_NaturalSpeech2.md) still requires 150 sampling steps. &gt; As a result, although these methods can produce human-like speech, they require significant computational time and cost. &gt; &gt; Some efforts have been made to accelerate the generation process. &gt; [Voicebox (2023)](../../Models/Speech_LLM/2023.06.23_VoiceBox.md) adopts [Flow-Matching (2022)](../_Basis/2022.10.06_Flow_Matching.md) so that fewer sampling steps (Number of function evaluations (NFE): 64) can be achieved because of the optimal transport path. &gt; [CLaM-TTS (2024)](../Speech_LLM/2024.04.03_CLaM-TTS.md) proposes a mel-codec with a superior compression rate and a latent language model that generates a stack of tokens at once. &gt; Although the slow generation speed issue has been somewhat alleviated, the inference speed is still far from satisfactory for practical applications. &gt; Moreover, the substantial computational time of these approaches leads to significant computational cost overheads, presenting another challenge.   <p>\u8fd1\u5e74\u6765, \u968f\u7740\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u51fa\u73b0, \u8bed\u97f3\u5408\u6210\u9886\u57df\u53d1\u751f\u4e86\u5de8\u5927\u8f6c\u53d8. \u56e0\u6b64, \u6700\u65b0\u7684\u7814\u7a76\u5de5\u4f5c\u901a\u8fc7\u663e\u8457\u589e\u52a0\u6570\u636e\u96c6\u548c\u6a21\u578b\u5927\u5c0f, \u5728\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55. \u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210, \u5982\u6587\u672c\u8f6c\u8bed\u97f3, \u58f0\u97f3\u8f6c\u6362, \u7f16\u8f91, \u65e8\u5728\u5728\u63a8\u7406\u65f6\u5c06\u672a\u89c1\u8fc7\u7684\u53d1\u8a00\u4eba\u7279\u5f81\u5d4c\u5165\u5230\u8bed\u97f3\u4e2d, \u800c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3. \u5f53\u524d\u5148\u8fdb\u7684\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u7c7b\u6a21\u578b\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e0a\u4e0b\u6587\u6587\u672c\u751f\u6210.</p> <p>\u7136\u800c, \u8fd9\u4e9b\u65b9\u6cd5\u7684\u751f\u6210\u8fc7\u7a0b\u9700\u8981\u5f88\u957f\u65f6\u95f4\u7684\u8fed\u4ee3. - VALL-E (2023) \u5728\u7b2c\u4e00\u9636\u6bb5\u7684\u81ea\u56de\u5f52\u6807\u8bc6\u7b26\u5e8f\u5217\u751f\u6210\u8fc7\u7a0b\u4e2d, \u8bed\u8a00\u6a21\u578b\u9884\u6d4b 1 \u79d2\u8bed\u97f3\u9700\u8981\u9884\u6d4b 75 \u4e2a\u97f3\u9891\u6807\u8bc6\u7b26\u5e8f\u5217. - NaturalSpeech2 (2023) \u4f7f\u7528\u57fa\u4e8e\u975e\u81ea\u56de\u5f52\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6, \u4f46\u4ecd\u7136\u9700\u8981 150 \u4e2a\u91c7\u6837\u6b65\u9aa4.</p> <p>\u56e0\u6b64, \u5c3d\u7ba1\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u8bed\u97f3, \u4f46\u5b83\u4eec\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u65f6\u95f4\u548c\u6210\u672c.</p> <p>\u5df2\u7ecf\u6709\u4e00\u4e9b\u5de5\u4f5c\u5c1d\u8bd5\u52a0\u901f\u751f\u6210\u8fc7\u7a0b. - Voicebox (2023) \u91c7\u7528\u6d41\u5339\u914d (Flow Matching, FM), \u7531\u4e8e\u6700\u4f18\u4f20\u8f93\u8def\u5f84\u53ef\u4ee5\u5b9e\u73b0\u8f83\u5c11\u7684\u91c7\u6837\u6b65\u9aa4. - CLaM-TTS (2024) \u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u4f18\u538b\u7f29\u7387\u7684 mel-codec, \u4ee5\u53ca\u4e00\u4e2a\u6f5c\u5728\u8bed\u8a00\u6a21\u578b, \u4e00\u6b21\u751f\u6210\u4e00\u7cfb\u5217 token. \u5c3d\u7ba1\u6162\u901f\u751f\u6210\u901f\u5ea6\u95ee\u9898\u6709\u6240\u7f13\u89e3, \u4f46\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u6765\u8bf4, \u63a8\u7406\u901f\u5ea6\u4ecd\u7136\u4e0d\u8db3. \u6b64\u5916, \u8fd9\u4e9b\u65b9\u6cd5\u7684\u5927\u91cf\u8ba1\u7b97\u65f6\u95f4\u5bfc\u81f4\u4e86\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500, \u8fd9\u53c8\u662f\u4e00\u4e2a\u6311\u6218.</p> \u539f\u6587  &gt; The fundamental limitation of speech generation stems from the intrinsic mechanisms of language models and diffusion models, which require considerable time either auto-regressively or through a large number of denoising steps. &gt; Hence, the primary objective of this work is to accelerate inference speed and reduce computational costs while preserving generation quality at levels comparable to the prior research. &gt; &gt; In this paper, we propose FlashSpeech as the next step towards efficient zero-shot speech synthesis. &gt; To address the challenge of slow generation speed, we leverage the [Latent Consistency Model (LCM) (2023)](), a recent advancement in generative models. &gt; Building upon the previous non-autoregressive TTS system ([NaturalSpeech2 (2023)](2023.04.18_NaturalSpeech2.md)), we adopt the encoder of a neural audio codec to convert speech waveforms into latent vectors as the training target for our LCM. &gt; To train this model, we propose a novel technique called adversarial consistency training, which utilizes the capabilities of pre-trained speech language models ([WavLM](); [HuBERT (2021)](../Speech_Representaion/2021.06.14_HuBERT.md); [Wav2Vec2.0 (2020)](../Speech_Representaion/2020.06.20_Wav2Vec2.0.md)) as discriminators. &gt; This facilitates the transfer of knowledge from large pre-trained speech language models to speech generation tasks, efficiently integrating adversarial and consistency training to improve performance. &gt; The LCM is conditioned on prior vectors obtained from a phoneme encoder, a prompt encoder, and a prosody generator. &gt; Furthermore, we demonstrate that our proposed prosody generator leads to more diverse expressions and prosody while preserving stability.   <p>\u8bed\u97f3\u751f\u6210\u7684\u6839\u672c\u9650\u5236\u5728\u4e8e\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7684\u5185\u5728\u673a\u5236, \u4ee5\u81ea\u56de\u5f52\u65b9\u5f0f\u6216\u9700\u8981\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\u4ece\u800c\u9700\u8981\u76f8\u5f53\u7684\u65f6\u95f4. \u56e0\u6b64, \u672c\u9879\u5de5\u4f5c\u7684\u4e3b\u8981\u76ee\u6807\u662f\u52a0\u901f\u63a8\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c, \u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u4e0e\u4e4b\u524d\u7814\u7a76\u6c34\u5e73\u76f8\u5f53.</p> <p>\u672c\u6587\u63d0\u51fa\u4e86 FlashSpeech \u4f5c\u4e3a\u9ad8\u6548\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7684\u4e0b\u4e00\u6b65. \u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u901f\u5ea6\u6162\u7684\u95ee\u9898, \u6211\u4eec\u91c7\u7528\u4e86\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b (Latent Consistency Model, LCM), \u8fd9\u662f\u8fd1\u5e74\u6765\u751f\u6210\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55. \u57fa\u4e8e\u5148\u524d\u975e\u81ea\u56de\u5f52\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf NaturalSpeech2 (2023), \u6211\u4eec\u91c7\u7528\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684\u7f16\u7801\u5668, \u5c06\u8bed\u97f3\u6ce2\u5f62\u8f6c\u6362\u4e3a\u9690\u5411\u91cf, \u4f5c\u4e3a LCM \u7684\u8bad\u7ec3\u76ee\u6807. \u4e3a\u4e86\u8bad\u7ec3\u8be5\u6a21\u578b, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u540d\u4e3a\u5bf9\u6297\u4e00\u81f4\u6027\u8bad\u7ec3 (Adversarial Consistency Training) \u7684\u6280\u672f, \u8be5\u6280\u672f\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u4f5c\u4e3a\u5224\u522b\u5668. \u8fd9\u6709\u52a9\u4e8e\u5c06\u77e5\u8bc6\u4ece\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u8fc1\u79fb\u5230\u8bed\u97f3\u751f\u6210\u4efb\u52a1\u4e2d, \u6709\u6548\u5730\u96c6\u6210\u5bf9\u6297\u6027\u548c\u4e00\u81f4\u6027\u8bad\u7ec3, \u63d0\u9ad8\u6027\u80fd. LCM \u5c06\u97f3\u7d20\u7f16\u7801\u5668, \u63d0\u793a\u7f16\u7801\u5668\u548c\u97f5\u5f8b\u751f\u6210\u5668\u83b7\u5f97\u7684\u5148\u9a8c\u5411\u91cf\u4f5c\u4e3a\u6761\u4ef6. \u6b64\u5916, \u6211\u4eec\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u97f5\u5f8b\u751f\u6210\u5668\u80fd\u591f\u4ea7\u751f\u66f4\u5177\u591a\u6837\u6027\u7684\u8868\u8fbe\u548c\u97f5\u5f8b, \u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027.</p> \u539f\u6587  &gt; Our contributions can be summarized as follows: &gt; - We propose FlashSpeech, an efficient zero-shot speech synthesis system that generates voice with high audio quality and speaker similarity in zero-shot scenarios. &gt; - We introduce adversarial consistency training, a novel combination of consistency and adversarial training leveraging pre-trained speech language models, for training the latent consistency model from scratch, achieving speech generation in one or two steps. &gt; - We propose a prosody generator module that enhances the diversity of prosody while maintaining stability. &gt; - FlashSpeech significantly outperforms strong baselines in audio quality and matches them in speaker similarity. &gt; Remarkably, it achieves this at a speed approximately 20 times faster than comparable systems, demonstrating unprecedented efficiency.   <p>\u6211\u4eec\u7684\u8d21\u732e\u53ef\u4ee5\u603b\u7ed3\u5982\u4e0b: - \u6211\u4eec\u63d0\u51fa FlashSpeech, \u4e00\u79cd\u9ad8\u6548\u7684\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7cfb\u7edf, \u80fd\u591f\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u751f\u6210\u9ad8\u97f3\u8d28\u548c\u53d1\u8a00\u4eba\u76f8\u4f3c\u5ea6\u7684\u8bed\u97f3. - \u6211\u4eec\u5f15\u5165\u5bf9\u6297\u4e00\u81f4\u6027\u8bad\u7ec3, \u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u5bf9\u6297\u8bad\u7ec3\u7684\u65b0\u578b\u7ec4\u5408, \u7528\u4e8e\u4ece\u96f6\u8bad\u7ec3\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b, \u5b9e\u73b0\u8bed\u97f3\u751f\u6210\u5728\u4e00\u6216\u4e24\u6b65. - \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u97f5\u5f8b\u751f\u6210\u5668\u6a21\u5757, \u589e\u5f3a\u4e86\u97f5\u5f8b\u591a\u6837\u6027, \u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027. - FlashSpeech \u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u5728\u97f3\u9891\u8d28\u91cf\u548c\u53d1\u8a00\u4eba\u76f8\u4f3c\u5ea6\u65b9\u9762\u7684\u8868\u73b0, \u4e0e\u5b83\u4eec\u76f8\u5f53. \u503c\u5f97\u6ce8\u610f\u7684\u662f, \u5b83\u5728\u901f\u5ea6\u4e0a\u4e0e\u76f8\u5f53\u7684\u7cfb\u7edf\u76f8\u5f53, \u5c55\u793a\u4e86\u524d\u6240\u672a\u6709\u7684\u6548\u7387.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#2related-work","title":"2.Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#21larger-scale-speech-synthesis","title":"2.1.Larger-Scale Speech Synthesis","text":"\u539f\u6587  &gt; Motivated by the success of the large language model, the speech research community has recently shown increasing interest in scaling the sizes of model and training data to bolster generalization capabilities, producing natural speech with diverse speaker identities and prosody under zero-shot settings. &gt; The pioneering work is [VALL-E (2023)](../Speech_LLM/2023.01.05_VALL-E.md), which adopts the [EnCodec (2022)](../Speech_Neural_Codec/2022.10.24_EnCodec.md) to discretize the audio waveform into tokens. &gt; Therefore, a language model can be trained via in-context learning that can generate the target utterance where the style is consistent with prompt utterance. &gt; However, generating audio in such an autoregressive manner ([LM-VC (2023)](2023_LM-VC.md); [VoiceCraft (2024)](../_tmp/2024.03.25_VoiceCraft.md)) can lead to unstable prosody, word skipping, and repeating issues ([FastSpeech2](../TTS2_Acoustic/2020.06.08_FastSpeech2.md); Tan et al.(2021); [NaturalSpe2023.04.18_NaturalSpeech2.mdralSpeech2.md)) &gt; To ensure the robustness of the system, non-autoregressive methods such as [NaturalSpeech2 (2023)](2023.04.18_NaturalSpeech2.md) and [Voicebox (2023)](../../Models/Speech_LLM/2023.06.23_VoiceBox.md) utilize diffusion-style model (VP-diffusion Song et al.(2020) or [Flow-Matching (2022)](../_Basis/2022.10.06_Flow_Matching.md)) to learn the distribution of a continuous intermediate vector such as mel-spectrogram or latent vector of codec. &gt; Both LM-based methods Zhao et al.(2023) and diffusion-based methods show superior performance in speech generation tasks. &gt; However, their generation is slow due to the iterative computation. &gt; Considering that many speech generation scenarios require real-time inference and low computational costs, we employ the latent consistency model for large-scale speech generation that inference with one or two steps while maintaining high audio quality.   <p>\u7531\u4e8e LLM \u7684\u6210\u529f, \u97f3\u9891\u7814\u7a76\u793e\u533a\u8fd1\u671f\u5c55\u793a\u4e86\u5728\u6269\u5927\u6a21\u578b\u5c3a\u5bf8\u548c\u8bad\u7ec3\u6570\u636e\u6570\u91cf\u4ee5\u652f\u6301\u6cdb\u5316\u80fd\u529b, </p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#22acceleration-of-speech-synthesis","title":"2.2.Acceleration of Speech Synthesis","text":"\u539f\u6587  &gt; Since early neural speech generation models Tan et al.(2021) use autoregressive models such as Tacotron Wang et al.(2017) and TransformerTTS Li et al.(2019), causing slow inference speed, with $\\mathcal{O}(n)$ computation, whereNis the sequence length. &gt; To address the slow inference speed, [FastSpeech (2019)](../TTS2_Acoustic/2019.05.22_FastSpeech.md), [FastSpeech2](../TTS2_Acoustic/2020.06.08_FastSpeech2.md) proposes to generate a mel-spectrogram in a non-autoregressive manner. &gt; However, these models Ren et al.(2022) result in blurred and over-smoothed mel-spectrograms due to the regression loss they used and the capability of modeling methods. &gt; To further enhance the speech quality, diffusion models are utilized Popov et al.(2021a); Jeong et al.(2021); Popov et al.(2021b) which increase the computation to $\\mathcal{O}(T)$, where $T$ is the diffusion steps. &gt; Therefore, distillation techniques Luo (2023) for diffusion-based methods such as CoMoSpeech Ye et al.(2023), CoMoSVC Lu et al.(2024) and Reflow-TTS Guan et al.(2023) emerge to reduce the sampling steps back to $\\mathcal{O}(1)$, but require additional pre-trained diffusion as the teacher model. &gt; Unlike previous distillation techniques, which require extra training for the diffusion model as a teacher and are limited by its performance, our proposed adversarial consistency training technique can directly train from scratch, significantly reducing training costs. &gt; In addition, previous acceleration methods only validate speaker-limited recording-studio datasets with limited data diversity. &gt; To the best of our knowledge, FlashSpeech is the first work that reduces the computation of a large-scale speech generation system back to $\\mathcal{O}(1)$."},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#23consistency-model","title":"2.3.Consistency Model","text":"\u539f\u6587  &gt; The Consistency Model is proposed in [Song et al.(2023)](); [Song and Dhariwal.(2023)]() to generate high-quality samples by directly mapping noise to data. &gt; Furthermore, many variants [Kong et al.(2023)](); [Lu et al.(2023)](); [Sauer et al.(2023)](); [Kim et al.(2023a)]() are proposed to further increase the generation quality of images. &gt; The [Latent Consistency Model (2023)](../_Basis/2023.10.06_LCM.md) is proposed by Luo et al. which can directly predict the solution of PF-ODE in latent space. &gt; However, the [original LCM](../_Basis/2023.10.06_LCM.md) employs consistency distillation on the pre-trained **Latent Diffusion Model (LDM)** which leverages large-scale off-the-shelf image diffusion models Rombach et al.(2022). &gt; Since there are no pre-trained large-scale TTS models in the speech community, and inspired by the techniques Song and Dhariwal (2023); Kim et al.(2023a); Lu et al.(2023); Sauer et al.(2023); Kong et al.(2023), we propose the novel adversarial consistency training method which can directly train the large-scale latent consistency model from scratch utilizing the large pre-trained speech language model Chen et al.(2022b); [HuBERT (2021)](../Speech_Representaion/2021.06.14_HuBERT.md); Baevski et al. (2020) such as WavLM for speech generation."},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#3method","title":"3.Method\u00b7\u65b9\u6cd5","text":""},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#31overview","title":"3.1.Overview\u00b7\u603b\u89c8","text":"<p>Our work is dedicated to advancing the speech synthesis efficiency, achieving $\\mathcal{O}(1)$ computation cost while maintaining comparable performance to prior studies that require $\\mathcal{O}(T)$ or $\\mathcal{O}(N)$ computations.  The framework of the proposed method, FlashSpeech, is illustrated in Fig.02. FlashSpeech integrates a neural codec, an encoder for phonemes and prompts, a prosody generator, and an LCM, which are utilized during both the training and inference stages. Exclusively during training, a conditional discriminator is employed. FlashSpeech adopts the in-context learning paradigm VALL-E (2023), initially segmenting the latent vector $\\mathbf{z}$, extracted from the codec, into $\\mathbf{z}{target}$ and $\\mathbf{z}{prompt}$. Subsequently, the phoneme and $\\mathbf{z}_{prompt}$ are processed through the encoder to produce the hidden feature. A prosody generator then predicts pitch and duration based on the hidden feature. The pitch and duration embeddings are combined with the hidden feature and inputted into the LCM as the conditional feature. The LCM model is trained from scratch using adversarial consistency training. After training, FlashSpeech can achieve efficient generation within one or two sampling steps.</p> <p>\u672c\u9879\u5de5\u4f5c\u4e13\u6ce8\u4e8e\u63d0\u9ad8\u8bed\u97f3\u5408\u6210\u6548\u7387, \u5728\u4fdd\u6301\u548c\u5148\u524d\u5de5\u4f5c\u76f8\u8fd1\u6027\u80fd\u7684\u540c\u65f6, \u5c06 $\\mathcal{O}(T)$ \u6216 $\\mathcal{O}(N)$ \u8ba1\u7b97\u91cf\u964d\u4f4e\u5230 $\\mathcal{O}(1)$. \u56fe 02 \u5c55\u793a\u4e86\u6240\u63d0\u65b9\u6cd5 FlashSpeech \u7684\u6846\u67b6.</p> <p></p> <p>FlashSpeech \u7ee7\u627f\u4e86\u795e\u7ecf\u7f16\u89e3\u7801\u5668, \u97f3\u7d20\u548c\u63d0\u793a\u7684\u7f16\u7801\u5668, \u97f5\u5f8b\u751f\u6210\u5668, \u548c\u4e00\u4e2a\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b (Latent Consistency Model, LCM), \u5e76\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u4f7f\u7528\u5b83\u4eec. \u4ec5\u5728\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u6761\u4ef6\u5224\u522b\u5668 (Conditional Discriminator). FlashSpeech \u91c7\u7528\u4e86 VALL-E (2023) \u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u8303\u5f0f, \u9996\u5148\u5c06\u4ece\u7f16\u89e3\u7801\u5668\u63d0\u53d6\u51fa\u7684\u9690\u5411\u91cf $\\mathbf{z}$ \u5206\u5272\u4e3a $\\mathbf{z}{target}$ \u548c $\\mathbf{z}{prompt}$. \u7136\u540e, \u97f3\u7d20\u548c $\\mathbf{z}_{prompt}$ \u901a\u8fc7\u7f16\u7801\u5668\u4ea7\u751f\u9690\u7279\u5f81. \u97f5\u5f8b\u751f\u6210\u5668\u57fa\u4e8e\u9690\u7279\u5f81\u9884\u6d4b\u97f3\u9ad8\u548c\u65f6\u957f. \u97f3\u9ad8\u548c\u65f6\u957f\u5d4c\u5165\u548c\u9690\u7279\u5f81\u7ed3\u5408\u540e\u4f5c\u4e3a\u6761\u4ef6\u7279\u5f81\u8f93\u5165\u5230 LCM \u79cd. LCM \u6a21\u578b\u4f7f\u7528\u5bf9\u6297\u4e00\u81f4\u8bad\u7ec3\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3. \u8bad\u7ec3\u5b8c\u6210\u540e, FlashSpeech \u53ef\u4ee5\u5728\u4e00\u5230\u4e24\u4e2a\u91c7\u6837\u6b65\u5185\u5b9e\u73b0\u9ad8\u6548\u751f\u6210.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#32latent-consistency-model","title":"3.2.Latent Consistency Model","text":"<p>The Consistency Model (Song et al.(2023)) is a new family of generative models that enables one-step or few-step generation. </p> <p>\u4e00\u81f4\u6027\u6a21\u578b\u662f\u4e00\u7c7b\u65b0\u7684\u751f\u6210\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u4e00\u6b65\u6216\u51e0\u6b65\u751f\u6210.</p> <p>Let us denote the data distribution by $p_{data}(\\mathbf{x})$.  The core idea of the consistency model is to learn the function that maps any points on a trajectory of the PF-ODE to that trajectory\u2019s origin, which can be formulated as:</p> <p>\u6570\u636e\u5206\u5e03\u8bb0\u4e3a $p_{data}(\\mathbf{x})$. \u4e00\u81f4\u6027\u6a21\u578b\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5b66\u4e60\u4e00\u4e2a\u6620\u5c04\u51fd\u6570, \u5c06 PF-ODE \u8f68\u8ff9\u4e0a\u7684\u4efb\u610f\u70b9\u6620\u5c04\u5230\u8be5\u8f68\u8ff9\u7684\u8d77\u70b9, \u53ef\u4ee5\u8868\u793a\u4e3a:</p> <p>$$   f(\\mathbf{x}\\sigma, \\sigma) = \\mathbf{x}{\\sigma_{\\min}},\\tag{01} $$</p> <p>where $f(\\cdot,\\cdot)$ is the consistency function and $\\mathbf{x}{\\sigma}$ represents the data $\\mathbf{x}$ perturbed by adding zero-mean Gaussian noise with standard deviation $\\sigma$. $\\sigma{\\min}$ is a fixed small positive number.</p> <p>\u5176\u4e2d $f(\\cdot, \\cdot)$ \u662f\u4e00\u81f4\u6027\u51fd\u6570, $\\mathbf{x}{\\sigma}$ \u8868\u793a\u6570\u636e $\\mathbf{x}$ \u6dfb\u52a0\u96f6\u5747\u503c $\\sigma$ \u6807\u51c6\u5dee\u7684\u9ad8\u65af\u566a\u58f0\u6270\u52a8\u540e\u7684\u6570\u636e. $\\sigma{\\min}$ \u662f\u56fa\u5b9a\u7684\u5c0f\u6b63\u6570.</p> <p>Then $\\mathbf{x}{\\sigma{\\min}}$ can then be viewed as an approximation sample from the data distribution $p_{data}(\\mathbf{x})$. To satisfy property in Eq.01, following Song et al.(2023), we parameterize the consistency model as</p> <p>$\\mathbf{x}{\\sigma{\\min}}$ \u53ef\u4ee5\u89c6\u4e3a\u6570\u636e\u5206\u5e03 $p_{data}(\\mathbf{x})$ \u7684\u8fd1\u4f3c\u91c7\u6837. \u4e3a\u4e86\u6ee1\u8db3\u5982\u65b9\u7a0b 01 \u6240\u793a\u7684\u6027\u8d28, \u9075\u5faa Song et al., \u6211\u4eec\u5c06\u4e00\u81f4\u6027\u6a21\u578b\u53c2\u6570\u5316\u4e3a</p> <p>$$   f_{\\theta}(\\mathbf{x}{\\sigma}, \\sigma) = c{skip}(\\sigma)\\mathbf{x} + c_{out}(\\sigma) F_{\\theta}(\\mathbf{x}_{\\sigma}, \\sigma),\\tag{02} $$</p> <p>where $f_{\\theta}$ is to estimate consistency function $f$ by learning from data, $F_{\\theta}$ is a deep neural network with parameter $\\theta$, $c_{skip}(\\sigma)$ and $c_{out}(\\sigma)$ are differentiable functions with $c_{skip}(\\sigma_{\\min})=1$ and $c_{out}(\\sigma_{\\min})=0$ to ensure boundary condition.</p> <p>\u5176\u4e2d  - $f_{\\theta}$ \u901a\u8fc7\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5f97\u5230, \u7528\u4e8e\u4f30\u8ba1\u4e00\u81f4\u6027\u51fd\u6570 $f$, - $F_{\\theta}$ \u662f\u5177\u6709\u53c2\u6570 $\\theta$ \u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, - $c_{skip}(\\sigma)$ \u548c $c_{out}(\\sigma)$ \u662f\u53ef\u5fae\u51fd\u6570, \u4e14 $c_{skip}(\\sigma_{\\min})=1$ \u548c $c_{out}(\\sigma_{\\min})=0$ \u7528\u4e8e\u786e\u4fdd\u8fb9\u754c\u6761\u4ef6.</p> <p>A valid consistency model should satisfy the self-consistency property (Song et al.(2023))</p> <p>\u4e00\u4e2a\u6709\u6548\u7684\u4e00\u81f4\u6027\u6a21\u578b\u9700\u8981\u6ee1\u8db3\u81ea\u4e00\u81f4\u6027:</p> <p>$$   f_{\\theta}(\\mathbf{x}\\sigma, \\sigma) = f{\\theta}(\\mathbf{x}{\\sigma'}, \\sigma'),\\quad \\forall \\sigma, \\sigma' \\in [\\sigma{\\min}, \\sigma_{\\max}].\\tag{03} $$</p> <p>where $\\sigma_{\\max}=80$ and $\\sigma_{\\min}=0.002$ following Karras et al. (2022); Song et al. (2023); Song and Dhariwal (2023). Then the model can generate samples in one step by evaluating </p> <p>$$   \\mathbf{x}{\\sigma{\\min}} = f_{\\theta}(\\mathbf{x}{\\sigma{\\max}}, \\sigma_{\\max})\\tag{04} $$</p> <p>from distribution $\\mathbf{x}{\\sigma{\\max}}\\sim\\mathcal{N}(0,\\sigma_{\\max}^2 \\mathbf{I})$.</p> <p>As we apply a consistency model on the latent space of audio, we use the latent feature $\\mathbf{z}$ which are extracted prior to the residual quantization layer of the codec.</p> <p>$$   \\mathbf{z} = CodecEncoder(\\mathbf{y})\\tag{05} $$</p> <p>where $\\mathbf{y}$ is the speech waveform.</p> <p>Futhermore, we add the feature from the prosody generator and encoder as the conditional feature $c$, our objective has changed to achieve</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#33adversarial-consistency-training","title":"3.3.Adversarial Consistency Training","text":"<p>A major drawback of the LCM is that it needs to pre-train a diffusion-based teacher model in the first stage, and then perform distillation to produce the final model. This would make the training process complicated, and the performance would be limited as a result of the distillation. To eliminate the reliance on the teacher model training, in this paper, we propose a novel adversarial consistency training method to train LCM from scratch. Our training procedure is outlined in Fig.03, which has three parts:</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#331consistency-training","title":"3.3.1.Consistency Training","text":""},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#332adversarial-training","title":"3.3.2.Adversarial Training","text":""},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#333combined-together","title":"3.3.3.Combined Together","text":""},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#34prosody-generator","title":"3.4.Prosody Generator","text":""},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#341analysis-of-prosody-prediction","title":"3.4.1.Analysis of Prosody Prediction","text":"<p>Previous regression methods for prosody prediction (FastSpeech2; NaturalSpeech2 (2023)), due to their deterministic mappings and assumptions of unimodal distribution, often fail to capture the inherent diversity and expressiveness of human speech prosody. This leads to predictions that lack variation and can appear over-smoothed. On the other hand, diffusion methods (Voicebox (2023); Li et al.(2023)) for prosody prediction offer a promising alternative by providing greater prosody diversity. However, they come with challenges regarding stability, and the potential for unnatural prosody. Additionally, the iterative inference process in DMs requires a significant number of sampling steps that may also hinder real-time appli cation. Meanwhile, LM-based methods (Mega-TTS2; VALL-E (2023)) also need a long time for inference. To alleviate these issues, our prosody generator consists of a prosody regression module and a prosody refinement module to enhance the diversity of prosody regression results with efficient one-step consistency model sampling.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#342prosody-refinement-via-consistency-model","title":"3.4.2.Prosody Refinement via Consistency Model","text":"<p>As shown in 4, our prosody generator consists of two parts which are prosody regression and prosody refinement. We first train the prosody regression module to get a deterministic output. Next, we freeze the parameters of the prosody regression module and use the residual of ground truth prosody and deterministic predicted prosody as the training target for prosody refinement. We adopt a consistency model as a prosody refinement module. The conditional feature of the consistency model is the feature from prosody regression before the final projection layer. Thus, the residual from a stochastic sampler refines the output of a deterministic prosody regression and produces a diverse set of plausible prosody under the same transcription and audio prompt. One option for the final prosody output p final can be represented as: p final= pres+ p init,(18) where p final denotes the final prosody output, p res represents the residual output from the prosody refinement module, capturing the variations between the ground truth prosody and the deterministic prediction, p init is the initial deterministic prosody prediction from the prosody regression module. However, this formulation may negatively affect prosody stability, a similar observation is found in (Audiobox; Voicebox (2023)). More specifically, higher diversity may cause less stability and sometimes produce unnatural prosody. To address this, we introduce a control factor  $\\alpha$  that finely tunes the balance between stability and diversity in the prosodic output: p final=  $\\alpha$  p res+ p init(19) where  $\\alpha$  is a scalar value ranging between 0 and 1. This adjustment allows for controlled incorporation of variability into the prosody, mitigating issues related to stability while still benefiting from the diversity offered by the prosody refinement module.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#35applications","title":"3.5.Applications","text":"<p>This section elaborates on the practical applications of FlashSpeech. We delve into its deployment across various tasks such as zero-shot TTS, speech editing, voice conversion, and diverse speech sampling. All the sample audios of applications are available on the demo page.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#351zero-shot-tts","title":"3.5.1.Zero-Shot TTS","text":"<p>Given a target text and reference audio, we first convert the text to phoneme using g2p (grapheme-to-phoneme conversion). Then we use the codec encoder to convert the reference audio into $z_{prompt}$. Speech can be synthesized efficiently through FlashSpeech with the phoneme input and $z_{prompt}$, achieving high-quality text-to-speech results without requiring pre-training on the specific voice.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#352voice-conversion","title":"3.5.2.Voice Conversion","text":"<p>Voice conversion aims to convert the source audio into the target audio using the speaker\u2019s voice of the reference audio. Following NaturalSpeech2 (2023); Preechakul et al.(2022), we first apply the reverse of ODE to diffuse the source audio into a starting point that still maintains some information in the source audio. After that, we run the sampling process from this starting point with the reference audio as z prompt and condition c. The condition c uses the phoneme and duration from the source audio and the pitch is predicted by the prosody generator. This method allows for zero-shot voice conversion while preserving the linguistic content of the source audio, and achieving the same timbre as the reference audio.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#353speech-editing","title":"3.5.3.Speech Editing","text":"<p>Given the speech, the original transcription, and the new transcription, we first use MFA (Montreal Forced Aligner) to align the speech and the original transcription to get the duration of each word. Then we remove the part that needs to be edited to construct the reference audio. Next, we use the new transcription and reference to synthesize new speech. Since this task is consistent with the in-context learning, we can concatenate the remaining part of the raw speech and the synthesized part as the final speech, thus enabling precise and seamless speech editing.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#354diverse-speech-sampling","title":"3.5.4.Diverse Speech Sampling","text":"<p>FlashSpeech leverages its inherent stochastic it y to generate a variety of speech outputs under the same conditions. By employing stochastic sampling in its prosody generation and LCM, FlashSpeech can produce diverse variations in pitch, duration, and overall audio characteristics from the same phoneme input and audio prompt. This feature is particularly useful for generating a wide range of speech expressions and styles from a single input, enhancing applications like voice acting, synthetic voice variation for virtual assistants, and more personalized speech synthesis. In addition, the synthetic data via speech sampling can also benefit other tasks such as ASR (Rossenbach et al. (2020)).</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#4experiment","title":"4.Experiment","text":"<p>In the experimental section, we begin by introducing the datasets and the configurations for training in our experiments. Following this, we show the evaluation metrics and demonstrate the comparative results against various zero-shot TTS models. Subsequently, ablation studies are conducted to test the effectiveness of several design choices. Finally, we also validate the effectiveness of other tasks such as voice conversion. We show our speech editing and diverse speech sampling results on our demo page.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#41experimental-settings","title":"4.1.Experimental Settings","text":""},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#411data-and-preprocessing","title":"4.1.1.Data and Preprocessing","text":"<p>We use the English subset of Multilingual LibriSpeech (MLS) Pratap et al.(2020), including 44.5k hours of transcribed audiobook data and it contains 5490 distinct speakers. The audio data is resampled at a frequency of 16kHz. The input text is transformed into a sequence of phonemes through grapheme-to-phoneme conversion Sun et al.(2019) and then we use our internal alignment tool aligned with speech to obtain the phoneme-level duration. We adopt a hop size of 200 for all frame-level features. The pitch sequence is extracted using PyWorld2. we adopt EnCodec (2022) as our audio codec. We use a modified version3and train it on MLS. We use the dense features extracted before the residual quantization layer as our latent vector z.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#412training-details","title":"4.1.2.Training Details","text":"<p>Our training consists of two stages, in the first stage we train LCM and the prosody regression part. We use 8 H800 80GB GPUs with a batch size of 20k frames of latent vectors per GPU for 650k steps. We use the AdamW optimizer with a learning rate of 3e-4, warm up the learning rate for the first 30k updates and then linear decay it. We deactivate adversarial training with\u03bbadv= 0 before 600K training iterations. For hyper-parameters, we setain Equation(12)to 0.03. In equation(10), \u03c3i=\ufffd\u03c31/\u03c1min+i\u22121N(k)\u22121\ufffd\u03c31/\u03c1max\u2212 \u03c31/\u03c1min\ufffd\ufffd\u03c1,wherei \u2208 [1, N(k)],\u03c1 = 7, \u03c3min= 0.002, \u03c3max= 80. For N(k) in Equation(11), we sets0= 10, s1= 1280, K = 600k. After 600k steps, we activate adversarial loss, and N(k) can be considered as fixed to 1280. We crop the waveform length fed into the discriminator into minimum waveform length in a minibatch. In addition, the weight of the feature extractor WavLM and the codec decoder are frozen. In the second stage, we train 150k steps for the prosody refinement module with consistency training in Equation(10). Different from the above setting, we empirically sets1= 160,K = 150k. During training, only the weight of the prosody refinement part is updated.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#413model-details","title":"4.1.3.Model Details","text":"<p>The model structures of the prompt encoder and phoneme encoder are follow NaturalSpeech2 (2023). The neural function part in LCM is almost the same as the NaturalSpeech2 (2023). We rescale the sinusoidal position embedding in the neural function part by a factor of 1000. As for the prosody generator, we adopt 30 non-casual WaveNet (2016) layers for the neural function part in the prosody refinement module and the same configurations for prosody regression parts in NaturalSpeech2 (2023). And we set $\\alpha= 0.2$ for the prosody refinement module empirically. For the discriminator\u2019s head, we stack 5 convolutional layers with weight normalization Salimans and Kingma (2016) for binary classification.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#42evaluation-metrics","title":"4.2.Evaluation Metrics","text":"<p>We use both objective and subjective evaluation metrics, including - Real-time-factor (RTF): RTF measures the time taken for the system to generate one second of speech. This metric is crucial for evaluating the efficiency of our system, particularly for applications requiring real-time processing. We measure the time of our system end-to-end on an NVIDIA V100 GPU following NaturalSpeech2 (2023). - Sim-O and Sim-R: These metrics assess the speaker similarity. Sim-R measures the objective similarity between the synthesized speech and the reconstruction reference speech through the audio codec, using features embedding extracted from the pre-trained speaker verification model VALL-E (2023); [ClaM-TTS (2024)](2024.04.../_tmp/2024.04.03_CLaM-TTS.md Sim-O is calculated with the original reference speech. Higher scores in Sim-O and Sim-R indicate a higher speaker similarity. - WER (Word Error Rate): To evaluate the accuracy and clarity of synthesized speech from the TTS system, we employ the Automatic Speech Recognition (ASR) model Wang et al. (2023a)5to transcribe generated audio. The discrepancies between these transcriptions and original texts are quantified using the Word Error Rate (WER), a crucial metric indicating intelligibility and robustness. - CMOS, SMOS, UTMOS: we rank the comparative mean option score (CMOS) and similarity mean option score (SMOS) using mturk. The prompt for CMOS refers to \u2019Please focus on the audio quality and naturalness and ignore other factors.\u2019. The prompt for SMOS refers to \u2019Please focus on the similarity of the speaker to the reference, and ignore the differences of content, grammar or audio quality.\u2019 Each audio has been listened to by at least 10 listeners. UTMOS Saeki et al.(2022) is a Speech MOS predictor6to measure the naturalness of speech. We use it in ablation studies which reduced the cost for evaluation. - Prosody JS Divergence: To evaluate the diversity and accuracy of the prosody prediction in our TTS system, we include the Prosody JS Divergence metric. This metric employs the Jensen-Shannon (JS) divergence Men\u00e9ndez et al.(1997) to quantify the divergence between the predicted and ground truth prosody feature distributions. Prosody features, including pitch, and duration, are quantized and their distributions in both synthesized and natural speech are compared. Lower JS divergence values indicate closer similarity between the predicted prosody features and those of the ground truth, suggesting a higher diversity of the synthesized speech.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#43experimental-results-on-zero-shot-tts","title":"4.3.Experimental Results on Zero-shot TTS","text":"<p>Following VALL-E (2023), We employ LibriSpeech Panayotov et al.(2015) test-clean for zero-shot TTS evaluation. We adopt the cross-sentence setting in VALL-E (2023) that we randomly select 3-second clips as prompts from the same speaker\u2019s speech. The results are summarized in table 1 and figure 5.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#431evaluation-baselines","title":"4.3.1.Evaluation Baselines","text":"<ul> <li>VALL-E (2023): VALL-E (2023) predicts codec tokens using both AR and NAR models. RTF is obtained from ClaM-TTS (2024); Voicebox (2023). We use our reproduced results for MOS, Sim, and WER. Additionally, we do a preference test with their official demo.</li> <li>Voicebox (2023): Voicebox uses flow-matching to predict maksed mel-spectrogram. RTF is from the original paper. We use our reproduced results for MOS, Sim, and WER. We also implement a preference test with their official demo.</li> <li>NaturalSpeech2 (2023): NaturalSpeech2 uses a latent diffusion model to predict latent features of codec. The RTF is from the original paper. the Sim, WER and samples for MOS are obtained through communication with the authors. We also do a preference test with their official demo.</li> <li>Mega-TTS (2023) 8: Mega-TTS uses both language model and GAN to predict mel-spectrogram. We obtain RTF from mobilespeech Ji et al.(2024) and WER from the original paper. We do a preference test with their official demo.</li> <li>ClaM-TTS (2024): ClaM-TTS uses the AR model to predict mel codec tokens. We obtain the objective evaluation results from the original paper and do a preference test with their official demo.</li> </ul>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#432generation-quality","title":"4.3.2.Generation Quality","text":"<p>FlashSpeech stands out significantly in terms of speaker quality, surpassing other baselines in both CMOS and audio quality preference tests. Notably, our method closely approaches ground truth recordings, underscoring its effectiveness. These results affirm the superior quality of FlashSpeech in speech synthesis. our method.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#433generation-similarity","title":"4.3.3.Generation Similarity","text":"<p>Our evaluation of speaker similarity utilizes Sim, SMOS, and speaker similarity preference tests, where our methods achieve 1st, 2nd, and 3rd place rankings, respectively. These findings validate our methods\u2019 ability to achieve comparable speaker similarity to other methods. Despite our training data (MLS) containing approximately 5k speakers, fewer than most other methods (e.g., Librilight with about 7k speakers or self-collected data), we believe that increasing the number of speakers in our methods can further enhance speaker similarity.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#434robustness","title":"4.3.4.Robustness","text":"<p>Our methods achieve a WER of 2.7, placing them in the first echelon. This is due to the non-autoregressive nature of our methods, which ensures robustness.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#435generation-speed","title":"4.3.5.Generation Speed","text":"<p>FlashSpeech achieves a remarkable approximately 20x faster inference speed compared to previous work. Considering its excellent audio quality, robustness, and comparable speaker similarity, our method stands out as an efficient and effective solution in the field of large-scale speech synthesis.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#44ablation-studies","title":"4.4.Ablation Studies","text":""},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#441ablation-studies-of-lcm","title":"4.4.1.Ablation Studies of LCM","text":"<p>We explored the impact of different pre-trained models in adversarial training on UTMOS and Sim-O. As shown in the table 2, the baseline, which employs consistency training alone, achieved a UTMOS of 3.62 and a Sim-O of 0.45. Incorporating adversarial training using wav2vec2-large9, hubert-large10, and wavlm-large11as discriminators significantly improved both UTMOS and Sim-O scores. Notably, the application of adversarial training with Wavlm-large achieved the highest scores (UTMOS: 4.00, Sim-O: 0.52), underscoring the efficacy of this pre-trained model in enhancing the quality and speaker similarity of synthesized speech. Additionally, without using the audio prompt\u2019s feature as a condition the discriminator shows a slight decrease in performance (UTMOS: 3.97, Sim-O: 0.51), highlighting the importance of conditional features in guiding the adversarial training process. As shown in table 3, the effect of sampling steps (NFE) on UTMOS and Sim-O revealed that increasing NFE from 1 to 2 marginally improves UTMOS (3.99 to 4.00) and Sim-O (0.51 to 0.52). However, further increasing to 4 sampling steps slightly reduced UTMOS to 3.91 due to the accumulation of score estimation errors Chen et al.(2022a); Lyu et al.(2024). Therefore, we use 2 steps as the default setting for LCM.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#442ablation-studies-of-prosody-generator","title":"4.4.2.Ablation Studies of Prosody Generator","text":"<p>In this part, we investigated the effects of a control factor, denoted as $\\alpha$, on the prosodic features of pitch and duration in speech synthesis, by setting another influencing factor to zero. Our study specifically conducted an ablation analysis to assess how $\\alpha$ influences these features, emphasizing its critical role in balancing stability and diversity within our framework\u2019s prosodic outputs.</p> <p>Tab.04 elucidates the effects of varying $\\alpha$ on the pitch component. With $\\alpha$ set to 0, indicating no inclusion of the residual output from prosody refinement, we observed a Pitch JSD of 0.072 and a WER of 2.8. A slight modification to $\\alpha= 0.2$ resulted in a reduced Pitch JSD of 0.067, maintaining the same WER. Notably, setting $\\alpha$ to 1, fully incorporating the prosody refinement\u2019s residual output, further decreased the Pitch JSD to 0.063, albeit at the cost of increased WER to 3.7, suggesting a trade-off between prosody diversity and speech intelligibility.</p> <p>Similar trends in table 5 are observed in the duration component analysis. With $\\alpha$  = 0, the Duration JSD was 0.0175 with a WER of 2.8. Adjusting $\\alpha$ to 0.2 slightly improved the Duration JSD to 0.0168, without affecting WER. However, fully embracing the refinement module\u2019s output by setting $\\alpha$  = 1 yielded the most significant improvement in Duration JSD to 0.0153, which, similar to pitch analysis, came with an increased WER of 3.9. The results underline the delicate balance required in tuning $\\alpha$  to optimize between diversity and stability of prosody without compromising speech intelligibility.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#45evaluation-results-for-voice-conversion","title":"4.5.Evaluation Results for Voice Conversion","text":"<p>In this section, we present the evaluation results of our voice conversion system, FlashSpeech, in comparison with state-of-the-art methods, including YourTTS (2021) 12 and DDDM-VC 13 Choi et al.(2024).  We conduct the experiments with their official checkpoints in our internal test set.</p> <p>Our system outperforms both YourTTS and DDDM-VC in terms of CMOS, SMOS and Sim-O, demonstrating its capability to produce converted voices with high quality and similarity to the target speaker.  These results confirm the effectiveness of our FlashSpeech approach in voice conversion tasks.</p>"},{"location":"TTS/Models/Diffusion/2024.04.23_FlashSpeech/#46conclusions-and-future-work","title":"4.6.Conclusions and Future Work","text":"<p>In this paper, we presented FlashSpeech, a novel speech generation system that significantly reduces computational costs while maintaining high-quality speech output. Utilizing a novel adversarial consistency training method and an LCM, FlashSpeech outperforms existing zero-shot TTS systems in efficiency, achieving speeds about 20 times faster without compromising on voice quality, similarity, and robustness. In the future, we aim to further refine the model to improve the inference speed and reduce computational demands. In addition, we will expand the data scale and enhance the system\u2019s ability to convey a broader range of emotions and more nuanced prosody. For future applications, FlashSpeech can be integrated for real-time interactions in applications such as virtual assistants and educational tools.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/","title":"SimpleSpeech","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models - \u4f5c\u8005:   - [Dongchao Yang](../../Authors/Dongchao_Yang_(\u6768\u4e1c\u8d85).md)   - [Dongdong Wang](../../Authors/Dingdong_Wang.md)   - [Haohan Guo](../../Authors/Haohan_Guo.md)   - [Xueyuan Chen](../../Authors/Xueyuan_Chen.md)   - [Xixin Wu](../../Authors/Xixin_Wu.md)   - [Helen Meng](../../Authors/Helen_Meng_(\u8499\u7f8e\u73b2).md) - \u673a\u6784:   - [\u9999\u6e2f\u4e2d\u6587\u5927\u5b66](../../Institutions/CUHK_\u9999\u6e2f\u4e2d\u6587\u5927\u5b66.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.06.04 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.06 - \u53d1\u8868:   - [InterSpeech 2024](../../Publications/InterSpeech.md) - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2406.02328)   - [Demo](https://simplespeech.github.io/simplespeechDemo/) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u6269\u6563\u6a21\u578b](../../Tags/Model_Diffusion.md)   - [\u7f16\u89e3\u7801\u5668](../../Tags/Codec.md) - \u9875\u6570: 6 - \u5f15\u7528: 36 - \u88ab\u5f15: ?"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>In this study,we propose a simple and efficient Non-AutoRegressive (NAR) Text-To-Speech (TTS) system based on diffusion, named SimpleSpeech.  Its simpleness shows in three aspects: </p> <ol> <li>It can be trained on the speech-only dataset, without any alignment information; </li> <li>It directly takes plain text as input and generates speech through an NAR way; </li> <li>It tries to model speech in a finite and compact latent space, which alleviates the modeling difficulty of diffusion. </li> </ol> <p>More specifically, we propose a novel speech codec model (SQ-Codec) with scalar quantization, SQ-Codec effectively maps the complex speech signal into a finite and compact latent space, named scalar latent space.  Benefits from SQ-Codec, we apply a novel Transformer diffusion model in the scalar latent space of SQ-Codec.  We train SimpleSpeech on 4k hours of a speech-only dataset, it shows natural prosody and voice cloning ability.  Compared with previous large-scale TTS models, it presents significant speech quality and generation speed improvement.  Demos are released at this URL.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Text-to-speech synthesis (TTS) aims to synthesize intelligible and natural speech given text, which has made great progress in the past years [1, 2]. Most previous TTS systems trained on small-scale high-quality labeled speech datasets. In terms of model training, they rely on a relatively complicated pipeline (e.g.prosody prediction, G2P conversion), and fine-grained alignment information (e.g. phone-level duration) is needed. However, the recently proposed approaches based on large-scale speech data significantly simplify the TTS system (AudioLM, VALL-E, SPEAR-TTS, UniAudio, Make-A_Voice). For instance, language model (LM) based TTS, e.g. VALL-E uses a pre-trained audio codec model e.g. EnCodec to map the speech signal into a sequence of discrete tokens, then an auto-regressive language model is trained to translate the phoneme into speech tokens. It can generate more expressive speech, but also is troubled by the slow and unstable inference. To address these issues, Non-autoregressive (NAR) models, e.g. NaturalSpeech 2, SoundStorm, and VoiceBox, are proposed. They have higher inference speed and better stability, but with the cost of (1) relying on phoneme-acoustic alignment information, and (2) a more complicated training process. In this work, we propose a simple and efficient TTS system, SimpleSpeech, which does not rely on any alignment information and generates high-quality speech in a NAR way.2To build such a TTS system, where we meet with the following research problems: </p> <ul> <li>(1) how to use the large-scale speech-only dataset to train a TTS model without any alignment information e.g. phoneme-level duration; </li> <li>(2) how to design a generative model that can generate high-quality speech in a NAR way. </li> <li>(3) how to solve the duration alignment problem without using a specific duration model when we train the NAR model; </li> </ul> <p>The main contributions of this study are summarized as follows:</p> <ul> <li>(1) We demonstrate that large-scale unlabeled speech data can be used to build an NAR TTS system.</li> <li>(2) We propose a novel generation model, a scalar latent transformer diffusion model, which models the speech data in a finite and compact latent space. More specifically, we propose a speech codec model (SQ-Codec) based on scalar quantization, which maps the complex speech signal into a finite and compact latent space, named scalar latent space. Then we apply the diffusion model in the scalar latent space of SQ-Codec.</li> <li>(3) We propose to use sentence duration instead of phone-level duration for the NAR-based TTS model. The sentence duration is used to determine the length of the target sequence, then an in-context conditioning strategy is introduced to learn the fine-grained alignment between the condition and target sequence implicitly. Compared to previous works that predict the duration of each phoneme, the sentence duration gives more diversity in the generation process. More importantly, the sentence duration is easy to obtain in both the training and inference stages. Please refer to Section 3.2 finds more details.</li> <li>(4) Extensive experimental results show the effectiveness of SimpleSpeech. We also conduct a lot of ablation studies to explore the effectiveness of each part in SimpleSpeech.</li> </ul>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#2related-works","title":"2.Related Works\u00b7\u76f8\u5173\u5de5\u4f5c","text":"<p><code>None</code></p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#3methodology","title":"3.Methodology\u00b7\u65b9\u6cd5\u8bba","text":"<p>The overall architecture of the SimpleSpeech framework is demonstrated in Figure 1 (b), which mainly consists of two parts: SQ-Codec and scalar transformer diffusion. The detailed design of each part will be introduced in this section.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#31text-encoder-speaker-encoder","title":"3.1.Text Encoder &amp; Speaker Encoder","text":"<p>This work explores to use the large-scale speech-only datasets to train a TTS system. The first step is to obtain the text label for these speech samples. We propose to use the open available ASR model to get transcripts, Whisper-base model [12] is used in this study. To take advantage of the large language model and simply the traditional TTS frontend, we use a pre-trained language model to extract the textual representations. Then we directly take these textual representations as the conditional information for TTS. To realize zero-shot voice cloning, we use the 1st layer of XLSR-53 [13] to extract global embedding to represent the speaker timbre.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#32sentence-duration","title":"3.2.Sentence Duration","text":"<p>Previous works [2, 9] try to model the phone-level duration in TTS. In general, a duration predictor is built to predict the duration of each phone. Training such modules increases the complexity of data pre-processing and training pipeline.In this study, we propose to model the sentence-level duration prior by using the in-context learning of LLMs (gpt-3.5-turbo is used). Our motivation is that LLMs can easily estimate how much time to read a sentence based on the number of words in the sentence and the prior knowledge. The prompt for ChatGPT can be found on the demo page. After we obtain the sentence-level duration, we let the model learn the alignment between words and latent features implicitly. Such a design will bring more diversity to speech synthesis. In the training stage, we can directly get the duration based on the length of the waveform. We follow Stable Audio [14] uses a timing module to encode the duration into a global embedding. In the inference stage, the predicted duration by LLMs first determines the length of the noisy sequence, then input into the timing module.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#33sq-codec","title":"3.3.SQ-Codec","text":"<p>Although residual vector quantization (RVQ) based audio codec models have shown effectiveness for audio compression, training a good codec model needs a lot of tricks and complicated loss design [15]. In this study, we propose to use scalar quantization [16, 17] to replace the residual vector quantization in audio codec models, which can be trained with the reconstruction loss and adversarial loss without any training tricks. Furthermore, we also find that the scalar quantization effectively maps the complex speech signal into a finite and compact latent space, which is suitable for diffusion model (refer to Section 2.4 and 3.2.4 for more details). Assuming h \u2208 RT \u2217ddenotes the output features of Encoder in the codec model. T and d denote the number of frames and the dimension of each vector. For any vector hi, we use a parameter-free scalar quantization module to quantize hiinto a fixed scalar space: hi= torch.tanh(hi),si= torch.round(hi\u2217 S)/S,(1) where S is a hyper-parameter that determines the scope of scalar space. To get gradients through the rounding operation, we use a straight-through estimator like VQ-VAE [18]. We can see that the scalar quantization first uses a tanh function to map the value of features into [\u22121, 1], then a round operation further reduces the value of range into 2*S+1 different numbers. We named such value domain as scalar latent space. We note that previous works [17, 19] also try to use scalar quantization as the image tokenizer for image generation. We claim that our implementation is different from theirs, and better adapts audio codec tasks in our experiments.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#encoder-and-decoder","title":"Encoder and Decoder","text":"<p>Our encoder consists of 5 convolution blocks, each block includes 2 causal 1D-convolutional layers and one down-sample layer. The down-sample strides are set as [2, 2, 4, 4, 5], resulting in 320 times down-sample along the time dimension. The decoder mirrors the encoder and uses transposed convolutions instead of stride convolutions.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#discriminator-and-training-loss","title":"Discriminator and Training Loss","text":"<p>Following [6], a multi-scale discriminator is used. The training loss of SQ-Codec consists of two parts: (1) reconstruction loss Lrec, which includes time domain and frequency domain losses: the L1 loss between the reconstructed waveform and the original waveform and the MSE loss for the STFT spectrogram. (2) adversarial loss, which is calculated based on the results of the discriminator.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#34scalar-latent-transformer-diffusion-models","title":"3.4.Scalar Latent Transformer Diffusion Models","text":"<p>Based on the previous discussion, we can map the speech data into a scalar latent space based on our proposed SQ-Codec model. Inspired by the success of latent diffusion models [20] in both image and audio generation [9,21], we propose to model the speech data in the scalar latent space. Our motivation is that the sampling space of scalar latent space is simple because SQ effectively limits the value range of each element. The details of the scalar latent transformer diffusion model are as follows.</p> <p>$$ $$</p> <p>where \u03b8 denotes the parameter of the neural network. T denotes the timestep, xTdenotes the sampling features from the Gaussian distribution. c denotes the condition information.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#transformer-based-diffusion-backbone","title":"Transformer-Based Diffusion Backbone","text":"<p>U-Net backbone has been widely used in diffusion models, especially in the speech synthesis field [9,22\u201324]. We also noted that many transformer-based diffusion models, such as DiT [25] and Sora [26] have been used in image/video generation. Inspired by the success of the transformer-based audio language models [4\u20136] and DiT, we propose a transformer-based diffusion backbone for speech synthesis. Specifically, a GPT2-like transformer backbone is used: 12 attention layers, 8 attention heads, and the model dimension is 768.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#in-context-conditioning","title":"In-Context Conditioning","text":"<p>Inspired by LLMs and previous audio language models [4\u20136], we simply append the features of time step t and condition c as the prefix sequence in the input sequence. Such a condition way allows us to use a standard GPT-like structure without modification. After the final block, we remove the conditioning sequence from the output sequence.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#scalar-latent-diffusion","title":"Scalar Latent Diffusion","text":"<p>Latent diffusion models (LDM) have been demonstrated to fit complex data distributions, including VAE latent features [9, 20\u201322], Mel-spectrogram features [27], and waveform [23, 24]. We speculate that these data distributions are very complex because their search space is infinite. Instead, SQ-Codec provides a finite and compact scalar latent space, thus we can consider modeling speech data in this space. Specifically, a network is trained to transfer the Gaussian distribution to the scalar latent space. We follow the training strategy of DDPM [28], and the mean squared error (MSE) loss is used. To make sure the final output belongs to the scalar latent space, we use the scalar quantization (SQ) operation to limit the final prediction.</p>"},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#4experiments","title":"4.Experiments\u00b7\u5b9e\u9a8c","text":""},{"location":"TTS/Models/Diffusion/2024.06.04_SimpleSpeech/#5conclusion","title":"5.Conclusion\u00b7\u7ed3\u8bba","text":"<p>In this study, we propose a simple and efficient TTS model named SimpleSpeech. SimpleSpeech can be trained on large-scale speech-only datasets without any additional data pre-processing, which significantly simplifies the efforts to train a TTS model.We propose a novel speech codec (SQ-Codec) based on scalar quantization.Then we apply a novel latent transformer diffusion model to the scalar latent space of SQ-Codec. Experimental results show the proposed model has better performance than the previous U-Net-based latent diffusion model. Due to the NAR generation strategy, SimpleSpeech significantly improves the generation efficiency compared to previous LM-based TTS models. In the future, we will explore to scale the model and data size.</p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/","title":"DiTTo-TTS","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer - \u4f5c\u8005:   - 01 [Keon Lee](../../Authors/Keon_Lee.md)   - 02 [Dong Won Kim](../../Authors/Dong_Won_Kim.md)   - 03 [Jaehyeon Kim](../../Authors/Jaehyeon_Kim.md)   - 04 [Jaewoong Cho](../../Authors/Jaewoong_Cho.md) - \u673a\u6784:   - [KRAFTON.AI](../../Institutions/KRAFTON.AI.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.06.17 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.18 - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2406.11427)      - [Demo](https://ditto-tts.github.io/)   - [Scholar](https://scholar.google.com/scholar?cluster=) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md)   - [\u6269\u6563\u6a21\u578b](../../Tags/Model_Diffusion.md) - \u9875\u6570: 21 - \u5f15\u7528: 80 - \u88ab\u5f15: 0 - \u6570\u636e:"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#abstract","title":"Abstract: \u6458\u8981","text":"\u539f\u6587  &gt; Large-scale diffusion models have shown outstanding generative abilities across multiple modalities including images, videos, and audio. &gt; However, text-to-speech (TTS) systems typically involve domain-specific modeling factors (e.g., phonemes and phoneme-level durations) to ensure precise temporal alignments between text and speech, which hinders the efficiency and scalability of diffusion models for TTS. &gt; In this work, we present an efficient and scalable Diffusion Transformer (DiT) that utilizes off-the-shelf pre-trained text and speech encoders. &gt; Our approach addresses the challenge of text-speech alignment via cross-attention mechanisms with the prediction of the total length of speech representations. &gt; To achieve this, we enhance the DiT architecture to suit TTS and improve the alignment by incorporating semantic guidance into the latent space of speech. &gt; We scale the training dataset and the model size to 82K hours and 790M parameters, respectively. &gt; Our extensive experiments demonstrate that the large-scale diffusion model for TTS without domain-specific modeling not only simplifies the training pipeline but also yields superior or comparable zero-shot performance to state-of-the-art TTS models in terms of naturalness, intelligibility, and speaker similarity. &gt; Our speech samples are available at https://ditto-tts.github.io.   <p>\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u5df2\u7ecf\u8bc1\u660e\u4e86\u5728\u591a\u79cd\u6a21\u6001 (\u5305\u62ec\u56fe\u50cf, \u89c6\u9891\u548c\u97f3\u9891) \u4e0a\u7684\u5353\u8d8a\u751f\u6210\u80fd\u529b. \u7136\u800c, \u6587\u672c\u8f6c\u8bed\u97f3 (TTS) \u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u5efa\u6a21\u56e0\u7d20 (\u4f8b\u5982, \u97f3\u7d20\u548c\u97f3\u7d20\u7ea7\u522b\u7684\u6301\u7eed\u65f6\u95f4), \u4ee5\u786e\u4fdd\u6587\u672c\u548c\u8bed\u97f3\u4e4b\u95f4\u7684\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50, \u8fd9\u4f1a\u59a8\u788d\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u7528\u4e8e TTS. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6269\u6563 Transformer (DiT), \u5b83\u5229\u7528\u73b0\u6210\u7684\u9884\u8bad\u7ec3\u6587\u672c\u548c\u8bed\u97f3\u7f16\u7801\u5668. \u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u89e3\u51b3\u4e86\u6587\u672c-\u8bed\u97f3\u5bf9\u9f50\u95ee\u9898, \u5e76\u901a\u8fc7\u9884\u6d4b\u8bed\u97f3\u8868\u793a\u7684\u603b\u957f\u5ea6\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9, \u6211\u4eec\u589e\u5f3a\u4e86 DiT \u67b6\u6784, \u4ee5\u9002\u5e94 TTS \u5e76\u6539\u8fdb\u4e86\u5bf9\u9f50. \u6211\u4eec\u5c06\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6a21\u578b\u5927\u5c0f\u6269\u5c55\u5230 82K \u5c0f\u65f6\u548c 790M \u53c2\u6570, \u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c, \u8bc1\u660e\u4e86\u65e0\u9700\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u5efa\u6a21\u7684\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u5bf9\u4e8e TTS \u800c\u8a00, \u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u6570\u636e, \u5c31\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u6700\u5148\u8fdb\u7684 TTS \u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u96f6\u6837\u672c\u6027\u80fd. \u6211\u4eec\u7684\u8bed\u97f3\u793a\u4f8b\u53ef\u5728 https://ditto-tts.github.io \u83b7\u5f97.</p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#1introduction","title":"1.Introduction: \u5f15\u8a00","text":"<p>Large-scale diffusion models have demonstrated impressive generative abilities in a wide range of fields including images [1], [2], videos [3], [4], and audio (WaveGrad; DiffWave). The integration of latent diffusion models (LDMs) further amplifies their popularity [8], [9], [10], as these models significantly enhance computational efficiency. This efficiency is achieved through the reduction of input dimensionality using autoencoders, which also enables the diffusion models to focus on the most critical features of data (LDM). However, applying LDMs to text-to-speech (TTS) presents unique challenges because TTS requires precise alignment between text and generated speech over time. Hence, their application to TTS often requires a complex pipeline, incorporating speech domain-specific modeling such as phoneme and duration (Grad-TTS, NaturalSpeech2). Without these components, generation performance tends to be suboptimal (Simple-TTS, E3-TTS), while their inclusion hinders the model efficiency and scalability.</p> <p>In this work, we present a latent diffusion model for TTS that is integrated with off-the-shelf pre-trained text and speech encoders without relying on the speech domain-specific modeling. Our method addresses the challenge of aligning text and speech solely through cross-attention mechanisms. Furthermore, we introduce a module that predicts the total duration of the generated speech from a given text and speech prompt, rather than determining the duration of each individual input token. To accomplish this, we conduct a network architecture ablation to identify a model specifically suited for TTS applications. Consequently, we adopt the Diffusion Transformer (DiT) to TTS, naming our method DiTTo-TTS (or simply DiTTo). We also explore the significance of leveraging aligned text and speech representations, showing that performance can be enhanced by either using a text encoder jointly trained with speech data or a speech autoencoder with an auxiliary language modeling objective alongside the reconstruction objective.</p> <p>Our comprehensive experiments on English-only and multilingual evaluation demonstrate that our model not only simplifies the training process but also achieves superior or comparable zero-shot performance to state-of-the-art models in terms of naturalness, intelligibility, and speaker similarity. The base-sized DiTTo surpasses a state-of-the-art autoregressive model (CLaM-TTS), offering an inference speed 4.6 times faster and a model size 3.84 times smaller. Additionally, we demonstrate that our model scales effectively with increases in both data and model sizes.</p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#2related-works","title":"2.Related Works: \u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#large-scale-tts","title":"Large-scale TTS","text":"<p>Recently, large-scale TTS research progresses actively in two main directions: LLM-based autoregressive (AR) TTS and non-autoregressive (Non-AR) TTS. A prominent feature of LLMs is the scalability [16], [17] and their proficiency in zero-shot learning tasks, demonstrating significant capabilities without prior specific training on those tasks [18], [19], [20], [21]. Efforts to replicate LLM\u2019s capability in different modalities have shown progress, including vision [22], [23], [24] and audio (VALL-E; AudioLM; SPEAR-TTS; Qwen-Audio). VALL-E employs EnCodec for speech-to-token mapping, posing TTS tasks as AR language modeling tasks, thus enabling zero-shot capabilities in the speech domain. CLaM-TTS introduces Mel-VAE to achieve superior token length compression, and enables a language model to predict multiple tokens simultaneously. Although this approach removes the need for cascaded modeling (VALL-E) to manage the number of token streams, their resource-intensive inference processes limit their applications [30]. On the other hand, Non-AR generative models are employed to enhance the efficiency of TTS systems. VoiceBox utilizes a flow matching [32] to generate speech, effectively casting the TTS task into a speech infilling task. NaturalSpeech series (NaturalSpeech2, NaturalSpeech3), building upon recent advances in the Latent Diffusion Model (LDM), incorporate auxiliary modules for controllability of various speech attribute such as content, prosody, and timbre. However, requiring supplementary data beyond speech-transcription pairs hinders scalability. Simple-TTS simplifies the data preparation and training process by removing auxiliary modules and the need for phoneme-level durations. Similarly, E3-TTS follows this approach but does not utilize a pre-trained latent autoencoder. However, both models have limitations in audio quality and impose a fixed length on the target audio, which is a significant constraint for speech generation.</p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#latent-diffusion-model-ldm","title":"Latent Diffusion Model (LDM)","text":"<p>LDM improves modeling efficiency of the diffusion process model [1], [34] by operating in a latent space, achieving remarkable performance in generating realistic samples. Initially applied in image generation, their success is attributed to the reduced dimensionality of the latent space, facilitating efficient training and sampling (LDM). Notably, guided diffusion [35], [36] has been expanded to various applications of LDMs, such as image editing [37] and image retrieval [38]. In the field of audio signals, techniques such as style transfer, inpainting, and super-resolution have been explored, along with text-guided audio and speech generation [39], [40]. In the context of TTS, however, applying LDMs to TTS (NaturalSpeech2; NaturalSpeech3) necessitates domain-specific elements such as phonemes, phoneme-level durations, and pitch. This is primarily due to the need for precise temporal alignment between text and speech, as well as the higher fidelity requirements inherent in audio data.</p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#neural-audio-codec","title":"Neural Audio Codec","text":"<p>Neural audio codecs, which effectively compress various types of audio using neural networks, are used as part of many TTS systems (NaturalSpeech2; CLaM-TTS; VALL-E). Recent advancements employ an encoder-decoder architecture coupled with Residual Vector Quantization (RVQ) [41], [42], [43] to transform raw audio waves into discretized tokens. For example, EnCodec converts 24,000 Hz mono waveforms into 75 Hz latents. With a similar architecture, by focusing the compression specifically on speech rather than general audio signals, Mel-VAE (CLaM-TTS) achieves approximately 10.76 Hz latents by compressing the mel-spectrogram. This reduction significantly lowers the computational cost of the speech generation module. Another research direction of improving neural audio codecs for TTS systems is injecting semantic information using large language models (LLMs) (SpeechTokenizer).</p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#3methodology","title":"3.Methodology: \u65b9\u6cd5","text":"\u539f\u6587  &gt; We present a latent diffusion model (LDM) for TTS that enables the use of off-the-shelf pre-trained text and speech encoders without relying on speech domain-specific modeling, such as phoneme and duration. &gt; Toward this goal, we employ the following two approaches:  &gt; (1) introducing a speech length predictor that predicts the total length of the generated speech without relying on phoneme-level durations or requiring a fixed speech length; and  &gt; (2) fine-tuning the pre-trained neural audio codec using a pre-trained language model to enhance the alignment between text and speech embeddings.   <p>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b (Latent Diffusion Model, LDM) \u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u8bed\u97f3\u9886\u57df\u7279\u5b9a\u5efa\u6a21 (\u4f8b\u5982\u97f3\u7d20\u548c\u65f6\u957f) \u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u73b0\u6210\u7684\u9884\u8bad\u7ec3\u6587\u672c\u548c\u8bed\u97f3\u7f16\u7801\u5668. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807, \u6211\u4eec\u91c7\u7528\u4e86\u5982\u4e0b\u4e24\u79cd\u65b9\u6cd5: 1. \u5f15\u5165\u4e00\u4e2a\u8bed\u97f3\u957f\u5ea6\u9884\u6d4b\u5668, \u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u97f3\u7d20\u7ea7\u522b\u65f6\u957f\u6216\u8981\u6c42\u56fa\u5b9a\u8bed\u97f3\u957f\u5ea6\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u751f\u6210\u8bed\u97f3\u7684\u603b\u957f\u5ea6; 2. \u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u9884\u8bad\u7ec3\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5fae\u8c03\u4ee5\u589e\u5f3a\u6587\u672c\u548c\u8bed\u97f3\u5d4c\u5165\u4e4b\u95f4\u7684\u5bf9\u9f50.</p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#31preliminary","title":"3.1.Preliminary: \u9884\u5907\u77e5\u8bc6","text":"\u539f\u6587  &gt; Diffusion models [1], [34] are a class of generative models that iteratively transform a simple noise distribution into a complex data distribution through a stochastic denoising process. &gt; They define a forward process that progressively adds Gaussian noise to the input data as time step increases. &gt; The reverse generative process then estimates the added noise to reconstruct the original data. &gt; Conditional diffusion models enhance this framework by incorporating additional information, such as text descriptions in text-to-image generation [35], [45] or phonemes and their durations in TTS ([Grad-TTS](../../Models/TTS2_Acoustic/2021.05.13_Grad-TTS.md)), [46]. &gt; While diffusion models can operate directly on real-world data, many of them are applied in the latent space ([LDM](../../Models/_Basis/2021.12.20_LDM.md); [8], [10], [47]. &gt; Thanks to the reduced dimensionality, this approach improves computational efficiency and output quality by allowing diffusion models to focus on the semantic information of the data while the autoencoder handles the high-frequency details that are less perceptible ([LDM](../../Models/_Basis/2021.12.20_LDM.md)).   <p>\u6269\u6563\u6a21\u578b\u662f\u4e00\u7c7b\u8fed\u4ee3\u5730\u5c06\u7b80\u5355\u566a\u58f0\u5206\u5e03\u8f6c\u6362\u4e3a\u590d\u6742\u6570\u636e\u5206\u5e03\u7684\u751f\u6210\u6a21\u578b. \u5b83\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u6b63\u5411\u8fc7\u7a0b, \u968f\u7740\u65f6\u95f4\u6b65\u957f\u7684\u589e\u52a0, \u8be5\u8fc7\u7a0b\u9010\u6e10\u5411\u8f93\u5165\u6570\u636e\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0. \u7136\u540e, \u53cd\u5411\u751f\u6210\u8fc7\u7a0b\u4f30\u8ba1\u6dfb\u52a0\u7684\u566a\u58f0\u4ee5\u91cd\u5efa\u539f\u59cb\u6570\u636e. \u6761\u4ef6\u6269\u6563\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u989d\u5916\u7684\u4fe1\u606f (\u5982\u6587\u751f\u56fe\u4e2d\u7684\u6587\u672c\u63cf\u8ff0\u6216\u6587\u672c\u8f6c\u8bed\u97f3\u7684\u97f3\u7d20\u53ca\u5176\u65f6\u957f). \u867d\u7136\u6269\u6563\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u64cd\u4f5c, \u4f46\u5927\u591a\u90fd\u5e94\u7528\u4e8e\u9690\u7a7a\u95f4. \u7531\u4e8e\u7ef4\u5ea6\u7684\u964d\u4f4e, \u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u5141\u8bb8\u6269\u6563\u6a21\u578b\u4e13\u6ce8\u4e8e\u6570\u636e\u7684\u8bed\u4e49\u4fe1\u606f, \u540c\u65f6\u81ea\u7f16\u7801\u5668\u5904\u7406\u90a3\u4e9b\u4e0d\u592a\u660e\u663e\u7684, \u9ad8\u9891\u7684\u7ec6\u8282, \u4ece\u800c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8f93\u51fa\u8d28\u91cf.</p> \u539f\u6587  &gt; In our setting, a conditional LDM can be formulated as follows. &gt; Given speech audio, an autoencoder produces its latent representation $z_{speech}$, and the diffusion model is trained to predict $z_{speech}$ at each diffusion step $t \\in [1, T]$ conditioned on a text token sequence $x$. &gt; Specifically, the noised latent $z(t)$ is expressed as $\\alpha_t z_{speech} + \\sigma_t \\epsilon$, where $\\epsilon$ is sampled from the standard normal distribution and $\\alpha_t$ and $\\sigma_t$ are defined by a noise schedule. &gt; Note that $z^{(1)}$ is $z_{speech}$ and $z^{(T)}$ follows the standard normal distribution. &gt; We use $v$-prediction [48] as our model output $v_{\\theta} (z^{(t)}, x, t)$, which predicts $v^{(t)} := \\alpha_t \\epsilon \u2212 \\sigma_t z^{(t)}$. &gt; This setup provides a mean squared error objective as the training loss:   <p></p> <p>\u5728\u6211\u4eec\u7684\u8bbe\u7f6e\u4e2d, \u4e00\u4e2a\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u5f62\u5f0f\u5316\u5982\u4e0b. \u7ed9\u5b9a\u8bed\u97f3\u97f3\u9891, \u81ea\u7f16\u7801\u5668\u4ea7\u751f\u5176\u9690\u8868\u793a $z_{speech}$, \u6269\u6563\u6a21\u578b\u88ab\u8bad\u7ec3\u6210\u5728\u6bcf\u4e2a\u6269\u6563\u6b65 $t\\in [1,T]$ \u4ee5\u6587\u672c\u6807\u8bc6\u7b26\u5e8f\u5217 $x$ \u4e3a\u6761\u4ef6, \u9884\u6d4b $z_{speech}$. \u5177\u4f53\u6765\u8bf4, \u566a\u58f0\u5316\u7684\u9690\u8868\u793a $z(t)$ \u88ab\u8868\u8fbe\u4e3a $\\alpha_t z_{speech} + \\sigma_t \\epsilon$, \u5176\u4e2d $\\epsilon$ \u662f\u4ece\u6807\u51c6\u6b63\u6001\u5206\u5e03\u91c7\u6837\u5f97\u5230\u7684, $\\alpha_t$ \u548c $\\sigma_t$ \u662f\u7531\u566a\u58f0\u8c03\u5ea6\u5b9a\u4e49\u7684. \u6ce8\u610f\u5230 $z^{(1)}$ \u662f $z_{speech}$ \u800c $z^{(T)}$ \u9075\u5faa\u6807\u51c6\u6b63\u6001\u5206\u5e03. \u6211\u4eec\u4f7f\u7528 $v$-\u9884\u6d4b\u4f5c\u4e3a\u6a21\u578b\u8f93\u51fa $v_{\\theta} (z^{(t)}, x, t)$, \u5b83\u9884\u6d4b $v^{(t)} := \\alpha_t \\epsilon \u2212 \\sigma_t z^{(t)}$.</p> <p>\u8fd9\u79cd\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5747\u65b9\u8bef\u5dee\u76ee\u6807\u4e3a\u8bad\u7ec3\u635f\u5931:</p> <p>$$     Loss_{diffusion} = E_{t\\sim \\mathcal{U}(1,T),\\epsilon\\sim\\mathcal{N}(0,I)} \\left[ |v^{(t)}-v_{\\theta}(z^{(t)},x,t)|^2\\right]. $$</p> \u539f\u6587  &gt; To enrich the contextual information and facilitate zero-shot audio prompting, we incorporate a random span masking into the model training following ([VoiceBox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md); [Audiobox](../../Models/Speech_LLM/2023.12.25_Audiobox.md)). &gt; We input $z_{mask}^{(t)} := m\\odot z^{(t)} + (1-m)\\odot z_{speech}$ to the model, where $\\odot$ indicates element-wise multiplication and the binary masking $m$ fully masks with the probability of $0.1$ or partially masks a random contiguous segment whose size is between $70\\%$ and $100\\%$ of data. &gt; We also use the binary span masking as an additional input to the model. &gt; This allows the model to explicitly identify which part needs to be generated. &gt; The inclusion of masking modifies the training loss to:   <p></p> <p>\u4e3a\u4e86\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u4fc3\u8fdb\u96f6\u6837\u672c\u97f3\u9891\u63d0\u793a, \u6211\u4eec\u9075\u5faa VoiceBox; Audiobox \u7684\u505a\u6cd5, \u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u5f15\u5165\u968f\u673a\u8de8\u5ea6\u63a9\u819c.</p> <p>\u6211\u4eec\u5c06 $z_{mask}^{(t)} := m\\odot z^{(t)} + (1-m)\\odot z_{speech}$ \u8f93\u5165\u5230\u6a21\u578b\u4e2d, \u5176\u4e2d $\\odot$ \u8868\u793a\u9010\u5143\u7d20\u76f8\u4e58, \u4e8c\u8fdb\u5236\u63a9\u819c $m$ \u5b8c\u5168\u63a9\u76d6\u6982\u7387\u4e3a $0.1$ \u6216\u90e8\u5206\u63a9\u76d6\u968f\u673a\u8fde\u7eed\u7247\u6bb5, \u5176\u5927\u5c0f\u4ecb\u4e8e\u6570\u636e $70\\%$ \u548c $100\\%$ \u4e4b\u95f4. \u6211\u4eec\u4e5f\u4f7f\u7528\u4e8c\u8fdb\u5236\u8de8\u5ea6\u63a9\u819c\u4f5c\u4e3a\u6a21\u578b\u7684\u989d\u5916\u8f93\u5165. \u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u663e\u5f0f\u8bc6\u522b\u54ea\u4e9b\u90e8\u5206\u9700\u8981\u751f\u6210. \u5f15\u5165\u63a9\u819c\u4f1a\u4fee\u6539\u8bad\u7ec3\u635f\u5931:</p> <p>$$     Loss_{diffusion} = E_{t\\sim \\mathcal{U}(1,T),\\epsilon\\sim\\mathcal{N}(0,I)} \\left[ |m\\odot (v^{(t)}-v_{\\theta}(z_{mask}^{(t)},m,x,t))|^2\\right]. $$</p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#32model-training","title":"3.2.Model &amp; Training: \u6a21\u578b\u4e0e\u8bad\u7ec3","text":"\u539f\u6587  &gt; An overview of our proposed method is presented in Figure.01.   <p>\u672c\u6587\u65b9\u6cd5\u5982\u56fe 01 \u6240\u793a.</p> <p></p> \u539f\u6587  &gt; #### Text Encoder &gt; We employ a text encoder from a pre-trained large language model $p_{\\phi}$, which is parameterized by $\\phi$. &gt; The model was pre-trained to maximize the log-likelihood of the text token sequence $\\log p_{\\phi}(x)$. &gt; The parameters of the model are kept frozen while training the diffusion model for TTS. &gt; We denote the output of the text encoder by $z_{text}$.   <p></p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#_1","title":"\u6587\u672c\u7f16\u7801\u5668","text":"<p>\u6211\u4eec\u91c7\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u7f16\u7801\u5668, \u5176\u53c2\u6570\u7531 $\\phi$ \u51b3\u5b9a. \u6a21\u578b\u88ab\u9884\u8bad\u7ec3\u4ee5\u6700\u5927\u5316\u6587\u672c\u6807\u8bc6\u7b26\u5e8f\u5217\u7684\u5bf9\u6570\u4f3c\u7136 $\\log p_{\\phi}(x)$. \u5728\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4ee5\u8fdb\u884c\u6587\u672c\u8f6c\u8bed\u97f3\u65f6, \u6a21\u578b\u7684\u53c2\u6570\u4fdd\u6301\u56fa\u5b9a. \u6211\u4eec\u7528 $z_{text}$ \u8868\u793a\u6587\u672c\u7f16\u7801\u5668\u7684\u8f93\u51fa.</p> \u539f\u6587  &gt; #### Neural Audio Codec &gt; A neural audio codec, which is parameterized by $\\psi$, comprises of three components:  &gt; (1) an encoder that maps a speech into a sequence of latent representations $z_{speech}$;  &gt; (2) a vector quantizer converting the latent vector into the discrete code representation; and  &gt; (3) a decoder that reconstructs the speech from a sequence of the quantized latent representations $z'_{speech}$. &gt; To enhance alignment between text and speech embeddings, we fine-tune the neural audio codec using the pre-trained language model. &gt; We introduce a learnable linear projection $f(\\cdot)$ to match the dimension of the latent representation $z_{speech}$ to the language model's hidden space. &gt; Subsequently, we use this projected embedding in place of the embedding from the pre-trained text encoder within the cross-attention operation of the language model\u2019s decoder. &gt; The neural audio codec is fine-tuned with auxiliary loss that infuse semantic content into the generated representations:   <p></p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#_2","title":"\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801","text":"<p>\u7531 $\\psi$ \u53c2\u6570\u5316\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7531\u4e09\u90e8\u5206\u7ec4\u6210: (1) \u4e00\u4e2a\u7f16\u7801\u5668, \u5b83\u5c06\u8bed\u97f3\u6620\u5c04\u4e3a\u4e00\u7cfb\u5217\u9690\u8868\u793a $z_{speech}$; (2) \u5411\u91cf\u91cf\u5316\u5668, \u5c06\u9690\u5411\u91cf\u8f6c\u6362\u4e3a\u79bb\u6563\u4ee3\u7801\u8868\u793a; (3) \u4e00\u4e2a\u89e3\u7801\u5668, \u4ece\u91cf\u5316\u9690\u8868\u793a\u5e8f\u5217\u4e2d\u91cd\u6784\u8bed\u97f3. \u4e3a\u4e86\u589e\u5f3a\u6587\u672c\u548c\u8bed\u97f3\u5d4c\u5165\u4e4b\u95f4\u7684\u5bf9\u9f50, \u6211\u4eec\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5bf9\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u8fdb\u884c\u5fae\u8c03. \u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u7ebf\u6027\u6295\u5f71 $f(\\cdot)$, \u4ee5\u5339\u914d\u9690\u8868\u793a $z_{speech}$ \u7684\u7ef4\u5ea6\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u7a7a\u95f4\u7684\u7ef4\u5ea6. \u968f\u540e, \u6211\u4eec\u5c06\u8fd9\u4e2a\u6295\u5f71\u5d4c\u5165\u7528\u4f5c\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u5668\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u64cd\u4f5c\u4e2d\u7684\u6587\u672c\u5d4c\u5165. \u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684\u5fae\u8c03\u91c7\u7528\u8f85\u52a9\u635f\u5931, \u5b83\u5c06\u8bed\u4e49\u5185\u5bb9\u6ce8\u5165\u751f\u6210\u8868\u793a\u4e2d:</p> <p>$$     Loss(\\psi) = Loss_{NAC}(\\psi) + \\lambda Loss_{LM}(\\psi),\\quad Loss_{LM}(\\psi) = -\\log p_{\\phi}(x|f_{z_{speech}}) $$</p> \u539f\u6587  &gt; where $Loss_{NAC}(\\psi)$ indicate the loss function which is used when the neural audio codec is pre-trained ([CLaM-TTS](../../Models/Speech_LLM/2024.04.03_CLaM-TTS.md)).  &gt; $\\lambda$ controls the contribution of $Loss_{LM}$, with $\\lambda = 0$ indicating the pre-training phase. &gt; When $\\lambda &gt; 0$, a pre-trained language decoder performs causal language modeling on text token sequences $x$, based on the speech latent vector $z_{speech}$. &gt; While the parameters of language model decoder are fixed, gradients are backpropagated to adjust the linear mappings $f(z_{speech})$. &gt; This training strategy aligns the speech latents with the linguistic latents of the pretrained language model during autoencoding.   <p></p> <p>\u5176\u4e2d $Loss_{NAC}(\\psi)$ \u8868\u793a\u7528\u4e8e\u9884\u8bad\u7ec3\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684\u635f\u5931\u51fd\u6570, $\\lambda$ \u63a7\u5236 $Loss_{LM}$ \u7684\u8d21\u732e, \u5176\u503c\u4e3a $0$ \u65f6\u8868\u793a\u9884\u8bad\u7ec3\u9636\u6bb5. \u5f53 $\\lambda &gt; 0$ \u65f6, \u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u5668\u4ee5\u57fa\u4e8e\u8bed\u97f3\u9690\u5411\u91cf $z_{speech}$ \u7684\u6587\u672c\u6807\u8bc6\u7b26\u5e8f\u5217 $x$ \u4e3a\u6761\u4ef6\u8fdb\u884c\u8bed\u8a00\u5efa\u6a21. \u867d\u7136\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u5668\u7684\u53c2\u6570\u56fa\u5b9a, \u4f46\u68af\u5ea6\u4f1a\u53cd\u5411\u4f20\u64ad\u5230\u8c03\u6574\u7ebf\u6027\u6620\u5c04 $f(z_{speech})$ \u4e0a. \u8fd9\u79cd\u8bad\u7ec3\u7b56\u7565\u5728\u81ea\u7f16\u7801\u8fc7\u7a0b\u4e2d\u5c06\u8bed\u97f3\u9690\u5411\u91cf\u4e0e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50.</p> \u539f\u6587  &gt; #### Diffusion Model &gt; We are given text embedding $z_{text}$ and speech embedding $z_{speech}$. &gt; We train the diffusion model $v_{\\theta}(\\cdot)$ using the objective in Eq.01, replacing $x$ with $z_{text}$:  $$     Loss_{Diffusion} = E_{t\\sim \\mathcal{U}(1,T),\\epsilon\\sim\\mathcal{N}(0,I)} \\left[ \\|m\\odot (v^{(t)}-v_{\\theta}(z_{mask}^{(t)},m,z_{text},t))\\|^2\\right]. $$  &gt; where $z^{(t)}_{mask} = m \\odot z(t) + (1 \u2212 m) \\odot z_{speech}$ is the masked input latent, $m$ is the binary span masking, and $t$ is the diffusion time step. &gt; We apply classifier-free guidance (CFG) [36] and adopt the diffusion noise schedule from [Simple-TTS](../../Models/_tmp/Simple-TTS.md).   <p></p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#_3","title":"\u6269\u6563\u6a21\u578b","text":"<p>\u7ed9\u5b9a\u6587\u672c\u5d4c\u5165 $z_{text}$ \u548c\u8bed\u97f3\u5d4c\u5165 $z_{speech}$,  \u6211\u4eec\u4f7f\u7528\u524d\u6587\u7684\u76ee\u6807\u635f\u5931\u8bad\u7ec3\u6269\u6563\u6a21\u578b $v_{\\theta}(\\cdot)$, \u5177\u4f53\u5c06 $x$ \u66ff\u6362\u4e3a $z_{text}$ \u5982\u4e0b:</p> <p>$$     Loss_{Diffusion} = E_{t\\sim \\mathcal{U}(1,T),\\epsilon\\sim\\mathcal{N}(0,I)} \\left[ |m\\odot (v^{(t)}-v_{\\theta}(z_{mask}^{(t)},m,z_{text},t))|^2\\right]. $$</p> <p>\u5176\u4e2d $z^{(t)}{mask} = m \\odot z(t) + (1 \u2212 m) \\odot z{speech}$ \u662f\u63a9\u76d6\u8f93\u5165\u9690\u5411\u91cf, $m$ \u662f\u4e8c\u8fdb\u5236\u8de8\u5ea6\u63a9\u819c, $t$ \u662f\u6269\u6563\u65f6\u95f4\u6b65. \u6211\u4eec\u91c7\u7528\u5206\u7c7b\u65e0\u76d1\u7763\u6307\u5bfc (CFG) [36], \u5e76\u91c7\u7528 Simple-TTS \u4e2d\u7684\u6269\u6563\u566a\u58f0\u8c03\u5ea6.</p> \u539f\u6587  &gt; #### Speech Length Predictor &gt; We introduce a model designed to predict the total length of a generated speech for a given text rather than to estimate each phoneme\u2019s duration, and the input noise of diffusion model is set by the length from the speech length predictor at inference time. &gt; As shown in Figure.01, we employ an encoder-decoder transformer for the speech length predictor. &gt; The encoder processes text input bidirectionally to capture comprehensive textual context, while the decoder, equipped with causal masking to prevent future lookahead, receives an audio token sequence from the encoder of the neural codec for speech prompting at inference time. &gt; We use cross-attention mechanisms to integrate text features from the encoder. &gt; We use the softmax activation in the final layer to predict the number of tokens to be generated within the given maximum length $N$. &gt; Specifically, the ground truth label for the remaining audio length decreases by one at each subsequent time step. &gt; The model is trained separate from the diffusion model, using the cross-entropy loss function.   <p></p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#_4","title":"\u8bed\u97f3\u957f\u5ea6\u9884\u6d4b\u5668","text":"<p>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b, \u7528\u4e8e\u7ed9\u5b9a\u6587\u672c\u9884\u6d4b\u751f\u6210\u8bed\u97f3\u7684\u603b\u957f\u5ea6, \u800c\u4e0d\u662f\u4f30\u8ba1\u6bcf\u4e2a\u97f3\u7d20\u7684\u6301\u7eed\u65f6\u95f4, \u6269\u6563\u6a21\u578b\u7684\u8f93\u5165\u566a\u58f0\u7531\u8bed\u97f3\u957f\u5ea6\u9884\u6d4b\u5668\u5728\u63a8\u7406\u65f6\u8bbe\u7f6e. \u5982\u56fe 01 \u6240\u793a, \u6211\u4eec\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u53d8\u538b\u5668\u4f5c\u4e3a\u8bed\u97f3\u957f\u5ea6\u9884\u6d4b\u5668. \u7f16\u7801\u5668\u53cc\u5411\u5904\u7406\u6587\u672c\u8f93\u5165, \u4ee5\u6355\u83b7\u5168\u9762\u7684\u6587\u672c\u4e0a\u4e0b\u6587, \u800c\u89e3\u7801\u5668, \u914d\u5907\u4e86\u56e0\u679c\u63a9\u819c, \u9632\u6b62\u672a\u6765\u9884\u6d4b, \u63a5\u6536\u6765\u81ea\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u7684\u97f3\u9891\u6807\u8bc6\u7b26\u5e8f\u5217\u4f5c\u4e3a\u63a8\u7406\u65f6\u523b\u7684\u63d0\u793a. \u6211\u4eec\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u96c6\u6210\u6587\u672c\u7279\u5f81. \u6211\u4eec\u5728\u6700\u7ec8\u5c42\u4f7f\u7528 softmax \u6fc0\u6d3b\u51fd\u6570, \u9884\u6d4b\u5728\u7ed9\u5b9a\u6700\u5927\u957f\u5ea6 $N$ \u5185\u751f\u6210\u7684\u6807\u8bb0\u6570\u91cf. \u5177\u4f53\u6765\u8bf4, \u968f\u7740\u65f6\u95f4\u6b65\u7684\u589e\u52a0, \u5269\u4f59\u97f3\u9891\u957f\u5ea6\u7684\u771f\u503c\u6807\u7b7e\u51cf\u5c11\u4e00. \u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u5206\u5f00\u8bad\u7ec3, \u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570.</p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#33model-architecture","title":"3.3.Model Architecture: \u6a21\u578b\u67b6\u6784","text":"\u539f\u6587  &gt; We conduct a comprehensive model architecture search to identify the most suitable diffusion-based model for TTS, resulting in the adoption of the Diffusion Transformer (DiT) [10] model (see Section 6.1). &gt; We adopt the DiT model in TTS while incorporating recent architectural advancements &gt; for transformer variants, such as the gated linear unit with GELU activation [50], rotary position embeddings [51], and cross-attention with global adaptive layer normalization (AdaLN) [52]. &gt; For the latent space, we employ Mel-VAE introduced in ([CLaM-TTS](../../Models/Speech_LLM/2024.04.03_CLaM-TTS.md)) which is able to compress audio sequences approximately seven times more than [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md), yet maintaining superior quality. &gt; Due to space limitations, additional details regarding model configurations are provided in Appendix 8.4. &gt; We also detail down our noise scheduler and CFG in Appendix 8.6.   <p>\u6211\u4eec\u6784\u9020\u4e00\u4e2a\u5168\u9762\u7684\u6a21\u578b\u67b6\u6784\u641c\u7d22, \u4ee5\u786e\u5b9a\u9002\u5408 TTS \u7684\u6269\u6563\u6a21\u578b, \u5e76\u91c7\u7528\u6269\u6563 Transformer (DiT) [10] \u6a21\u578b (\u53c2\u89c1\u7b2c 6.1 \u8282). \u6211\u4eec\u5728 TTS \u4e2d\u91c7\u7528 DiT \u6a21\u578b, \u5e76\u878d\u5165\u4e86\u6700\u8fd1\u7684 Transformer \u53d8\u4f53\u7684\u67b6\u6784\u8fdb\u6b65, \u5982\u95e8\u63a7\u7ebf\u6027\u5355\u5143 (GELU) \u6fc0\u6d3b\u7684\u95e8\u63a7\u7ebf\u6027\u5355\u5143 [50], \u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165 [51], \u4ee5\u53ca\u4ea4\u53c9\u6ce8\u610f\u529b\u4e0e\u5168\u5c40\u81ea\u9002\u5e94\u5c42\u5f52\u4e00\u5316 (AdaLN) [52]. \u5bf9\u4e8e\u9690\u7a7a\u95f4, \u6211\u4eec\u91c7\u7528 Mel-VAE (CLaM-TTS) \u4f5c\u4e3a\u66ff\u4ee3\u54c1, \u5b83\u80fd\u591f\u5728\u97f3\u9891\u5e8f\u5217\u7684\u8fd1\u4f3c\u538b\u7f29\u7387\u4e0a\u6bd4 EnCodec \u63d0\u9ad8\u4e86 7 \u500d, \u540c\u65f6\u4fdd\u6301\u4f18\u79c0\u7684\u8d28\u91cf. \u7531\u4e8e\u7bc7\u5e45\u9650\u5236, \u6211\u4eec\u5728\u9644\u5f55 8.4 \u4e2d\u63d0\u4f9b\u4e86\u6a21\u578b\u914d\u7f6e\u7684\u8be6\u7ec6\u4fe1\u606f. \u6211\u4eec\u8fd8\u5728\u9644\u5f55 8.6 \u4e2d\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u566a\u58f0\u8c03\u5ea6\u548c CFG.</p>"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#4experiments","title":"4.Experiments: \u5b9e\u9a8c","text":"\u5b9e\u9a8c\u5360\u4f4d  ### Dataset  ### Training  ### Inference  ### Metrics  ### Baselines  ### Tasks  ## 5.Results: \u7ed3\u679c  ### 5.1.Comparison with Baselines  ### 5.2.Scaling Model Size  ### 5.3.Aligned Text-Speech Embeddings Improve Performances  ## 6.Ablation Study: \u6d88\u878d\u7814\u7a76"},{"location":"TTS/Models/Diffusion/2024.06.17_DiTTo-TTS/#7conclusions","title":"7.Conclusions: \u7ed3\u8bba","text":"\u539f\u6587  &gt; We presented ***DiTTo-TTS***, a latent diffusion model for text-to-speech (TTS) that leverages cross-attention and the prediction of the total length of latent speech representations to achieve text-speech alignment. &gt; We demonstrated that the proposed method shows exceptional zero-shot performance in naturalness, intelligibility, and speaker similarity, all without relying on domain-specific elements such as phonemes and durations, while simplifying the training process as well. &gt; In the process of obtaining this result, we also found that fine-tuning a speech autoencoder with an auxiliary language modeling objective can significantly enhance the text-speech alignment. &gt; Moreover, ***DiTTo-TTS*** shows effective scalability with respect to data and model sizes. &gt; Our future work includes:  &gt; (1) exploring various noise schedules to improve output quality and distillation methods to reduce inference times;  &gt; (2) enhancing pronunciation accuracy by improving character input normalization;  &gt; (3) enabling ***DiTTo*** to understand and learn from natural language instructions.   <p>\u6211\u4eec\u63d0\u51fa\u4e86 DiTTo-TTS, \u4e00\u79cd\u57fa\u4e8e\u9690\u7a7a\u95f4\u6269\u6563\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b, \u5b83\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u9884\u6d4b\u9690\u8bed\u97f3\u8868\u793a\u7684\u603b\u957f\u5ea6, \u5b9e\u73b0\u6587\u672c-\u8bed\u97f3\u5bf9\u9f50. \u6211\u4eec\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u81ea\u7136\u5ea6\u3001\u6613\u8bfb\u6027\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u96f6\u6837\u672c\u6027\u80fd, \u800c\u65e0\u9700\u4f9d\u8d56\u8bf8\u5982\u97f3\u7d20\u548c\u6301\u7eed\u65f6\u95f4\u7b49\u7279\u5b9a\u5143\u7d20, \u540c\u65f6\u7b80\u5316\u4e86\u8bad\u7ec3\u8fc7\u7a0b. \u5728\u53d6\u5f97\u8fd9\u4e9b\u7ed3\u679c\u7684\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u8fd8\u53d1\u73b0\u5fae\u8c03\u8bed\u97f3\u81ea\u7f16\u7801\u5668\u4e0e\u8f85\u52a9\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u6587\u672c-\u8bed\u97f3\u5bf9\u9f50. \u6b64\u5916, DiTTo-TTS \u5177\u6709\u4e0e\u6570\u636e\u548c\u6a21\u578b\u5927\u5c0f\u6210\u6bd4\u4f8b\u7684\u6709\u6548\u6269\u5c55\u6027. \u6211\u4eec\u7684\u672a\u6765\u5de5\u4f5c\u5305\u62ec: (1) \u63a2\u7d22\u5404\u79cd\u566a\u58f0\u8c03\u5ea6\u4ee5\u63d0\u9ad8\u8f93\u51fa\u8d28\u91cf\u548c\u84b8\u998f\u65b9\u6cd5\u4ee5\u51cf\u5c11\u63a8\u7406\u65f6\u95f4; (2) \u901a\u8fc7\u6539\u8fdb\u5b57\u7b26\u8f93\u5165\u6807\u51c6\u5316\u63d0\u9ad8\u53d1\u97f3\u51c6\u786e\u6027; (3) \u4f7f DiTTo \u80fd\u591f\u7406\u89e3\u548c\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u6307\u4ee4.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/","title":"VITS","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech - \u4f5c\u8005:   - [Jaehyeon Kim](../../Authors/Jaehyeon_Kim.md)   - [Jungil Kong](../../Authors/Jungil_Kong.md)   - [Juhee Son](../../Authors/Juhee_Son.md) - \u673a\u6784:   - [Kakao Enterprise](../../Institutions/Kakao_Enterprise.md)   - [KAIST](../../Institutions/KAIST_\u97e9\u56fd\u79d1\u5b66\u6280\u672f\u7814\u7a76\u6240.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2021.06.11 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.06 - \u53d1\u8868:   - [ICML](../../Publications/ICML.md) - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2106.06103)   - [DOI](https://proceedings.mlr.press/v139/kim21f)   - [Github](https://github.com/jaywalnut310/vits)   - [Demo](https://jaywalnut310.github.io/vits-demo/index.html)   - [Scholar](https://scholar.google.com/scholar?cluster=12414540587288194560) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u6a21\u578b-\u6d41](../../Tags/Model_Flow.md)   - [\u6a21\u578b-\u53d8\u5206\u81ea\u7f16\u7801\u5668](../../Tags/Model_VAE.md)   - [\u6a21\u578b-\u751f\u6210\u5bf9\u6297\u7f51\u7edc](../../Tags/Model_GAN.md)   - [\u5bf9\u6297\u5b66\u4e60](../../Tags/Learning_Adversarial.md)   - [\u5f00\u6e90](../../Tags/OpenSource.md) - \u9875\u6570: 15 - \u5f15\u7528: 45 - \u88ab\u5f15: 610 - \u6570\u636e:   - [LJSpeech](../../Datasets/LJSpeech.md)   - [VCTK](../../Datasets/VCTK.md) - \u5bf9\u6bd4   - [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md) + [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md)   - [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) + [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md) - \u590d\u73b0:   - 2023.11.28 [open-mmlab/Amphion](https://github.com/open-mmlab/Amphion/tree/main/models/tts/vits)"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#abstract","title":"Abstract: \u6458\u8981","text":"\u539f\u6587  &gt; Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. &gt; In this work, we present a parallel end to-end TTS method that generates more natural sounding audio than current two-stage models. &gt; Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. &gt; We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. &gt; With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. &gt; A subjective human evaluation ([Mean Opinion Score, or MOS](../../Evaluations/MOS.md)) on the [LJ Speech](../../Datasets/LJSpeech.md), a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS com parable to ground truth.   <p>\u8fd1\u671f\u51e0\u9879\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u4f7f\u5f97\u5355\u9636\u6bb5\u8bad\u7ec3\u548c\u5e76\u884c\u91c7\u6837\u6210\u4e3a\u53ef\u80fd, \u4f46\u5b83\u4eec\u7684\u91c7\u6837\u8d28\u91cf\u4e0e\u4e24\u9636\u6bb5\u7cfb\u7edf\u7684\u8d28\u91cf\u76f8\u6bd4\u4ecd\u6709\u5dee\u8ddd. \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e76\u884c\u7684\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u65b9\u6cd5, \u5176\u751f\u6210\u7684\u97f3\u9891\u66f4\u81ea\u7136. \u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u6807\u51c6\u5316\u6d41\u548c\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u7684\u53d8\u5206\u63a8\u65ad, \u8fd9\u53ef\u4ee5\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u8868\u73b0\u529b. \u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7528\u4e8e\u4ece\u8f93\u5165\u6587\u672c\u751f\u6210\u591a\u79cd\u8282\u594f\u7684\u8bed\u97f3. \u901a\u8fc7\u5bf9\u9690\u53d8\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u8868\u8fbe\u81ea\u7136\u7684\u4e00\u5bf9\u591a\u5173\u7cfb, \u5373\u4e00\u4e2a\u6587\u672c\u8f93\u5165\u53ef\u4ee5\u88ab\u8bf4\u6210\u591a\u79cd\u97f3\u9ad8\u548c\u8282\u594f. \u5728 LJ Speech \u6570\u636e\u96c6\u7684\u4e3b\u89c2\u8bc4\u4ef7 (MOS) \u4e2d, \u6211\u4eec\u5bf9\u6bd4\u4e86\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u7684\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf, \u5e76\u8fbe\u5230\u4e86\u53ef\u6bd4\u7684MOS.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#1introduction","title":"1.Introduction: \u5f15\u8a00","text":"\u539f\u6587  &gt; Text-to-speech (TTS) systems synthesize raw speech waveforms from given text through several components. &gt; With the rapid development of deep neural networks, TTS system pipelines have been simplified to two-stage generative modeling apart from text preprocessing such as text normalization and phonemization. &gt; The first stage is to produce intermediate speech representations such as mel-spectrograms ([Tacotron2 (2017)](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md)) or linguistic features (Oord et al., 2016) from the preprocessed text (Although there is a text preprocessing step in TTS systems, We herein use preprocessed text interchangeably with the word \"text\".), and the second stage is to generate raw waveforms conditioned on the intermediate representations ([WaveNet (2016)](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md); [WaveRNN (2018)](../../Models/TTS3_Vocoder/2018.02.23_WaveRNN.md)). &gt; Models at each of the two-stage pipelines have been developed independently.   <p>\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u901a\u8fc7\u6570\u4e2a\u7ec4\u4ef6\u4ece\u7ed9\u5b9a\u7684\u6587\u672c\u751f\u6210\u539f\u59cb\u8bed\u97f3\u6ce2\u5f62. \u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8fc5\u901f\u53d1\u5c55, \u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u5df2\u7ecf\u7b80\u5316\u4e3a\u9664\u4e86\u6587\u672c\u9884\u5904\u7406 (\u6587\u672c\u89c4\u8303\u5316\u548c\u97f3\u7d20\u5316) \u4e4b\u5916\u7684\u4e24\u9636\u6bb5\u751f\u6210\u6a21\u578b. \u7b2c\u4e00\u9636\u6bb5\u662f\u751f\u6210\u4e2d\u95f4\u8bed\u97f3\u8868\u793a\u5982\u6885\u5c14\u9891\u8c31\u56fe\u6216\u6765\u81ea\u9884\u5904\u7406\u6587\u672c\u7684\u8bed\u8a00\u7279\u5f81, \u7b2c\u4e8c\u9636\u6bb5\u662f\u6839\u636e\u4e2d\u95f4\u8868\u793a\u751f\u6210\u539f\u59cb\u6ce2\u5f62. \u6bcf\u4e2a\u9636\u6bb5\u7684\u6a21\u578b\u90fd\u5355\u72ec\u5efa\u7acb.</p> \u539f\u6587  &gt; Neural network-based autoregressive TTS systems have shown the capability of synthesizing realistic speech ([Tacotron2 (2017)](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md); [Transformer-TTS (2018)](../../Models/TTS2_Acoustic/2018.09.19_Transformer_TTS.md)), but their sequential generative process makes it difficult to fully utilize modern parallel processors. &gt; To overcome this limitation and improve synthesis speed, several non-autoregressive methods have been proposed. &gt; In the text-to-spectrogram generation step, extracting attention maps from pre-trained autoregressive teacher networks ([FastSpeech (2019)](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md); [ParaNet](../../Models/TTS2_Acoustic/2019.05.21_ParaNet.md)) is attempted to decrease the difficulty of learning alignments between text and spectrograms. &gt; More recently, likelihood-based methods further eliminate the dependency on external aligners by estimating or learning alignments that maximize the likelihood of target mel-spectrograms ([AlignTTS (2020)](../../Models/TTS2_Acoustic/2020.03.04_AlignTTS.md); [Flow-TTS (2020)](../../Models/TTS2_Acoustic/2020.05.04_Flow-TTS.md); [Glow-TTS (2020)](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md)). &gt; Meanwhile, [Generative Adversarial Networks (GANs)](../../Models/_Basis/2014.06.10_GAN.md) have been explored in second stage models. &gt; GAN-based feed-forward networks with multiple discriminators, each distinguishing samples at different scales or periods, achieve high-quality raw waveform synthesis ([MelGAN (2019)](../../Models/TTS3_Vocoder/2019.10.08_MelGAN.md); [GAN-TTS (2019)](../../Models/TTS3_Vocoder/2019.09.25_GAN-TTS.md).   <p></p> <p>\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u5df2\u7ecf\u5c55\u793a\u4e86\u5408\u6210\u771f\u5b9e\u8bed\u97f3\u7684\u80fd\u529b, \u4f46\u5b83\u4eec\u7684\u987a\u5e8f\u751f\u6210\u8fc7\u7a0b\u4f7f\u5f97\u5b83\u4eec\u96be\u4ee5\u5145\u5206\u5229\u7528\u73b0\u4ee3\u5e76\u884c\u5904\u7406\u5668. \u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\u5e76\u63d0\u5347\u5408\u6210\u901f\u5ea6, \u5df2\u7ecf\u6709\u6570\u79cd\u975e\u81ea\u56de\u5f52\u65b9\u6cd5\u88ab\u63d0\u51fa. \u5728\u6587\u672c\u5230\u9891\u8c31\u56fe\u751f\u6210\u6b65\u9aa4\u4e2d, \u4ece\u9884\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u6559\u5e08\u7f51\u7edc\u4e2d\u63d0\u53d6\u6ce8\u610f\u529b\u7279\u5f81\u56fe\u8bd5\u56fe\u51cf\u5c11\u5b66\u4e60\u6587\u672c\u548c\u9891\u8c31\u56fe\u4e4b\u95f4\u5bf9\u9f50\u7684\u96be\u5ea6. \u8fd1\u671f, \u57fa\u4e8e\u4f3c\u7136\u7684\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6d88\u9664\u4e86\u5bf9\u5916\u90e8\u5bf9\u9f50\u5668\u7684\u4f9d\u8d56, \u901a\u8fc7\u6700\u5927\u5316\u76ee\u6807\u9891\u8c31\u56fe\u7684\u4f3c\u7136\u6765\u4f30\u8ba1\u6216\u5b66\u4e60\u5bf9\u9f50. \u540c\u65f6, \u751f\u6210\u5bf9\u6297\u7f51\u7edc\u88ab\u7528\u4e8e\u7b2c\u4e8c\u9636\u6a21\u578b. \u57fa\u4e8e GAN \u7684\u524d\u9988\u7f51\u7edc\u5177\u6709\u591a\u4e2a\u5224\u522b\u5668, \u6bcf\u4e2a\u5224\u522b\u4e0d\u540c\u5c3a\u5ea6\u6216\u5468\u671f\u7684\u6837\u672c, \u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u539f\u59cb\u6ce2\u5f62\u5408\u6210.</p> \u539f\u6587  &gt; Despite the progress of parallel TTS systems, two-stage pipelines remain problematic because they require sequential training or fine-tuning ([Tacotron2 (2017)](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md); [Wave-Tacotron (2020)]()) for high-quality production wherein latter stage models are trained with the generated samples of earlier stage models. &gt; In addition, their dependency on predefined intermediate features precludes applying learned hidden representations to obtain further improvements in performance. &gt; Recently, several works, i.e., [FastSpeech 2s (2020)](../TTS2_Acoustic/2020.06.08_FastSpeech2.md) and [EATS (2020)](../../Models/E2E/2020.06.05_EATS.md), have proposed efficient end-to-end training methods such as training over short audio clips rather than entire waveforms, leveraging a mel-spectrogram decoder to aid text representation learning, and designing a specialized spectrogram loss to relax length-mismatch between target and generated speech.  &gt; However, despite potentially improving performance by utilizing the learned representations, their synthesis quality lags behind two-stage systems.   <p></p> <p>\u5c3d\u7ba1\u5e76\u884c\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u53d6\u5f97\u4e86\u8fdb\u5c55, \u4f46\u4e24\u9636\u6bb5\u65b9\u6848\u4ecd\u7136\u5b58\u5728\u95ee\u9898. \u56e0\u4e3a\u5b83\u4eec\u8981\u6c42\u987a\u5e8f\u8bad\u7ec3\u6216\u5fae\u8c03\u4ee5\u83b7\u53d6\u9ad8\u8d28\u91cf\u7ed3\u679c, \u5373\u540e\u4e00\u9636\u6bb5\u7684\u6a21\u578b\u9700\u8981\u5728\u524d\u4e00\u9636\u6bb5\u6a21\u578b\u7684\u751f\u6210\u6837\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3. \u6b64\u5916, \u4ed6\u4eec\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u4e2d\u95f4\u7279\u5f81\u9650\u5236\u4e86\u5c06\u5b66\u4e60\u5230\u7684\u9690\u85cf\u8868\u793a\u4ee5\u83b7\u5f97\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347. \u8fd1\u671f, \u6709\u51e0\u9879\u5de5\u4f5c\u5982 FastSpeech 2s (2020) \u548c EATS (2020) \u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5, \u5982\u8bad\u7ec3\u4e8e\u8f83\u77ed\u7684\u97f3\u9891\u7247\u6bb5\u800c\u4e0d\u662f\u5b8c\u6574\u6ce2\u5f62, \u5229\u7528\u6885\u5c14\u9891\u8c31\u56fe\u89e3\u7801\u5668\u8f85\u52a9\u6587\u672c\u8868\u793a\u5b66\u4e60, \u5e76\u8bbe\u8ba1\u4e13\u95e8\u7684\u9891\u8c31\u56fe\u635f\u5931\u4ee5\u677e\u5f1b\u76ee\u6807\u548c\u751f\u6210\u8bed\u97f3\u7684\u957f\u5ea6\u4e0d\u5339\u914d. \u7136\u800c, \u5c3d\u7ba1\u5229\u7528\u5b66\u4e60\u5230\u7684\u8868\u793a\u53ef\u4ee5\u63d0\u5347\u6027\u80fd, \u4f46\u5b83\u4eec\u7684\u5408\u6210\u8d28\u91cf\u4ecd\u7136\u843d\u540e\u4e8e\u4e24\u9636\u6bb5\u7cfb\u7edf.</p> \u539f\u6587  &gt; In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models.  &gt; Using a Variational AutoEncoder (VAE), we connect two modules of TTS systems through latent variables to enable efficient end-to-end learning.  &gt; To improve the expressive power of our method so that high-quality speech waveforms can be synthesized, we apply normalizing flows to our conditional prior distribution and adversarial training on the waveform domain.  &gt; In addition to generating fine-grained audio, it is important for TTS systems to express the one-to-many relationship in which text input can be spoken in multiple ways with different variations (e.g., pitch and duration).  &gt; To tackle the one-to-many problem, we also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. &gt; With the uncertainty modeling over latent variables and the stochastic duration predictor, our method captures speech variations that cannot be represented by text.    <p></p> <p>\u672c\u9879\u5de5\u4f5c\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e76\u884c\u7684\u7aef\u5230\u7aef\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u65b9\u6cd5, \u5176\u751f\u6210\u7684\u97f3\u9891\u66f4\u81ea\u7136. \u6211\u4eec\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAE) \u5c06\u4e24\u4e2a\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u7684\u6a21\u5757\u8fde\u63a5\u8d77\u6765, \u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u5b66\u4e60. \u4e3a\u4e86\u63d0\u9ad8\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u8868\u73b0\u529b, \u6211\u4eec\u5c06\u6807\u51c6\u5316\u6d41\u5e94\u7528\u5230\u6761\u4ef6\u5148\u9a8c\u5206\u5e03, \u5e76\u5728\u6ce2\u5f62\u57df\u4e0a\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3. \u9664\u4e86\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u97f3\u9891\u4e4b\u5916, \u5bf9\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u6765\u8bf4, \u8868\u8fbe\u4e00\u5bf9\u591a\u5173\u7cfb\u662f\u81f3\u5173\u91cd\u8981\u7684, \u5373\u8f93\u5165\u6587\u672c\u53ef\u4ee5\u4ee5\u4e0d\u540c\u7684\u97f3\u9ad8\u548c\u8282\u594f\u88ab\u8bf4\u51fa\u6765. \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898, \u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u7528\u4e8e\u4ece\u8f93\u5165\u6587\u672c\u751f\u6210\u591a\u79cd\u8282\u594f\u7684\u8bed\u97f3. \u901a\u8fc7\u5bf9\u9690\u53d8\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u6355\u6349\u5230\u4e0d\u80fd\u7528\u6587\u672c\u8868\u793a\u7684\u8bed\u97f3\u53d8\u4f53.</p> \u539f\u6587  &gt; Our method obtains more natural sounding speech and higher sampling efficiency than the best publicly available TTS system, [Glow-TTS (2020)](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) with [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md).  &gt; We make both our [demo page](https://jaywalnut310.github.io/vits-demo/index.html) and [source-code](https://github.com/jaywalnut310/vits) publicly available.   <p></p> <p>\u6211\u4eec\u7684\u65b9\u6cd5\u83b7\u5f97\u66f4\u81ea\u7136\u7684\u8bed\u97f3\u548c\u66f4\u9ad8\u7684\u91c7\u6837\u6548\u7387.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#2related-works","title":"2.Related Works: \u76f8\u5173\u5de5\u4f5c","text":"<p>\u6ce8: \u539f\u6587\u7b2c\u4e94\u8282 Section 5 in Original.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#21end-to-end-text-to-speech","title":"2.1.End-to-End Text-to-Speech\u00b7\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3","text":"\u539f\u6587  &gt; Currently, neural TTS models with a two-stage pipeline can synthesize human-like speech ([WaveNet (2016)](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md); [DeepVoice3](../../Models/TTS2_Acoustic/2017.10.20_DeepVoice3.md); [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md)). &gt; However, they typically require vocoders trained or fine-tuned with first stage model output, which causes training and deployment inefficiency. &gt; They are also unable to reap the potential benefits of an end-to-end approach that can use learned hidden representations rather than predefined intermediate features.   <p>\u5f53\u524d\u4e24\u9636\u6bb5\u7684\u795e\u7ecf\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u53ef\u4ee5\u5408\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8bed\u97f3. \u7136\u800c\u4ed6\u4eec\u540c\u6837\u8981\u6c42\u4f7f\u7528\u7b2c\u4e00\u9636\u6bb5\u6a21\u578b\u7684\u8f93\u51fa\u7528\u4e8e\u8bad\u7ec3\u6216\u5fae\u8c03\u58f0\u7801\u5668, \u8fd9\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u548c\u90e8\u7f72\u6548\u7387\u4f4e\u4e0b. \u4ed6\u4eec\u8fd8\u65e0\u6cd5\u5229\u7528\u7aef\u5230\u7aef\u65b9\u6cd5\u7684\u6f5c\u5728\u4f18\u52bf, \u5373\u53ef\u4ee5\u5229\u7528\u5b66\u4e60\u5230\u7684\u9690\u85cf\u8868\u793a\u800c\u975e\u9884\u5b9a\u4e49\u7684\u4e2d\u95f4\u7279\u5f81.</p> \u539f\u6587  &gt; Recently, single-stage end-to-end TTS models have been proposed to tackle the more challenging task of generating raw waveforms, which contain richer information (e.g., high-frequency response and phase) than mel-spectrograms, directly from text. &gt; [FastSpeech 2s (2020)](../../Models/E2E/2020.06.08_FastSpeech2s.md) is an extension of [FastSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md) that enables end-to-end parallel generation by adopting adversarial training and an auxiliary mel-spectrogram decoder that helps learn text representations. &gt; However, to resolve the one-to-many problem, FastSpeech 2s must extract phoneme duration, pitch, and energy from speech used as input conditions in training. &gt; [EATS (2020)](../../Models/E2E/2020.06.05_EATS.md) employs adversarial training as well and a differentiable alignment scheme. &gt; To resolve the length mismatch problem between generated and target speech, EATS adopts soft dynamic time warping loss that is calculated by dynamic programming. &gt; [Wave-Tacotron (2020)]() combines normalizing flows with [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md) for an end-to-end structure but remains autoregressive. &gt; The audio quality of all the aforementioned end-to-end TTS models is less than that of two-stage models.   <p></p> <p>\u6700\u8fd1\u5355\u9636\u6bb5\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u88ab\u63d0\u51fa\u7528\u4e8e\u89e3\u51b3\u751f\u6210\u539f\u59cb\u6ce2\u5f62\u7684\u66f4\u5177\u6311\u6218\u6027\u4efb\u52a1, \u5b83\u5305\u542b\u6bd4\u6885\u5c14\u9891\u8c31\u56fe\u66f4\u591a\u7684\u4fe1\u606f (\u5982\u9ad8\u9891\u54cd\u5e94\u548c\u76f8\u4f4d), \u76f4\u63a5\u4ece\u6587\u672c\u751f\u6210. FastSpeech 2s \u662f FastSpeech 2 \u7684\u6269\u5c55, \u5b83\u901a\u8fc7\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u548c\u4e00\u4e2a\u8f85\u52a9\u7684\u6885\u5c14\u9891\u8c31\u56fe\u89e3\u7801\u5668\u6765\u5b9e\u73b0\u7aef\u5230\u7aef\u5e76\u884c\u751f\u6210. \u7136\u800c, \u4e3a\u4e86\u89e3\u51b3\u4e00\u5bf9\u591a\u95ee\u9898, FastSpeech 2s \u5fc5\u987b\u4ece\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u8bed\u97f3\u4e2d\u63d0\u53d6\u97f3\u7d20\u65f6\u957f, \u97f3\u9ad8\u548c\u80fd\u91cf. EATS \u4e5f\u662f\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u540c\u7684\u5bf9\u9f50\u65b9\u6848. \u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u548c\u76ee\u6807\u8bed\u97f3\u4e4b\u95f4\u7684\u957f\u5ea6\u4e0d\u5339\u914d\u95ee\u9898, EATS \u91c7\u7528\u8f6f\u52a8\u6001\u65f6\u95f4\u7a97\u635f\u5931, \u5b83\u662f\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u8ba1\u7b97\u7684. Wave Tacotron \u662f\u5c06\u6807\u51c6\u5316\u6d41\u4e0e Tacotron2 \u7ed3\u5408\u7528\u4e8e\u7aef\u5230\u7aef\u7ed3\u6784, \u4f46\u4ecd\u7136\u662f\u81ea\u56de\u5f52\u7684. \u6240\u6709\u8fd9\u4e9b\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u7684\u8bed\u97f3\u8d28\u91cf\u90fd\u4e0d\u5982\u4e24\u9636\u6bb5\u6a21\u578b.</p> \u539f\u6587  &gt; Unlike the aforementioned end-to-end models, by utilizing a conditional VAE, our model  &gt;  &gt; 1. learns to synthesize raw waveforms directly from text without requiring additional input conditions, &gt; 2. uses a dynamic programming method, MAS, to search the optimal alignment rather than to calculate loss,  &gt; 3. generates samples in parallel,  &gt; 4. outperforms the best publicly available two-stage models.   <p></p> <p>\u548c\u524d\u9762\u63d0\u53ca\u7684\u7aef\u5230\u7aef\u6a21\u578b\u4e0d\u540c, \u901a\u8fc7\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668, \u6211\u4eec\u7684\u6a21\u578b: 1. \u76f4\u63a5\u4ece\u6587\u672c\u751f\u6210\u539f\u59cb\u6ce2\u5f62\u800c\u4e0d\u9700\u8981\u989d\u5916\u7684\u8f93\u5165\u6761\u4ef6, 2. \u4f7f\u7528\u52a8\u6001\u89c4\u5212\u65b9\u6cd5 MAS \u641c\u7d22\u6700\u4f18\u5bf9\u9f50\u800c\u4e0d\u662f\u8ba1\u7b97\u635f\u5931, 3. \u4ee5\u5e76\u884c\u7684\u65b9\u5f0f\u751f\u6210\u6837\u672c, 4. \u6027\u80fd\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u516c\u5f00\u53ef\u7528\u7684\u4e24\u9636\u6bb5\u6a21\u578b.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#22variational-autoencoders","title":"2.2.Variational Autoencoders\u00b7\u53d8\u5206\u81ea\u7f16\u7801\u5668","text":"\u539f\u6587  &gt; VAEs are one of the most widely used likelihood-based deep generative models. &gt; We adopt a conditional VAE to a TTS system. &gt; A conditional VAE is a conditional generative model where the observed conditions modulate the prior distribution of latent variables used to generate outputs. &gt; In speech synthesis, [GMVAE-Tacotron (2018)]() and [Learning latent representations for style control and transfer in end-to-end speech synthesis]() combine [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md) and VAEs to learn speech style and prosody. &gt; [BVAE-TTS (2020)]() generates mel-spectrograms in parallel based on a bidirectional VAE ([IAF (2016)]()). &gt; Unlike the previous works that applied VAEs to first stage models, we adopt a VAE to a parallel end-to-end TTS system. &gt; [Variational inference with normalizing flows](), [Variational Lossy Autoencoder]() and [Latent normalizing flows for discrete sequences]() improve VAE performance by enhancing the expressive power of prior and posterior distribution with normalizing flows. &gt; To improve the representation power of the prior distribution, we add normalizing flows to our conditional prior network, leading to the generation of more realistic samples.   <p>\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAEs) \u662f\u6700\u5e38\u7528\u7684\u57fa\u4e8e\u4f3c\u7136\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e4b\u4e00. \u6211\u4eec\u5c06\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf. \u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u662f\u4e00\u79cd\u6761\u4ef6\u751f\u6210\u6a21\u578b, \u5176\u4e2d\u89c2\u5bdf\u5230\u7684\u6761\u4ef6\u4f1a\u5f71\u54cd\u7528\u4e8e\u751f\u6210\u8f93\u51fa\u7684\u9690\u53d8\u91cf\u7684\u5148\u9a8c\u5206\u5e03. \u5728\u8bed\u97f3\u5408\u6210\u4e2d, Hsu \u548c Zhang \u5c06 Tacotron2 \u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7ed3\u5408\u4ee5\u5b66\u4e60\u8bed\u97f3\u98ce\u683c\u548c\u97f5\u5f8b. BVAE-TTS \u57fa\u4e8e\u53cc\u5411\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ee5\u5e76\u884c\u751f\u6210\u6885\u5c14\u9891\u8c31. \u548c\u8fd9\u4e9b\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u7b2c\u4e00\u9636\u6bb5\u6a21\u578b\u7684\u5de5\u4f5c\u4e0d\u540c, \u6211\u4eec\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u5e76\u884c\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf. Rezende, Chen, Ziegler \u901a\u8fc7\u4f7f\u7528\u6807\u51c6\u5316\u6d41\u589e\u5f3a\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\u8868\u73b0\u6027\u4ee5\u63d0\u9ad8\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6027\u80fd. \u4e3a\u4e86\u63d0\u5347\u5148\u9a8c\u5206\u5e03\u7684\u8868\u793a\u80fd\u529b, \u6211\u4eec\u5728\u6761\u4ef6\u5148\u9a8c\u7f51\u7edc\u4e2d\u52a0\u5165\u4e86\u6807\u51c6\u5316\u6d41, \u4ece\u800c\u80fd\u591f\u751f\u6210\u66f4\u771f\u5b9e\u7684\u6837\u672c.</p> \u539f\u6587  &gt; Similar to our work, [Flowseq]() proposed a conditional VAE with normalizing flows in a conditional prior network for non-autoregressive neural machine translation, FlowSeq. &gt; However, the fact that our model can explicitly align a latent sequence with the source sequence differs from FlowSeq, which needs to learn implicit alignment through attention mechanisms. &gt; Our model removes the burden of transforming the latent sequence into standard normal random variables by matching the latent sequence with the time-aligned source sequence via MAS, which allows for simpler architecture of normalizing flows.   <p></p> <p>\u548c\u6211\u4eec\u7684\u5de5\u4f5c\u7c7b\u4f3c, Ma \u7b49\u4eba\u63d0\u51fa\u4e86 FlowSeq \u4e00\u4e2a\u5e26\u6709\u6807\u51c6\u5316\u6d41\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668, \u7528\u4e8e\u975e\u81ea\u56de\u5f52\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1. \u7136\u800c, \u7531\u4e8e\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u663e\u5f0f\u5730\u5bf9\u9f50\u9690\u53d8\u91cf\u5e8f\u5217\u548c\u6e90\u5e8f\u5217, \u4e0e FlowSeq \u4e0d\u540c, \u5b83\u9700\u8981\u901a\u8fc7\u6ce8\u610f\u673a\u5236\u5b66\u4e60\u9690\u5f0f\u5bf9\u9f50. \u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7 MAS \u5339\u914d\u9690\u53d8\u91cf\u5e8f\u5217\u548c\u65f6\u95f4\u5bf9\u9f50\u7684\u6e90\u5e8f\u5217, \u4ece\u800c\u6d88\u9664\u4e86\u5c06\u9690\u53d8\u91cf\u5e8f\u5217\u8f6c\u6362\u4e3a\u6807\u51c6\u6b63\u6001\u968f\u673a\u53d8\u91cf\u7684\u8d1f\u62c5, \u8fd9\u4f7f\u5f97\u6807\u51c6\u5316\u6d41\u7684\u67b6\u6784\u66f4\u7b80\u5355.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#23duration-prediction-in-non-autoregressive-text-to-speech","title":"2.3.Duration Prediction in Non-Autoregressive Text-to-Speech\u00b7\u975e\u81ea\u56de\u5f52\u6587\u672c\u8f6c\u8bed\u97f3\u4e2d\u7684\u65f6\u957f\u9884\u6d4b","text":"\u539f\u6587  &gt; Autoregressive TTS models ([VoiceLoop (2017.07)](); [Tacotron](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md)); [Flowtron (2021)](../../Models/TTS2_Acoustic/2020.05.12_Flowtron.md) diverse speech with different rhythms through their autoregressive structure and several tricks including maintaining dropout probability during inference and priming ([Generating sequences with recurrent neural networks]()). &gt; Parallel TTS models ([FastSpeech (2019)](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md); [ParaNet (2020)](../../Models/TTS2_Acoustic/2019.05.21_ParaNet.md); [Glow-TTS (2020)](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md); [FastSpeech 2s (2020)](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md), on the other hand, have been relied on deterministic duration prediction. &gt; It is because parallel models have to predict target phoneme duration or the total length of target speech in one feed-forward path, which makes it hard to capture the correlated joint distribution of speech rhythms. &gt; In this work, we suggest a flow-based stochastic duration predictor that learns the joint distribution of the estimated phoneme duration, resulting in the generation of diverse speech rhythms in parallel.   <p>\u81ea\u56de\u5f52\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u901a\u8fc7\u4ed6\u4eec\u7684\u81ea\u56de\u5f52\u7ed3\u6784\u548c\u4e00\u4e9b\u6280\u5de7\u5305\u62ec\u5728\u63a8\u7406\u7ef4\u6301\u968f\u673a\u5931\u6d3b\u7387\u7b49\u7b49\u751f\u6210\u5177\u6709\u4e0d\u540c\u8282\u594f\u7684\u591a\u6837\u8bed\u97f3. \u5e76\u884c\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u5219\u4f9d\u8d56\u4e8e\u786e\u5b9a\u6027\u7684\u65f6\u957f\u9884\u6d4b, \u8fd9\u662f\u56e0\u4e3a\u5e76\u884c\u6a21\u578b\u5fc5\u987b\u5728\u4e00\u4e2a\u524d\u9988\u8def\u5f84\u4e2d\u9884\u6d4b\u76ee\u6807\u97f3\u7d20\u65f6\u957f\u6216\u76ee\u6807\u8bed\u97f3\u7684\u603b\u957f\u5ea6, \u8fd9\u4f7f\u5f97\u96be\u4ee5\u6355\u6349\u8bed\u97f3\u8282\u594f\u7684\u76f8\u5173\u8054\u5408\u5206\u5e03. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6d41\u7684\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u5b83\u5b66\u4e60\u4f30\u8ba1\u7684\u97f3\u7d20\u65f6\u957f\u7684\u8054\u5408\u5206\u5e03, \u4ece\u800c\u5728\u5e76\u884c\u751f\u6210\u4e2d\u751f\u6210\u5177\u6709\u4e0d\u540c\u8282\u594f\u7684\u591a\u6837\u8bed\u97f3.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#3method","title":"3.Method","text":"\u539f\u6587  &gt; In this section, we explain our proposed method and the architecture of it. &gt; The proposed method is mostly described in the first three subsections: a conditional VAE formulation; alignment estimation derived from variational inference; adversarial training for improving synthesis quality. &gt; The overall architecture is described at the end of this section. &gt; Fig.01a and Fig.01b show the training and inference procedures of our method, respectively. &gt; From now on, we will refer to our method as ***Variational Inference with adversarial learning for end-to-end Text-to-Speech (VITS)***.   <p>\u672c\u8282\u5c06\u89e3\u91ca\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ca\u5176\u67b6\u6784. \u5728\u524d\u4e09\u4e2a\u5c0f\u8282\u4e2d, \u6211\u4eec\u5c06\u4ecb\u7ecd\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u4e3b\u8981\u5185\u5bb9: \u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5f62\u5f0f; \u7531\u53d8\u5206\u63a8\u65ad\u5f97\u5230\u7684\u5bf9\u9f50\u4f30\u8ba1; \u5bf9\u6297\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u5408\u6210\u8d28\u91cf. \u6700\u540e\u4e00\u5c0f\u8282\u5c06\u4ecb\u7ecd\u6574\u4f53\u67b6\u6784. \u56fe 01a \u548c\u56fe 01b \u5206\u522b\u5c55\u793a\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b. \u4e4b\u540e, \u6211\u4eec\u5c06\u4f7f\u7528 VITS \u6765\u6307\u4ee3\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#31variational-inference","title":"3.1.Variational Inference\u00b7\u53d8\u5206\u63a8\u65ad","text":""},{"location":"TTS/Models/E2E/2021.06.11_VITS/#311overview","title":"3.1.1.Overview\u00b7\u6982\u89c8","text":"\u539f\u6587  &gt; ***VITS*** can be expressed as a conditional VAE with the objective of maximizing the variational lower bound, also called the evidence lower bound (ELBO), of the intractable marginal log-likelihood of data $\\log p_{\\theta}(x|c)$:   <p>VITS \u53ef\u4ee5\u8868\u793a\u4e3a\u4e00\u4e2a\u6761\u4ef6\u81ea\u7f16\u7801\u5668, \u5176\u76ee\u6807\u662f\u6700\u5927\u5316\u96be\u4ee5\u5904\u7406\u7684\u6570\u636e\u7684\u8fb9\u9645\u5206\u5e03\u5bf9\u6570\u4f3c\u7136 $\\log p_{\\theta}(x|c)$ \u7684\u53d8\u5206\u4e0b\u754c, \u4e5f\u79f0\u4e3a\u8bc1\u636e\u4e0b\u754c (Evidence Lower BOund, ELBO).</p> <p>$$   \\log p_{\\theta}(x|c) \\geq E_{q_{\\phi}(z|x)}\\left[\\log p_{\\theta}(x|z)-\\log\\dfrac{q_{\\phi}(z|x)}{p_{\\theta}(z|c)}\\right] \\tag{1} $$</p> \u539f\u6587  &gt; where $p_{\\theta}(z|c)$ denotes a prior distribution of the latent variables $z$ given condition $c$,  &gt; $p_{\\theta}(x|z)$ is the likelihood function of a data point $x$, &gt; and $q_{\\phi}(z|x)$ is an approximate posterior distribution.   <p></p> <ul> <li>$p_{\\theta}(z|c)$: \u7ed9\u5b9a\u6761\u4ef6 $c$ \u7684\u9690\u53d8\u91cf $z$ \u7684\u5148\u9a8c\u5206\u5e03.</li> <li>$p_{\\theta}(x|z)$: \u6570\u636e\u70b9 $x$ \u7684\u4f3c\u7136\u51fd\u6570.</li> <li>$q_{\\phi}(z|x)$: \u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03.</li> </ul> \u539f\u6587  &gt; The training loss is then the negative ELBO, which can be viewed as the sum of reconstruction loss $-\\log p_{\\theta}(x|z)$ and KL-divergence $\\log q_{\\phi}(z|x) -\\log \\| p_{\\theta}(z|c)$, where $z\\sim q_{\\phi}(z|x)$.   <p></p> <p>\u8bad\u7ec3\u635f\u5931\u4e3a\u8d1f\u7684 ELBO, \u5b83\u53ef\u4ee5\u88ab\u89c6\u4e3a\u91cd\u6784\u635f\u5931 $-\\log p_{\\theta}(x|z)$ \u548c KL \u6563\u5ea6 $\\log q_{\\phi}(z|x) -\\log | p_{\\theta}(z|c)$ \u7684\u603b\u548c, \u5176\u4e2d $z\\sim q_{\\phi}(z|x)$.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#312reconstruction-loss","title":"3.1.2.Reconstruction Loss\u00b7\u91cd\u6784\u635f\u5931","text":"\u539f\u6587  &gt; As a target data point in the reconstruction loss, we use a mel-spectrogram instead of a raw waveform, denoted by $x_{mel}$.  &gt; We upsample the latent variables $z$ to the waveform domain $\\hat{y}$ through a decoder and transform $\\hat{y}$ to the mel-spectrogram domain $\\hat{x}_{mel}$.  &gt; Then the $L_1$ loss between the predicted and target mel-spectrogram is used as the reconstruction loss:   <p>\u91cd\u6784\u635f\u5931\u4e2d\u4f7f\u7528\u7684\u76ee\u6807\u6570\u636e\u70b9\u662f\u6885\u5c14\u9891\u8c31\u800c\u4e0d\u662f\u539f\u59cb\u6ce2\u5f62, \u8bb0\u4e3a $x_{mel}$. \u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u89e3\u7801\u5668\u5c06\u9690\u53d8\u91cf $z$ \u4e0a\u91c7\u6837\u5230\u6ce2\u5f62\u57df $\\hat{y}$, \u5e76\u5c06 $\\hat{y}$ \u8f6c\u6362\u5230\u6885\u5c14\u9891\u8c31\u57df $\\hat{x}_{mel}$. \u7136\u540e, \u6211\u4eec\u4f7f\u7528\u9884\u6d4b\u7684\u548c\u76ee\u6807\u7684\u6885\u5c14\u9891\u8c31\u4e4b\u95f4\u7684 $L_1$ \u635f\u5931\u4f5c\u4e3a\u91cd\u6784\u635f\u5931:</p> <p>$$   Loss_{recon}= |x_{mel}\u2212\\hat{x}_{mel}|_1 \\tag{2} $$</p> \u539f\u6587  &gt; This can be viewed as maximum likelihood estimation assuming a Laplace distribution for the data distribution and ignoring constant terms.    <p></p> <p>\u8fd9\u53ef\u4ee5\u89c6\u4e3a\u5047\u8bbe\u6570\u636e\u5206\u5e03\u4e3a Laplace \u5206\u5e03, \u5e76\u5ffd\u7565\u5e38\u6570\u9879\u540e\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1.</p> \u539f\u6587  &gt; We define the reconstruction loss in the mel-spectrogram domain to improve the perceptual quality by using a mel-scale that approximates the response of the human auditory system.  &gt; Note that the mel-spectrogram estimation from a raw waveform does not require trainable parameters as it only uses STFT and linear projection onto the mel-scale.  &gt; Furthermore, the estimation is only employed during training, not inference.  &gt; In practice, we do not upsample the whole latent variables $z$ but use partial sequences as an input for the decoder, which is the windowed generator training used for efficient end-to-end training ([FastSpeech 2s (2020)](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md); [EATS (2020)](../../Models/E2E/2020.06.05_EATS.md)).   <p></p> <p>\u6211\u4eec\u5728\u6885\u5c14\u9891\u8c31\u57df\u4e2d\u5b9a\u4e49\u4e86\u91cd\u6784\u635f\u5931, \u4ee5\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf, \u56e0\u4e3a\u5b83\u4f7f\u7528\u4e00\u4e2a\u8fd1\u4f3c\u4eba\u7c7b\u542c\u89c9\u7cfb\u7edf\u54cd\u5e94\u7684\u6885\u5c14\u523b\u5ea6. \u6ce8\u610f, \u4ece\u539f\u59cb\u6ce2\u5f62\u4f30\u8ba1\u6885\u5c14\u9891\u8c31\u4e0d\u9700\u8981\u53ef\u8bad\u7ec3\u53c2\u6570, \u56e0\u4e3a\u5b83\u53ea\u4f7f\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\u548c\u7ebf\u6027\u6295\u5f71\u5230\u6885\u5c14\u523b\u5ea6. \u6b64\u5916, \u4f30\u8ba1\u4ec5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528, \u800c\u63a8\u7406\u65f6\u4e0d\u4f7f\u7528. \u5b9e\u9645\u4e0a, \u6211\u4eec\u4e0d\u4e0a\u91c7\u6837\u6574\u4e2a\u9690\u53d8\u91cf $z$, \u800c\u662f\u4f7f\u7528\u90e8\u5206\u5e8f\u5217\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165, \u8fd9\u4e0e\u7528\u4e8e\u9ad8\u6548\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u7a97\u53e3\u751f\u6210\u5668\u8bad\u7ec3\u76f8\u540c.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#313kl-divergencekl","title":"3.1.3.KL-Divergence\u00b7KL \u6563\u5ea6","text":"\u539f\u6587  &gt; The input condition of the prior encoder $c$ is composed of phonemes $c_{text}$ extracted from text and an alignment $A$ between phonemes and latent variables.    <p>\u5148\u9a8c\u7f16\u7801\u5668\u7684\u8f93\u5165\u6761\u4ef6 $c$ \u7531\u6587\u672c\u4e2d\u63d0\u53d6\u7684\u97f3\u7d20 $c_{text}$, \u97f3\u7d20\u548c\u9690\u53d8\u91cf\u4e4b\u95f4\u7684\u5bf9\u9f50 $A$ \u7ec4\u6210.</p> \u539f\u6587  &gt; The alignment is a hard monotonic attention matrix with $|c_{text}|\\times|z|$ dimensions representing how long each input phoneme expands to be time-aligned with the target speech. &gt; Because there are no ground truth labels for the alignment, we must estimate the alignment at each training iteration, which we will discuss in Section 2.2.1.    <p></p> <p>\u8fd9\u4e2a\u5bf9\u9f50\u662f\u4e00\u4e2a\u7ef4\u5ea6\u4e3a $|c_{text}|\\times|z|$ \u786c\u6027\u5355\u8c03\u6ce8\u610f\u529b\u77e9\u9635, \u8868\u793a\u6bcf\u4e2a\u8f93\u5165\u97f3\u7d20\u6269\u5c55\u5230\u4e0e\u76ee\u6807\u8bed\u97f3\u65f6\u95f4\u5bf9\u9f50\u7684\u65f6\u95f4\u957f\u5ea6. \u56e0\u4e3a\u5bf9\u9f50\u6ca1\u6709\u771f\u5b9e\u6807\u7b7e, \u6211\u4eec\u5fc5\u987b\u5728\u6bcf\u4e00\u6b21\u8bad\u7ec3\u8fed\u4ee3\u4e2d\u4f30\u8ba1\u5bf9\u9f50, \u6211\u4eec\u5c06\u5728\u7b2c 3.2.1 \u8282\u4e2d\u8ba8\u8bba. </p> \u539f\u6587   &gt; In our problem setting, we aim to provide more high-resolution information for the posterior encoder. &gt; We, therefore, use the linear-scale spectrogram of target speech $x_{lin}$ as input rather than the mel-spectrogram.  &gt; Note that the modified input does not violate the properties of variational inference.    <p></p> <p>\u5728\u6211\u4eec\u7684\u95ee\u9898\u8bbe\u7f6e\u4e2d, \u6211\u4eec\u5e0c\u671b\u4e3a\u540e\u9a8c\u7f16\u7801\u5668\u63d0\u4f9b\u66f4\u591a\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f. \u56e0\u6b64, \u6211\u4eec\u4f7f\u7528\u76ee\u6807\u8bed\u97f3\u7684\u7ebf\u6027\u9891\u8c31 $x_{lin}$ \u4f5c\u4e3a\u8f93\u5165, \u800c\u4e0d\u662f\u6885\u5c14\u9891\u8c31. \u6ce8\u610f, \u6211\u4eec\u4fee\u6539\u7684\u8f93\u5165\u5e76\u4e0d\u8fdd\u53cd\u53d8\u5206\u63a8\u65ad\u7684\u6027\u8d28.</p> \u539f\u6587  &gt; The KL divergence is then:  $$   Loss_{kl} = \\log q_{\\phi}(z|x) -\\log p_{\\theta}(z|c_{text},A) \\tag{3} $$  $$   z\\sim q_{\\phi}(z|x) =\\mathcal{N}(z;\\mu_{\\phi}(x_{lin}), \\sigma_{\\phi}(x_{lin})) $$  &gt; The factorized normal distribution is used to parameterize our prior and posterior encoders.     <p></p> <p>KL \u6563\u5ea6\u4e3a:</p> <p>$$   Loss_{kl} = \\log q_{\\phi}(z|x) -\\log p_{\\theta}(z|c_{text},A) \\tag{3} $$</p> <p>$$   z\\sim q_{\\phi}(z|x) =\\mathcal{N}(z;\\mu_{\\phi}(x_{lin}), \\sigma_{\\phi}(x_{lin})) $$</p> <p>\u6211\u4eec\u4f7f\u7528\u56e0\u5b50\u6b63\u6001\u5206\u5e03\u6765\u53c2\u6570\u5316\u6211\u4eec\u7684\u5148\u9a8c\u548c\u540e\u9a8c\u7f16\u7801\u5668.</p> \u539f\u6587  &gt; We found that increasing the expressiveness of the prior distribution is important for generating realistic samples.  &gt; We, therefore, apply a normalizing flow $f_{\\theta}$ ([Variational inference with normalizing flows]()), which allows an invertible transformation of a simple distribution into a more complex distribution following the rule of change-of-variables, on top of the factorized normal prior distribution:   <p></p> <p>\u6211\u4eec\u53d1\u73b0\u589e\u52a0\u5148\u9a8c\u5206\u5e03\u7684\u8868\u73b0\u6027\u5bf9\u4e8e\u751f\u6210\u771f\u5b9e\u6837\u672c\u81f3\u5173\u91cd\u8981. \u56e0\u6b64, \u6211\u4eec\u5728\u56e0\u5b50\u6b63\u6001\u5148\u9a8c\u5206\u5e03\u4e0a\u5e94\u7528\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u6d41 $f_{\\theta}$, \u5b83\u80fd\u591f\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u5206\u5e03\u901a\u8fc7\u53d8\u91cf\u53d8\u6362\u89c4\u5219\u8f6c\u6362\u4e3a\u5230\u4e00\u4e2a\u66f4\u590d\u6742\u5206\u5e03\u7684\u53ef\u9006\u53d8\u6362:</p> <p>$$   p_{\\theta}(z|c) = \\mathcal{N}(f_{\\theta}(z);\\mu_{\\theta}(c),\\sigma_{\\theta}(c))|\\det\\dfrac{\\partial f_{\\theta}(z)}{\\partial z}| \\tag{4} $$</p> <p>\u5176\u4e2d $c=[c_{text}, A]$</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#32alignment-estimation","title":"3.2.Alignment Estimation\u00b7\u5bf9\u9f50\u4f30\u8ba1","text":""},{"location":"TTS/Models/E2E/2021.06.11_VITS/#321monotonic-alignment-search","title":"3.2.1.Monotonic Alignment Search\u00b7\u5355\u8c03\u5bf9\u9f50\u641c\u7d22","text":"\u539f\u6587  &gt; To estimate an alignment $A$ between input text and target speech, we adopt Monotonic Alignment Search (MAS), a method to search an alignment that maximizes the likelihood of data parameterized by a normalizing flow $f$:   <p>\u4e3a\u4e86\u4f30\u8ba1\u6587\u672c\u548c\u76ee\u6807\u8bed\u97f3\u4e4b\u95f4\u7684\u5bf9\u9f50 $A$, \u6211\u4eec\u91c7\u7528\u5355\u8c03\u5bf9\u9f50\u641c\u7d22 (Monotonic Alignment Search, MAS), \u8be5\u65b9\u6cd5\u641c\u7d22\u4e00\u4e2a\u5bf9\u9f50, \u4f7f\u5f97\u7531\u6807\u51c6\u5316\u6d41 $f$ \u53c2\u6570\u5316\u7684\u6570\u636e\u7684\u4f3c\u7136\u6700\u5927\u5316:</p> <p>$$ \\begin{aligned}   A &amp;= \\arg\\max_{\\hat{A}}\\log p_{\\theta}(x_{mel}|c_{text},\\hat{A}) \\   &amp;= \\arg\\max_{\\hat{A}}\\log\\mathcal{N}(f(x);\\mu(c_{text},\\hat{A}),\\sigma(c_{text},\\hat{A})) \\end{aligned}\\tag{5} $$</p> \u539f\u6587  &gt; where the candidate alignments are restricted to be monotonic and non-skipping following the fact that humans read text in order without skipping any words.   <p></p> <p>\u5176\u4e2d\u5019\u9009\u5bf9\u9f50\u88ab\u9650\u5236\u4e3a\u5355\u8c03\u4e14\u4e0d\u8df3\u8fc7\u4efb\u4f55\u5355\u8bcd, \u56e0\u4e3a\u4eba\u7c7b\u9605\u8bfb\u6587\u672c\u662f\u6709\u5e8f\u7684\u4e14\u4e0d\u4f1a\u8df3\u8fc7\u4efb\u4f55\u5355\u8bcd.</p> \u539f\u6587  &gt; To find the optimal alignment, [Glow-TTS (2020)](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) use dynamic programming.  &gt; Applying MAS directly in our setting is difficult because our objective is the ELBO, not the exact log-likelihood.  &gt; We, therefore, redefine MAS to find an alignment that maximizes the ELBO, which reduces to finding an alignment that maximizes the log-likelihood of the latent variables $z$:   <p></p> <p>\u4e3a\u4e86\u627e\u5230\u6700\u4f18\u5bf9\u9f50, Glow-TTS (2020) \u4f7f\u7528\u52a8\u6001\u89c4\u5212. \u5728\u6211\u4eec\u7684\u8bbe\u7f6e\u4e2d\u76f4\u63a5\u5e94\u7528 MAS \u56f0\u96be, \u56e0\u4e3a\u6211\u4eec\u7684\u76ee\u6807\u662f ELBO, \u800c\u4e0d\u662f\u7cbe\u786e\u7684\u5bf9\u6570\u4f3c\u7136. \u56e0\u6b64, \u6211\u4eec\u91cd\u65b0\u5b9a\u4e49 MAS, \u5bfb\u627e\u4e00\u4e2a\u5bf9\u9f50, \u4f7f\u5f97 ELBO \u6700\u5927\u5316, \u8fd9\u7b49\u4ef7\u4e8e\u5bfb\u627e\u4e00\u4e2a\u5bf9\u9f50, \u4f7f\u5f97\u9690\u53d8\u91cf $z$ \u7684\u5bf9\u6570\u4f3c\u7136\u6700\u5927\u5316:</p> <p>$$ \\begin{aligned}   &amp;\\arg\\max_{\\hat{A}}\\log p_{\\theta}(x_{mel}|z)-\\log\\frac{q_{\\phi}(z|x_{lin})}{p_{\\theta}(z|c_{text},\\hat{A})} \\   &amp;=\\arg\\max_{\\hat{A}}\\log p_{\\theta}(z|c_{text},\\hat{A}) \\   &amp;=\\arg\\max_{\\hat{A}}\\log \\mathcal{N}(f_\\theta(z);\\mu_\\theta(c_{text},\\hat{A}),\\sigma_\\theta(c_{text},\\hat{A}))  \\end{aligned} $$</p> \u539f\u6587  &gt; Due to the resemblance of Eq.05 to Eq.06, we can use the original MAS implementation without modification. &gt; Appendix A includes pseudocode for MAS. &gt; Although we search the alignment which maximizes the ELBO not the exact log-likelihood of data, we can use the MAS implementation of [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) as described in Section 2.2.1.   <p></p> <p>\u7531\u4e8e\u65b9\u7a0b 05 \u548c\u65b9\u7a0b 06 \u7684\u76f8\u4f3c\u6027, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u539f\u59cb\u7684 MAS \u5b9e\u73b0\u800c\u65e0\u9700\u4fee\u6539. \u9644\u5f55 A \u5305\u542b MAS \u7684\u4f2a\u4ee3\u7801. \u5c3d\u7ba1\u6211\u4eec\u641c\u7d22 ELBO \u6700\u5927\u7684\u5bf9\u9f50, \u800c\u4e0d\u662f\u6570\u636e\u7cbe\u786e\u7684\u5bf9\u6570\u4f3c\u7136, \u4f46\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 Glow-TTS \u7684 MAS \u5b9e\u73b0.</p> \u4ee3\u7801 <pre><code>def monotonic_alignment_search(value):\n    \"\"\"Returns the most likely alignment for the given log-likelihood matrix.\n    Args:\n        value: the log-likelihood matrix. Its (i, j)-th entry contains\n        the log-likelihood of the j-th latent variable\n        for the given i-th prior mean and variance:\n        .. math::\n            value_{i,j} = log N(f(z)_{j}; \\mu_{i}, \\sigma_{i})\n        (dtype=float, shape=[text_length, latent_variable_length])\n    Returns:\n        path: the most likely alignment.\n        (dtype=float, shape=[text_length, latent_variable_length])\n    \"\"\"\n    t_x, t_y = value.shape # [text_length, letent_variable_length]\n    path = zeros([t_x, t_y])\n    # A cache to store the log-likelihood for the most likely alignment so far.\n    Q = -INFINITY * ones([t_x, t_y])\n    for y in range(t_y):\n        for x in range(max(0, t_x + y - t_y), min(t_x, y + 1)):\n            if y == 0: # Base case. If y is 0, the possible x value is only 0.\n                Q[x, 0] = value[x, 0]\n            else:\n                if x == 0:\n                    v_prev = -INFINITY\n                else:\n                    v_prev = Q[x-1, y-1]\n                v_cur = Q[x, y-1]\n                Q[x, y] = value[x, y] + max(v_prev, v_cur)\n    # Backtrack from last observation.\n    index = t_x - 1\n    for y in range(t_y - 1, -1, -1):\n        path[index, y] = 1\n        if index != 0 and (index == y or Q[index, y-1] &lt; Q[index-1, y-1]):\n            index = index - 1\nreturn path\n</code></pre>  \u6e90\u4ee3\u7801\u91c7\u7528 Cython \u7f16\u5199: - \u5b9a\u4e49\u51fd\u6570 `maximum_path_each()`, \u63a5\u53d7\u4e00\u4e2a\u4e8c\u7ef4\u6570\u7ec4 `path`, \u4e8c\u7ef4\u6570\u7ec4 `value`, \u4e24\u4e2a\u6574\u6570 `t_x`, `t_y`, \u4ee5\u53ca\u4e00\u4e2a\u6d6e\u70b9\u6570 `max_neg_val`. \u7528\u4e8e\u8ba1\u7b97\u6bcf\u4e2a\u8def\u5f84\u7684\u6700\u5927\u503c. - \u5b9a\u4e49\u51fd\u6570 `maximum_path_c()`, \u63a5\u53d7\u4e09\u4e2a\u4e09\u7ef4\u6570\u7ec4 `paths`, `values` \u548c\u4e24\u4e2a\u4e00\u7ef4\u6570\u7ec4 `t_xs` \u548c `t_ys`, \u4ee5\u53ca\u4e00\u4e2a\u6d6e\u70b9\u6570 `max_neg_val`. \u7528\u4e8e\u8ba1\u7b97\u6240\u6709\u8def\u5f84\u7684\u6700\u5927\u503c.   <p></p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#322duration-prediction-from-text","title":"3.2.2.Duration Prediction from Text\u00b7\u6587\u672c\u4e2d\u7684\u65f6\u957f\u9884\u6d4b","text":"\u539f\u6587  &gt; We can calculate the duration of each input token $d_i$ by summing all the columns in each row of the estimated alignment $\\sum_j A_{i,j}$.  &gt; The duration could be used to train a deterministic duration predictor, as proposed in previous work ([HiFi-GAN (2020)](../TTS3_Vocoder/2020.10.12_HiFi-GAN.md)), but it cannot express the way a person utters at different speaking rates each time.  &gt; To generate human-like rhythms of speech, we design a stochastic duration predictor so that its samples follow the duration distribution of given phonemes.  &gt; The stochastic duration predictor is a flow-based generative model that is typically trained via maximum likelihood estimation. &gt; The direct application of maximum likelihood estimation, however, is difficult because the duration of each input phoneme is  &gt; 1. a discrete integer, which needs to be dequantized for using continuous normalizing flows, &gt; 2. a scalar, which prevents high-dimensional transformation due to invertibility.    <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5bf9\u4f30\u8ba1\u7684\u5bf9\u9f50\u7684\u6bcf\u4e00\u884c\u4e2d\u6240\u6709\u5217\u6c42\u548c\u4ee5\u8ba1\u7b97\u6bcf\u4e2a\u8f93\u5165 Token $d_i$ \u7684\u65f6\u957f, \u5373 $\\sum_j A_{i,j}$. \u65f6\u957f\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3\u4e00\u4e2a\u786e\u5b9a\u6027\u7684\u65f6\u957f\u9884\u6d4b\u5668, \u5982 HiFi-GAN (2020) \u6240\u63d0\u51fa\u7684, \u4f46\u5b83\u4e0d\u80fd\u8868\u8fbe\u4eba\u7c7b\u5728\u4e0d\u540c\u8bed\u901f\u4e0b\u8bf4\u8bdd\u7684\u65b9\u5f0f. \u4e3a\u4e86\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u8bed\u8c03\u7684\u8bed\u97f3, \u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u4f7f\u5f97\u5176\u91c7\u6837\u7b26\u5408\u7ed9\u5b9a\u97f3\u7d20\u7684\u65f6\u957f\u5206\u5e03. \u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b, \u901a\u5e38\u901a\u8fc7\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u8fdb\u884c\u8bad\u7ec3. \u7136\u800c, \u76f4\u63a5\u5e94\u7528\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u662f\u56f0\u96be\u7684, \u56e0\u4e3a\u6bcf\u4e2a\u8f93\u5165\u97f3\u7d20\u7684\u65f6\u957f\u662f: 1. \u4e00\u4e2a\u79bb\u6563\u6574\u6570, \u8fd9\u9700\u8981\u5bf9\u5176\u53bb\u91cf\u5316\u4ee5\u4fbf\u4f7f\u7528\u8fde\u7eed\u6807\u51c6\u5316\u6d41, 2. \u4e00\u4e2a\u6807\u91cf, \u7531\u4e8e\u53ef\u9006\u6027\u800c\u963b\u788d\u4e86\u9ad8\u7ef4\u53d8\u6362.</p> \u539f\u6587  &gt; We apply variational dequantization ([Flow++ (2019)](../_tmp/2019.02_Flowpp.md)) and variational data augmentation ([VFlow (2020)]()) to solve these problems.  &gt; To be specific, we introduce two random variables $u$ and $\u03bd$, which have the same time resolution and dimension as that of the duration sequenced, for variational dequantization and variational data augmentation, respectively.  &gt; We restrict the support of $u$ to be $[0, 1)$ so that the difference $d\u2212u$ becomes a sequence of positive real numbers, and we concatenate $\u03bd$ and $d$ channel-wise to make a higher dimensional latent representation.  &gt; We sample the two variables through an approximate posterior distribution $q_{\\phi}(u, \u03bd|d, c_{text})$.  &gt; The resulting objective is a variational lower bound of the log-likelihood of the phoneme duration:    <p></p> <p>\u6211\u4eec\u5e94\u7528\u53d8\u5206\u53bb\u91cf\u5316\u548c\u53d8\u5206\u6570\u636e\u589e\u5f3a\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898. \u5177\u4f53\u5730, \u6211\u4eec\u5f15\u5165\u4e24\u4e2a\u968f\u673a\u53d8\u91cf $u$ \u548c $v$, \u62e5\u6709\u548c\u65f6\u957f\u5e8f\u5217\u76f8\u540c\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7ef4\u5ea6, \u5206\u522b\u7528\u4e8e\u53d8\u5206\u53bb\u91cf\u5316\u548c\u53d8\u5206\u6570\u636e\u589e\u5f3a. \u6211\u4eec\u5c06 $u$ \u7684\u652f\u6301\u9650\u5236\u4e3a $[0, 1)$, \u8fd9\u6837 $d-u$ \u53d8\u6210\u4e00\u7cfb\u5217\u6b63\u5b9e\u6570, \u5e76\u5c06 $\u03bd$ \u548c $d$ \u9010\u901a\u9053\u62fc\u63a5\u4ee5\u751f\u6210\u66f4\u9ad8\u7ef4\u7684\u6f5c\u5728\u8868\u793a. \u6211\u4eec\u901a\u8fc7\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03 $q_{\\phi}(u, v|d, c_{text})$ \u91c7\u6837\u8fd9\u4e24\u4e2a\u53d8\u91cf. \u6700\u540e\u5f97\u5230\u7684\u76ee\u6807\u662f\u97f3\u7d20\u65f6\u957f\u7684\u5bf9\u6570\u4f3c\u7136\u7684\u53d8\u5206\u4e0b\u754c:</p> <p>$$   \\log p_{\\theta}(d|c_{text}) \\geq E_{q_{\\phi}(u, \u03bd|d, c_{text})} \\left[\\log\\dfrac{p_{\\theta}(d-u,v|c_{text})}{q_{\\phi}(u,v|d,c_{text})}\\right]\\tag{7} $$</p> \u539f\u6587  &gt; The training loss $Loss_{dur}$ is then the negative variational lower bound.  &gt; We apply the stop gradient operator ([VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md)), which prevents back-propagating the gradient of inputs, to the input conditions so that the training of the duration predictor does not affect that of other modules.   <p></p> <p>\u8bad\u7ec3\u635f\u5931 $Loss_{dur}$ \u5219\u662f\u8d1f\u53d8\u5206\u4e0b\u754c. \u6211\u4eec\u5e94\u7528\u5bf9\u8f93\u5165\u6761\u4ef6\u5e94\u7528\u505c\u6b62\u68af\u5ea6\u7b97\u5b50 (\u9632\u6b62\u68af\u5ea6\u53cd\u5411\u4f20\u64ad) \u4ee5\u4fbf\u8bad\u7ec3\u65f6\u957f\u9884\u6d4b\u5668\u4e0d\u5f71\u54cd\u5176\u4ed6\u6a21\u5757.</p> \u539f\u6587  &gt; The sampling procedure is relatively simple; the phoneme duration is sampled from random noise through the inverse transformation of the stochastic duration predictor, and then it is converted to integers.   <p></p> <p>\u91c7\u6837\u8fc7\u7a0b\u76f8\u5bf9\u7b80\u5355; \u97f3\u7d20\u65f6\u957f\u901a\u8fc7\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7684\u9006\u53d8\u6362\u4ece\u968f\u673a\u566a\u58f0\u4e2d\u91c7\u6837, \u7136\u540e\u5c06\u5176\u8f6c\u6362\u4e3a\u6574\u6570.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#33adversarial-training","title":"3.3.Adversarial Training\u00b7\u5bf9\u6297\u8bad\u7ec3","text":"\u539f\u6587  &gt; To adopt adversarial training in our learning system, we add a discriminator $D$ that distinguishes between the output generated by the decoder $G$ and the ground truth waveform $y$. &gt; In this work, we use two types of loss successfully applied in speech synthesis; the least-squares loss function ([LSGAN (2016)]()) for adversarial training, and the additional feature-matching loss ([Autoencoding beyond pixels using a learned similarity metric]()) for training the generator:   <p>\u4e3a\u4e86\u5728\u6211\u4eec\u7684\u5b66\u4e60\u7cfb\u7edf\u4e2d\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3, \u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e2a\u5224\u522b\u5668 $D$ \u6765\u533a\u5206\u7531\u89e3\u7801\u5668 $G$ \u751f\u6210\u7684\u8f93\u51fa\u548c\u771f\u5b9e\u6ce2\u5f62 $y$. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u6210\u529f\u5730\u5e94\u7528\u4e86\u4e24\u79cd\u635f\u5931, \u5373\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u6700\u5c0f\u4e8c\u4e58\u635f\u5931\u548c\u989d\u5916\u7684\u7279\u5f81\u5339\u914d\u635f\u5931\u7528\u4e8e\u8bad\u7ec3\u751f\u6210\u5668:</p> <p>$$ \\begin{align}   Loss_{adv}(D) &amp;= E_{(y,z)} [(D(y)-1)^2 + (D(G(z)))^2]\\tag{8}\\   Loss_{adv}(G) &amp;= E_{z} [(D(G(z)) - 1)^2]\\tag{9}\\   Loss_{fm}(G) &amp;= E_{(y,z)} \\left[\\sum_{l=1}^T \\dfrac{1}{N_l}| D^l(y)-D^l(G(z))|_1\\right]\\tag{10} \\end{align} $$</p> \u539f\u6587  &gt; where $T$ denotes the total number of layers in the discriminator and $D^l$ outputs the feature map of the $l$-th layer of the discriminator with $N_l$ number of features.   <p></p> <p>\u5176\u4e2d $T$ \u8868\u793a\u5224\u522b\u5668\u4e2d\u7684\u5c42\u6570, $D^l$ \u8f93\u51fa\u7b2c $l$ \u5c42\u5224\u522b\u5668\u7684\u7279\u5f81\u56fe, \u5176\u6709 $N_l$ \u4e2a\u7279\u5f81.</p> \u539f\u6587  &gt; Notably, the feature matching loss can be seen as reconstruction loss that is measured in the hidden layers of the discriminator suggested as an alternative to the element-wise reconstruction loss of VAEs ([Autoencoding beyond pixels using a learned similarity metric]()).   <p></p> <p>\u7279\u5f81\u5339\u914d\u635f\u5931\u53ef\u4ee5\u89c6\u4e3a\u91cd\u5efa\u635f\u5931, \u5b83\u5728\u5224\u522b\u5668\u7684\u9690\u85cf\u5c42\u4e0a\u8fdb\u884c\u5ea6\u91cf, \u88ab\u5efa\u8bae\u4f5c\u4e3a VAE \u7684\u9010\u5143\u7d20\u91cd\u5efa\u635f\u5931\u7684\u66ff\u4ee3.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#34final-loss","title":"3.4.Final Loss\u00b7\u6700\u7ec8\u635f\u5931","text":"\u539f\u6587  &gt; With the combination of VAE and GAN training, the total loss for training our conditional VAE can be expressed as follows:   <p>\u901a\u8fc7\u5bf9 VAE \u548c GAN \u8bad\u7ec3\u7684\u7ec4\u5408, \u6211\u4eec\u7684\u6761\u4ef6 VAE \u7684\u603b\u635f\u5931\u53ef\u4ee5\u8868\u793a\u5982\u4e0b:</p> <p>$$   Loss_{vae} = Loss_{recon} + Loss_{kl} + Loss_{dur} + Loss_{adv}(G) + Loss_{fm}(G) \\tag{11} $$</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#35model-architecture","title":"3.5.Model Architecture\u00b7\u6a21\u578b\u67b6\u6784","text":"\u539f\u6587  &gt; The overall architecture of the proposed model consists of a posterior encoder, prior encoder, decoder, discriminator, and stochastic duration predictor.  &gt; The posterior encoder and discriminator are only used for training, not for inference. &gt; Architectural details are available in Appendix B.   <p>\u6240\u63d0\u65b9\u6cd5\u7684\u6574\u4f53\u67b6\u6784\u7531\u540e\u9a8c\u7f16\u7801\u5668, \u5148\u9a8c\u7f16\u7801\u5668, \u89e3\u7801\u5668, \u5224\u522b\u5668\u548c\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7ec4\u6210. \u540e\u9a8c\u7f16\u7801\u5668\u548c\u5224\u522b\u5668\u4ec5\u7528\u4e8e\u8bad\u7ec3, \u800c\u4e0d\u7528\u4e8e\u63a8\u65ad.</p> \u539f\u6587  &gt; In this section, we mainly describe the newly added parts of ***VITS*** as we followed configurations of [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) and [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md) for several parts of our model:  &gt; we use the same transformer encoder and WaveNet residual blocks as those of [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md);  &gt; our decoder and the multi-period discriminator is the same as the generator and multi-period discriminator of [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md), respectively, except that we use different input dimension for the decoder and append a sub-discriminator.   <p></p> <p>\u672c\u8282\u4e3b\u8981\u63cf\u8ff0\u4e86\u6240\u63d0\u65b9\u6cd5\u4e2d\u65b0\u589e\u7684 VITS \u7684\u90e8\u5206, \u6211\u4eec\u9075\u5faa Glow-TTS \u548c HiFi-GAN \u7684\u914d\u7f6e, \u5e76\u5bf9\u6a21\u578b\u7684\u4e0d\u540c\u90e8\u5206\u8fdb\u884c\u4e86\u63cf\u8ff0: - \u6211\u4eec\u4f7f\u7528\u4e0e Glow-TTS \u76f8\u540c\u7684 Transformer \u7f16\u7801\u5668\u548c WaveNet \u6b8b\u5dee\u5757; - \u6211\u4eec\u4f7f\u7528\u4e0e HiFi-GAN \u76f8\u540c\u7684\u751f\u6210\u5668\u548c\u591a\u5468\u671f\u5224\u522b\u5668, \u9664\u4e86\u5bf9\u89e3\u7801\u5668\u4f7f\u7528\u4e0d\u540c\u7684\u8f93\u5165\u7ef4\u5ea6\u5e76\u6dfb\u52a0\u4e86\u4e00\u4e2a\u5b50\u5224\u522b\u5668.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#351posterior-encoder","title":"3.5.1.Posterior Encoder\u00b7\u540e\u9a8c\u7f16\u7801\u5668","text":"\u539f\u6587  &gt; For the posterior encoder, we use the non-causal WaveNet residual blocks used in [WaveGlow (2018)](../TTS3_Vocoder/2018.10.31_WaveGlow.md) and [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md). &gt; A WaveNet residual block consists of layers of dilated convolutions with a gated activation unit and skip connection.  &gt; The linear projection layer above the blocks produces the mean and variance of the normal posterior distribution.  &gt; For the multi-speaker case, we use global conditioning (Oord et al., 2016) in residual blocks to add speaker embedding.   <p>\u5bf9\u4e8e\u540e\u9a8c\u7f16\u7801\u5668, \u6211\u4eec\u91c7\u7528 WaveGlow \u548c Glow-TTS \u4e2d\u4f7f\u7528\u7684\u975e\u56e0\u679c\u7684 WaveNet \u6b8b\u5dee\u5757. WaveNet \u6b8b\u5dee\u5757\u7531\u5e26\u6709\u95e8\u63a7\u6fc0\u6d3b\u5355\u5143\u7684\u7a7a\u6d1e\u5377\u79ef\u5c42\u548c\u8df3\u8dc3\u8fde\u63a5\u7ec4\u6210. \u5757\u4e0a\u7684\u7ebf\u6027\u6620\u5c04\u5c42\u4ea7\u751f\u6b63\u6001\u540e\u9a8c\u5206\u5e03\u7684\u5747\u503c\u548c\u65b9\u5dee. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u60c5\u5f62, \u6211\u4eec\u5728\u6b8b\u5dee\u5757\u4e2d\u4f7f\u7528\u5168\u5c40\u6761\u4ef6\u5316\u4ee5\u6dfb\u52a0\u8bf4\u8bdd\u4eba\u5d4c\u5165.</p> \u539f\u6587  &gt; The posterior encoder, consisting of 16 WaveNet residual blocks, takes linear-scale log magnitude spectrograms and produce latent variables with 192 channels.   <p></p> <p>\u540e\u9a8c\u7f16\u7801\u5668\u7531 16 \u4e2a WaveNet \u6b8b\u5dee\u5757\u7ec4\u6210, \u5b83\u63a5\u53d7\u7ebf\u6027\u7684\u5bf9\u6570\u5e45\u5ea6\u8c31\u56fe\u5e76\u751f\u6210\u5177\u6709 192 \u901a\u9053\u7684\u9690\u53d8\u91cf.</p> \u4ee3\u7801 <pre><code>class PosteriorEncoder(nn.Module):\n    def __init__(self,\n        in_channels,\n        out_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        gin_channels=0):\n        super().__init__()\n        self.in_channels     = in_channels\n        self.out_channels    = out_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size     = kernel_size\n        self.dilation_rate   = dilation_rate\n        self.n_layers        = n_layers\n        self.gin_channels    = gin_channels\n\n        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n        self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\n        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, x, x_lengths, g=None):\n        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n        x = self.pre(x) * x_mask\n        x = self.enc(x, x_mask, g=g)\n        stats = self.proj(x) * x_mask\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n        return z, m, logs, x_mask\n</code></pre> <p></p> <p>\u9996\u5148\u5bf9\u8f93\u5165 x \u751f\u6210\u76f8\u5e94\u7684 x_mask \u4ee5\u53bb\u6389\u65e0\u6548\u90e8\u5206. \u4f7f\u7528\u4e00\u7ef4\u5377\u79ef\u5bf9\u8f93\u5165 x \u7684\u901a\u9053\u8fdb\u884c\u4fee\u6539. \u7136\u540e\u5c06\u4fee\u6539\u540e\u7684 x \u548c\u8bf4\u8bdd\u4eba\u5d4c\u5165\u4e00\u540c\u8f93\u5165\u5230 WaveNet \u4e2d. \u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u7ebf\u6027\u6620\u5c04\u8c03\u6574\u901a\u9053\u6570\u4e3a\u9690\u85cf\u7ef4\u5ea6\u7684\u4e24\u500d, \u5207\u5206\u540e\u5f97\u5230\u5747\u503c\u548c\u5bf9\u6570\u6807\u51c6\u5dee. \u7136\u540e\u91c7\u6837\u9690\u53d8\u91cf z \u5e76\u8fd4\u56de.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#352prior-encoder","title":"3.5.2.Prior Encoder\u00b7\u5148\u9a8c\u7f16\u7801\u5668","text":"\u539f\u6587  &gt; The prior encoder consists of a text encoder that processes the input phonemes $c_{text}$ and a normalizing flow $f_{\\theta}$ that improves the flexibility of the prior distribution. &gt; The text encoder is a [transformer](../_Basis/2017.06.12_Transformer.md) encoder  that uses [relative positional representation](Self-Attention with Relative Position Representations) instead of absolute positional encoding. &gt; We can obtain the hidden representation $h_{text}$ from $c_{text}$ through the text encoder and a linear projection layer above the text encoder that produces the mean and variance used for constructing the prior distribution.   <p>\u5148\u9a8c\u7f16\u7801\u5668\u7531\u4e00\u4e2a\u7528\u4e8e\u5904\u7406\u8f93\u5165\u97f3\u7d20\u7684\u6587\u672c\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u5148\u9a8c\u5206\u5e03\u7075\u6d3b\u6027\u7684\u6807\u51c6\u5316\u6d41\u7ec4\u6210. \u6587\u672c\u7f16\u7801\u5668\u662f\u4e00\u4e2a Transformer \u7f16\u7801\u5668, \u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u8868\u793a\u800c\u4e0d\u662f\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801. \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u7f16\u7801\u5668\u548c\u5176\u4e0a\u7684\u7ebf\u6027\u6620\u5c04\u5c42\u6765\u83b7\u5f97\u9690\u85cf\u8868\u793a $h_{text}$ \u5e76\u7528\u4e8e\u6784\u9020\u5148\u9a8c\u5206\u5e03\u7684\u5747\u503c\u548c\u65b9\u5dee.</p> \u4ee3\u7801 <pre><code>class TextEncoder(nn.Module):\n    def __init__(self,\n        n_vocab,\n        out_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout):\n        super().__init__()\n        self.n_vocab = n_vocab\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n\n        self.emb = nn.Embedding(n_vocab, hidden_channels)\n        nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n\n        self.encoder = attentions.Encoder(\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout)\n        self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, x, x_lengths):\n        x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\n        x = torch.transpose(x, 1, -1) # [b, h, t]\n        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n\n        x = self.encoder(x * x_mask, x_mask)\n        stats = self.proj(x) * x_mask\n\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        return x, m, logs, x_mask\n</code></pre> <p></p> <p>\u6587\u672c\u7f16\u7801\u5668\u5bf9\u8f93\u5165 x \u8fdb\u884c\u5d4c\u5165\u67e5\u627e\u540e, \u4e58\u4ee5\u6839\u53f7\u4e0b\u9690\u85cf\u901a\u9053\u6570, \u7136\u540e\u8f6c\u7f6e. \u6784\u9020 mask \u5bf9\u8f93\u5165 x \u8fdb\u884c\u65e0\u6548\u4f4d\u7f6e\u63a9\u819c. \u8f93\u5165\u5230 Transformer \u7f16\u7801\u5668\u4e2d, \u901a\u8fc7\u4e00\u7ef4\u5377\u79ef\u5c42\u8c03\u6574\u901a\u9053\u6570\u4e3a\u9690\u85cf\u7ef4\u5ea6\u7684\u4e24\u500d, \u5207\u5206\u540e\u5f97\u5230\u5747\u503c\u548c\u5bf9\u6570\u6807\u51c6\u5dee.</p> \u539f\u6587  &gt; The normalizing flow is a stack of affine coupling layers ([Density estimation using Real NVP]()) consisting of a stack of WaveNet residual blocks. &gt; For simplicity, we design the normalizing flow to be a volume-preserving transformation with the Jacobian determinant of one. &gt; For the multi-speaker setting, we add speaker embedding to the residual blocks in the normalizing flow through global conditioning.  &gt; The normalizing flow in the prior encoder is a stack of four affine coupling layers, each coupling layer consisting of four WaveNet residual blocks.  &gt; As we restrict the affine coupling layers to be volume-preserving transformations, the coupling layers do not produce scale parameters.   <p></p> <p>\u6807\u51c6\u5316\u6d41\u662f\u7531\u4eff\u5c04\u8026\u5408\u5c42\u5806\u53e0\u800c\u6210, \u6bcf\u4e2a\u4eff\u5c04\u8026\u5408\u5c42\u7531 WaveNet \u6b8b\u5dee\u5757\u5806\u53e0\u800c\u6210. \u4e3a\u4e86\u7b80\u5316, \u6211\u4eec\u8bbe\u8ba1\u6807\u51c6\u5316\u6d41\u4e3a\u5177\u6709\u5355\u4f4d\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u7684\u4f53\u79ef\u4fdd\u6301\u53d8\u6362. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u8bbe\u7f6e, \u6211\u4eec\u901a\u8fc7\u5168\u5c40\u6761\u4ef6\u5316\u5728\u6807\u51c6\u5316\u6d41\u7684\u6b8b\u5dee\u5757\u4e2d\u6dfb\u52a0\u8bf4\u8bdd\u4eba\u5d4c\u5165.</p> <p>\u5177\u4f53\u5730, \u5148\u9a8c\u7f16\u7801\u5668\u4e2d\u7684\u6807\u51c6\u5316\u6d41\u7531\u56db\u4e2a\u4eff\u5c04\u8026\u5408\u5c42\u7ec4\u6210, \u6bcf\u4e2a\u4eff\u5c04\u8026\u5408\u5c42\u7531\u56db\u4e2a WaveNet \u6b8b\u5dee\u5757\u7ec4\u6210. \u7531\u4e8e\u6211\u4eec\u9650\u5236\u4eff\u5c04\u8026\u5408\u5c42\u4e3a\u4f53\u79ef\u4fdd\u6301\u53d8\u6362, \u56e0\u6b64\u8026\u5408\u5c42\u4e0d\u4ea7\u751f\u5c3a\u5ea6\u53c2\u6570.</p> \u4ee3\u7801 <pre><code>class ResidualCouplingBlock(nn.Module):\n    def __init__(self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        n_flows=4,\n        gin_channels=0):\n        super().__init__()\n        self.channels = channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.n_flows = n_flows\n        self.gin_channels = gin_channels\n\n        self.flows = nn.ModuleList()\n        for i in range(n_flows):\n            self.flows.append(modules.ResidualCouplingLayer(channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels, mean_only=True))\n            self.flows.append(modules.Flip())\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        if not reverse:\n            for flow in self.flows:\n                x, _ = flow(x, x_mask, g=g, reverse=reverse)\n        else:\n            for flow in reversed(self.flows):\n                x = flow(x, x_mask, g=g, reverse=reverse)\n        return x\n</code></pre> <p></p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#353decoder","title":"3.5.3.Decoder\u00b7\u89e3\u7801\u5668","text":"\u539f\u6587  &gt; The decoder is essentially the [HiFi-GAN (2020)](../TTS3_Vocoder/2020.10.12_HiFi-GAN.md) V1 generator. &gt; It is composed of a stack of transposed convolutions, each of which is followed by a multi-receptive field fusion module (MRF). &gt; The output of the MRF is the sum of the output of residual blocks that have different receptive field sizes. &gt; For the multi-speaker setting, we add a linear layer that transforms speaker embedding and add it to the input latent variables $z$.  &gt; The input of our decoder is latent variables generated from the prior or posterior encoders, so the input channel size of the decoder is 192.  &gt; For the last convolutional layer of the decoder, we remove a bias parameter, as it causes unstable gradient scales during mixed precision training.   <p>\u89e3\u7801\u5668\u4e3a HiFi-GAN V1 \u751f\u6210\u5668\u7684\u57fa\u672c\u7ed3\u6784. \u5b83\u7531\u8f6c\u7f6e\u5377\u79ef\u5c42\u548c\u591a\u611f\u53d7\u91ce\u878d\u5408\u6a21\u5757 (MRF) \u5806\u53e0\u800c\u6210. \u6bcf\u4e2a MRF \u7684\u8f93\u51fa\u662f\u4e0d\u540c\u611f\u53d7\u91ce\u5927\u5c0f\u7684\u6b8b\u5dee\u5757\u8f93\u51fa\u7684\u603b\u548c. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u8bbe\u7f6e, \u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e2a\u7ebf\u6027\u5c42, \u7528\u4e8e\u8f6c\u6362\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u5165\u9690\u53d8\u91cf $z$ \u4e2d.</p> <p>\u89e3\u7801\u5668\u7684\u8f93\u5165\u662f\u4ece\u5148\u9a8c\u6216\u540e\u9a8c\u7f16\u7801\u5668\u751f\u6210\u7684\u56e0\u53d8\u91cf\u6240\u4ee5\u7f16\u7801\u5668\u7684\u8f93\u5165\u901a\u9053\u6570\u4e3a 192. \u5bf9\u4e8e\u89e3\u7801\u5668\u7684\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42, \u6211\u4eec\u79fb\u9664\u504f\u7f6e\u53c2\u6570, \u56e0\u4e3a\u5b83\u4f1a\u5bfc\u81f4\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u65f6\u7684\u4e0d\u7a33\u5b9a\u68af\u5ea6\u5c3a\u5ea6.</p> \u4ee3\u7801 <pre><code>class Generator(torch.nn.Module):\n    def __init__(self, initial_channel, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):\n        super(Generator, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\n        resblock = modules.ResBlock1 if resblock == '1' else modules.ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(weight_norm(\n                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n                                k, u, padding=(k-u)//2)))\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel//(2**(i+1))\n            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n    def forward(self, x, g=None):\n        x = self.conv_pre(x)\n        if g is not None:\n          x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i*self.num_kernels+j](x)\n                else:\n                    xs += self.resblocks[i*self.num_kernels+j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n</code></pre> <p></p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#354discriminator","title":"3.5.4.Discriminator\u00b7\u5224\u522b\u5668","text":"\u539f\u6587  &gt; We follow the discriminator architecture of the multi-period discriminator proposed in [HiFi-GAN (2020)](../TTS3_Vocoder/2020.10.12_HiFi-GAN.md). &gt; The multi-period discriminator is a mixture of Markovian window-based sub-discriminators ([MelGAN (2019)](../TTS3_Vocoder/2019.10.08_MelGAN.md)), each of which operates on different periodic patterns of input waveforms.  &gt; For the discriminator, [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md) uses the multi-period discriminator containing five sub-discriminators with periods [2, 3, 5, 7, 11] and the multi-scale discriminator containing three sub-discriminators.  &gt; To improve training efficiency, we leave only the first sub-discriminator of the multi-scale discriminator that operates on raw waveforms and discard two sub-discriminators operating on average-pooled waveforms.  &gt; The resultant discriminator can be seen as the multi-period discriminator with periods [1, 2, 3, 5, 7, 11].   <p>\u6211\u4eec\u9075\u5faa HiFi-GAN \u63d0\u51fa\u7684\u591a\u5468\u671f\u5224\u522b\u5668\u7684\u67b6\u6784. \u591a\u5468\u671f\u5224\u522b\u5668\u662f\u4e00\u4e2a\u7531\u9a6c\u5c14\u53ef\u592b\u7a97\u53e3\u5b50\u5224\u522b\u5668\u7ec4\u6210\u7684\u6df7\u5408\u6a21\u578b, \u5b83\u4eec\u5206\u522b\u64cd\u4f5c\u4e8e\u8f93\u5165\u6ce2\u5f62\u7684\u4e0d\u540c\u5468\u671f\u6a21\u5f0f.</p> <p>\u5bf9\u4e8e\u5224\u522b\u5668, HiFi-GAN \u4f7f\u7528\u5305\u542b\u4e94\u4e2a\u5b50\u5224\u522b\u5668\u7684\u591a\u5468\u671f\u5224\u522b\u5668\u548c\u5305\u542b\u4e09\u4e2a\u5b50\u5224\u522b\u5668\u7684\u591a\u5c3a\u5ea6\u5224\u522b\u5668. \u4e3a\u4e86\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387, \u6211\u4eec\u4ec5\u4fdd\u7559\u591a\u5c3a\u5ea6\u5224\u522b\u5668\u7684\u7b2c\u4e00\u4e2a\u5b50\u5224\u522b\u5668, \u5e76\u4e22\u5f03\u4e24\u4e2a\u64cd\u4f5c\u4e8e\u5e73\u5747\u6c60\u5316\u6ce2\u5f62\u7684\u5b50\u5224\u522b\u5668. \u6700\u7ec8\u7684\u5224\u522b\u5668\u53ef\u4ee5\u770b\u4f5c\u662f\u5177\u6709\u5468\u671f [1, 2, 3, 5, 7, 11] \u7684\u591a\u5468\u671f\u5224\u522b\u5668.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#355stochastic-duration-predictor","title":"3.5.5.Stochastic Duration Predictor\u00b7\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668","text":"\u539f\u6587  &gt; The stochastic duration predictor estimates the distribution of phoneme duration from a conditional input $h_{text}$. &gt; For the efficient parameterization of the stochastic duration predictor, we stack residual blocks with dilated and depth-separable convolutional layers. &gt; We also apply [Neural Spline Flows](), which take the form of invertible nonlinear transformations by using monotonic rational-quadratic splines, to coupling layers. &gt; Neural spline flows improve transformation expressiveness with a similar number of parameters compared to commonly used affine coupling layers. &gt; For the multi-speaker setting, we add a linear layer that transforms speaker embedding and add it to the input $h_{text}$.   <p>\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u4ece\u6761\u4ef6\u8f93\u5165 $h_{text}$ \u4f30\u8ba1\u97f3\u7d20\u65f6\u957f\u7684\u5206\u5e03. \u4e3a\u4e86\u6709\u6548\u5730\u53c2\u6570\u5316\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u6211\u4eec\u5806\u53e0\u5177\u6709\u81a8\u80c0\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5c42\u7684\u6b8b\u5dee\u5757. \u6211\u4eec\u8fd8\u5e94\u7528\u4e86\u795e\u7ecf\u6837\u6761\u6d41, \u5b83\u662f\u4e00\u79cd\u4f7f\u7528\u5355\u8c03\u5206\u6bb5\u4e8c\u6b21\u6837\u6761\u7684\u975e\u7ebf\u6027\u53d8\u6362, \u7528\u4e8e\u8026\u5408\u5c42. \u795e\u7ecf\u6837\u6761\u6d41\u7528\u548c\u5e38\u7528\u7684\u4eff\u5c04\u8026\u5408\u5c42\u53c2\u6570\u6570\u91cf\u76f8\u5f53\u7684\u53c2\u6570\u6765\u63d0\u9ad8\u53d8\u6362\u7684\u8868\u8fbe\u80fd\u529b. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u8bbe\u7f6e, \u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e2a\u7ebf\u6027\u5c42, \u7528\u4e8e\u8f6c\u6362\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u5165 $h_{text}$.</p> \u539f\u6587  &gt; Fig.05a and Fig.05b show the training and inference procedures of the stochastic duration predictor, respectively.  &gt; The main building block of the stochastic duration predictor is the dilated and depth-wise separable convolutional (DDSConv) residual block as in Fig.05c.  &gt; Each convolutional layer in DDSConv blocks is followed by a layer normalization layer and GELU activation function.  &gt; We choose to use dilated and depth-wise separable convolutional layers for improving parameter efficiency while maintaining large receptive field size.   <p></p> <p></p> <p>\u56fe 05a \u548c\u56fe 05b \u5206\u522b\u5c55\u793a\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b. \u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7684\u4e3b\u8981\u6784\u5efa\u5757\u662f\u5177\u6709\u81a8\u80c0\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7684\u6b8b\u5dee\u5757 (DDSConv), \u5982\u56fe 05c \u6240\u793a. DDSConv \u4e2d\u7684\u6bcf\u4e2a\u5377\u79ef\u5c42\u540e\u8ddf\u968f\u4e00\u4e2a\u5c42\u5f52\u4e00\u5316\u5c42\u548c GELU \u6fc0\u6d3b\u51fd\u6570. \u6211\u4eec\u9009\u62e9\u4f7f\u7528\u81a8\u80c0\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5c42\u6765\u63d0\u9ad8\u53c2\u6570\u6548\u7387, \u540c\u65f6\u4fdd\u6301\u5927\u7684\u611f\u53d7\u91ce\u5c3a\u5bf8.</p> \u539f\u6587  &gt; The posterior encoder and normalizing flow module in the duration predictor are flow-based neural networks and have the similar architecture.  &gt; The difference is that the posterior encoder transforms a Gaussian noise sequence into two random variables $\u03bd$ and $u$ to express the approximate posterior distribution $q_{\\phi}(u, \u03bd|d, c_{text})$, and the normalizing flow module transforms $d\u2212u$ and $\u03bd$ into a Gaussian noise sequence to express the log-likelihood of the augmented and dequantized data $\\log p_{\\theta}(d \u2212 u, \u03bd|c_{text})$ as described in Section 2.2.2.   <p></p> <p>\u65f6\u957f\u9884\u6d4b\u5668\u4e2d\u7684\u540e\u9a8c\u7f16\u7801\u5668\u548c\u6807\u51c6\u5316\u6d41\u6a21\u5757\u90fd\u662f\u57fa\u4e8e\u6d41\u7684\u795e\u7ecf\u7f51\u7edc, \u4e14\u5177\u6709\u76f8\u4f3c\u67b6\u6784. \u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u540e\u9a8c\u7f16\u7801\u5668\u5c06\u9ad8\u65af\u566a\u58f0\u5e8f\u5217\u8f6c\u6362\u4e3a\u4e24\u4e2a\u968f\u673a\u53d8\u91cf $\u03bd$ \u548c $u$, \u7528\u4e8e\u8868\u793a\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03 $q_{\\phi}(u, \u03bd|d, c_{text})$, \u800c\u6807\u51c6\u5316\u6d41\u6a21\u5757\u5c06 $d\u2212u$ \u548c $\u03bd$ \u8f6c\u6362\u4e3a\u9ad8\u65af\u566a\u58f0\u5e8f\u5217, \u7528\u4e8e\u8868\u793a\u589e\u5f3a\u548c\u53bb\u91cf\u5316\u6570\u636e\u7684\u5bf9\u6570\u4f3c\u7136 $\\log p_{\\theta}(d \u2212 u, \u03bd|c_{text})$, \u5982\u7b2c 2.2.2 \u8282\u6240\u8ff0.</p> \u539f\u6587  &gt; All input conditions are processed through condition encoders, each consisting of two 1x1 convolutional layers and a DDSConv residual block.  &gt; The posterior encoder and normalizing flow module have four coupling layers of neural spline flows.  &gt; Each coupling layer first processes input and input conditions through a DDSConv block and produces 29-channel parameters that are used to construct 10 rational-quadratic functions.  &gt; We set the hidden dimension of all coupling layers and condition encoders to 192.  &gt; Fig.06a and Fig.6b show the architecture of a condition encoder and a coupling layer used in the stochastic duration predictor.   <p></p> <p></p> <p>\u6240\u6709\u7684\u8f93\u5165\u6761\u4ef6\u901a\u8fc7\u6761\u4ef6\u7f16\u7801\u5668\u5904\u7406, \u6bcf\u4e00\u4e2a\u7531\u4e24\u4e2a 1x1 \u5377\u79ef\u5c42\u548c DDSConv \u6b8b\u5dee\u5757\u7ec4\u6210. \u540e\u9a8c\u7f16\u7801\u5668\u548c\u6807\u51c6\u5316\u6d41\u6a21\u5757\u90fd\u6709\u56db\u4e2a\u8026\u5408\u5c42\u7684\u795e\u7ecf\u6837\u6761\u6d41. \u6bcf\u4e2a\u8026\u5408\u5c42\u9996\u5148\u901a\u8fc7 DDSConv \u5757\u5904\u7406\u8f93\u5165\u548c\u8f93\u5165\u6761\u4ef6, \u5e76\u4ea7\u751f 29 \u901a\u9053\u7684\u53c2\u6570, \u7528\u4e8e\u6784\u9020 10 \u4e2a\u5206\u6bb5\u4e8c\u6b21\u51fd\u6570. \u6211\u4eec\u5c06\u6240\u6709\u8026\u5408\u5c42\u548c\u6761\u4ef6\u7f16\u7801\u5668\u7684\u9690\u85cf\u7ef4\u5ea6\u8bbe\u7f6e\u4e3a 192. \u56fe 06a \u548c\u56fe 06b \u5c55\u793a\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u4e2d\u6761\u4ef6\u7f16\u7801\u5668\u548c\u8026\u5408\u5c42\u7684\u67b6\u6784.</p> \u4ee3\u7801 <pre><code>class StochasticDurationPredictor(nn.Module):\n        def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\n        super().__init__()\n        filter_channels = in_channels # it needs to be removed from future version.\n        self.in_channels = in_channels\n        self.filter_channels = filter_channels\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.n_flows = n_flows\n        self.gin_channels = gin_channels\n\n        self.log_flow = modules.Log()\n        self.flows = nn.ModuleList()\n        self.flows.append(modules.ElementwiseAffine(2))\n        for i in range(n_flows):\n            self.flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n            self.flows.append(modules.Flip())\n\n        self.post_pre = nn.Conv1d(1, filter_channels, 1)\n        self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\n        self.post_convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n        self.post_flows = nn.ModuleList()\n        self.post_flows.append(modules.ElementwiseAffine(2))\n        for i in range(4):\n            self.post_flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n            self.post_flows.append(modules.Flip())\n\n        self.pre = nn.Conv1d(in_channels, filter_channels, 1)\n        self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\n        self.convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\n\n        def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\n        x = torch.detach(x)\n        x = self.pre(x)\n        if g is not None:\n            g = torch.detach(g)\n            x = x + self.cond(g)\n        x = self.convs(x, x_mask)\n        x = self.proj(x) * x_mask\n\n        if not reverse:\n            flows = self.flows\n            assert w is not None\n\n            logdet_tot_q = 0 \n            h_w = self.post_pre(w)\n            h_w = self.post_convs(h_w, x_mask)\n            h_w = self.post_proj(h_w) * x_mask\n            e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n</code></pre> <p></p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#4experiments","title":"4.Experiments\u00b7\u5b9e\u9a8c","text":""},{"location":"TTS/Models/E2E/2021.06.11_VITS/#41datasets","title":"4.1.Datasets\u00b7\u6570\u636e\u96c6","text":"\u539f\u6587  &gt; We conducted experiments on two different datasets. &gt; We used the [LJ Speech dataset](../../Datasets/LJSpeech.md) for comparison with other publicly available models and the [VCTK dataset](../../Datasets/VCTK.md) to verify whether our model can learn and express diverse speech characteristics. &gt; The [LJ Speech dataset](../../Datasets/LJSpeech.md) consists of 13,100 short audio clips of a single speaker with a total length of approximately 24 hours. &gt; The audio format is 16-bit PCM with a sample rate of 22 kHz, and we used it without any manipulation. &gt; We randomly split the dataset into a training set (12,500 samples), validation set (100 samples), and test set (500 samples). &gt; The [VCTK dataset](../../Datasets/VCTK.md) consists of approximately 44,000 short audio clips uttered by 109 native English speakers with various accents. &gt; The total length of the audio clips is approximately 44 hours. &gt; The audio format is 16-bit PCM with a sample rate of 44 kHz. &gt; We reduced the sample rate to 22 kHz. &gt; We randomly split the dataset into a training set (43,470 samples), validation set (100 samples), and test set (500 samples).   <p>\u6211\u4eec\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c. \u6211\u4eec\u4f7f\u7528 LJ Speech \u6570\u636e\u96c6 \u7528\u4e8e\u548c\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4, \u7136\u540e\u7528 VCTK \u6570\u636e\u96c6 \u9a8c\u8bc1\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u5b66\u4e60\u548c\u8868\u8fbe\u591a\u6837\u5316\u7684\u8bed\u97f3\u7279\u5f81.</p> <p>LJ Speech \u6570\u636e\u96c6 \u7531\u5355\u4e2a\u8bf4\u8bdd\u4eba\u7684 13,100 \u77ed\u97f3\u9891\u7247\u6bb5\u7ec4\u6210, \u603b\u65f6\u957f\u7ea6\u4e3a 24 \u5c0f\u65f6. \u97f3\u9891\u683c\u5f0f\u4e3a 16 \u4f4d PCM, \u91c7\u6837\u7387\u4e3a 22 kHz, \u6211\u4eec\u6ca1\u6709\u5bf9\u5176\u8fdb\u884c\u4efb\u4f55\u5904\u7406. \u6211\u4eec\u968f\u673a\u5c06\u6570\u636e\u96c6\u5206\u4e3a\u8bad\u7ec3\u96c6 (12,500 \u4e2a\u6837\u672c), \u9a8c\u8bc1\u96c6 (100 \u4e2a\u6837\u672c), \u548c\u6d4b\u8bd5\u96c6 (500 \u4e2a\u6837\u672c).</p> <p>VCTK \u6570\u636e\u96c6 \u7531 109 \u540d\u82f1\u8bed\u6bcd\u8bed\u53d1\u8a00\u4eba\u53d1\u51fa\u7684\u7ea6 44,000 \u77ed\u97f3\u9891\u7247\u6bb5\u7ec4\u6210, \u603b\u65f6\u957f\u7ea6\u4e3a 44 \u5c0f\u65f6. \u97f3\u9891\u683c\u5f0f\u4e3a 16 \u4f4d PCM, \u91c7\u6837\u7387\u4e3a 44 kHz. \u6211\u4eec\u5c06\u91c7\u6837\u7387\u964d\u4f4e\u5230 22 kHz. \u6211\u4eec\u968f\u673a\u5c06\u6570\u636e\u96c6\u5206\u4e3a\u8bad\u7ec3\u96c6 (43,470 \u4e2a\u6837\u672c), \u9a8c\u8bc1\u96c6 (100 \u4e2a\u6837\u672c), \u548c\u6d4b\u8bd5\u96c6 (500 \u4e2a\u6837\u672c).</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#42preprocessing","title":"4.2.Preprocessing\u00b7\u9884\u5904\u7406","text":"\u539f\u6587  &gt; We use linear spectrograms which can be obtained from raw waveforms through the Short-time Fourier transform (STFT), as input of the posterior encoder. &gt; The FFT size, window size and hop size of the transform are set to 1024, 1024 and 256, respectively. &gt; We use 80 bands mel-scale spectrograms for reconstruction loss, which is obtained by applying a mel-filter bank to linear spectrograms. &gt; We use International Phonetic Alphabet (IPA) sequences as input to the prior encoder. &gt; We convert text sequences to IPA phoneme sequences using open-source software (Bernard, 2021), and the converted sequences are interspersed with a blank token following the implementation of [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md).   <p>\u6211\u4eec\u4f7f\u7528\u7ebf\u6027\u9891\u8c31\u56fe, \u5b83\u53ef\u4ee5\u4ece\u539f\u59cb\u6ce2\u5f62\u901a\u8fc7\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362 (STFT) \u83b7\u5f97. FFT \u5927\u5c0f, \u7a97\u53e3\u5927\u5c0f\u548c\u8df3\u8dc3\u5927\u5c0f\u5206\u522b\u8bbe\u7f6e\u4e3a 1024, 1024 \u548c 256. \u6211\u4eec\u4f7f\u7528 80 bands \u6885\u5c14\u9891\u8c31\u56fe\u7528\u4e8e\u8ba1\u7b97\u91cd\u6784\u635f\u5931, \u5b83\u662f\u901a\u8fc7\u5bf9\u7ebf\u6027\u9891\u8c31\u56fe\u5e94\u7528\u6885\u5c14\u6ee4\u6ce2\u5668\u83b7\u5f97\u7684. \u6211\u4eec\u4f7f\u7528 IPA \u5e8f\u5217\u4f5c\u4e3a\u5148\u9a8c\u7f16\u7801\u5668\u7684\u8f93\u5165. \u6211\u4eec\u4f7f\u7528\u5f00\u6e90\u8f6f\u4ef6\u5c06\u6587\u672c\u5e8f\u5217\u8f6c\u6362\u4e3a IPA \u97f3\u7d20\u5e8f\u5217, \u5e76\u9075\u5faa Glow-TTS \u7684\u5b9e\u73b0, \u5728\u97f3\u7d20\u5e8f\u5217\u4e4b\u95f4\u63d2\u5165\u7a7a\u767d Token.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#43training","title":"4.3.Training\u00b7\u8bad\u7ec3","text":"\u539f\u6587  &gt; The networks are trained using the [AdamW optimizer](../_Basis/2017.11.14_AdamW.md) with $\\beta_1= 0.8$, $\\beta_2=0.99$ and weight decay $\\lambda = 0.01$. &gt; The learning rate decay is scheduled by a 0.9991/8 factor in every epoch with an initial learning rate of $2\\times 10^{\u22124}$. &gt; Following previous work ([FastSpeech 2s (2020)](../TTS2_Acoustic/2020.06.08_FastSpeech2.md); [EATS (2020)]()), we adopt the windowed generator training, a method of generating only a part of raw waveforms to reduce the training time and memory usage during training. &gt; We randomly extract segments of latent representations with a window size of 32 to feed to the decoder instead of feeding entire latent representations and also extract the corresponding audio segments from the ground truth raw waveforms as training targets. &gt; We use mixed precision training on 4 NVIDIA V100 GPUs. &gt; The batch size is set to 64 per GPU and the model is trained up to 800k steps.   <p>\u7f51\u7edc\u91c7\u7528 AdamW \u4f18\u5316\u5668 \u8fdb\u884c\u8bad\u7ec3, \u5176\u53c2\u6570\u4e3a $\\beta_1= 0.8$, $\\beta_2=0.99$ \u548c\u6743\u91cd\u8870\u51cf $\\lambda = 0.01$. \u5b66\u4e60\u7387\u8870\u51cf\u662f\u6bcf\u4e2a Epoch \u6309 0.9991/8 \u56e0\u5b50\u8fdb\u884c\u8c03\u5ea6, \u521d\u59cb\u5b66\u4e60\u7387\u4e3a $2\\times 10^{\u22124}$. \u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u7c7b\u4f3c, \u6211\u4eec\u91c7\u7528\u7a97\u53e3\u751f\u6210\u5668\u8bad\u7ec3, \u4e00\u79cd\u53ea\u751f\u6210\u90e8\u5206\u539f\u59cb\u6ce2\u5f62\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u5360\u7528. \u6211\u4eec\u968f\u673a\u63d0\u53d6 32 \u957f\u5ea6\u7684\u9690\u8868\u793a\u7247\u6bb5, \u5e76\u5c06\u5176\u8f93\u5165\u5230\u89e3\u7801\u5668, \u800c\u4e0d\u662f\u8f93\u5165\u6574\u4e2a\u9690\u8868\u793a, \u5e76\u4e14\u63d0\u53d6\u76f8\u5e94\u7684\u771f\u5b9e\u97f3\u9891\u7247\u6bb5\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807. \u6211\u4eec\u5728 4 \u5757 NVIDIA V100 GPU \u4e0a\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3. \u6bcf\u5757 GPU \u7684\u6279\u91cf\u5927\u5c0f\u8bbe\u7f6e\u4e3a 64, \u6a21\u578b\u8bad\u7ec3 800k \u6b65.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#44experiment-setup-for-comparison","title":"4.4.Experiment Setup for Comparison\u00b7\u5bf9\u6bd4\u5b9e\u9a8c\u8bbe\u7f6e","text":"\u539f\u6587  &gt; We compared our model with the best publicly available models. &gt; We used [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md), an autoregressive model, and [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md), a flow-based non-autoregressive model, as first stage models and [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md) as a second stage model. &gt; We used their public implementations and pre-trained weights. &gt; Since a two-stage TTS system can theoretically achieve higher synthesis quality through sequential training, we included the fine-tuned [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md) up to 100k steps with the predicted outputs from the first stage models. &gt; We empirically found that fine-tuning [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md) with the generated mel-spectrograms from [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md) under teacher-forcing mode, led to better quality for both [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md) and [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) than fine-tuning with the generated mel-spectrograms from [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md), so we appended the better fine-tuned [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md) to both [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md) and [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md). &gt; As each model has a degree of randomness during sampling, we fixed hyper-parameters that controls the randomness of each model throughout our experiments. &gt; The probability of dropout in the pre-net of [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md) was set to 0.5. &gt; For [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md), the standard deviation of the prior distribution was set to 0.333. &gt; For ***VITS***, the standard deviation of input noise of the stochastic duration predictor was set to 0.8 and we multiplied a scale factor of 0.667 to the standard deviation of the prior distribution.   <p>\u6211\u4eec\u5c06\u6a21\u578b\u548c\u6700\u4f73\u7684\u516c\u5f00\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83. \u6211\u4eec\u4f7f\u7528\u81ea\u56de\u5f52\u6a21\u578b Tacotron2, \u57fa\u4e8e\u6d41\u7684\u975e\u81ea\u56de\u5f52\u6a21\u578b Glow-TTS \u4f5c\u4e3a\u4e00\u9636\u6bb5\u6a21\u578b, HiFi-GAN \u4f5c\u4e3a\u4e8c\u9636\u6bb5\u6a21\u578b. \u6211\u4eec\u4f7f\u7528\u5b83\u4eec\u7684\u516c\u5f00\u5b9e\u73b0\u548c\u9884\u8bad\u7ec3\u6743\u91cd. \u7531\u4e8e\u4e24\u9636\u6bb5 TTS \u7cfb\u7edf\u7406\u8bba\u4e0a\u53ef\u4ee5\u901a\u8fc7\u4e32\u884c\u8bad\u7ec3\u5b9e\u73b0\u66f4\u9ad8\u7684\u5408\u6210\u8d28\u91cf, \u6211\u4eec\u5c06 HiFi-GAN \u4f7f\u7528 Tacotron2 \u548c Glow-TTS \u7684\u9884\u6d4b\u8f93\u51fa\u5fae\u8c03\u5230 100k \u6b65. \u6211\u4eec\u7ecf\u9a8c\u6027\u5730\u53d1\u73b0, \u4f7f\u7528 Tacotron2 \u751f\u6210\u7684\u6885\u5c14\u9891\u8c31\u56fe\u4f5c\u4e3a\u6559\u5e08\u5f3a\u5236\u6a21\u5f0f, \u5fae\u8c03 HiFi-GAN \u80fd\u4ea7\u751f\u66f4\u597d\u7684\u8d28\u91cf, \u56e0\u6b64\u6211\u4eec\u5c06\u66f4\u597d\u7684\u5fae\u8c03 HiFi-GAN \u8ffd\u52a0\u5230 Tacotron2 \u548c Glow-TTS. \u7531\u4e8e\u6bcf\u4e2a\u6a21\u578b\u5728\u91c7\u6837\u65f6\u90fd\u6709\u4e00\u5b9a\u7684\u968f\u673a\u6027, \u6211\u4eec\u5728\u5b9e\u9a8c\u4e2d\u56fa\u5b9a\u4e86\u6bcf\u4e2a\u6a21\u578b\u7684\u8d85\u53c2\u6570, \u4ee5\u63a7\u5236\u968f\u673a\u6027. Tacotron2 \u7684\u9884-\u7f51\u7edc\u7684\u4e22\u5f03\u6982\u7387\u8bbe\u7f6e\u4e3a 0.5. Glow-TTS \u7684\u5148\u9a8c\u5206\u5e03\u7684\u6807\u51c6\u5dee\u8bbe\u7f6e\u4e3a 0.333. VITS \u7684\u8f93\u5165\u566a\u58f0\u7684\u6807\u51c6\u5dee\u8bbe\u7f6e\u4e3a 0.8, \u5e76\u5c06\u5148\u9a8c\u5206\u5e03\u7684\u6807\u51c6\u5dee\u4e58\u4ee5 0.667.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#5results","title":"5.Results\u00b7\u7ed3\u679c","text":""},{"location":"TTS/Models/E2E/2021.06.11_VITS/#51speech-synthesis-quality","title":"5.1.Speech Synthesis Quality\u00b7\u8bed\u97f3\u5408\u6210\u8d28\u91cf","text":"\u539f\u6587  &gt; We conducted crowd-sourced MOS tests to evaluate the quality. &gt; Raters listened to randomly selected audio samples, and rated their naturalness on a 5 point scale from 1 to 5. &gt; Raters were allowed to evaluate each audio sample once, and we normalized all the audio clips to avoid the effect of amplitude differences on the score. &gt; All of the quality assessments in this work were conducted in this manner. &gt; The evaluation results are shown in Tab.01.   <p>\u6211\u4eec\u8fdb\u884c\u4e86\u4f17\u5305 MOS \u6d4b\u8bd5, \u4ee5\u8bc4\u4f30\u8d28\u91cf. \u8bc4\u5206\u5458\u968f\u673a\u9009\u62e9\u4e86\u97f3\u9891\u6837\u672c, \u5e76\u5bf9\u5176\u81ea\u7136\u5ea6\u8fdb\u884c\u4e86 5 \u7ea7\u8bc4\u5206, 1 \u5230 5 \u4e4b\u95f4. \u8bc4\u5206\u5458\u4ec5\u5141\u8bb8\u5bf9\u6bcf\u4e2a\u97f3\u9891\u6837\u672c\u8fdb\u884c\u4e00\u6b21\u8bc4\u5206, \u6211\u4eec\u5bf9\u6240\u6709\u97f3\u9891\u7247\u6bb5\u8fdb\u884c\u4e86\u5f52\u4e00\u5316, \u4ee5\u907f\u514d\u632f\u5e45\u5dee\u5f02\u5bf9\u5f97\u5206\u7684\u5f71\u54cd. \u672c\u6587\u4e2d\u6240\u6709\u8d28\u91cf\u8bc4\u4f30\u90fd\u91c7\u7528\u4e86\u8fd9\u79cd\u65b9\u5f0f. \u8868 01 \u663e\u793a\u4e86\u8bc4\u4f30\u7ed3\u679c.</p> <p>Tab.01.Comparison of evaluated MOS with 95% confidence intervals on the LJ Speech dataset.</p> Model MOS (CI) Ground Truth 4.46 (\u00b10.06) Tacotron 2 + HiFi-GAN 3.77 (\u00b10.08) Tacotron 2 + HiFi-GAN (Fine-tuned) 4.25 (\u00b10.07) Glow-TTS + HiFi-GAN 4.14 (\u00b10.07) Glow-TTS + HiFi-GAN (Fine-tuned) 4.32 (\u00b10.07) VITS (DDP) 4.39 (\u00b10.06) VITS 4.43 (\u00b10.06) \u539f\u6587  &gt; ***VITS*** outperforms other TTS systems and achieves a similar MOS to that of ground truth. &gt; The ***VITS*** (DDP), which employs the same deterministic duration predictor architecture used in [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) rather than the stochastic duration predictor, scores the second-highest among TTS systems in the MOS evaluation. &gt; These results imply that 1) the stochastic duration predictor generates more realistic phoneme duration than the deterministic duration predictor and 2) our end-to-end training method is an effective way to make better samples than other TTS models even if maintaining the similar duration predictor architecture.     <p></p> <p>VITS \u4f18\u4e8e\u5176\u4ed6 TTS \u7cfb\u7edf, \u5e76\u8fbe\u5230\u548c\u771f\u5b9e\u503c\u76f8\u4f3c\u7684 MOS. VITS (DDP) \u4f7f\u7528\u548c Glow-TTS \u76f8\u540c\u7684\u786e\u5b9a\u6027\u65f6\u957f\u9884\u6d4b\u5668\u67b6\u6784\u800c\u4e0d\u662f\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u5728 MOS \u8bc4\u4f30\u4e2d\u6392\u540d\u7b2c\u4e8c. \u8fd9\u4e9b\u7ed3\u679c\u8868\u660e: 1. \u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u6bd4\u786e\u5b9a\u6027\u65f6\u957f\u9884\u6d4b\u5668\u751f\u6210\u66f4\u771f\u5b9e\u7684\u97f3\u7d20\u65f6\u957f, 2. \u6211\u4eec\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5, \u5373\u4f7f\u4fdd\u6301\u76f8\u4f3c\u7684\u65f6\u957f\u9884\u6d4b\u5668\u67b6\u6784, \u4e5f\u80fd\u4ea7\u751f\u66f4\u597d\u7684\u6837\u672c.</p> \u539f\u6587  &gt; We conducted an ablation study to demonstrate the effectiveness of our methods, including the normalized flow in the prior encoder and linear-scale spectrogram posterior input. &gt; All models in the ablation study were trained up to 300k steps. &gt; The results are shown in Tab.02.   <p></p> <p>\u6211\u4eec\u8fdb\u884c\u4e86\u6d88\u878d\u5b9e\u9a8c\u4ee5\u8bf4\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027, \u5305\u62ec\u5148\u9a8c\u7f16\u7801\u5668\u4e2d\u7684\u5f52\u4e00\u5316\u6d41\u548c\u7ebf\u6027\u9891\u8c31\u56fe\u540e\u9a8c\u8f93\u5165. \u6d88\u878d\u5b9e\u9a8c\u4e2d\u6240\u6709\u6a21\u578b\u90fd\u8bad\u7ec3\u4e86 300k \u6b65. \u8868 02 \u663e\u793a\u4e86\u7ed3\u679c.</p> <p>Tab.02. MOS comparison in the ablation studies.</p> Model MOS (CI) Ground Truth 4.50 (\u00b10.06) Baseline 4.50 (\u00b10.06) without Normalizing Flow 2.98 (\u00b10.08) with Mel-spectrogram 4.31 (\u00b10.08) \u539f\u6587  &gt; Removing the normalizing flow in the prior encoder results in a 1.52 MOS decrease from the baseline, demonstrating that the prior distribution\u2019s flexibility significantly influences the synthesis quality. &gt; Replacing the linear-scale spectrogram for posterior input with the mel-spectrogram results in a quality degradation (-0.19 MOS), indicating that the high-resolution information is effective for ***VITS*** in improving the synthesis quality.   <p></p> <p>\u79fb\u9664\u5148\u9a8c\u7f16\u7801\u5668\u4e2d\u7684\u6807\u51c6\u5316\u6d41\u4f1a\u964d\u4f4e 1.52 \u7684 MOS \u5f97\u5206, \u8bf4\u660e\u5148\u9a8c\u5206\u5e03\u7684\u7075\u6d3b\u6027\u5bf9\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd. \u7528\u6885\u5c14\u9891\u8c31\u56fe\u66ff\u6362\u7ebf\u6027\u9891\u8c31\u56fe\u540e\u9a8c\u8f93\u5165\u4f1a\u5bfc\u81f4\u8d28\u91cf\u964d\u4f4e (-0.19 MOS), \u8868\u660e\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\u5bf9\u4e8e VITS \u5728\u63d0\u5347\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u6709\u4f5c\u7528.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#52generalization-to-multi-speaker-text-to-speech","title":"5.2.Generalization to Multi-Speaker Text-to-Speech\u00b7\u591a\u8bf4\u8bdd\u4eba\u6587\u672c\u8f6c\u8bed\u97f3","text":"\u539f\u6587  &gt; To verify that our model can learn and express diverse speech characteristics, we compared our model to [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md), [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) and [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md), which showed the ability to extend to multi-speaker speech synthesis ([Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis](); [Glow-TTS (2020)](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md); [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md)). &gt; We trained the models on the [VCTK dataset](../../Datasets/VCTK.md). &gt; We added speaker embedding to our model as described in Section 2.5. &gt; For [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md), we broadcasted speaker embedding and concatenated it with the encoder output, and for [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md), we applied the global conditioning following the previous work. &gt; The evaluation method is the same as that described in Section 4.1. &gt; As shown in Tab.03, our model achieves a higher MOS than the other models.   <p>\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u548c\u8868\u793a\u591a\u6837\u7684\u8bed\u97f3\u7279\u6027, \u6211\u4eec\u5c06\u6a21\u578b\u4e0e Tacotron2, Glow-TTS \u548c HiFi-GAN \u8fdb\u884c\u4e86\u6bd4\u8f83, \u5b83\u4eec\u90fd\u5177\u6709\u6269\u5c55\u5230\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u5408\u6210\u80fd\u529b. \u6211\u4eec\u5728 VCTK \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u6a21\u578b. \u6211\u4eec\u5728\u6a21\u578b\u4e2d\u52a0\u5165\u4e86\u8bf4\u8bdd\u4eba\u5d4c\u5165, \u5982\u7b2c 2.5 \u8282\u6240\u8ff0. \u5bf9\u4e8e Tacotron2, \u6211\u4eec\u5e7f\u64ad\u4e86\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5e76\u5c06\u5176\u4e0e\u7f16\u7801\u5668\u8f93\u51fa\u8fde\u63a5\u8d77\u6765, \u5bf9\u4e8e Glow-TTS, \u6211\u4eec\u9075\u5faa\u4e4b\u524d\u7684\u5de5\u4f5c, \u5e94\u7528\u5168\u5c40\u6761\u4ef6. \u8bc4\u4f30\u65b9\u6cd5\u4e0e\u7b2c 4.1 \u8282\u4e2d\u63cf\u8ff0\u7684\u76f8\u540c. \u5982\u8868 3 \u6240\u793a, \u6211\u4eec\u7684\u6a21\u578b\u8d85\u8fc7\u4e86\u5176\u4ed6\u6a21\u578b.</p> <p>Table 3.Comparison of evaluated MOS with 95% confidence intervals on the VCTK dataset.</p> Model MOS (CI) Ground Truth 4.38 (\u00b10.07) Tacotron 2 + HiFi-GAN 3.14 (\u00b10.09) Tacotron 2 + HiFi-GAN (Fine-tuned) 3.19 (\u00b10.09) Glow-TTS + HiFi-GAN 3.76 (\u00b10.07) Glow-TTS + HiFi-GAN (Fine-tuned) 3.82 (\u00b10.07) VITS 4.38 (\u00b10.06) <p>This demonstrates that our model learns and expresses various speech characteristics in an end-to-end manner.  </p> <p>\u8fd9\u8868\u660e, \u6211\u4eec\u7684\u6a21\u578b\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u5b66\u4e60\u548c\u8868\u793a\u5404\u79cd\u8bed\u97f3\u7279\u6027.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#53speech-variation","title":"5.3.Speech Variation\u00b7\u8bed\u97f3\u53d8\u5316","text":"\u539f\u6587  &gt; We verified how many different lengths of speech the stochastic duration predictor produces, and how many different speech characteristics the synthesized samples have. &gt; Similar to [Flowtron (2020)](../../Models/TTS2_Acoustic/2020.05.12_Flowtron.md), all samples here were generated from a sentence \u201cHow much variation is there?\u201d. &gt; Fig.02a shows histograms of the lengths of 100 generated utterances from each model. &gt; While [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) generates only fixed-length utterances due to the deterministic duration predictor, samples from our model follow a similar length distribution to that of [Tacotron2](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md). &gt; Fig.02b shows the lengths of 100 utterances generated with each of five speaker identities from our model in the multi-speaker setting, implying that the model learns the speaker-dependent phoneme duration. &gt; F0 contours of 10 utterances extracted with the YIN algorithm ([a fundamental frequency estimator for speech and music]()) in Fig.03 shows that our model generates speech with diverse pitches and rhythms, and five samples generated with each of different speaker identities in Fig.03d demonstrates our model expresses very different lengths and pitches of speech for each speaker identity. &gt; Note that [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) could increase the diversity of pitch by increasing the standard deviation of the prior distribution, but on the contrary, it could lower the synthesis quality.   <p>\u6211\u4eec\u9a8c\u8bc1\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u751f\u6210\u7684\u8bed\u97f3\u957f\u5ea6\u6709\u591a\u5c11\u79cd, \u5408\u6210\u7684\u6837\u672c\u6709\u591a\u5c11\u79cd\u4e0d\u540c\u7684\u8bed\u97f3\u7279\u6027. \u4e0e Flowtron (2020) \u7c7b\u4f3c, \u8fd9\u91cc\u6240\u6709\u7684\u6837\u672c\u90fd\u6765\u81ea\u4e8e\u4e00\u53e5\u8bdd \"How much variation is there?\". \u56fe 2a \u663e\u793a\u4e86\u6765\u81ea\u6bcf\u4e2a\u6a21\u578b\u7684 100 \u4e2a\u5408\u6210\u53e5\u5b50\u7684\u957f\u5ea6\u76f4\u65b9\u56fe. \u867d\u7136 Glow-TTS \u7531\u4e8e\u786e\u5b9a\u6027\u65f6\u957f\u9884\u6d4b\u5668\u800c\u4ea7\u751f\u56fa\u5b9a\u957f\u5ea6\u7684\u53e5\u5b50, \u4f46\u6765\u81ea\u6211\u4eec\u7684\u6a21\u578b\u7684\u6837\u672c\u4e0e Tacotron2 \u7684\u6837\u672c\u6709\u7740\u76f8\u4f3c\u7684\u957f\u5ea6\u5206\u5e03. \u56fe 2b \u663e\u793a\u4e86\u6765\u81ea\u6211\u4eec\u7684\u6a21\u578b\u7684 100 \u4e2a\u53e5\u5b50\u7684\u957f\u5ea6, \u8fd9\u8868\u660e\u6a21\u578b\u5b66\u4e60\u4e86\u8bf4\u8bdd\u4eba\u76f8\u5173\u7684\u97f3\u7d20\u65f6\u957f. \u56fe 3 \u663e\u793a\u4e86\u4f7f\u7528 YIN \u7b97\u6cd5\u63d0\u53d6\u7684 10 \u4e2a\u53e5\u5b50\u7684 F0 \u8f6e\u5ed3, \u8fd9\u8868\u660e\u6211\u4eec\u7684\u6a21\u578b\u751f\u6210\u4e86\u5177\u6709\u4e0d\u540c\u97f3\u9ad8\u548c\u97f5\u5f8b\u7684\u8bed\u97f3, \u56fe 3d \u663e\u793a\u4e86\u6765\u81ea\u4e94\u4e2a\u4e0d\u540c\u8bf4\u8bdd\u4eba\u6807\u8bc6\u7684 10 \u4e2a\u53e5\u5b50, \u8fd9\u8868\u660e\u6a21\u578b\u4e3a\u6bcf\u4e2a\u8bf4\u8bdd\u4eba\u6807\u8bc6\u751f\u6210\u4e86\u4e0d\u540c\u7684\u8bed\u97f3\u957f\u5ea6\u548c\u97f3\u9ad8. \u6ce8\u610f\u5230 Glow-TTS \u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u5148\u9a8c\u5206\u5e03\u7684\u6807\u51c6\u5dee\u6765\u589e\u52a0\u97f3\u9ad8\u7684\u591a\u6837\u6027, \u4f46\u76f8\u53cd, \u5b83\u4f1a\u964d\u4f4e\u5408\u6210\u8d28\u91cf.</p>"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#54synthesis-speed","title":"5.4.Synthesis Speed\u00b7\u5408\u6210\u901f\u5ea6","text":"\u539f\u6587  &gt; We compared the synthesis speed of our model with a parallel two-stage TTS system, [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md) and [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md). &gt; We measured the synchronized elapsed time over the entire process to generate raw waveforms from phoneme sequences with 100 sentences randomly selected from the test set of the [LJ Speech dataset](../../Datasets/LJSpeech.md). &gt; We used a single NVIDIA V100 GPU with a batch size of 1. &gt; The results are shown in Table 4. &gt; Since our model does not require modules for generating predefined intermediate representations, its sampling efficiency and speed are greatly improved.   <p>\u6211\u4eec\u6bd4\u8f83\u4e86\u6211\u4eec\u7684\u6a21\u578b\u4e0e Glow-TTS \u548c HiFi-GAN \u7684\u5408\u6210\u901f\u5ea6. \u6211\u4eec\u6d4b\u91cf\u4e86\u4ece LJ Speech \u6d4b\u8bd5\u96c6\u4e2d\u968f\u673a\u9009\u62e9 100 \u4e2a\u53e5\u5b50\u7684\u97f3\u7d20\u5e8f\u5217\u751f\u6210\u539f\u59cb\u6ce2\u5f62\u6240\u9700\u7684\u65f6\u95f4. \u6211\u4eec\u4f7f\u7528\u5355\u4e2a NVIDIA V100 GPU, \u6279\u5927\u5c0f\u4e3a 1. \u7ed3\u679c\u5982\u8868 4 \u6240\u793a. \u7531\u4e8e\u6211\u4eec\u7684\u6a21\u578b\u4e0d\u9700\u8981\u751f\u6210\u9884\u5b9a\u4e49\u7684\u4e2d\u95f4\u8868\u793a, \u5176\u91c7\u6837\u6548\u7387\u548c\u901f\u5ea6\u5927\u5e45\u63d0\u9ad8.</p> <p>Table 4.Comparison of the synthesis speed.  Speed of n kHz means that the model can generate n\u00d71000 raw audio samples per second.  Real-time means the synthesis speed over real-time.</p> Model Speed (kHz) Real-time Glow-TTS + HiFi-GAN 606.05 \u00d727.48 VITS 1480.15 \u00d767.12 VITS (DDP) 2005.03 \u00d790.93"},{"location":"TTS/Models/E2E/2021.06.11_VITS/#6conclusion","title":"6.Conclusion\u00b7\u7ed3\u8bba","text":"\u539f\u6587  &gt; In this work, we proposed a parallel TTS system, ***VITS***, that can learn and generate in an end-to-end manner. &gt; We further introduced the stochastic duration predictor to express diverse rhythms of speech. &gt; The resulting system synthesizes natural sounding speech waveforms directly from text, without having to go through predefined intermediate speech representations. &gt; Our experimental results show that our method outperforms two-stage TTS systems and achieves close to human quality. &gt; We hope the proposed method will be used in many speech synthesis tasks, where two-stage TTS systems have been used, to achieve performance improvement and enjoy the simplified training procedure. &gt; We also want to point out that even though our method integrates two separated generative pipelines in TTS systems, there remains a problem of text preprocessing. &gt; Investigating self-supervised learning of language representations could be a possible direction for removing the text preprocessing step. &gt; We will release our source-code and pre-trained models to facilitate research in plenty of future directions.   <p>\u672c\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e76\u884c\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf, VITS, \u80fd\u591f\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u5b66\u4e60\u548c\u751f\u6210\u8bed\u97f3. \u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u4ee5\u751f\u6210\u5177\u6709\u591a\u6837\u97f5\u5f8b\u7684\u8bed\u97f3. \u8be5\u7cfb\u7edf\u80fd\u76f4\u63a5\u4ece\u6587\u672c\u5408\u6210\u81ea\u7136\u7684\u8bed\u97f3\u6ce2\u5f62, \u800c\u4e0d\u9700\u8981\u7ecf\u8fc7\u9884\u5148\u5b9a\u4e49\u7684\u4e2d\u95f4\u8bed\u97f3\u8868\u793a. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, \u6211\u4eec\u7684\u6a21\u578b\u4f18\u4e8e\u4e24\u9636\u6bb5\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf, \u5e76\u8fbe\u5230\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u8d28\u91cf. \u6211\u4eec\u5e0c\u671b\u8be5\u65b9\u6cd5\u80fd\u591f\u7528\u4e8e\u8bb8\u591a\u4e4b\u524d\u5df2\u7ecf\u4f7f\u7528\u4e24\u9636\u6bb5\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u7684\u8bed\u97f3\u5408\u6210\u4efb\u52a1, \u4ee5\u63d0\u5347\u6027\u80fd\u5e76\u7b80\u5316\u8bad\u7ec3\u8fc7\u7a0b. \u6211\u4eec\u8fd8\u9700\u8981\u6307\u51fa\u5c3d\u7ba1\u6211\u4eec\u7684\u65b9\u6cd5\u5408\u5e76\u4e86\u4e24\u79cd\u5355\u72ec\u7684\u751f\u6210 pipelines, \u4f46\u4ecd\u7136\u5b58\u5728\u6587\u672c\u9884\u5904\u7406\u7684\u95ee\u9898. \u63a2\u7d22\u81ea\u76d1\u7763\u5b66\u4e60\u8bed\u8a00\u8868\u5f81\u53ef\u80fd\u662f\u6d88\u9664\u6587\u672c\u9884\u5904\u7406\u6b65\u9aa4\u7684\u4e00\u79cd\u53ef\u80fd\u65b9\u5411. \u6211\u4eec\u5c06\u4f1a\u53d1\u5e03\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b, \u4ee5\u4fc3\u8fdb\u7814\u7a76\u7684\u5e7f\u9614\u524d\u666f.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/","title":"VITS2","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: ***VITS2***: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design - \u4f5c\u8005:   - [Jungil Kong](../../Authors/Jungil_Kong.md)   - [Jihoon Park](../../Authors/Jihoon_Park.md)   - [Beomjeong Kim](../../Authors/Beomjeong_Kim.md)   - [Jeongmin Kim](../../Authors/Jeongmin_Kim.md)   - [Dohee Kong](../../Authors/Dohee_Kong.md)   - [Sangjin Kim](../../Authors/Sangjin_Kim.md) - \u673a\u6784:   - [SK Telecom](../../Institutions/SK_Telecom.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2023.07.31 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.06 - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2307.16430)   - [DOI]()   - [Github]()   - [Demo]() - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u5bf9\u6297\u8bad\u7ec3](../../Tags/Learning_Adversarial.md) - \u9875\u6570: 5 - \u5f15\u7528: ? - \u88ab\u5f15: 5"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>Single-stage text-to-speech models have been actively studied recently, and their results have outperformed two-stage pipeline systems. Although the previous single-stage model has made great progress, there is room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and present that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows a fully end-to-end single-stage approach.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Recent developments in deep neural network-based text-to-speech have seen significant advancements. Deep neural network-based text-to-speech is a method for generating corresponding raw waveforms from input texts; it has several interesting features that often make the text-to-speech task challenging. A quick review of the features reveals that the text-to-speech task involves converting text, which is a discontinuous feature, into continuous waveforms. The input and output have a time step difference of hundreds of times, and the alignment between them must be very precise to synthesize high-quality speech audio. Additionally, prosody and speaker characteristics not present in the input text should be expressed naturally and it is a one-to-many problem in which text input can be spoken in multiple ways. Another factor that makes synthesizing high-quality speech challenging is that humans focus on individual components when listening to an audio; therefore, even if a fraction of the hundreds of thousands of signals that constitute the entire audio are unnatural, humans can easily sense them. Efficiency is another factor that makes the task difficult. The synthesized audio has a substantial time resolution, which generally comprises more than 20,000 data per second, demanding highly efficient sampling methods.</p> <p>Owing to the text-to-speech task features, the solution can also be sophisticated. Previous works have addressed these problems by dividing the process of generating waveforms from input texts into two cascaded stages. A popular method involves producing intermediate speech representations such as mel-spectrograms or linguistic features from the input texts in the first stage (Tacotron2, Transformer-TTS, FastSpeech, Glow-TTS, Flowtron, Grad-TTS, FastSpeech2) and then generating raw waveforms conditioned on those intermediate representations in the second stage (WaveNet, WaveRNN, WaveGlow, MelGAN, GAN-TTS, Parallel WaveGAN, HiFi-GAN, WaveGrad). Two-stage pipeline systems have the advantages of simplifying each model and facilitating training; however, they also have the following limitations.  (1) Error propagation from the first stage to the second stage.  (2) Rather than utilizing the learned representation inside the model, it is mediated through human-defined features such as mel-spectrogram or linguistic features.  (3) Computation required to generate intermediate features. Recently, to address these limitations, single-stage models that directly generate waveforms from input texts have been actively studied [16, 7, 17, 18]. The single-stage models not only outperformed the two-stage pipeline systems, but also showed an ability to generate high-quality speech nearly indistinguishable from humans.</p> <p>Although the previous work (VITS) has achieved great success with the single-stage approach, the model has the following problems: intermittent unnaturalness, low efficiency of the duration predictor, complex input format to alleviate the limitations of alignment and duration modeling (use of blank token), insufficient speaker similarity in the multi-speaker model, slow training, and strong dependence on the phoneme conversion. In this work, we provide methods to address these problems. We propose a stochastic duration predictor trained through adversarial learning, normalizing flows improved by utilizing the transformer block and a speaker-conditioned text encoder to model multiple speakers\u2019 characteristics better. We confirm that the proposed methods improve quality and efficiency. Furthermore, we show that the methods reduce the dependency on the phoneme conversion through the experiment using normalized texts as the input of the model. Thus, the methods move closer to a fully end-to-end single-stage approach.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#2related-works","title":"2.Related Works\u00b7\u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#3methodology","title":"3.Methodology\u00b7\u65b9\u6cd5","text":"<p>In this section, we describe improvements in four subsections: duration prediction, augmented variational autoencoder with normalizing flows, alignment search, and speaker-conditioned text encoder. We propose a method that uses adversarial learning to train the duration predictor to synthesize natural speech with high efficiency in both training and synthesis. Our model essentially learns alignments using the Monotonic Alignment Search (MAS) proposed in the previous work (Glow-TTS, VITS), and we further suggest a modification to improve the quality. In addition, we propose a method to improve naturalness by introducing the transformer block into the normalizing flows, which enables capturing long-term dependencies when transforming the distribution. Furthermore, we modify the speaker conditioning to improve the speaker similarity in a multi-speaker model.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#31stochastic-duration-predictor-with-time-step-wise-conditional-discriminator","title":"3.1.Stochastic Duration Predictor with Time Step-wise Conditional Discriminator","text":"<p>The previous work (VITS) has shown that the flow-based stochastic duration predictor is more effective in improving the naturalness of synthesized speech than the deterministic approach. It showed great results; however, the flow-based method requires relatively more computations and some sophisticated techniques. We propose a stochastic duration predictor with adversarial learning to synthesize more natural speech with higher efficiency in both training and synthesis than the previous work (VITS). The overview of the proposed duration predictor and discriminator is shown in Figure 1a. We apply adversarial learning to train the duration predictor with a conditional discriminator that is fed the same input as the generator to appropriately discriminate the predicted duration. We use the hidden representation of the text $h_{text}$ and Gaussian noise $z_d$ as the input of the generator $G$; and the $h_{text}$ and duration obtained using MAS in the logarithmic scale denoted as $d$ or predicted from the duration predictor denoted as $\\hat{d}$, are used as the input of the discriminator $D$. Discriminators of general generative adversarial networks are fed inputs of a fixed length, whereas the duration for each input token is predicted, and the length of the input sequence varies for each training instance. To properly discriminate the inputs of variable length, we propose a time step-wise discriminator that discriminates each of the predicted durations of all tokens. We use two types of losses; the least squares loss function (LSGAN) for adversarial learning and the mean squared error loss function:</p> <p>$$   Loss_{adv}(D) = E_{(d,z_d,h_{text})} [(D(d, h_{text})-1)^2 + (D(G(z_d, h_{text}), h_{text}))^2]\\tag{01} $$</p> <p>$$   Loss_{adv}(G) = E_{(z_d, h_{text})} [(D(G(z_d, h_{text}))-1)^2]\\tag{02} $$</p> <p>$$   Loss_{mse} = MSE(G(z_d, h_{text}), d)\\tag{03} $$</p> <p>Our proposed duration predictor and training mechanism allow for a learning duration in short steps, and the duration predictor is separately trained as the last training step, which reduces the overall computation time for training.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#32monotonic-alignment-search-with-gaussian-noise","title":"3.2.Monotonic Alignment Search with Gaussian Noise","text":"<p>Following the previous work (Glow-TTS, VITS), we introduce MAS into our model to learn the alignment. The algorithm yields the alignment between text and audio that has the highest probability among all possible monotonic alignments, and the model is trained to maximize its probability. The method is efficient; however, after searching and optimizing a particular alignment, it is limited in exploration to search for other alignments that are more appropriate. To mitigate this, we add a small Gaussian noise to the calculated probabilities. This gives the model extra opportunities to search for other alignments. We only add this noise at the beginning of training because MAS enables the model to learn the alignments quickly. Referring to a previous work Glow-TTS, which described the algorithm in detail, $Q$ values have the maximum log-likelihood calculated for all possible positions in the forward operation. We add small Gaussian noise $\\varepsilon$ to the calculated $Q$ values in the operation.</p> <p>$$   P_{i,j} = \\log\\mathcal{N}(z_j;\\mu_i, \\sigma_i^2)\\tag{04} $$</p> <p>$$   Q_{i,j} = \\max_{A}\\sum_{k=1}^{j}\\log \\mathcal{N}(z_k;\\mu_{A(k)}, \\sigma_{A(k)}^2) = \\max (Q_{i-1,j-1}, Q_{i,j-1}) + P_{i,j}+\\varepsilon\\tag{05} $$</p> <p>where $i$ and $j$ denote a specific position on the input sequence and posterior, respectively, $z$ represents transformed latent variables from the normalizing flows. $\\varepsilon$ is obtained as the product of noise sampled from the standard normal distribution, the standard deviation of $P$, and the noise scale starting at $0.01$ and decreasing by $2\\times 10^{\u22126}$ for every step.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#33normalizing-flows-using-transformer-block","title":"3.3.Normalizing Flows using Transformer Block","text":"<p>The previous work (VITS) demonstrated the capability of the variational autoencoder augmented with normalizing flows to synthesize high-quality speech audio. The normalizing flows comprise convolution blocks, which are effective structures for capturing the patterns of adjacent data and enabling the model to synthesize high-quality speech. The ability to capture long-term dependencies can be crucial when transforming distribution because each part of the speech is related to other parts that are not adjacent. Although a convolution block captures adjacent patterns effectively, it has a disadvantage in capturing long-term dependencies owing to the limitations of its receptive field. Therefore, we add a small transformer block with the residual connection into the normalizing flows to enable the capturing of long-term dependencies, as shown in Figure 1b. Figure 2 shows an actual attention score map and the receptive field of the convolution block. We can confirm that the transformer block collects information at various positions when transforming the distribution, which is impossible with the receptive field.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#34speaker-conditioned-text-encoder","title":"3.4.Speaker-Conditioned Text Encoder","text":"<p>Because the multi-speaker model is to synthesize speech in multiple characteristics according to the speaker condition with one single model, expressing individual speech characteristics of each speaker is an important quality factor as well as naturalness. The previous work showed that the single-stage model can model multiple speakers with high quality. Considering some features, such as a speaker\u2019s particular pronunciation and intonation, significantly influences the expression of the speech characteristics of each speaker but are not contained in the input text, we design a text encoder conditioned with the speaker information to better mimic various speech characteristics of each speaker by learning the features while encoding the input text. We condition the speaker vector on the third transformer block of the text encoder, as shown in Figure 1c.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#4experiments","title":"4.Experiments\u00b7\u5b9e\u9a8c","text":"<p>We conducted experiments on two different datasets. We used the LJ Speech dataset to confirm the improvement in naturalness and the VCTK dataset to verify whether our model could reproduce speaker characteristics better. The LJ Speech dataset consists of 13,100 short audio clips of a single speaker with a total length of approximately 24 hours. The audio format is 16-bit PCM with a sample rate of 22.05 kHz, and we used it without any manipulation. We randomly split the dataset into a training set (12,500 samples), validation set (100 samples), and test set (500 samples). The VCTK dataset consists of approximately 44,000 short audio clips uttered by 109 native English speakers with various accents. The total length of the audio clips is approximately 44 hours. The audio format is 16-bit PCM with a sample rate of 44.1 kHz. We reduced the sample rate to 22.05 kHz. We randomly split the dataset into a training set (43,470 samples), validation set (100 samples), and test set (500 samples).</p> <p>We used 80 bands mel-scale spectrograms for calculating the reconstruction loss. In contrast with the previous work (VITS), we used the same spectrograms as the input of the posterior encoder. The fast Fourier transform, window, and hop sizes were set to 1024, 1024, and 256, respectively.</p> <p>We conducted experiments using both phoneme sequences and normalized texts as the input of the model. We converted text sequences into International Phonetic Alphabet sequences using open-source software (Phonemizer) and fed the text encoder with the sequences. Contrasting with the previous work (VITS), we did not use the blank token. For the experiment with normalized texts, we normalized the input text with simple rules using open-source software (Tacotron) and fed the text encoder with it.</p> <p>The networks were trained using the AdamW optimizer with $\\beta_{1} = 0.8$, $\\beta_{2} = 0.99$, and weight decay $\\lambda = 0.01$. The learning rate decay was scheduled by a $0.999^{1/8}$ factor in every epoch, with an initial learning rate of $2\\times 10^{\u22124}$. We fed the networks with 256 training instances per step. Following the previous work (VITS), the windowed generator training was applied. We used mixed precision training on four NVIDIA V100 GPUs. The networks to generate waveforms and the duration predictor were trained up to 800k and 30k steps, respectively.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#5results","title":"5.Results\u00b7\u7ed3\u679c","text":""},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#51evaluation-of-naturalness","title":"5.1.Evaluation of Naturalness","text":"<p>To confirm that the proposed model synthesizes natural speech, crowdsourced mean opinion score (MOS) tests were conducted. Raters rated their naturalness on a 5-point scale from 1 to 5 after listening to randomly selected audio samples from the test sets. Considering that the previous work (VITS) has already demonstrated similar quality to human recordings, we also conducted a comparative mean opinion score (CMOS) test, which is appropriate for evaluating high-quality samples by direct comparison. Raters rated their relative preference in terms of naturalness on a 7-point scale from 3 to -3 after listening to randomly selected audio samples from the test sets.1 Raters were allowed to evaluate each audio sample once. All audio samples were normalized to avoid the effect of amplitude differences on the score. We used the official implementation and pre-trained weights of the previous work (VITS) as the comparison model. The evaluation results are presented in Table 1 and Table 2a. The MOS difference between our method and the previous work (VITS) was 0.09, and the CMOS and confidence interval were 0.201 and \u00b10.105, respectively. The results demonstrate that the our method significantly improves the quality of synthesized speech. Additionally, we evaluated CMOS with the method (JETS) that showed good performance using different structures and training mechanisms. For evaluation, we generated samples using the official implementation and pre-trained weights. The CMOS and confidence intervals of the evaluation are 0.176 and \u00b10.125, respectively, indicating that our method significantly outperforms the method.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#52ablation-studies","title":"5.2.Ablation Studies","text":"<p>Ablation studies were also conducted to verify the validity of the proposed methods. To verify the validity of the stochastic duration predictor trained with adversarial learning, it was substituted with the deterministic duration predictor that had the same structure and was trained with L2 loss. The deterministic duration predictor was trained up to the same steps as the previous work (VITS). To verify the efficacy of the noise scheduling used in the alignment search, the model was trained without the noise. We trained the model without the transformer block in the normalizing flows to verify its effectiveness. The evaluation results are presented in Table 1. The MOS differences of the ablation studies on the deterministic duration predictor, alignment search without the noise, and normalizing flows without the transformer block are 0.14, 0.15, and 0.06, respectively. As we do not use the blank token and linear spectrogram, the computational efficiency would be improved, and removing some of the proposed methods shows lower performance compared with the previous work (VITS). The results show that the proposed methods are effective in improving the quality.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#53evaluation-of-speaker-similarity","title":"5.3.Evaluation of Speaker Similarity","text":"<p>To confirm the improvement in speaker similarity in the multispeaker model, similarity MOS tests similar to the previous work (Transfer Learning from Speaker Verification to Multispeaker Text-to-Speech Synthesis) were conducted through crowdsourcing. In the test, randomly sampled human recorded audio from the test set was presented as a reference, and raters scored the similarity between the reference and the corresponding synthesized audio on a five-point scale from 1 to 5. As in section 4.1, raters were allowed to evaluate each audio sample once, and the audio samples were normalized. The evaluation results are presented in Table 2b. VITS2 was rated 0.2 MOS higher than the previous work (VITS), which shows the effectiveness of our method in improving speaker similarity when modeling multiple speakers.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#54reduced-dependency-on-the-phoneme-conversion","title":"5.4.Reduced Dependency on the Phoneme Conversion","text":"<p>Previous works [17, 26] have shown good performance with single-stage approaches but continue to have a strong dependence on phoneme conversion. Because normalized text does not inform its actual pronunciation, it makes learning accurate pronunciations challenging. It is currently a crucial barrier to achieving a fully end-to-end single-stage speech synthesis. We present that our method significantly improves this problem through intelligibility tests. After transcribing 500 synthesized audio in the test set using Google\u2019s automatic speech recognition API, we calculated the character error rate (CER) with the ground truth text as the reference. We compared the results of the following four models with the ground truth: the proposed model using phoneme sequences, the proposed model using normalized texts, the previous work using phoneme sequences, and the previous work using normalized texts. Table 3 presents the comparison, which confirms that not only the proposed model outperforms the previous work, but also the performance of our model using normalized texts is comparable to that of the model using phoneme sequences. It demonstrates the possibility of a data-driven, fully end-to-end approach.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#55comparison-of-synthesis-and-training-speed","title":"5.5.Comparison of Synthesis and Training Speed","text":"<p>We compared our model\u2019s synthesis and training speed with those of the previous work (VITS). We measured the synchronized elapsed time over the entire process to generate raw waveforms from input sequences with 500 sentences randomly selected from the LJ Speech dataset. We used a single NVIDIA V100 GPU with a batch size of 1. We also measured and averaged the elapsed time for the training computation of each step for five epochs on four NVIDIA V100 GPUs. Table 4 shows the results. As the duration predictor is more efficient and can be trained separately and the input sequences are shorter than in the previous work, its training and synthesis speed are improved; the improvements are 20.5% and 22.7%, respectively.</p>"},{"location":"TTS/Models/E2E/2023.07.31_VITS2/#6conclusions","title":"6.Conclusions\u00b7\u7ed3\u8bba","text":"<p>We propose VITS2, a single-stage text-to-speech model that can efficiently synthesize more natural speech. We improved the training and inference efficiency and naturalness by introducing adversarial learning into the duration predictor. The transformer block was added to the normalizing flows to capture the long-term dependency when transforming the distribution. The synthesis quality was improved by incorporating Gaussian noise into the alignment search. The dependency on phoneme conversion, which was posing a challenge in achieving a fully end-to-end single-stage speech synthesis, was significantly reduced. The test results also show that overall intelligibility was improved. We demonstrated the validity of our proposed methods through experiments, quality evaluation, and computation speed measurement. Various problems still exist in the field of speech synthesis that must be addressed, and we hope that our work can be a basis for future research.</p>"},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/","title":"OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework","text":"\u4f5c\u8005 \u673a\u6784 Sachin Mehta Apple Mohammad Hossein Sekhavat Apple Qingqing Cao Apple Maxwell Horton Apple Yanzi Jin Apple Chenfan Sun Apple Iman Mirzadeh Apple Mahyar Najibi Apple Dmitry Belenko Apple Peter Zatloukal Apple Mohammad Rastegari Apple","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release ==OpenELM==, a state-of-the-art open language model. ==OpenELM== uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the Transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, ==OpenELM== exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2\u00d7 fewer pre-training tokens.</p> <p>\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u590d\u73b0\u6027\u548c\u900f\u660e\u5ea6\u5bf9\u4e8e\u5148\u8fdb\u5f00\u653e\u7814\u7a76\u81f3\u5173\u91cd\u8981, \u786e\u4fdd\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6, \u80fd\u591f\u6df1\u5165\u7814\u7a76\u6570\u636e\u548c\u6a21\u578b\u504f\u89c1, \u4ee5\u53ca\u6f5c\u5728\u98ce\u9669. \u4e3a\u6b64, \u6211\u4eec\u53d1\u5e03\u4e86 OpenELM, \u4e00\u4e2a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u8bed\u8a00\u6a21\u578b. OpenELM \u4f7f\u7528\u9010\u5c42\u7f29\u653e\u7b56\u7565\u4ee5\u6709\u6548\u5730\u5728 Transformer \u6a21\u578b\u7684\u6bcf\u4e2a\u5c42\u4e2d\u5206\u914d\u53c2\u6570, \u4ece\u800c\u589e\u5f3a\u7cbe\u5ea6. \u4f8b\u5982, \u5bf9\u4e8e\u4e00\u4e2a\u53c2\u6570\u89c4\u6a21\u7ea6\u4e3a 1 \u4ebf\u7684\u6a21\u578b, OpenELM \u76f8\u8f83\u4e8e OLMo \u63d0\u9ad8\u4e86 2.36% \u7684\u51c6\u786e\u7387, \u800c\u53ea\u8981\u6c42 2 \u500d\u5c11\u4e8e OLMo \u7684\u9884\u8bad\u7ec3\u6570\u636e.</p> <p>Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.</p> <p>\u548c\u4e4b\u524d\u7684\u7814\u7a76\u53ea\u63d0\u4f9b\u6a21\u578b\u6743\u91cd\u548c\u63a8\u7406\u4ee3\u7801, \u4ee5\u53ca\u5728\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u4e0d\u540c, \u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u5b8c\u6574\u6846\u67b6, \u5305\u62ec\u8bad\u7ec3\u65e5\u5fd7, \u591a\u4e2a\u68c0\u67e5\u70b9, \u4ee5\u53ca\u9884\u8bad\u7ec3\u914d\u7f6e. \u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u7528\u4e8e\u5c06\u6a21\u578b\u8f6c\u6362\u4e3a MLX \u5e93\u7528\u4e8e\u5728 Apple \u8bbe\u5907\u4e0a\u8fdb\u884c\u63a8\u7406\u548c\u5fae\u8c03\u7684\u4ee3\u7801. \u6b64\u5b8c\u6574\u7684\u53d1\u5e03\u65e8\u5728\u6fc0\u53d1\u548c\u52a0\u5f3a\u5f00\u653e\u7814\u7a76\u793e\u533a, \u4e3a\u672a\u6765\u5f00\u653e\u7814\u7a76\u5960\u5b9a\u57fa\u7840.</p> <p>Our source code along with pre-trained model weights and training recipes is available at https://github.com/apple/corenet. Additionally, ==OpenELM== models can be found on HuggingFace at: https://huggingface.co/apple/OpenELM.</p> <p>\u6211\u4eec\u7684\u6e90\u4ee3\u7801, \u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u548c\u8bad\u7ec3\u65b9\u6cd5\u90fd\u5728 Github \u4e0a\u53d1\u5e03, \u5730\u5740\u4e3a https://github.com/apple/corenet. \u53e6\u5916, OpenELM \u6a21\u578b\u53ef\u4ee5\u5728 HuggingFace \u4e0a\u627e\u5230, \u5730\u5740\u4e3a https://huggingface.co/apple/OpenELM.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Transformer-based [48] large language models (LLM) are revolutionizing the field of natural language processing [7,46]. These models are isotropic, meaning that they have the same configuration (e.g., number of heads and feed-forward network dimensions) for each Transformer layer. Though such isotropic models are simple, they may not allocate parameters efficiently inside the model.</p> <p>\u57fa\u4e8e Transformer \u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6b63\u5728\u53d8\u9769\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df. \u8fd9\u4e9b\u6a21\u578b\u662f\u5404\u5411\u540c\u6027\u7684, \u8fd9\u610f\u5473\u7740\u5b83\u4eec\u7684\u6bcf\u4e2a Transformer \u5c42\u90fd\u5177\u6709\u76f8\u540c\u7684\u914d\u7f6e (\u4f8b\u5982, \u6ce8\u610f\u529b\u5934\u6570\u548c\u524d\u9988\u7f51\u7edc\u7ef4\u5ea6). \u867d\u7136\u8fd9\u6837\u7684\u5404\u5411\u540c\u6027\u6a21\u578b\u5f88\u7b80\u5355, \u4f46\u5b83\u4eec\u53ef\u80fd\u5728\u6a21\u578b\u5185\u90e8\u4e0d\u5145\u5206\u5730\u5206\u914d\u53c2\u6570.</p> <p>In this work, we develop and release ==OpenELM==, a family of pre-trained and fine-tuned models on publicly available datasets. At the core of ==OpenELM== lies layer-wise scaling [30], enabling more efficient parameter allocation across layers. This method utilizes smaller latent dimensions in the attention and feed-forward modules of the Transformer layers closer to the input, and gradually widening the layers as they approach the output.</p> <p>\u672c\u9879\u5de5\u4f5c\u5f00\u53d1\u5e76\u53d1\u5e03\u4e86 OpenELM, \u4e00\u7c7b\u5728\u516c\u5f00\u53ef\u7528\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u6a21\u578b. OpenELM \u7684\u6838\u5fc3\u662f\u9010\u5c42\u7f29\u653e, \u5b83\u4f7f\u5f97\u53c2\u6570\u5206\u914d\u66f4\u52a0\u6709\u6548. \u8fd9\u79cd\u65b9\u6cd5\u5728 Transformer \u5c42\u7684\u6ce8\u610f\u529b\u6a21\u5757\u548c\u524d\u9988\u6a21\u5757\u4e2d\u4f7f\u7528\u8f83\u5c0f\u7684\u9690\u85cf\u5c42\u7ef4\u5ea6, \u5e76\u968f\u7740\u63a5\u8fd1\u8f93\u51fa\u4e0d\u65ad\u52a0\u5bbd.</p> <p>We release the complete framework, encompassing data preparation, training, fine-tuning, and evaluation procedures, alongside multiple pre-trained checkpoints and training logs, to facilitate open research. Importantly, ==OpenELM== outperforms existing open LLMs that are pre-trained using publicly available datasets (Tab.01).  For example, ==OpenELM== with 1.1 billion parameters outperforms OLMo [17], which has 1.2 billion parameters, by 2.36% while requiring 2\u00d7 fewer pre-training tokens.</p> <p>\u6211\u4eec\u53d1\u5e03\u4e86\u5b8c\u6574\u6846\u67b6, \u5305\u62ec\u6570\u636e\u51c6\u5907, \u8bad\u7ec3, \u5fae\u8c03, \u8bc4\u4f30\u8fc7\u7a0b, \u4ee5\u53ca\u591a\u4e2a\u9884\u8bad\u7ec3\u68c0\u67e5\u70b9\u548c\u8bad\u7ec3\u65e5\u5fd7, \u4ee5\u4fc3\u8fdb\u5f00\u653e\u7814\u7a76. \u91cd\u8981\u7684\u662f, OpenELM \u7684\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u7684\u5728\u516c\u5f00\u53ef\u7528\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b, \u53c2\u9605\u8868\u683c 01. \u4f8b\u5982, 1.1 \u4ebf\u53c2\u6570\u7684 OpenELM \u4f18\u4e8e 1.2 \u4ebf\u53c2\u6570\u7684 OLMo, \u4ec5\u9700 2 \u500d\u5c11\u4e8e OLMo \u7684\u9884\u8bad\u7ec3\u6570\u636e, \u4e14\u51c6\u786e\u7387\u63d0\u9ad8 2.36%.</p> <p></p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#2pre-training","title":"2.Pre-Training\u00b7\u9884\u8bad\u7ec3","text":"<p>This section describes the framework, including model architecture (2.1), pre-training data (2.2), training hyper-parameters (2.3), and evaluation (2.4).</p> <p>\u672c\u8282\u4ecb\u7ecd\u4e86\u6574\u4f53\u6846\u67b6, \u5305\u62ec\u6a21\u578b\u67b6\u6784, \u9884\u8bad\u7ec3\u6570\u636e, \u8bad\u7ec3\u8d85\u53c2\u6570\u548c\u8bc4\u4f30.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#21openelm-architecture","title":"2.1.OpenELM Architecture\u00b7\u67b6\u6784","text":"<p>We adopt the decoder-only Transformer-based architecture. Following state-of-the-art LLMs, we: 1. do not use learnable bias parameters in any fully-connected (a.k.a., linear) layers,  2. apply pre-normalization using RMSNorm [53] and also, use rotatory positional embedding (ROPE) [43] for encoding positional information,  3. use Grouped Query Attention (GQA) [1] instead of Multi-Head Attention (MHA),  4. replace the feed forward network (FFN) with SwiGLU FFN [41],  5. use flash attention [13] for computing the scaled dot-product attention, 6. use the same tokenizer as LLaMA.</p> <p>\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u4ec5\u7528 Transformer \u89e3\u7801\u5668\u7684\u67b6\u6784. \u9075\u5faa\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b, \u6211\u4eec: 1. \u4e0d\u5728\u4efb\u4f55\u5168\u8fde\u63a5\u5c42\u4e2d\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u504f\u7f6e\u53c2\u6570; 2. \u4f7f\u7528 RMSNorm \u8fdb\u884c\u9884\u5f52\u4e00\u5316, \u5e76\u4f7f\u7528\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165 (ROPE) \u6765\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f; 3. \u4f7f\u7528 Grouped Query Attention (GQA) \u800c\u4e0d\u662f\u591a\u5934\u6ce8\u610f\u529b; 4. \u5c06\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u66ff\u6362\u4e3a SwiGLU FFN; 5. \u4f7f\u7528 Flash Attention \u8ba1\u7b97\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b; 6. \u4f7f\u7528\u548c LLaMA \u76f8\u540c\u7684\u5206\u8bcd\u5668.</p> <p>Existing LLMs use the same configuration for each Transformer layer in the model, resulting in a uniform allocation of parameters across layers. Unlike these models, each Transformer layer in ==OpenELM== has a different configuration (e.g., number of heads and feed forward network dimension), resulting in variable number of parameters in each layer of the model. This lets ==OpenELM== to better utilize the available parameter budget for achieving higher accuracies. We implement this non-uniform allocation of parameters across layers using layer-wise scaling (also referred as block-wise scaling in [30]).</p> <p>\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u6bcf\u4e2a Transformer \u5c42\u90fd\u4f7f\u7528\u76f8\u540c\u7684\u914d\u7f6e, \u4f7f\u5f97\u53c2\u6570\u5206\u914d\u5728\u5404\u5c42\u4e0a\u662f\u5747\u5300\u7684. \u548c\u8fd9\u4e9b\u6a21\u578b\u4e0d\u540c, OpenELM \u4e2d\u6bcf\u4e2a Transformer \u5c42\u90fd\u6709\u4e0d\u540c\u914d\u7f6e (\u4f8b\u5982, \u6ce8\u610f\u529b\u5934\u6570\u548c\u524d\u9988\u7f51\u7edc\u7ef4\u5ea6), \u4f7f\u5f97\u6a21\u578b\u6bcf\u5c42\u90fd\u5177\u6709\u4e0d\u540c\u53c2\u6570\u6570\u91cf. \u8fd9\u4f7f\u5f97 OpenELM \u80fd\u591f\u66f4\u597d\u5730\u5229\u7528\u53ef\u7528\u53c2\u6570\u9884\u7b97\u5b9e\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u7387. \u6211\u4eec\u4f7f\u7528\u9010\u5c42\u7f29\u653e (\u4e5f\u79f0\u4e3a\u5757\u7ea7\u7f29\u653e) \u6765\u5b9e\u73b0\u975e\u5747\u5300\u5206\u914d\u53c2\u6570.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#layer-wise-scaling","title":"Layer-Wise Scaling\u00b7\u9010\u5c42\u7f29\u653e","text":"<p>A standard Transformer layer is composed of multi-head attention (MHA) and feed-forward network (FFN). For non-uniform allocation of parameters in the Transformer layer, we adjust the number of attention heads and the FFN multiplier in each Transformer layer.</p> <p>\u4e00\u4e2a\u6807\u51c6\u7684 Transformer \u5c42\u7531\u591a\u5934\u6ce8\u610f\u529b (MHA) \u548c\u524d\u9988\u7f51\u7edc (FFN) \u7ec4\u6210. \u4e3a\u4e86\u5b9e\u73b0\u975e\u5747\u5300\u5206\u914d\u53c2\u6570, \u6211\u4eec\u8c03\u6574\u6bcf\u4e2a Transformer \u5c42\u7684\u6ce8\u610f\u529b\u5934\u6570\u548c FFN \u4e58\u6570.</p> <p>Assume that the standard Transformer model with uniform parameter allocation has $N$ Transformer layers and the dimensionality of the input to each layer is $d_{model}$. The MHA has $n_h$ heads and dimension of each head is $d_h=\\dfrac{d_{model}}{n_h}$. Also, the hidden dimension for FFN is $d_{FFN}= m\\cdot d_{model}$, where $m$ is a scalar FFN multiplier.</p> <p>\u5047\u8bbe\u5747\u5300\u53c2\u6570\u5206\u914d\u7684\u6807\u51c6\u7684 Transformer \u6a21\u578b\u6709 $N$ \u4e2a Transformer \u5c42, \u8f93\u5165\u5230\u6bcf\u4e2a\u5c42\u7684\u7ef4\u5ea6\u4e3a $d_{model}$. \u591a\u5934\u6ce8\u610f\u529b\u6709 $n_h$ \u4e2a\u6ce8\u610f\u529b\u5934, \u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u7684\u7ef4\u5ea6\u4e3a $d_h=\\dfrac{d_{model}}{n_h}$. \u524d\u9988\u7f51\u7edc\u7684\u9690\u85cf\u5c42\u7ef4\u5ea6\u4e3a $d_{FFN}= m\\cdot d_{model}$, \u5176\u4e2d $m$ \u662f\u6807\u91cf\u7684 FFN \u4e58\u6570.</p> <p>We introduce parameters $\\alpha$ and $\\beta$ to scale the number of attention heads $n_h$ and FFN multiplier $m$ per layer respectively. For the $i$-th layer, $n_h$ and $m$ are computed as </p> <p>\u6211\u4eec\u5f15\u5165\u53c2\u6570 $\\alpha$ \u548c $\\beta$ \u6765\u5206\u522b\u7f29\u653e\u6bcf\u4e2a\u5c42\u7684\u6ce8\u610f\u529b\u5934\u6570 $n_h$ \u548c\u524d\u9988\u7f51\u7edc\u4e58\u6570 $m$. \u5bf9\u4e8e\u7b2c $i$ \u5c42, $n_h$ \u548c $m$ \u8ba1\u7b97\u5982\u4e0b:</p> <p>$$   n_h^i = \\dfrac{\\alpha^i\\cdot d_{model}}{d_h},\\quad m^i = \\beta^i $$</p> <p>where</p> <p>\u5176\u4e2d</p> <p>$$   \\alpha^i = \\alpha_{min} + \\dfrac{\\alpha_{max} - \\alpha_{min}}{N-1}\\cdot i $$</p> <p>$$   \\beta^i = \\beta_{min} + \\dfrac{\\beta_{max} - \\beta_{min}}{N-1}\\cdot i $$</p> <p>$$   0\\leq i &lt;N $$</p> <p>Here, $\\alpha_{min}$ and $\\alpha_{max}$ are the hyper-parameters that allow us to scale the attention heads. Similarly, $\\beta_{min}$ and $\\beta_{max}$ let us to vary the width of FFN layers. Therefore, varying the configuration of standard Transformer layers using $\\alpha$ and $\\beta$ results in non-uniform allocation of parameters in the model. Note, setting $\\alpha_{min}=\\alpha_{max}= 1.0$ and $m_i= m$ produces the standard uniform Transformer model.</p> <p>\u8fd9\u91cc\u7684 $\\alpha_{min}$ \u548c $\\alpha_{max}$ \u662f\u5141\u8bb8\u6211\u4eec\u7f29\u653e\u6ce8\u610f\u529b\u5934\u7684\u8d85\u53c2\u6570. \u7c7b\u4f3c\u5730, $\\beta_{min}$ \u548c $\\beta_{max}$ \u5141\u8bb8\u6211\u4eec\u8c03\u6574 FFN \u5c42\u7684\u5bbd\u5ea6. \u56e0\u6b64, \u4f7f\u7528 $\\alpha$ \u548c $\\beta$ \u8c03\u6574\u6807\u51c6 Transformer \u5c42\u7684\u914d\u7f6e, \u4f7f\u5f97\u6a21\u578b\u4e2d\u7684\u53c2\u6570\u5206\u914d\u4e0d\u5747\u5300. \u6ce8\u610f, \u8bbe\u7f6e $\\alpha_{min}=\\alpha_{max}= 1.0$ \u548c $m_i= m$ \u5c06\u5f97\u5230\u6807\u51c6\u5747\u5300\u5206\u914d\u53c2\u6570\u7684 Transformer \u6a21\u578b.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#22pre-training-data","title":"2.2.Pre-Training Data\u00b7\u9884\u8bad\u7ec3\u6570\u636e","text":"<p>For pre-training, we use public datasets. Specifically, our pre-training dataset contains RefinedWeb [35], deduplicated PILE [15], a subset of RedPajama [11], and a subset of Dolma v1.6 [42], totaling approximately 1.8 trillion tokens. These details are also summarized in Tab.02.</p> <p>\u5bf9\u4e8e\u9884\u8bad\u7ec3, \u6211\u4eec\u4f7f\u7528\u516c\u5f00\u6570\u636e\u96c6. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5305\u62ec RefinedWeb, \u53bb\u91cd\u7684 PILE, RedPajama \u7684\u5b50\u96c6, Dolma v1.6 \u7684\u5b50\u96c6, \u603b\u8ba1\u7ea6 1.8 \u4e07\u4ebf\u4e2a Token. \u8fd9\u4e9b\u7ec6\u8282\u5728\u8868\u683c 02 \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3.</p> Source Subset Tokens RefinedWeb 665 B RedPajama GithubBooksArXivWikipediaStackExchangeC4 59 B26 B28 B24 B20 B175 B PILE 207 B Dolma The StackRedditPeS2oProject GutenbergWikipedia + Wikibooks 411 B89 B70 B6 B4.3B","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#on-the-fly-tokenization-data-filtering","title":"On-the-Fly Tokenization &amp; Data Filtering\u00b7\u5373\u65f6\u5206\u8bcd\u548c\u6570\u636e\u8fc7\u6ee4","text":"<p>Unlike previous approaches that utilize pre-tokenized data [5,17], we filter and tokenize text data on-the-fly. This facilitates seamless experimentation with various tokenizers, thereby significantly simplifying prototyping and research endeavors. In our experiments, we use the same tokenizer as used in LLaMA.</p> <p>\u4e0e\u4e4b\u524d\u4f7f\u7528\u9884\u5206\u8bcd\u6570\u636e\u7684\u65b9\u6cd5\u4e0d\u540c, \u6211\u4eec\u8fc7\u6ee4\u5e76\u5373\u65f6\u5206\u8bcd\u6587\u672c\u6570\u636e. \u8fd9\u6837\u53ef\u4ee5\u4fc3\u8fdb\u4e0d\u540c\u5206\u8bcd\u5668\u7684\u65e0\u7f1d\u5b9e\u9a8c, \u4ece\u800c\u5927\u5927\u7b80\u5316\u539f\u578b\u8bbe\u8ba1\u548c\u7814\u7a76\u5de5\u4f5c. \u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u91c7\u7528 LLaMA \u7684\u5206\u8bcd\u5668.</p> <p>To filter out low-length sequences, we apply two filtering methods. The first method operates at the character-level, checking if the number of characters in the sequence is below a specified threshold. The second method operates at the token-level, where it examines whether the sequence contains fewer tokens than a specified threshold. Sequences that are shorter than either of these thresholds are skipped. In our experiments, we use 200 characters and 256 tokens as character and token-level filtering thresholds.</p> <p>\u4e3a\u4e86\u8fc7\u6ee4\u6389\u4f4e\u957f\u5ea6\u7684\u5e8f\u5217, \u6211\u4eec\u91c7\u7528\u4e24\u79cd\u8fc7\u6ee4\u65b9\u6cd5. \u7b2c\u4e00\u79cd\u65b9\u6cd5\u5728\u5b57\u7b26\u7ea7\u522b\u4e0a\u8fdb\u884c, \u68c0\u67e5\u5e8f\u5217\u4e2d\u5b57\u7b26\u7684\u6570\u91cf\u662f\u5426\u4f4e\u4e8e\u6307\u5b9a\u9608\u503c. \u7b2c\u4e8c\u79cd\u65b9\u6cd5\u5728 Token \u7ea7\u522b\u4e0a\u8fdb\u884c, \u5b83\u68c0\u67e5\u5e8f\u5217\u4e2d Token \u7684\u6570\u91cf\u662f\u5426\u5305\u542b\u5c11\u4e8e\u6307\u5b9a\u9608\u503c. \u5982\u679c\u5e8f\u5217\u957f\u5ea6\u4f4e\u4e8e\u8fd9\u4e24\u79cd\u9608\u503c\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a, \u5219\u8df3\u8fc7\u8be5\u5e8f\u5217. \u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u4f7f\u7528 200 \u4e2a\u5b57\u7b26\u548c 256 \u4e2a Token \u4f5c\u4e3a\u5b57\u7b26\u7ea7\u522b\u548c Token \u7ea7\u522b\u7684\u8fc7\u6ee4\u9608\u503c.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#23training-details","title":"2.3.Training Details\u00b7\u8bad\u7ec3\u7ec6\u8282","text":"<p>We train ==OpenELM== variants for 350k iterations (or training steps) using CoreNet (formerly CVNets [29]). We use AdamW as an optimizer. We use a cosine learning rate schedule [27], with warm up of 5k iterations, and decay the final learning rate down to 10% of maximum learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We train four variants of ==OpenELM== (270M, 450M, 1.1B, and 3B), and for some, we use FSDP [56] and activation checkpointing [8]. Please refer to Appendix A for additional pre-training details.</p> <p>\u6211\u4eec\u4f7f\u7528 CoreNet (\u524d\u8eab\u4e3a CVNets) \u8bad\u7ec3 350k \u6b21\u8fed\u4ee3/\u8bad\u7ec3\u6b65\u6570\u5f97\u5230 OpenELM \u53d8\u4f53. \u6211\u4eec\u4f7f\u7528 AdamW \u4f5c\u4e3a\u4f18\u5316\u5668. \u6211\u4eec\u4f7f\u7528\u4f59\u5f26\u5b66\u4e60\u7387\u8c03\u5ea6, \u5176\u4e2d\u4e94\u5343\u6b21\u8fed\u4ee3\u9884\u70ed, \u7136\u540e\u8870\u51cf\u6700\u7ec8\u5b66\u4e60\u7387\u5230\u6700\u5927\u5b66\u4e60\u7387\u7684 10%. \u6211\u4eec\u4f7f\u7528\u6743\u91cd\u8870\u51cf 0.1 \u548c\u68af\u5ea6\u88c1\u526a 1.0. \u6211\u4eec\u8bad\u7ec3\u56db\u4e2a OpenELM \u53d8\u4f53 (270M, 450M, 1.1B, 3B), \u5176\u4e2d\u4e00\u4e9b\u4f7f\u7528 FSDP \u548c\u6fc0\u6d3b\u68c0\u67e5\u70b9. \u8bf7\u53c2\u9605\u9644\u5f55 A (\u8868\u683c 09) \u83b7\u53d6\u66f4\u591a\u9884\u8bad\u7ec3\u7ec6\u8282.</p> <p></p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#24evaluation-details","title":"2.4.Evaluation Details\u00b7\u8bc4\u4f30\u7ec6\u8282","text":"<p>Following previous works, we evaluate the performance across different tasks using LM Evaluation Harness [16]:</p> <p>\u9075\u5faa\u4e4b\u524d\u7684\u5de5\u4f5c, \u6211\u4eec\u4f7f\u7528 LM Evaluation Harness [16] \u8bc4\u4f30\u5728\u4e0d\u540c\u4efb\u52a1\u7684\u6027\u80fd:</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#standard-zero-shot-tasks","title":"Standard Zero-Shot Tasks\u00b7\u6807\u51c6\u96f6\u6837\u672c\u4efb\u52a1","text":"<p>We consider 7 standard common-sense reasoning tasks: ARC easy and challenge [10], BoolQ [9], HellaSwag [52], PIQA [6], SciQ [49], and WinoGrande [39].</p> <p>\u6211\u4eec\u8003\u8651\u4e03\u4e2a\u6807\u51c6\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1: - ARC easy - ARC challenge - BoolQ - HellaSwag - PIQA - SciQ - WinoGrande</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#openllm-leaderboard-tasksopenllm","title":"OpenLLM Leaderboard Tasks\u00b7OpenLLM \u6392\u884c\u699c\u4efb\u52a1","text":"<p>We use 5 tasks from OpenLLM leaderboard [4]: ARC challenge, HellaSwag, MMLU [20], TruthfulQA [24], and WinoGrande.</p> <p>\u6211\u4eec\u4f7f\u7528 OpenLLM \u6392\u884c\u699c\u7684\u4e94\u4e2a\u4efb\u52a1: - ARC challenge - HellaSwag - MMLU - TruthfulQA - WinoGrande</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#llm360-leaderboard-tasksllm360","title":"LLM360 Leaderboard Tasks\u00b7LLM360 \u6392\u884c\u699c\u4efb\u52a1","text":"<p>We use 7 tasks from LLM360 leaderboard [26] for evaluation: ARC challenge, CrowS-Pairs (English version) [32], HellaSwag, WinoGrande, MMLU, PIQA, and RACE [23].</p> <p>\u6211\u4eec\u4f7f\u7528 LLM360 \u6392\u884c\u699c\u7684\u4e03\u4e2a\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30: - ARC challenge - CrowS-Pairs (\u82f1\u6587\u7248\u672c) - HellaSwag - WinoGrande - MMLU - PIQA - RACE</p> <p>These evaluation frameworks, built on top of LM Evaluation Harness, allows us to comprehensively evaluate ==OpenELM== in terms of reasoning (e.g., ARC-c, HellaSwag, and PIQA), knowledge understanding (e.g., MMLU and RACE), and misinformation &amp; bias (e.g., TruthfulQA and CrowS-Pairs).</p> <p>\u8fd9\u4e9b\u5efa\u7acb\u5728 LM Evaluation Harness \u4e4b\u4e0a\u7684\u8bc4\u4f30\u6846\u67b6, \u5141\u8bb8\u6211\u4eec\u5168\u9762\u8bc4\u4f30 OpenELM \u5728\u63a8\u7406 (ARC-c, HellaSwag, PIQA), \u77e5\u8bc6\u7406\u89e3 (MMLU \u548c RACE) \u548c\u8bef\u5bfc\u6027\u4fe1\u606f\u548c\u504f\u89c1 (TruthfulQA \u548c CrowS-Pairs) \u65b9\u9762\u7684\u80fd\u529b.</p> <p>While there may be some overlap in tasks among these frameworks, they primarily differ in the few-shot settings, as outlined in Tab.03.</p> <p>\u5c3d\u7ba1\u8fd9\u4e9b\u6846\u67b6\u4e2d\u7684\u4efb\u52a1\u53ef\u80fd\u5b58\u5728\u91cd\u53e0, \u4f46\u5b83\u4eec\u4e3b\u8981\u533a\u522b\u5728\u4e8e\u5c11\u6837\u672c\u8bbe\u7f6e, \u5982\u8868\u683c 03 \u6240\u793a.</p> <p></p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#3experimental-results","title":"3.Experimental Results\u00b7\u5b9e\u9a8c\u7ed3\u679c","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#pre-training-results","title":"Pre-Training Results\u00b7\u9884\u8bad\u7ec3\u7ed3\u679c","text":"<p>We evaluate the performance of ==OpenELM== on zero-shot and few-shot settings (Tab.03). We compare ==OpenELM== with publicly available LLMs, namely PyThia [5], Cerebras-GPT [14], TinyLlama [54], OpenLM [18], MobiLlama [44], and OLMo [17]. The works most closely related to ours are MobiLlama and OLMo. These models are trained on comparable dataset mixtures, with similar or larger number of pre-training tokens.</p> <p>\u6211\u4eec\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30 OpenELM \u7684\u6027\u80fd (\u8868\u683c 03). \u6211\u4eec\u6bd4\u8f83 OpenELM \u4e0e\u516c\u5f00\u53ef\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b, \u5305\u62ec PyThia, Cerebras-GPT, TinyLlama, OpenLM, MobiLlama, \u548c OLMo. \u4e0e\u6211\u4eec\u7684\u5de5\u4f5c\u6700\u63a5\u8fd1\u7684\u6a21\u578b\u662f MobiLlama \u548c OLMo. \u8fd9\u4e9b\u6a21\u578b\u90fd\u5728\u89c4\u6a21\u5dee\u8ddd\u4e0d\u5927\u7684\u6df7\u5408\u6570\u636e\u96c6\u4e2d\u8fdb\u884c\u8bad\u7ec3, \u5177\u6709\u76f8\u4f3c\u6216\u66f4\u5927\u7684\u9884\u8bad\u7ec3 Token \u6570\u91cf.</p> <p>In Fig.01, the accuracy of ==OpenELM== is plotted against training iterations for 7 standard zero-shot tasks. We observe an overall increase in accuracy with longer training durations across most tasks. Additionally, the checkpoint obtained by averaging the last five checkpoints, collected at intervals of 5000 iterations, demonstrates comparable or slightly better accuracy compared to the final checkpoint obtained after 350k iterations. This improvement is likely due to noise reduction through weight averaging. Consequently, we use the averaged checkpoint for our main evaluations in Tab.04, instruction tuning experiments in Tab.05, and parameter-efficient tuning experiments in Tab.06.</p> <p></p> <p>\u5728\u56fe 01 \u4e2d, \u6211\u4eec\u7ed8\u5236\u4e86 OpenELM \u5728 7 \u4e2a\u6807\u51c6\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u5ea6\u968f\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u7684\u53d8\u5316\u66f2\u7ebf. \u6211\u4eec\u89c2\u5bdf\u5230, \u5728\u5927\u591a\u6570\u4efb\u52a1\u4e2d, \u8bad\u7ec3\u6301\u7eed\u65f6\u95f4\u8d8a\u957f, \u51c6\u786e\u5ea6\u8d8a\u9ad8. \u6b64\u5916, \u6211\u4eec\u6536\u96c6\u7684\u6700\u540e\u4e94\u4e2a\u68c0\u67e5\u70b9\u7684\u5e73\u5747\u503c, \u95f4\u9694\u4e3a 5000 \u6b21\u8fed\u4ee3, \u4e0e 350k \u6b21\u8fed\u4ee3\u540e\u83b7\u5f97\u7684\u6700\u7ec8\u68c0\u67e5\u70b9\u7684\u51c6\u786e\u5ea6\u76f8\u5f53\u6216\u7a0d\u597d. \u8fd9\u79cd\u6539\u8fdb\u53ef\u80fd\u662f\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u800c\u5bfc\u81f4\u7684\u566a\u58f0\u964d\u4f4e. \u56e0\u6b64, \u6211\u4eec\u91c7\u7528\u5e73\u5747\u68c0\u67e5\u70b9\u7528\u4e8e\u8868\u683c 04 \u7684\u4e3b\u8981\u8bc4\u4f30, \u8868\u683c 05 \u7684\u6307\u4ee4\u8c03\u4f18\u5b9e\u9a8c, \u4ee5\u53ca\u8868\u683c 06 \u7684\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\u5b9e\u9a8c.</p> <p>The results in Tab.04 span across various evaluation frameworks, and highlights ==OpenELM==\u2019s effectiveness over existing methods.</p> <p>\u8868\u683c 04 \u4e2d\u7684\u7ed3\u679c\u6db5\u76d6\u4e86\u5404\u79cd\u8bc4\u4f30\u6846\u67b6, \u5e76\u7a81\u51fa\u4e86 OpenELM \u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6709\u6548\u6027.</p> <p></p> <p>For instance, an ==OpenELM== variant with 1.1 billion parameters achieves 1.28% (Tab.04a), 2.36% (Tab.04b), and 1.72% (Tab.04c) higher accuracy compared to OLMo with 1.2 billion parameters. Remarkably, ==OpenELM== achieves this level of accuracy while using 2\u00d7 less pretraining data.</p> <p>\u4f8b\u5982 1.1 \u4ebf\u53c2\u6570\u7684 OpenELM \u53d8\u4f53\u76f8\u5bf9\u4e8e 1.2 \u4ebf\u53c2\u6570\u7684 OLMo, \u51c6\u786e\u5ea6\u63d0\u9ad8\u4e86 1.28%, 2.36%, \u548c 1.72% \u70b9. \u9700\u8981\u6ce8\u610f\u7684\u662f, OpenELM \u53d6\u5f97\u4e86\u5982\u6b64\u9ad8\u7684\u51c6\u786e\u5ea6, \u800c\u4ec5\u4f7f\u7528\u5176\u4e00\u534a\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u6570\u636e.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#instruction-tuning-results","title":"Instruction Tuning Results\u00b7\u6307\u4ee4\u8c03\u4f18\u7ed3\u679c","text":"<p>We use the cleaned variant of UltraFeedback [3, 12] dataset that consists of 60k prompts for instruction tuning. We do instruction tuning using Alignment Handbook library [47]. For optimization, we use either the statistical rejection sampling method [25] or the direct preference optimization method [37]. These sampling method details along with other hyper-parameters and fine-tuning details are given in Appendix B.</p> <p>\u6211\u4eec\u4f7f\u7528 UltraFeedback \u6570\u636e\u96c6\u7684\u6e05\u6d17\u7248\u672c, \u5305\u542b\u516d\u4e07\u4e2a\u63d0\u793a\u7528\u4e8e\u6307\u4ee4\u8c03\u4f18. \u6211\u4eec\u4f7f\u7528 Alignment Handbook \u5e93\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18. \u5bf9\u4e8e\u4f18\u5316, \u6211\u4eec\u91c7\u7528\u7edf\u8ba1\u62d2\u7edd\u91c7\u6837\u65b9\u6cd5\u6216\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5. \u8fd9\u4e9b\u91c7\u6837\u65b9\u6cd5\u7684\u8be6\u7ec6\u4fe1\u606f\u4ee5\u53ca\u5176\u4ed6\u8d85\u53c2\u6570\u548c\u5fae\u8c03\u7ec6\u8282, \u8bf7\u53c2\u9605\u9644\u5f55 B.</p> <p>We conducted a grid search to determine optimal values for the learning rate and training epochs.  For the learning rate, we explored values in the range of [2e-5, 3e-5, 5e-5, 8e-5, 1e-4], while for training epochs, we investigated the range of [3, 5, 8, 10].  The final recipe selected is the one that yielded the highest average accuracy across various tasks as presented in Tab.03a and Tab.03c.</p> <p>\u6211\u4eec\u91c7\u7528\u7f51\u683c\u641c\u7d22\u786e\u5b9a\u6700\u4f73\u7684\u5b66\u4e60\u7387\u548c\u8bad\u7ec3 Epoch \u6570. \u5bf9\u4e8e\u5b66\u4e60\u7387, \u6211\u4eec\u63a2\u7d22\u8303\u56f4\u4e3a [2e-5, 3e-5, 5e-5, 8e-5, 1e-4], \u800c\u5bf9\u4e8e\u8bad\u7ec3 Epoch, \u6211\u4eec\u8c03\u67e5\u8303\u56f4\u4e3a [3, 5, 8, 10]. \u6700\u7ec8\u7684\u9009\u62e9\u662f\u90a3\u4e9b\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u83b7\u5f97\u5e73\u5747\u51c6\u786e\u5ea6\u6700\u9ad8\u7684\u914d\u7f6e.</p> <p>We fine-tune all the models with BFloat16 as a data type. We use activation checkpointing along with gradient accumulation with a step size of two.  We use the AdamW optimizer with default beta values.  We use the cosine learning rate scheduler with a warm-up ratio of 0.1, and we set the weight decay to 0 and loss temperature beta to 0.01. We set the maximum context length to 1024 and maximum prompt length to 512. Other hyper-parameters are included in Tab.10.</p> <p>\u6211\u4eec\u4f7f\u7528 BFloat16 \u4f5c\u4e3a\u6570\u636e\u7c7b\u578b\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03. \u6211\u4eec\u4f7f\u7528\u6fc0\u6d3b\u68c0\u67e5\u70b9\u548c\u68af\u5ea6\u7d2f\u79ef, \u6b65\u957f\u4e3a 2. \u6211\u4eec\u4f7f\u7528 AdamW \u4f18\u5316\u5668, \u5e76\u4f7f\u7528\u9ed8\u8ba4\u7684\u53c2\u6570. \u6211\u4eec\u4f7f\u7528\u4f59\u5f26\u5b66\u4e60\u7387\u8c03\u5ea6, \u9884\u70ed\u7387\u4e3a 0.1, \u6743\u91cd\u8870\u51cf\u4e3a 0, \u635f\u5931\u6e29\u5ea6\u53c2\u6570\u4e3a 0.01. \u6211\u4eec\u8bbe\u7f6e\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e3a 1024, \u6700\u5927\u63d0\u793a\u957f\u5ea6\u4e3a 512. \u5176\u4ed6\u8d85\u53c2\u6570\u8bf7\u53c2\u9605\u8868\u683c 10.</p> <p></p> <p>Tab.05 shows that instruction tuning consistently improves ==OpenELM==\u2019s average accuracy by 1-2% across different evaluation frameworks.</p> <p>\u8868\u683c 05 \u5c55\u793a\u4e86\u6307\u4ee4\u8c03\u4f18\u5728\u4e0d\u540c\u8bc4\u4f30\u6846\u67b6\u4e0b\u7684\u5e73\u5747\u51c6\u786e\u5ea6\u63d0\u9ad8 1-2% \u5de6\u53f3.</p> <p></p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#parameter-efficient-fine-tuning-peft-results","title":"Parameter-Efficient Fine-Tuning (PEFT) Results\u00b7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7ed3\u679c","text":"<p>We use the CommonSense reasoning training and evaluation setup [22]. This setup provides 170k training samples across 8 multiple-choice datasets for PEFT studies with different methods, including LoRA [21] and DoRA [51]. We integrate ==OpenELM== with these methods, and finetune the resulting model for three epochs using 8 NVIDIA H100 GPUs. Tab.06 shows that PEFT methods can be applied to ==OpenELM==. LoRA and DoRA deliver similar accuracy on average across the given CommonSense reasoning datasets.</p> <p>\u6211\u4eec\u4f7f\u7528 CommonSense \u63a8\u7406\u8bad\u7ec3\u548c\u8bc4\u4f30\u8bbe\u7f6e. \u8be5\u8bbe\u7f6e\u63d0\u4f9b\u4e86 170k \u8bad\u7ec3\u6837\u672c, 8 \u4e2a\u591a\u9009\u6570\u636e\u96c6, \u7528\u4e8e PEFT \u7814\u7a76, \u5305\u62ec LoRA \u548c DoRA. \u6211\u4eec\u5c06 OpenELM \u4e0e\u8fd9\u4e9b\u65b9\u6cd5\u96c6\u6210, \u5e76\u4f7f\u7528 8 \u4e2a NVIDIA H100 GPU \u5fae\u8c03\u7ed3\u679c\u6a21\u578b, \u5fae\u8c03 3 \u4e2a Epoch. \u8868\u683c 06 \u5c55\u793a\u4e86 PEFT \u65b9\u6cd5\u53ef\u4ee5\u5e94\u7528\u4e8e OpenELM. LoRA \u548c DoRA \u5728\u7ed9\u5b9a\u7684 CommonSense \u63a8\u7406\u6570\u636e\u96c6\u4e0a, \u5e73\u5747\u51c6\u786e\u5ea6\u76f8\u5f53.</p> <p></p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#4benchmarking","title":"4.Benchmarking\u00b7\u57fa\u51c6\u6d4b\u8bd5","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#hardware","title":"Hardware\u00b7\u786c\u4ef6","text":"<p>We benchmark on modern, consumer-grade hardware with BFloat16 as the data type. Specifically, CUDA benchmarks were performed on a workstation with an Intel i9-13900KF CPU, equipped with 64 GB of DDR5-4000 DRAM, and an NVIDIA RTX 4090 GPU with 24 GB of VRAM, running Ubuntu 22.04. PyTorch v2.2.2 [34] was used, with the most recent versions of models and the associated libraries. HuggingFace Transformers v4.39.3 [50] was used to benchmark HuggingFace models. We did not use Torch Inductor for model compilation.</p> <p>\u6211\u4eec\u5728\u73b0\u4ee3\u6d88\u8d39\u8005\u7ea7\u786c\u4ef6\u4e0a\u91c7\u7528 BFloat16 \u4f5c\u4e3a\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5. \u5177\u4f53\u6765\u8bf4, CUDA \u57fa\u51c6\u662f\u5728\u4e00\u53f0\u5de5\u4f5c\u7ad9\u4e0a\u8fdb\u884c\u7684, \u5176 CPU \u4e3a Intel i9-13900KF, \u5185\u5b58\u4e3a 64 GB DDR5-4000, GPU \u4e3a NVIDIA RTX 4090, \u64cd\u4f5c\u7cfb\u7edf\u4e3a Ubuntu 22.04. \u6211\u4eec\u4f7f\u7528 PyTorch v2.2.2, \u5e76\u4f7f\u7528\u6700\u65b0\u7248\u672c\u7684\u6a21\u578b\u548c\u76f8\u5173\u5e93. \u6211\u4eec\u4f7f\u7528 HuggingFace Transformers v4.39.3 \u4f5c\u4e3a HuggingFace \u6a21\u578b\u57fa\u51c6. \u6211\u4eec\u6ca1\u6709\u4f7f\u7528 Torch Inductor \u8fdb\u884c\u6a21\u578b\u7f16\u8bd1.</p> <p>To benchmark ==OpenELM== models on the Apple silicon, we used an Apple MacBook Pro with an M2 Max system-on-chip and 64GiB of RAM, running macOS 14.4.1. We ported the code and the weights of ==OpenELM== to Apple MLX v0.10.0 [19]. To maximize the throughput, lazy evaluation was used in MLX with 8 tokens evaluated at a time.</p> <p>\u4e3a\u4e86\u5728 Apple silicon \u4e0a\u5bf9 OpenELM \u6a21\u578b\u8fdb\u884c\u57fa\u51c6, \u6211\u4eec\u4f7f\u7528\u4e00\u53f0 Apple MacBook Pro, \u5176\u7cfb\u7edf\u82af\u7247\u4e3a M2 Max, \u5185\u5b58\u4e3a 64GiB, \u64cd\u4f5c\u7cfb\u7edf\u4e3a macOS 14.4.1. \u6211\u4eec\u5c06 OpenELM \u4ee3\u7801\u548c\u6743\u91cd\u79fb\u690d\u5230 Apple MLX v0.10.0. \u4e3a\u4e86\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u541e\u5410\u91cf, \u6211\u4eec\u5728 MLX \u4e2d\u4f7f\u7528\u60f0\u6027\u8bc4\u4f30, \u4e00\u6b21\u8bc4\u4f30 8 \u4e2a Token.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#evaluation","title":"Evaluation\u00b7\u8bc4\u4f30","text":"<p>We provide two separate measurements for token throughput (measured in terms of tokens processed per second): (1) prompt processing (pre-fill), and (2) token generation. Additionally, we also report the total combined throughput. We benchmark all models sequentially, and execute one full \u201cdry run\u201d generating 1024 tokens for the first model, since we found that this significantly increases the throughput of generation for subsequent models. Before measurement for each individual model, we warm up the model by executing a single forward pass to allow the frameworks to perform further auto-tuning, if any. In all experiments, we use key-value caching and generate 1024 tokens in addition to the prompt tokens in all tests. Static key-value cache was used whenever supported. The same prompt was used for all runs, resulting in prompt lengths of 35-36 tokens (depending on the tokenizer).</p> <p>\u6211\u4eec\u63d0\u4f9b\u4e24\u4e2a\u5355\u72ec\u7684\u5ea6\u91cf\u7528\u4e8e Token \u541e\u5410\u91cf (\u4ee5\u6bcf\u79d2\u5904\u7406 Token \u4e3a\u5355\u4f4d): 1. \u63d0\u793a\u5904\u7406 (\u9884\u586b\u5145) 2. Token \u751f\u6210</p> <p>\u6b64\u5916, \u6211\u4eec\u8fd8\u62a5\u544a\u603b\u4f53\u541e\u5410\u91cf. \u6211\u4eec\u6309\u987a\u5e8f\u5bf9\u6240\u6709\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5, \u5e76\u5bf9\u7b2c\u4e00\u4e2a\u6a21\u578b\u6267\u884c\u5b8c\u6574\u7684 \"\u5e72\u51c0\u8fd0\u884c\", \u56e0\u4e3a\u6211\u4eec\u53d1\u73b0, \u8fd9\u6837\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u540e\u7eed\u6a21\u578b\u751f\u6210\u7684\u541e\u5410\u91cf. \u5728\u5bf9\u6bcf\u4e2a\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u4e4b\u524d, \u6211\u4eec\u901a\u8fc7\u6267\u884c\u4e00\u6b21\u524d\u5411\u4f20\u64ad\u6765\u9884\u70ed\u6a21\u578b, \u4ee5\u4fbf\u6846\u67b6\u5b8c\u6210\u8fdb\u4e00\u6b65\u7684\u81ea\u52a8\u8c03\u4f18, \u5982\u679c\u6709. \u5728\u6240\u6709\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u90fd\u4f7f\u7528\u952e\u503c\u7f13\u5b58, \u5e76\u5728\u6240\u6709\u6d4b\u8bd5\u4e2d\u751f\u6210 1024 \u4e2a Token, \u800c\u63d0\u793a Token \u6570\u91cf\u53d6\u51b3\u4e8e Tokenizer. \u9759\u6001\u952e\u503c\u7f13\u5b58\u662f\u53ef\u9009\u7684. \u6240\u6709\u8fd0\u884c\u90fd\u662f\u7528\u76f8\u540c\u7684\u63d0\u793a, \u56e0\u6b64\u63d0\u793a\u957f\u5ea6\u4e3a 35-36 \u4e2a Token.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#results","title":"Results\u00b7\u7ed3\u679c","text":"<p>Tab.07a and Tab.07b shows the benchmarking results on GPU and MacBook Pro respectively. Despite ==OpenELM==\u2019s higher accuracy for a similar parameter count, we observe that it is slower than OLMo. While the primary focus of this study is reproducibility rather than inference performance, we did comprehensive profiling to understand the bottlenecks. Our analysis reveals that a significant portion of ==OpenELM==\u2019s processing time can be attributed to our naive implementation of RMSNorm (Tab.08). Specifically, naive RMSNorm implementation results in many individual kernel launches each of which processes a small input, rather than a launch of a single, fused kernel, as would be the case with e.g. LayerNorm. By replacing the naive RMSNorm with Apex\u2019s RMSNorm [33], we observe a notable increase in ==OpenELM==\u2019s throughput. However, a substantial performance gap persists compared to the models that use optimized LayerNorm, in part because (1) ==OpenELM== has 113 RMSNorm layers as compared to 33 LayerNorm layers in OLMo and (2) Apex\u2019s RMSNorm is not optimized for small inputs. To further illustrate the performance degradation attributable to RMSNorm, we replaced the LayerNorm in OLMo with RMSNorm, and observed a significant drop in generation throughput. In future work, we plan to explore optimization strategies to further improve the inference efficiency of ==OpenELM==.</p> <p>\u8868\u683c 07a \u548c\u8868\u683c 07b \u5206\u522b\u5c55\u793a\u4e86 GPU \u548c MacBook Pro \u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c. \u5c3d\u7ba1\u5728\u76f8\u4f3c\u7684\u53c2\u6570\u6570\u91cf\u4e0b OpenELM \u7684\u51c6\u786e\u5ea6\u66f4\u9ad8, \u4f46\u6211\u4eec\u89c2\u5bdf\u5230\u5b83\u6bd4 OLMo \u6162. \u5c3d\u7ba1\u672c\u7814\u7a76\u7684\u4e3b\u8981\u5173\u6ce8\u70b9\u662f\u53ef\u590d\u73b0\u6027\u800c\u4e0d\u662f\u63a8\u7406\u6027\u80fd, \u4f46\u6211\u4eec\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790, \u4ee5\u7406\u89e3\u74f6\u9888. \u6211\u4eec\u7684\u5206\u6790\u8868\u660e, OpenELM \u7684\u5927\u90e8\u5206\u5904\u7406\u65f6\u95f4\u90fd\u548c\u6211\u4eec\u5bf9 RMSNorm \u7684\u539f\u59cb\u5b9e\u73b0\u6709\u5173 (\u8868\u683c 08). \u5177\u4f53\u6765\u8bf4, \u539f\u59cb RMSNorm \u5b9e\u73b0\u4f7f\u5f97\u542f\u52a8\u8bb8\u591a\u5904\u7406\u5c0f\u8f93\u5165\u7684\u6838, \u800c\u4e0d\u662f\u542f\u52a8\u5355\u4e2a\u7684\u878d\u5408\u6838, \u5982 LayerNorm. \u901a\u8fc7\u5c06\u539f\u59cb RMSNorm \u66ff\u6362\u4e3a Apex \u7684 RMSNorm, \u6211\u4eec\u89c2\u5bdf\u5230 OpenELM \u7684\u541e\u5410\u91cf\u663e\u8457\u63d0\u9ad8. \u7136\u800c, \u548c\u4f7f\u7528 LayerNorm \u7684\u6a21\u578b\u76f8\u6bd4\u4ecd\u7136\u5b58\u5728\u5b9e\u8d28\u7684\u6027\u80fd\u5dee\u8ddd, \u90e8\u5206\u539f\u56e0\u662f (1) OpenELM \u6709 113 \u4e2a RMSNorm \u5c42, \u800c OLMo \u6709 33 \u4e2a LayerNorm \u5c42 (2) Apex \u7684 RMSNorm \u5e76\u672a\u9488\u5bf9\u5c0f\u8f93\u5165\u8fdb\u884c\u4f18\u5316. \u4e3a\u4e86\u8fdb\u4e00\u6b65\u8bf4\u660e RMSNorm \u5bfc\u81f4\u7684\u6027\u80fd\u9000\u5316, \u6211\u4eec\u5c06 OLMo \u4e2d\u7684 LayerNorm \u66ff\u6362\u4e3a RMSNorm, \u5e76\u89c2\u5bdf\u5230\u751f\u6210\u541e\u5410\u91cf\u663e\u8457\u4e0b\u964d. \u5728\u672a\u6765\u5de5\u4f5c\u4e2d, \u6211\u4eec\u8ba1\u5212\u63a2\u7d22\u4f18\u5316\u7b56\u7565, \u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8 OpenELM \u7684\u63a8\u7406\u6548\u7387.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#5conclusion","title":"5.Conclusion\u00b7\u7ed3\u8bba","text":"<p>This work releases ==OpenELM==, a decoder-only Transformer-based open language model. The ==OpenELM== uses a layer-wise scaling method for efficient parameter allocation within the Transformer model,resulting in improved accuracy compared to existing models. Additionally, we have made the entire framework open-source, including training logs, multiple checkpoints, pre-training configurations, and MLX inference code. This extensive release aims to empower and strengthen the open research community, facilitating future research efforts.</p> <p>\u672c\u9879\u5de5\u4f5c\u53d1\u5e03\u4e86 OpenELM, \u4e00\u4e2a\u57fa\u4e8e\u4ec5\u6709 Transformer \u89e3\u7801\u5668\u7684\u5f00\u6e90\u8bed\u8a00\u6a21\u578b. OpenELM \u4f7f\u7528\u9010\u5c42\u7f29\u653e\u65b9\u6cd5, \u6709\u6548\u5730\u5728 Transformer \u6a21\u578b\u4e2d\u5206\u914d\u53c2\u6570, \u4ece\u800c\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u6bd4, \u51c6\u786e\u5ea6\u5f97\u5230\u63d0\u9ad8. \u6b64\u5916, \u6211\u4eec\u5df2\u7ecf\u5c06\u6574\u4e2a\u6846\u67b6\u5f00\u6e90, \u5305\u62ec\u8bad\u7ec3\u65e5\u5fd7, \u591a\u4e2a\u68c0\u67e5\u70b9, \u9884\u8bad\u7ec3\u914d\u7f6e, \u548c MLX \u63a8\u7406\u4ee3\u7801. \u8fd9\u4e00\u8be6\u5c3d\u53d1\u5e03\u65e8\u5728\u6fc0\u53d1\u548c\u52a0\u5f3a\u5f00\u653e\u7814\u7a76\u793e\u533a, \u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u5de5\u4f5c.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/LLM/2024.04.22_OpenELM/#broader-impact","title":"Broader Impact\u00b7\u66f4\u5e7f\u6cdb\u7684\u5f71\u54cd","text":"<p>The release of ==OpenELM== models aims to empower and enrich the open research community by providing access to state-of-the-art language models. Trained on publicly available datasets, these models are made available without any safety guarantees. Consequently, there exists the possibility of these models producing outputs that are inaccurate, harmful, biased, or objectionable in response to user prompts. Thus, it is imperative for users and developers to undertake thorough safety testing and implement appropriate filtering mechanisms tailored to their specific requirements.</p> <p>OpenELM \u6a21\u578b\u7684\u53d1\u5e03\u65e8\u5728\u6fc0\u53d1\u548c\u4e30\u5bcc\u5f00\u653e\u7814\u7a76\u793e\u533a, \u63d0\u4f9b\u5bf9\u6700\u65b0\u8bed\u8a00\u6a21\u578b\u7684\u8bbf\u95ee. \u8fd9\u4e9b\u6a21\u578b\u5728\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3, \u65e0\u4efb\u4f55\u5b89\u5168\u4fdd\u8bc1. \u6240\u4ee5\u7528\u6237\u548c\u5f00\u53d1\u8005\u5e94\u8ba4\u771f\u6d4b\u8bd5\u8fd9\u4e9b\u6a21\u578b\u7684\u5b89\u5168\u6027, \u5e76\u6839\u636e\u5177\u4f53\u9700\u6c42\u5b9e\u73b0\u9002\u5f53\u7684\u8fc7\u6ee4\u673a\u5236. \u5bf9\u4e8e\u7528\u6237\u548c\u5f00\u53d1\u8005\u6765\u8bf4, \u4fdd\u8bc1\u6a21\u578b\u7684\u51c6\u786e\u6027, \u5b89\u5168\u6027, \u516c\u5e73\u6027, \u4ee5\u53ca\u4e0d\u53d7\u7528\u6237\u63d0\u793a\u7684\u5e72\u6270\u662f\u81f3\u5173\u91cd\u8981\u7684.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/","title":"TokSing","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: TokSing: Singing Voice Synthesis based on Discrete Tokens - \u4f5c\u8005:   - [Yuning Wu](../../Authors/Yuning_Wu.md)   - [Chunlei Zhang](../../Authors/Chunlei_Zhang.md)   - [Jiatong Shi](../../Authors/Jiatong_Shi.md)   - [Yuxun Tang](../../Authors/Yuxun_Tang.md)   - [Shan Yang](../../Authors/Shan_Yang.md)   - [Qin Jin](../../Authors/Qin_Jin.md) - \u673a\u6784:   - [\u4e2d\u56fd\u4eba\u6c11\u5927\u5b66](../../Institutions/RUC_\u4e2d\u56fd\u4eba\u6c11\u5927\u5b66.md)   - [\u817e\u8baf AI \u5b9e\u9a8c\u5ba4](../../Institutions/TecentAI.md)   - [CMU](../../Institutions/CMU_\u7f8e\u56fd\u5361\u5185\u57fa\u6885\u9686\u5927\u5b66.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.06.12 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.13 - \u53d1\u8868:   - [InterSpeech](../../Publications/InterSpeech.md) - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2406.08416)   - [DOI]()   - [Github]()   - [Demo]() - \u6807\u7b7e:   - [\u6b4c\u58f0\u5408\u6210](../../Tags/SingingVoiceSynthesis.md)   - [\u81ea\u76d1\u7763\u5b66\u4e60](../../Tags/Learning_Self-Supervised.md)   - [\u79bb\u6563\u8868\u793a](../../Tags/DiscreteRepresentation.md) - \u9875\u6570: 5 - \u5f15\u7528: 34 - \u88ab\u5f15: 0"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"\u539f\u6587  &gt; Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from **Self-Supervised Learning (SSL)** models. &gt; Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. &gt; However, when it comes to **Singing Voice Synthesis (SVS)**, achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. &gt; In this paper, we introduce ***TokSing***, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. &gt; We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. &gt; Extensive experiments demonstrate that our ***TokSing*** achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed.    <p>\u8fd1\u671f\u5728\u8bed\u97f3\u5408\u6210\u7684\u8fdb\u5c55\u663e\u793a, \u901a\u8fc7\u5229\u7528\u7531\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u7684\u79bb\u6563\u6807\u8bc6\u7b26\u53ef\u4ee5\u5e26\u6765\u663e\u8457\u7684\u597d\u5904. \u548c\u4f20\u7edf\u7684\u8fde\u7eed\u6885\u5c14\u9891\u8c31\u76f8\u6bd4, \u79bb\u6563\u6807\u8bc6\u7b26\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u80fd\u591f\u63d0\u4f9b\u66f4\u9ad8\u7684\u5b58\u50a8\u6548\u7387\u548c\u66f4\u5927\u7684\u53ef\u64cd\u4f5c\u6027. \u7136\u800c, \u6d89\u53ca\u5230\u6b4c\u58f0\u5408\u6210\u9886\u57df\u65f6, \u5229\u7528\u79bb\u6563\u6807\u8bc6\u7b26\u4ee5\u83b7\u5f97\u66f4\u9ad8\u6c34\u5e73\u7684\u65cb\u5f8b\u8868\u8fbe\u662f\u4e2a\u5de8\u5927\u7684\u6311\u6218. \u672c\u6587\u63d0\u51fa\u4e86 TokSing, \u4e00\u4e2a\u914d\u7f6e\u4e86\u6807\u8bc6\u7b26\u5f62\u5f0f\u5316\u5668\u7684\u79bb\u6563\u7684\u6b4c\u58f0\u5408\u6210\u7cfb\u7edf, \u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u6807\u8bc6\u7b26\u6df7\u5408. \u5728\u79bb\u6563\u5316\u8fc7\u7a0b\u4e2d\u89c2\u5bdf\u5230\u4e86\u65cb\u5f8b\u9000\u5316, \u8fd9\u63d0\u793a\u4e86\u6211\u4eec\u5c06\u65cb\u5f8b\u4fe1\u53f7\u548c\u79bb\u6563\u6807\u8bc6\u7b26\u76f8\u7ed3\u5408, \u5e76\u5728\u97f3\u4e50\u7f16\u7801\u5668\u4e2d\u52a0\u5165\u7279\u522b\u8bbe\u8ba1\u7684\u65cb\u5f8b\u589e\u5f3a\u7b56\u7565. \u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\u6211\u4eec\u7684 TokSing \u76f8\u6bd4\u6885\u5c14\u9891\u8c31\u57fa\u7ebf\u80fd\u8fbe\u5230\u66f4\u597d\u7684\u6027\u80fd, \u540c\u65f6\u5728\u4e2d\u95f4\u8868\u793a\u7a7a\u95f4\u6210\u672c\u548c\u6536\u655b\u901f\u7387\u8868\u73b0\u51fa\u4e86\u4f18\u52bf.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"\u539f\u6587  &gt; **Singing Voice Synthesis (SVS)** aims to generate vocal sounds given music scores with melody and lyrics. &gt; Traditional SVS systems ([Xiaoicesing](2020.06.11_Xiaoicesing.md); [Xiaoicesing2](2022.10.26_Xiaoicesing2.md); [DiffSinger](2021.05.06_Diffsinger.md); [Seq2Seq SVS with PE](../../Models/Singing_Voice/Seq2Seq_SVS_PE.md); [Singing-Tacotron](2022.02.16_Singing-Tacotron.md); [Bytesting](../../Models/Singing_Voice/Bytesting.md)) primarily focus on enhancing acoustic models to generate Mel spectrograms from scores, which are then converted into waveforms by vocoders ([HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md); [BigVGAN](../../Models/TTS3_Vocoder/2022.06.09_BigVGAN.md)). &gt; Recently, there has been a growing trend towards using discrete tokens, a representation with superior storage efficiency and controllability, for speech understanding and generation tasks ([Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning](), [Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS](), [SpeechGPT](../Speech_LLM/2023.05.18_SpeechGPT.md), [SPEAR-TTS](../../Models/Speech_LLM/2023.02.07_SPEAR-TTS.md)). &gt; Discrete tokens can be obtained from raw audio through vector quantization ([SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md); [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md); [DAC](../../Models/Speech_Neural_Codec/2023.06.11_Descript-Audio-Codec.md)) or generated by clustering ([Some Methods for Classification and Analysis of Multivariate Observations]()) on hidden embeddings of SSL models ([HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md); [Wav2Vec2.0](../../Models/Speech_Representaion/2020.06.20_Wav2Vec2.0.md); [WavLM](../../Models/Speech_Representaion/2021.10.26_WavLM.md); [VQ-Wav2Vec](../../Models/Speech_Representaion/2019.10.12_VQ-Wav2Vec.md)) pretrained on large-scale audios. &gt; Different from speech processing, singing adds the complexity of melodic expression on top of speech, requiring vocal sound that meets the musical score requirements and delivers high-quality listening experiences. &gt; Therefore, applying discrete tokens in SVS faces some unique challenges. &gt; Firstly, limited by copyright restrictions and strict recording environments, there is currently no dedicated singing SSL model, and existing SSL models contain scarce singing data ([Seq2Seq_SVS_PE](../../Models/Singing_Voice/Seq2Seq_SVS_PE.md); [SingAug](../../Models/Singing_Voice/SingAug.md); [ACE-Opencpop](../../Datasets/ACE-Opencpop.md)). &gt; Therefore, setting suitable tokens for singing synthesis remains challenging. &gt; Secondly, although lyrics are inherently discrete among the information encompassed within singing, the melody requires more refined expression, for example, the fundamental frequency of the same note can vary delicately between the frames it covers, especially in cases involving sustained notes, high pitches, vibratos, and other techniques requiring advanced vocal skills ([DAR-SVS](../../Models/Singing_Voice/DAR-SVS.md)). &gt; Therefore, there is a risk of losing the acoustic details of the melody during the process of discretization. &gt; Consequently, constructing a discrete token-based SVS system that meets the demands of melody expression poses a challenge.   <p>\u6b4c\u5531\u8bed\u97f3\u5408\u6210 (SVS) \u65e8\u5728\u6839\u636e\u5e26\u6709\u65cb\u5f8b\u548c\u6b4c\u8bcd\u7684\u4e50\u8c31\u751f\u6210\u6b4c\u58f0. \u4f20\u7edf\u7684SVS\u7cfb\u7edf (Xiaoicesing; Xiaoicesing2; DiffSinger; Seq2Seq SVS with PE; Singing-Tacotron; Bytesting) \u4e3b\u8981\u96c6\u4e2d\u5728\u901a\u8fc7\u589e\u5f3a\u58f0\u5b66\u6a21\u578b\u6765\u4ece\u4e50\u8c31\u751f\u6210\u6885\u5c14\u9891\u8c31\u56fe, \u7136\u540e\u901a\u8fc7\u58f0\u7801\u5668 (HiFi-GAN; BigVGAN) \u5c06\u5176\u8f6c\u6362\u4e3a\u6ce2\u5f62. \u6700\u8fd1, \u4f7f\u7528\u79bb\u6563\u6807\u8bc6\u7b26\u7684\u8d8b\u52bf\u65e5\u76ca\u589e\u957f, \u8fd9\u79cd\u8868\u793a\u5177\u6709\u4f18\u8d8a\u7684\u5b58\u50a8\u6548\u7387\u548c\u53ef\u63a7\u6027, \u9002\u7528\u4e8e\u8bed\u97f3\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1 (\u63a2\u7d22\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u79bb\u6563\u5316\u8f93\u5165\u7684\u9ad8\u6548\u7aef\u5230\u7aefASR, \u9762\u5411\u901a\u7528\u8bed\u97f3\u79bb\u6563\u6807\u8bc6\u7b26\uff1aASR\u548cTTS\u7684\u6848\u4f8b\u7814\u7a76, SpeechGPT, SPEAR-TTS) . \u79bb\u6563\u6807\u8bc6\u7b26\u53ef\u4ee5\u901a\u8fc7\u5411\u91cf\u91cf\u5316 (SoundStream; EnCodec; DAC) \u6216\u901a\u8fc7\u805a\u7c7b (\u591a\u53d8\u91cf\u89c2\u6d4b\u7684\u5206\u7c7b\u548c\u5206\u6790\u65b9\u6cd5) \u5728SSL\u6a21\u578b\u7684\u9690\u85cf\u5d4c\u5165\u4e0a\u751f\u6210 (HuBERT; Wav2Vec2.0; WavLM; VQ-Wav2Vec) , \u8fd9\u4e9b\u6a21\u578b\u5728\u5927\u89c4\u6a21\u97f3\u9891\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3.</p> <p>\u4e0e\u8bed\u97f3\u5904\u7406\u4e0d\u540c, \u6b4c\u5531\u5728\u8bed\u97f3\u4e4b\u4e0a\u589e\u52a0\u4e86\u65cb\u5f8b\u8868\u8fbe\u7684\u590d\u6742\u6027, \u8981\u6c42\u751f\u6210\u7684\u6b4c\u58f0\u7b26\u5408\u4e50\u8c31\u8981\u6c42\u5e76\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u542c\u89c9\u4f53\u9a8c. \u56e0\u6b64, \u5728SVS\u4e2d\u5e94\u7528\u79bb\u6563\u6807\u8bc6\u7b26\u9762\u4e34\u4e00\u4e9b\u72ec\u7279\u7684\u6311\u6218. \u9996\u5148, \u53d7\u9650\u4e8e\u7248\u6743\u9650\u5236\u548c\u4e25\u683c\u7684\u5f55\u97f3\u73af\u5883, \u76ee\u524d\u6ca1\u6709\u4e13\u95e8\u7684\u6b4c\u5531SSL\u6a21\u578b, \u73b0\u6709\u7684SSL\u6a21\u578b\u4e2d\u5305\u542b\u7684\u6b4c\u5531\u6570\u636e\u7a00\u7f3a (Seq2Seq_SVS_PE; SingAug; ACE-Opencpop) . \u56e0\u6b64, \u4e3a\u6b4c\u5531\u5408\u6210\u8bbe\u7f6e\u5408\u9002\u7684\u6807\u8bc6\u7b26\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027. \u5176\u6b21, \u5c3d\u7ba1\u6b4c\u8bcd\u5728\u6b4c\u5531\u6240\u5305\u542b\u7684\u4fe1\u606f\u4e2d\u672c\u8d28\u4e0a\u662f\u79bb\u6563\u7684, \u4f46\u65cb\u5f8b\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8868\u8fbe, \u4f8b\u5982, \u540c\u4e00\u97f3\u7b26\u7684\u57fa\u672c\u9891\u7387\u5728\u5176\u8986\u76d6\u7684\u5e27\u4e4b\u95f4\u53ef\u4ee5\u5fae\u5999\u5730\u53d8\u5316, \u7279\u522b\u662f\u5728\u6d89\u53ca\u6301\u7eed\u97f3\u7b26, \u9ad8\u97f3, \u98a4\u97f3\u548c\u5176\u4ed6\u9700\u8981\u9ad8\u7ea7\u58f0\u4e50\u6280\u5de7\u7684\u60c5\u51b5 (DAR-SVS) . \u56e0\u6b64, \u5728\u79bb\u6563\u5316\u8fc7\u7a0b\u4e2d\u5b58\u5728\u4e22\u5931\u65cb\u5f8b\u58f0\u5b66\u7ec6\u8282\u7684\u98ce\u9669. \u56e0\u6b64, \u6784\u5efa\u4e00\u4e2a\u6ee1\u8db3\u65cb\u5f8b\u8868\u8fbe\u9700\u6c42\u7684\u57fa\u4e8e\u79bb\u6563\u6807\u8bc6\u7b26\u7684SVS\u7cfb\u7edf\u662f\u4e00\u4e2a\u6311\u6218.</p> \u539f\u6587  &gt; In this study, we focus on addressing the above two challenges: finding suitable token formulations for singing and constructing a discrete token-based SVS system that meets the demands of melody expression. &gt; Firstly, we introduce a method for formulating tokens tailored to singing tasks and provide various flexible formulation strategies. &gt; Due to the diversity in pretraining tasks and datasets, different SSL models can offer valuable insights into singing semantics and acoustics, potentially providing complementary benefits to each other. &gt; Additionally, drawing from past research ([WavLM](../../Models/Speech_Representaion/2021.10.26_WavLM.md); [SUPERB](../../Evaluations/SUPERB.md); [Multi-Resolution_HuBERT](../Speech_Representaion/2023.10.04_Multi-Resolution_HuBERT.md)), it is evident that the influence of various intermediate layers in an SSL model differs across downstream tasks. &gt; This suggests a distribution of diverse knowledge across the layers. &gt; Based on the above reasons, we propose a token formulator that allows token blending across different models and layers. &gt; Secondly, for the discrete SVS system, we incorporate melody control signals to enhance the generated melody expression. &gt; Finally, combining the above methods, we propose a new SVS framework, namely ***TokSing***, using discrete token sequences and melody-oriented signals as system intermediates. &gt; Our main contributions include:  &gt; (1) We introduce a token formulator for training and provide multiple token formulations, offering flexibility in token sourcing and blending.  &gt; (2) We propose a discrete token based SVS framework, ***TokSing***, which achieves melody expression enhancement by integrating melody control signals to offset the loss of melodic intricacies in tokens.  &gt; (3) Extensive experiments in both single-singer and multi-singer scenarios demonstrate that our proposed ***TokSing*** framework achieves better performance with lower storage cost and higher convergence speed.   <p></p> <p>\u5728\u672c\u7814\u7a76\u4e2d, \u6211\u4eec\u4e13\u6ce8\u4e8e\u89e3\u51b3\u4e0a\u8ff0\u4e24\u4e2a\u6311\u6218\uff1a\u4e3a\u6b4c\u5531\u4efb\u52a1\u627e\u5230\u5408\u9002\u7684\u6807\u8bc6\u7b26\u6784\u6210\u65b9\u6cd5, \u5e76\u6784\u5efa\u4e00\u4e2a\u6ee1\u8db3\u65cb\u5f8b\u8868\u8fbe\u9700\u6c42\u7684\u57fa\u4e8e\u79bb\u6563\u6807\u8bc6\u7b26\u7684SVS\u7cfb\u7edf. \u9996\u5148, \u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4e3a\u6b4c\u5531\u4efb\u52a1\u91cf\u8eab\u5b9a\u5236\u7684\u6807\u8bc6\u7b26\u6784\u6210\u65b9\u6cd5, \u5e76\u63d0\u4f9b\u4e86\u591a\u79cd\u7075\u6d3b\u7684\u6784\u6210\u7b56\u7565. \u7531\u4e8e\u9884\u8bad\u7ec3\u4efb\u52a1\u548c\u6570\u636e\u96c6\u7684\u591a\u6837\u6027, \u4e0d\u540c\u7684SSL\u6a21\u578b\u53ef\u4ee5\u4e3a\u6b4c\u5531\u8bed\u4e49\u548c\u58f0\u5b66\u63d0\u4f9b\u5b9d\u8d35\u7684\u89c1\u89e3, \u53ef\u80fd\u76f8\u4e92\u63d0\u4f9b\u4e92\u8865\u7684\u597d\u5904. \u6b64\u5916, \u501f\u9274\u8fc7\u53bb\u7684\u7814\u7a76 (WavLM\uff1bSUPERB\uff1bMulti-Resolution_HuBERT) , \u5f88\u660e\u663e, SSL\u6a21\u578b\u4e2d\u4e0d\u540c\u4e2d\u95f4\u5c42\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u5404\u4e0d\u76f8\u540c. \u8fd9\u8868\u660e\u77e5\u8bc6\u5728\u5c42\u95f4\u5206\u5e03\u591a\u6837. \u57fa\u4e8e\u4e0a\u8ff0\u539f\u56e0, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u8bc6\u7b26\u6784\u6210\u5668, \u5141\u8bb8\u8de8\u4e0d\u540c\u6a21\u578b\u548c\u5c42\u7684\u6807\u8bc6\u7b26\u6df7\u5408. \u5176\u6b21, \u5bf9\u4e8e\u79bb\u6563SVS\u7cfb\u7edf, \u6211\u4eec\u6574\u5408\u4e86\u65cb\u5f8b\u63a7\u5236\u4fe1\u53f7\u4ee5\u589e\u5f3a\u751f\u6210\u7684\u65cb\u5f8b\u8868\u8fbe. \u6700\u540e, \u7ed3\u5408\u4e0a\u8ff0\u65b9\u6cd5, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684SVS\u6846\u67b6, \u5373TokSing, \u4f7f\u7528\u79bb\u6563\u6807\u8bc6\u7b26\u5e8f\u5217\u548c\u9762\u5411\u65cb\u5f8b\u7684\u4fe1\u53f7\u4f5c\u4e3a\u7cfb\u7edf\u4e2d\u95f4\u4ef6. \u6211\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a (1) \u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6807\u8bc6\u7b26\u6784\u6210\u5668\u7528\u4e8e\u8bad\u7ec3, \u5e76\u63d0\u4f9b\u4e86\u591a\u79cd\u6807\u8bc6\u7b26\u6784\u6210\u65b9\u6cd5, \u63d0\u4f9b\u4e86\u6807\u8bc6\u7b26\u6765\u6e90\u548c\u6df7\u5408\u7684\u7075\u6d3b\u6027. (2) \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u79bb\u6563\u6807\u8bc6\u7b26\u7684SVS\u6846\u67b6, TokSing, \u901a\u8fc7\u6574\u5408\u65cb\u5f8b\u63a7\u5236\u4fe1\u53f7\u6765\u62b5\u6d88\u6807\u8bc6\u7b26\u4e2d\u65cb\u5f8b\u7ec6\u8282\u7684\u635f\u5931, \u5b9e\u73b0\u4e86\u65cb\u5f8b\u8868\u8fbe\u7684\u589e\u5f3a. (3) \u5728\u5355\u6b4c\u624b\u548c\u591a\u6b4c\u624b\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e, \u6211\u4eec\u63d0\u51fa\u7684TokSing\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u8868\u73b0, \u540c\u65f6\u5177\u6709\u66f4\u4f4e\u7684\u5b58\u50a8\u6210\u672c\u548c\u66f4\u9ad8\u7684\u6536\u655b\u901f\u5ea6.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#2methodology","title":"2.Methodology\u00b7\u65b9\u6cd5","text":"\u539f\u6587  &gt; Figure 1 illustrates the overall framework of our discrete-based SVS system, containing:  &gt; (1) A token formulator extracts the hidden embeddings of the SSL model and quantizes them into tokens through clustering, with enhancements applied for better predictions.  &gt; (2) A musical encoder inputs the music score and predicts target tokens and melody signals, where enhancement is applied for better predictions;  &gt; (3) A vocoder converts tokens and melody signals into singing waveforms. &gt; Details of the three components are presented in the following subsections.   <p>Figure 1: Discrete-based SVS system architecture. The system contains three parts: a token formulator, a musical encoder and a vocoder. LF0, the logarithm of the fundamental frequency melody signal, serves as the melody signal. The ablations of melody prediction modules (purple blocks) and melody enhancing module (pink block) are discussed in Section 3.4. dji represents discrete token.</p> <p>\u56fe 01 \u5c55\u793a\u4e86\u6211\u4eec\u57fa\u4e8e\u79bb\u6563\u7684SVS\u7cfb\u7edf\u7684\u6574\u4f53\u6846\u67b6, \u5305\u542b\u4ee5\u4e0b\u4e09\u4e2a\u90e8\u5206\uff1a (1) \u6807\u8bc6\u7b26\u6784\u6210\u5668\u63d0\u53d6SSL\u6a21\u578b\u7684\u9690\u85cf\u5d4c\u5165, \u5e76\u901a\u8fc7\u805a\u7c7b\u5c06\u5176\u91cf\u5316\u4e3a\u6807\u8bc6\u7b26, \u540c\u65f6\u5e94\u7528\u589e\u5f3a\u4ee5\u63d0\u9ad8\u9884\u6d4b\u8d28\u91cf. (2) \u97f3\u4e50\u7f16\u7801\u5668\u8f93\u5165\u4e50\u8c31\u5e76\u9884\u6d4b\u76ee\u6807\u6807\u8bc6\u7b26\u548c\u65cb\u5f8b\u4fe1\u53f7, \u540c\u6837\u5e94\u7528\u589e\u5f3a\u4ee5\u63d0\u9ad8\u9884\u6d4b\u8d28\u91cf\uff1b (3) \u58f0\u7801\u5668\u5c06\u6807\u8bc6\u7b26\u548c\u65cb\u5f8b\u4fe1\u53f7\u8f6c\u6362\u4e3a\u6b4c\u5531\u6ce2\u5f62. \u4ee5\u4e0b\u5c0f\u8282\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u8fd9\u4e09\u4e2a\u7ec4\u4ef6.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#21token-formulation","title":"2.1.Token Formulation","text":"\u539f\u6587  &gt; There are typically two ways to formulate discrete tokens. &gt; One way is to use a [Variational Quantized Variational Autoencoder (VQ-VAE)](../_Basis/2017.11.02_VQ-VAE.md) to derive discrete representations from raw audio.  &gt; [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) introduces a residual vector quantizer (RVQ) and can reconstruct audios from multi-layer Codec tokens with Codec decoder. &gt; The other way involves quantizing tokens through clustering the hidden embeddings of SSL models. &gt; The pre-training task and the corpus of these models directly influence token generation. &gt; Previous research on speech understanding tasks ([WavLM](../../Models/Speech_Representaion/2021.10.26_WavLM.md); [Multi-Resolution_HuBERT](../Speech_Representaion/2023.10.04_Multi-Resolution_HuBERT.md); [MMM](../../Models/_tmp/MMM.md)) suggests that different hidden layers contain diverse information suitable for various downstream tasks, with shallow layers focusing more on identity and deeper layers on content-related features. &gt; We leverage and compare these tokens for SVS task.   <p>\u901a\u5e38\u6709\u4e24\u79cd\u65b9\u6cd5\u6765\u6784\u6210\u79bb\u6563\u6807\u8bc6\u7b26. \u4e00\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528\u53d8\u5206\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VQ-VAE) \u4ece\u539f\u59cb\u97f3\u9891\u4e2d\u63d0\u53d6\u79bb\u6563\u8868\u793a. SoundStream\u5f15\u5165\u4e86\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u5668 (RVQ) , \u5e76\u80fd\u591f\u4ece\u591a\u5c42\u7f16\u89e3\u7801\u5668\u6807\u8bc6\u7b26\u4e2d\u901a\u8fc7\u7f16\u89e3\u7801\u5668\u89e3\u7801\u5668\u91cd\u5efa\u97f3\u9891. \u53e6\u4e00\u79cd\u65b9\u6cd5\u6d89\u53ca\u901a\u8fc7\u805a\u7c7bSSL\u6a21\u578b\u7684\u9690\u85cf\u5d4c\u5165\u6765\u91cf\u5316\u6807\u8bc6\u7b26. \u8fd9\u4e9b\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\u548c\u8bed\u6599\u5e93\u76f4\u63a5\u5f71\u54cd\u6807\u8bc6\u7b26\u7684\u751f\u6210. \u5148\u524d\u5173\u4e8e\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u7684\u7814\u7a76 (WavLM\uff1bMulti-Resolution_HuBERT\uff1bMMM) \u8868\u660e, \u4e0d\u540c\u7684\u9690\u85cf\u5c42\u5305\u542b\u9002\u5408\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u591a\u6837\u5316\u4fe1\u606f, \u5176\u4e2d\u6d45\u5c42\u66f4\u591a\u5173\u6ce8\u8eab\u4efd\u7279\u5f81, \u800c\u6df1\u5c42\u66f4\u591a\u5173\u6ce8\u4e0e\u5185\u5bb9\u76f8\u5173\u7684\u7279\u5f81. \u6211\u4eec\u5229\u7528\u5e76\u6bd4\u8f83\u8fd9\u4e9b\u6807\u8bc6\u7b26\u7528\u4e8eSVS\u4efb\u52a1.</p> \u539f\u6587  &gt; These tokens can serve as intermediate representations, however, the information conveyed by a single token is limited. &gt; Inspired by [MMM](../../Models/_tmp/MMM.md), we propose a more flexible token formulation that involves blending tokens, which can be categorized into three basic types as in Figure 2: Type 1 involves selecting tokens from different layers of the same SSL model. &gt; Type 2 contains tokens from different SSL models that may relate to different pre-training corpora and tasks, taking advantages from different SSL models without additional training overhead. &gt; Lastly, by using RVQ, multiple-layer tokens can be obtained from the hidden embeddings of SSL models in a residual manner. &gt; Alternatively, we can utilize codec tokens directly obtained from audio encoders pre-trained on large-scale corpora ([SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md); [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md)). &gt; These three basic types can be utilized individually or combined strategically to offer greater flexibility and interpretability.   <p></p> <p></p> <p>\u8fd9\u4e9b\u6807\u8bc6\u7b26\u53ef\u4ee5\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a, \u7136\u800c, \u5355\u4e2a\u6807\u8bc6\u7b26\u4f20\u8fbe\u7684\u4fe1\u606f\u662f\u6709\u9650\u7684. \u53d7 MMM \u542f\u53d1, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u6807\u8bc6\u7b26\u6784\u6210\u65b9\u6cd5, \u6d89\u53ca\u6807\u8bc6\u7b26\u6df7\u5408, \u5982\u56fe2\u6240\u793a, \u53ef\u4ee5\u5206\u4e3a\u4e09\u79cd\u57fa\u672c\u7c7b\u578b\uff1a\u7c7b\u578b1\u6d89\u53ca\u4ece\u540c\u4e00SSL\u6a21\u578b\u7684\u4e0d\u540c\u5c42\u4e2d\u9009\u62e9\u6807\u8bc6\u7b26. \u7c7b\u578b2\u5305\u542b\u6765\u81ea\u4e0d\u540cSSL\u6a21\u578b\u7684\u6807\u8bc6\u7b26, \u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u4e0e\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u548c\u4efb\u52a1\u76f8\u5173, \u5229\u7528\u4e0d\u540cSSL\u6a21\u578b\u7684\u4f18\u52bf\u800c\u4e0d\u589e\u52a0\u989d\u5916\u7684\u8bad\u7ec3\u5f00\u9500. \u6700\u540e, \u901a\u8fc7\u4f7f\u7528RVQ, \u53ef\u4ee5\u4eceSSL\u6a21\u578b\u7684\u9690\u85cf\u5d4c\u5165\u4e2d\u4ee5\u6b8b\u5dee\u65b9\u5f0f\u83b7\u5f97\u591a\u5c42\u6807\u8bc6\u7b26. \u6216\u8005, \u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u5229\u7528\u4ece\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e0a\u9884\u8bad\u7ec3\u7684\u97f3\u9891\u7f16\u7801\u5668\u83b7\u5f97\u7684\u7f16\u89e3\u7801\u5668\u6807\u8bc6\u7b26 (SoundStream\uff1bEnCodec) . \u8fd9\u4e09\u79cd\u57fa\u672c\u7c7b\u578b\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\u6216\u7b56\u7565\u6027\u5730\u7ec4\u5408, \u4ee5\u63d0\u4f9b\u66f4\u5927\u7684\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#22musical-encoder","title":"2.2.Musical Encoder","text":"\u539f\u6587  &gt; The music encoder conducts acoustic modeling of the transition from musical scores to intermediate representations. &gt; It takes lyrics and corresponding note sequences containing pitch and duration information as input and outputs intermediate representations at the frame level. &gt; SVS requires adherence to the timing variations specified by the given musical scores, which demands higher accuracy in duration prediction. &gt; Hence, we employ a non-autoregressive (NAR) model with an explicit duration prediction module for modeling following ([Xiaoicesing2](2022.10.26_Xiaoicesing2.md); [FastSpeech](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md); [Phoneix](../../Models/_tmp/Phoneix.md).  &gt; In addition to the precise timing constraints, singing places a premium on pitch accuracy. &gt; As mentioned earlier, the discretization process may entail the degradation of pitch variations in singing, which is validated by our experiments in Section 3.3. &gt; To compensate for the loss of melody changes, we include a melody signal in addition to tokens by using the logarithm of the fundamental frequency. &gt; To further enhance the accuracy of melody prediction, we introduce a melody encoder to encode the input pitch and utilize a melody predictor for pitch prediction (see purple blocks in Figure 1).  &gt; We compute melody loss function $Loss_{m}$ using Euclidean distance between the extracted $m_i$ from the raw audio and the predicted $m'_i$ as: $Loss_{m} = \\sum_{i=1}^{N} |mi \u2212 m'_i|$, where $N$ represents the number of frames.  &gt; The tokens generated by the musical encoder might still encapsulate certain pitch-related details. &gt; Therefore, the output of the score decoder is enhanced with the predicted $m'_i$, jointly passed to the token predictor for better prediction. &gt; We compute the token prediction loss Ltok using the cross-entropy loss as: $Loss_{tok} = -\\dfrac{1}{N} \\sum_{i=1}^{N}\\sum_{j=1}^{M} (d_i^j)\\log (\\hat{d}_i^j)$, where $d_i^j$ represents the N ground truth token, $\\hat{d}_i^j$ represents the predicted token and $M$ is the number of tokens layers.   <p>\u97f3\u4e50\u7f16\u7801\u5668\u8fdb\u884c\u4ece\u4e50\u8c31\u5230\u4e2d\u95f4\u8868\u793a\u7684\u58f0\u5b66\u5efa\u6a21. \u5b83\u63a5\u53d7\u6b4c\u8bcd\u548c\u5305\u542b\u97f3\u9ad8\u53ca\u6301\u7eed\u65f6\u95f4\u4fe1\u606f\u7684\u76f8\u5e94\u97f3\u7b26\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165, \u5e76\u8f93\u51fa\u5e27\u7ea7\u522b\u7684\u4e2d\u95f4\u8868\u793a. SVS\u9700\u8981\u9075\u5faa\u4e50\u8c31\u6307\u5b9a\u7684\u65f6\u5e8f\u53d8\u5316, \u8fd9\u8981\u6c42\u5728\u6301\u7eed\u65f6\u95f4\u9884\u6d4b\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027. \u56e0\u6b64, \u6211\u4eec\u91c7\u7528\u4e86\u4e00\u4e2a\u975e\u81ea\u56de\u5f52 (NAR) \u6a21\u578b, \u8be5\u6a21\u578b\u5177\u6709\u660e\u786e\u7684\u6301\u7eed\u65f6\u95f4\u9884\u6d4b\u6a21\u5757, \u7528\u4e8e\u5efa\u6a21 (\u53c2\u89c1Xiaoicesing2\uff1bFastSpeech\uff1bPhoneix) .</p> <p>\u9664\u4e86\u7cbe\u786e\u7684\u65f6\u5e8f\u7ea6\u675f\u5916, \u6b4c\u5531\u5bf9\u97f3\u9ad8\u51c6\u786e\u6027\u6709\u5f88\u9ad8\u7684\u8981\u6c42. \u5982\u524d\u6240\u8ff0, \u79bb\u6563\u5316\u8fc7\u7a0b\u53ef\u80fd\u6d89\u53ca\u6b4c\u5531\u4e2d\u97f3\u9ad8\u53d8\u5316\u7684\u9000\u5316, \u8fd9\u5728\u7b2c3.3\u8282\u7684\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1. \u4e3a\u4e86\u8865\u507f\u65cb\u5f8b\u53d8\u5316\u7684\u635f\u5931, \u6211\u4eec\u5728\u6807\u8bc6\u7b26\u4e4b\u5916\u52a0\u5165\u4e86\u65cb\u5f8b\u4fe1\u53f7, \u4f7f\u7528\u57fa\u9891\u7684\u5bf9\u6570. \u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u65cb\u5f8b\u9884\u6d4b\u7684\u51c6\u786e\u6027, \u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65cb\u5f8b\u7f16\u7801\u5668\u6765\u7f16\u7801\u8f93\u5165\u97f3\u9ad8, \u5e76\u4f7f\u7528\u4e00\u4e2a\u65cb\u5f8b\u9884\u6d4b\u5668\u8fdb\u884c\u97f3\u9ad8\u9884\u6d4b (\u89c1\u56fe1\u4e2d\u7684\u7d2b\u8272\u5757) .</p> <p>\u6211\u4eec\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u8ba1\u7b97\u65cb\u5f8b\u635f\u5931\u51fd\u6570 $Loss_{m}$, \u8be5\u8ddd\u79bb\u662f\u539f\u59cb\u97f3\u9891\u4e2d\u63d0\u53d6\u7684 $m_i$ \u4e0e\u9884\u6d4b\u7684 $m'i $\u4e4b\u95f4\u7684\u8ddd\u79bb\uff1a $Loss{m} = \\sum_{i=1}^{N} |mi \u2212 m'_i|$, \u5176\u4e2d $N$ \u8868\u793a\u5e27\u7684\u6570\u91cf.</p> <p>\u97f3\u4e50\u7f16\u7801\u5668\u751f\u6210\u7684\u6807\u8bc6\u7b26\u53ef\u80fd\u4ecd\u7136\u5305\u542b\u67d0\u4e9b\u4e0e\u97f3\u9ad8\u76f8\u5173\u7684\u7ec6\u8282. \u56e0\u6b64, \u4e50\u8c31\u89e3\u7801\u5668\u7684\u8f93\u51fa\u4e0e\u9884\u6d4b\u7684 $m'i$ \u4e00\u8d77\u589e\u5f3a, \u5171\u540c\u4f20\u9012\u7ed9\u6807\u8bc6\u7b26\u9884\u6d4b\u5668\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u9884\u6d4b. \u6211\u4eec\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u8ba1\u7b97\u6807\u8bc6\u7b26\u9884\u6d4b\u635f\u5931 $Loss{tok} = -\\dfrac{1}{N} \\sum_{i=1}^{N}\\sum_{j=1}^{M} (d_i^j)\\log (\\hat{d}_i^j)$), \u5176\u4e2d $d_i^j$ \u8868\u793a\u771f\u5b9e\u6807\u8bc6\u7b26, $\\hat{d}_i^j$ \u8868\u793a\u9884\u6d4b\u7684\u6807\u8bc6\u7b26, $M$ \u662f\u6807\u8bc6\u7b26\u5c42\u7684\u6570\u91cf.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#23vocoder","title":"2.3.Vocoder","text":"\u539f\u6587  &gt; The backbone of the vocoder adopts the adversarial network from [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md), comprising a generator and two discriminators: the Multi-Period Discriminator (MPD) and the MultiScale Discriminator (MSD). &gt; Following previous works ([Direct S2ST](../../Models/_tmp/2021.07.12_Direct_S2ST.md); [ESPnet-ST-v2](../../Models/_tmp/ESPnet-ST-v2.md)), we replace Mel spectrograms with discrete tokens. &gt; The tokens are encoded by extra embedding layers and then concatenated with melody signal, passed to the upsampling layers. &gt; Different choices of vocoders are compared in Section 3.3.   <p>\u58f0\u7801\u5668\u7684\u4e3b\u5e72\u91c7\u7528\u4e86\u6765\u81ea HiFi-GAN \u7684\u5bf9\u6297\u7f51\u7edc, \u5305\u62ec\u4e00\u4e2a\u751f\u6210\u5668\u548c\u4e24\u4e2a\u5224\u522b\u5668\uff1a\u591a\u5468\u671f\u5224\u522b\u5668 (MPD) \u548c\u591a\u5c3a\u5ea6\u5224\u522b\u5668 (MSD) . \u9075\u5faa\u5148\u524d\u7684\u5de5\u4f5c (Direct S2ST\uff1bESPnet-ST-v2) , \u6211\u4eec\u7528\u79bb\u6563\u6807\u8bc6\u7b26\u66ff\u6362\u4e86\u6885\u5c14\u9891\u8c31\u56fe. \u6807\u8bc6\u7b26\u901a\u8fc7\u989d\u5916\u7684\u5d4c\u5165\u5c42\u8fdb\u884c\u7f16\u7801, \u7136\u540e\u4e0e\u65cb\u5f8b\u4fe1\u53f7\u8fde\u63a5, \u4f20\u9012\u7ed9\u4e0a\u91c7\u6837\u5c42. \u5728\u7b2c3.3\u8282\u4e2d\u6bd4\u8f83\u4e86\u4e0d\u540c\u9009\u62e9\u7684\u58f0\u7801\u5668.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#3experiments","title":"3.Experiments\u00b7\u5b9e\u9a8c","text":""},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#31experimental-setting","title":"3.1.Experimental Setting","text":"\u539f\u6587  &gt; We conduct experiments in single-singer and multi-singer scenarios. &gt; All singing audios have score notations of pitch, duration, and lyrics. &gt; During training, audios are preprocessed according to the SSL model\u2019s sampling rate and hop size. &gt; #### Datasets:  &gt; We carry out experiments on two public datasets:  &gt; (1) [Opencpop](../../Datasets/Opencpop.md) comprises 5.2 hours of 100 songs featuring a Mandarin female vocalist. &gt; We follow the official split in training and testing sets, with provided sentence-level segmentation.  &gt; (2) [ACE-Opencpop](../../Datasets/ACE-Opencpop.md) is a dataset derived from [Opencpop](../../Datasets/Opencpop.md)\u2019s music scores, containing multi-singers synthesized using the ACE Studio1 with detailed manual tuning. &gt; The dataset encompasses 30 singers with diverse genders and vocal styles, accumulating approximately 150 hours of total duration.  &gt; #### Token Formulation:  &gt; To acquire suitable token sources, we conduct resynthesis experiments across different hidden layers of various SSL models, aggregating them with a weighted sum approach. &gt; We select layers with higher weights as token sources for the following experiments. &gt; Eventually, the 6th and 23rd layers of the WavLM-large2 model, as well as the 6th layer of the HuBERT-large model3 are chosen, which also exhibit strong performance in speech understanding tasks ([SUPERB](../../Evaluations/SUPERB.md); [Multi-Resolution_HuBERT](../Speech_Representaion/2023.10.04_Multi-Resolution_HuBERT.md)). &gt; Additionally, setting the optimal number of clustering centers can be influenced by the phoneme inventory of the language and the number of singers involved. &gt; We compare the performance across exponential powers of 2 ranging from 32 to 1024. &gt; Ultimately, we set the number of clustering centers to 128 for the single-singer dataset and 1024 for the multi-singer dataset.  &gt; #### Model configurations:  &gt; The musical encoder adopts a NAR architecture in [FastSpeech](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md) with an explicit duration predictor. &gt; The encoding and decoding of score utilize a transformer based structure following [Xiaoicesing](2020.06.11_Xiaoicesing.md), employing a 384-dim embedding layer to encode lyrics, pitches, and durations from music scores. &gt; The prediction of melody aligns with [VISinger](../../Models/Singing_Voice/2021.10.17_VISinger.md) and the ground-truth melody signals are extracted from raw audios using pyworld 4 and computed by natural logarithm. &gt; The predicted ones are passed through a simple fully connected network (pink block in Figure 1) to the token predictor. &gt; In vocoder, a 768-dim and a 256dim embedding layer are used to encode the token and melody signal, respectively. &gt; The parameters of the generator and discriminator are consistent with [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md). &gt; For the multi-singer dataset, a singer embedding layer is integrated for musical encoder but is not used in vocoder.  &gt; #### Training and Inference:  &gt; The model employs the Adam optimizer with a learning rate of 0.001. &gt; The batch size is set to 16 for training. &gt; The inference is performed by averaging the best five models with the lowest loss on the validation set.  &gt; #### Evaluation Metric:  &gt; We use common objective metrics, including Mel Cepstral Distortion (MCD), Root Mean Square Error of Fundamental Frequency (F0), Semitone Accuracy (SA). &gt; Additionally, we conduct four subjective evaluations, including the clarity in lyric pronunciation (Pron), fluency in melody expression (Melody), proficiency in singing technique (Tech), rated on an integer scale from 1 to 3, followed by an overall listening experience (MOS) rating on a scale from 1 to 5. &gt; We randomly select 30 identical samples from each system and invite 20 native-speaker annotators to rate them. &gt; All annotators undergo pre-annotation tests to ensure they are musically competent. &gt; We report the MOS score at a confidence interval of 95%.   <p>\u6211\u4eec\u5728\u5355\u6b4c\u624b\u548c\u591a\u6b4c\u624b\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u5b9e\u9a8c. \u6240\u6709\u6b4c\u5531\u97f3\u9891\u90fd\u6709\u97f3\u9ad8, \u6301\u7eed\u65f6\u95f4\u548c\u6b4c\u8bcd\u7684\u4e50\u8c31\u7b26\u53f7. \u5728\u8bad\u7ec3\u671f\u95f4, \u97f3\u9891\u6839\u636eSSL\u6a21\u578b\u7684\u91c7\u6837\u7387\u548c\u8df3\u8dc3\u5927\u5c0f\u8fdb\u884c\u9884\u5904\u7406. \u6570\u636e\u96c6\uff1a\u6211\u4eec\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff1a (1) Opencpop \u5305\u542b5.2\u5c0f\u65f6\u7684100\u9996\u6b4c\u66f2, \u7531\u4e00\u4f4d\u666e\u901a\u8bdd\u5973\u6b4c\u624b\u6f14\u5531. \u6211\u4eec\u9075\u5faa\u5b98\u65b9\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\u5206\u5272, \u5e76\u63d0\u4f9b\u4e86\u53e5\u5b50\u7ea7\u522b\u7684\u5206\u5272. (2) ACE-Opencpop \u662f\u4ece Opencpop\u7684\u97f3\u4e50\u4e50\u8c31\u4e2d\u884d\u751f\u51fa\u6765\u7684\u6570\u636e\u96c6, \u4f7f\u7528ACE Studio1\u8fdb\u884c\u4e86\u591a\u6b4c\u624b\u7684\u5408\u6210, \u5e76\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u624b\u52a8\u8c03\u97f3. \u8be5\u6570\u636e\u96c6\u6db5\u76d6\u4e8630\u4f4d\u5177\u6709\u4e0d\u540c\u6027\u522b\u548c\u58f0\u4e50\u98ce\u683c\u7684\u6b4c\u624b, \u603b\u65f6\u957f\u7ea6\u4e3a150\u5c0f\u65f6. \u6807\u8bc6\u7b26\u6784\u6210\uff1a\u4e3a\u4e86\u83b7\u5f97\u5408\u9002\u7684\u6807\u8bc6\u7b26\u6765\u6e90, \u6211\u4eec\u8fdb\u884c\u4e86\u8de8\u4e0d\u540cSSL\u6a21\u578b\u9690\u85cf\u5c42\u7684\u91cd\u65b0\u5408\u6210\u5b9e\u9a8c, \u5e76\u91c7\u7528\u52a0\u6743\u6c42\u548c\u65b9\u6cd5\u8fdb\u884c\u805a\u5408. \u6211\u4eec\u9009\u62e9\u6743\u91cd\u8f83\u9ad8\u7684\u5c42\u4f5c\u4e3a\u4ee5\u4e0b\u5b9e\u9a8c\u7684\u6807\u8bc6\u7b26\u6765\u6e90. \u6700\u7ec8, \u9009\u62e9\u4e86WavLM-large2\u6a21\u578b\u7684\u7b2c6\u5c42\u548c\u7b2c23\u5c42, \u4ee5\u53caHuBERT-large\u6a21\u578b\u7684\u7b2c6\u5c42, \u8fd9\u4e9b\u5c42\u5728\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd (SUPERB\uff1bMulti-Resolution_HuBERT) . \u6b64\u5916, \u8bbe\u7f6e\u6700\u4f73\u7684\u805a\u7c7b\u4e2d\u5fc3\u6570\u91cf\u53ef\u80fd\u4f1a\u53d7\u5230\u8bed\u8a00\u7684\u97f3\u7d20\u5e93\u5b58\u548c\u6d89\u53ca\u7684\u6b4c\u624b\u6570\u91cf\u7684\u5f71\u54cd. \u6211\u4eec\u6bd4\u8f83\u4e86\u4ece32\u52301024\u7684\u6307\u65702\u7684\u5e42\u6b21\u7684\u8868\u73b0. \u6700\u7ec8, \u6211\u4eec\u4e3a\u5355\u6b4c\u624b\u6570\u636e\u96c6\u8bbe\u7f6e\u4e86128\u4e2a\u805a\u7c7b\u4e2d\u5fc3, \u4e3a\u591a\u6b4c\u624b\u6570\u636e\u96c6\u8bbe\u7f6e\u4e861024\u4e2a\u805a\u7c7b\u4e2d\u5fc3. \u6a21\u578b\u914d\u7f6e\uff1a\u97f3\u4e50\u7f16\u7801\u5668\u91c7\u7528\u4e86 FastSpeech \u4e2d\u7684 NAR \u67b6\u6784, \u5177\u6709\u660e\u786e\u7684\u6301\u7eed\u65f6\u95f4\u9884\u6d4b\u5668. \u4e50\u8c31\u7684\u7f16\u7801\u548c\u89e3\u7801\u91c7\u7528\u4e86\u57fa\u4e8eTransformer\u7684\u7ed3\u6784, \u9075\u5faa Xiaoicesing, \u4f7f\u7528384\u7ef4\u7684\u5d4c\u5165\u5c42\u6765\u7f16\u7801\u4e50\u8c31\u4e2d\u7684\u6b4c\u8bcd, \u97f3\u9ad8\u548c\u6301\u7eed\u65f6\u95f4. \u65cb\u5f8b\u7684\u9884\u6d4b\u4e0e VISinger \u4e00\u81f4, \u4ece\u539f\u59cb\u97f3\u9891\u4e2d\u4f7f\u7528 pyworld\u63d0\u53d6\u771f\u5b9e\u65cb\u5f8b\u4fe1\u53f7, \u5e76\u901a\u8fc7\u81ea\u7136\u5bf9\u6570\u8ba1\u7b97. \u9884\u6d4b\u7684\u65cb\u5f8b\u4fe1\u53f7\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u5168\u8fde\u63a5\u7f51\u7edc (\u56fe1\u4e2d\u7684\u7c89\u8272\u5757) \u4f20\u9012\u7ed9\u6807\u8bc6\u7b26\u9884\u6d4b\u5668. \u5728\u58f0\u7801\u5668\u4e2d, \u4f7f\u7528768\u7ef4\u548c256\u7ef4\u7684\u5d4c\u5165\u5c42\u5206\u522b\u7f16\u7801\u6807\u8bc6\u7b26\u548c\u65cb\u5f8b\u4fe1\u53f7. \u751f\u6210\u5668\u548c\u5224\u522b\u5668\u7684\u53c2\u6570\u4e0e HiFi-GAN \u4e00\u81f4. \u5bf9\u4e8e\u591a\u6b4c\u624b\u6570\u636e\u96c6, \u97f3\u4e50\u7f16\u7801\u5668\u4e2d\u96c6\u6210\u4e86\u4e00\u4e2a\u6b4c\u624b\u5d4c\u5165\u5c42, \u4f46\u5728\u58f0\u7801\u5668\u4e2d\u672a\u4f7f\u7528. \u8bad\u7ec3\u548c\u63a8\u7406\uff1a\u6a21\u578b\u91c7\u7528\u5b66\u4e60\u7387\u4e3a0.001\u7684Adam\u4f18\u5316\u5668. \u6279\u91cf\u5927\u5c0f\u8bbe\u7f6e\u4e3a16\u8fdb\u884c\u8bad\u7ec3. \u63a8\u7406\u662f\u901a\u8fc7\u5728\u9a8c\u8bc1\u96c6\u4e0a\u635f\u5931\u6700\u4f4e\u7684\u4e94\u4e2a\u6700\u4f73\u6a21\u578b\u8fdb\u884c\u5e73\u5747\u6765\u6267\u884c\u7684. \u8bc4\u4f30\u6307\u6807\uff1a\u6211\u4eec\u4f7f\u7528\u5e38\u89c1\u7684\u5ba2\u89c2\u6307\u6807, \u5305\u62ec\u6885\u5c14\u5012\u8c31\u5931\u771f (MCD) , \u57fa\u9891\u7684\u5747\u65b9\u6839\u8bef\u5dee (F0) , \u534a\u97f3\u51c6\u786e\u5ea6 (SA) . \u6b64\u5916, \u6211\u4eec\u8fdb\u884c\u4e86\u56db\u4e2a\u4e3b\u89c2\u8bc4\u4f30, \u5305\u62ec\u6b4c\u8bcd\u53d1\u97f3\u7684\u6e05\u6670\u5ea6 (Pron) , \u65cb\u5f8b\u8868\u8fbe\u7684\u6d41\u7545\u5ea6 (Melody) , \u6b4c\u5531\u6280\u5de7\u7684\u719f\u7ec3\u5ea6 (Tech) , \u8bc4\u5206\u8303\u56f4\u4e3a1\u52303\u7684\u6574\u6570, \u4ee5\u53ca\u6574\u4f53\u542c\u89c9\u4f53\u9a8c (MOS) , \u8bc4\u5206\u8303\u56f4\u4e3a1\u52305. \u6211\u4eec\u4ece\u6bcf\u4e2a\u7cfb\u7edf\u4e2d\u968f\u673a\u9009\u62e930\u4e2a\u76f8\u540c\u7684\u6837\u672c, \u5e76\u9080\u8bf720\u4f4d\u6bcd\u8bed\u4e3a\u666e\u901a\u8bdd\u7684\u6ce8\u91ca\u8005\u8fdb\u884c\u8bc4\u5206. \u6240\u6709\u6ce8\u91ca\u8005\u5728\u6ce8\u91ca\u524d\u90fd\u7ecf\u8fc7\u4e86\u9884\u6ce8\u91ca\u6d4b\u8bd5, \u4ee5\u786e\u4fdd\u4ed6\u4eec\u5728\u97f3\u4e50\u65b9\u9762\u6709\u8db3\u591f\u7684\u8d44\u8d28. \u6211\u4eec\u62a5\u544a\u4e8695%\u7f6e\u4fe1\u533a\u95f4\u4e0b\u7684MOS\u5206\u6570.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#32comparison-experiments","title":"3.2.Comparison Experiments","text":"\u539f\u6587  &gt; We compare our discrete-based SVS system, ***TokSing***, with a fastspeech-like Mel spectrograms-based system [Xiaoicesing2](2022.10.26_Xiaoicesing2.md) and a VAE-structured latent variance-based system [VISinger2](../../Models/Singing_Voice/2022.11.05_VISinger2.md). &gt; As mentioned in Section 2.2, singing is more sensitive to the duration variations of notes. &gt; Therefore, all systems employ acoustic models with explicit length regulation in a NAR manner. &gt; ***TokSing*** achieves better performance in both subjective and objective metrics, especially in the perception of melody. &gt; Cases of the generated segments (see Figure 3) also demonstrate that ***TokSing*** can exhibit more nuanced performance in melody variations.  &gt; As shown in Figure 4, ***TokSing*** also shows its advantages in both space cost and convergence speed as it only requires one token and one melody signal to represent a frame, resulting in much lower dimensions than Mel spectrogram-based and latent variance-based systems. &gt; Additionally, ***TokSing*** converges faster and more stably than the system based on Mel spectrograms. &gt; We calculate the MCD scores on the test set every five epochs. &gt; ***TokSing*** converges (TOurs) significantly earlier than the Mel spectrogram-based system (TMel).   <p>\u6211\u4eec\u5c06\u6211\u4eec\u7684\u57fa\u4e8e\u79bb\u6563\u7684SVS\u7cfb\u7edf, TokSing, \u4e0e\u57fa\u4e8e\u6885\u5c14\u9891\u8c31\u56fe\u7684\u5feb\u901f\u8bed\u97f3\u7c7b\u4f3c\u7cfb\u7edf Xiaoicesing2 \u548c\u57fa\u4e8eVAE\u7ed3\u6784\u7684\u6f5c\u5728\u53d8\u91cf\u7cfb\u7edf VISinger2 \u8fdb\u884c\u4e86\u6bd4\u8f83. \u5982\u7b2c2.2\u8282\u6240\u8ff0, \u6b4c\u5531\u5bf9\u97f3\u7b26\u6301\u7eed\u65f6\u95f4\u7684\u53d8\u5316\u66f4\u4e3a\u654f\u611f. \u56e0\u6b64, \u6240\u6709\u7cfb\u7edf\u90fd\u91c7\u7528\u4e86\u5177\u6709\u660e\u786e\u957f\u5ea6\u8c03\u8282\u7684NAR\u65b9\u5f0f\u7684\u58f0\u5b66\u6a21\u578b. TokSing \u5728\u4e3b\u89c2\u548c\u5ba2\u89c2\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u8868\u73b0, \u5c24\u5176\u662f\u5728\u65cb\u5f8b\u611f\u77e5\u65b9\u9762. \u751f\u6210\u7684\u7247\u6bb5\u6848\u4f8b (\u89c1\u56fe 03) \u4e5f\u8868\u660e, TokSing \u80fd\u591f\u5728\u65cb\u5f8b\u53d8\u5316\u65b9\u9762\u5c55\u73b0\u51fa\u66f4\u7ec6\u817b\u7684\u8868\u73b0.</p> <p>\u5982\u56fe 04 \u6240\u793a, TokSing \u5728\u7a7a\u95f4\u6210\u672c\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u4e5f\u663e\u793a\u51fa\u5176\u4f18\u52bf, \u56e0\u4e3a\u5b83\u53ea\u9700\u8981\u4e00\u4e2a\u6807\u8bc6\u7b26\u548c\u4e00\u4e2a\u65cb\u5f8b\u4fe1\u53f7\u6765\u8868\u793a\u4e00\u4e2a\u5e27, \u5bfc\u81f4\u7ef4\u5ea6\u8fdc\u4f4e\u4e8e\u57fa\u4e8e\u6885\u5c14\u9891\u8c31\u56fe\u548c\u57fa\u4e8e\u6f5c\u5728\u53d8\u91cf\u7684\u7cfb\u7edf. \u6b64\u5916, TokSing \u6bd4\u57fa\u4e8e\u6885\u5c14\u9891\u8c31\u56fe\u7684\u7cfb\u7edf\u6536\u655b\u5f97\u66f4\u5feb, \u66f4\u7a33\u5b9a. \u6211\u4eec\u6bcf\u4e94\u4e2a epoch \u8ba1\u7b97\u4e00\u6b21\u6d4b\u8bd5\u96c6\u4e0a\u7684MCD\u5206\u6570. TokSing \u6bd4\u57fa\u4e8e\u6885\u5c14\u9891\u8c31\u56fe\u7684\u7cfb\u7edf (TMel) \u663e\u8457\u66f4\u65e9\u5730\u6536\u655b (TOurs) .</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#33ablation-of-the-reconstruction-in-vocoder","title":"3.3.Ablation of the Reconstruction in Vocoder","text":"\u539f\u6587  &gt; We note in Section 2.2 that discretization may entail melody degradation. &gt; We conduct reconstruction experiments from the hidden layer of SSL models and from tokens extracted using the same layer. &gt; Results in Table 2 indicate a significant decrease in melody quality when using only tokens. &gt; To mitigate the melody loss during discretization, we introduce a melody signal described in Section 2.2 as an additional auxiliary representation. &gt; The results validate the effectiveness of this strategy both in the vocoder and when connected to the musical encoder. &gt; Although discrete tokens may not outperform Mel spectrogram-based vocoders in reconstruction experiments, they contribute to better overall performance in the SVS system. &gt; This suggests that discrete representations alleviate the modeling difficulty in the acoustic model. &gt; Compared to vocoders trained directly on the intermediate layers of self-supervised models, discrete-based vocoders still have room for further improvement.  &gt; Additionally, we test the reconstruction quality by training a new vocoder using codec tokens and those reconstructed by the codec decoder. &gt; For the former, we select the first layer of codec tokens from [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) (non-causal) (referred to as Codec Token + LF 0 in Table2). &gt; For the latter, we follow the encoder-decoder codec architecture of the same model to reconstruct singing audios (referred to as Codec Decoder in Table 2). &gt; Examples of more codec decoder reconstructions can be found in Figure 3 (e) and (f). &gt; The experiments indicate that the performance of codec tokens is inferior to tokens obtained from the SSL model. &gt; Furthermore, the codec decoder performs poorly in pitch accuracy.   <p>\u6211\u4eec\u5728\u7b2c2.2\u8282\u4e2d\u6ce8\u610f\u5230, \u79bb\u6563\u5316\u53ef\u80fd\u4f1a\u5bfc\u81f4\u65cb\u5f8b\u8d28\u91cf\u4e0b\u964d. \u6211\u4eec\u8fdb\u884c\u4e86\u4eceSSL\u6a21\u578b\u7684\u9690\u85cf\u5c42\u548c\u4f7f\u7528\u76f8\u540c\u5c42\u63d0\u53d6\u7684\u6807\u8bc6\u7b26\u8fdb\u884c\u91cd\u5efa\u7684\u5b9e\u9a8c. \u88682\u4e2d\u7684\u7ed3\u679c\u8868\u660e, \u4ec5\u4f7f\u7528\u6807\u8bc6\u7b26\u65f6\u65cb\u5f8b\u8d28\u91cf\u663e\u8457\u4e0b\u964d. \u4e3a\u4e86\u51cf\u8f7b\u79bb\u6563\u5316\u8fc7\u7a0b\u4e2d\u7684\u65cb\u5f8b\u635f\u5931, \u6211\u4eec\u5f15\u5165\u4e86\u7b2c2.2\u8282\u4e2d\u63cf\u8ff0\u7684\u65cb\u5f8b\u4fe1\u53f7\u4f5c\u4e3a\u989d\u5916\u7684\u8f85\u52a9\u8868\u793a. \u7ed3\u679c\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u7b56\u7565\u5728\u58f0\u7801\u5668\u548c\u8fde\u63a5\u5230\u97f3\u4e50\u7f16\u7801\u5668\u65f6\u7684\u6709\u6548\u6027. \u5c3d\u7ba1\u79bb\u6563\u6807\u8bc6\u7b26\u5728\u91cd\u5efa\u5b9e\u9a8c\u4e2d\u53ef\u80fd\u4e0d\u4f1a\u8d85\u8fc7\u57fa\u4e8e\u6885\u5c14\u9891\u8c31\u56fe\u7684\u58f0\u7801\u5668, \u4f46\u5b83\u4eec\u6709\u52a9\u4e8e\u5728SVS\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6574\u4f53\u6027\u80fd. \u8fd9\u8868\u660e\u79bb\u6563\u8868\u793a\u51cf\u8f7b\u4e86\u58f0\u5b66\u6a21\u578b\u4e2d\u7684\u5efa\u6a21\u96be\u5ea6. \u4e0e\u76f4\u63a5\u5728\u81ea\u76d1\u7763\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u4e0a\u8bad\u7ec3\u7684\u58f0\u7801\u5668\u76f8\u6bd4, \u57fa\u4e8e\u79bb\u6563\u7684\u58f0\u7801\u5668\u4ecd\u6709\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u7a7a\u95f4.</p> <p>\u6b64\u5916, \u6211\u4eec\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u65b0\u7684\u58f0\u7801\u5668\u6765\u6d4b\u8bd5\u91cd\u5efa\u8d28\u91cf, \u8be5\u58f0\u7801\u5668\u4f7f\u7528\u7f16\u89e3\u7801\u5668\u6807\u8bc6\u7b26\u548c\u7531\u7f16\u89e3\u7801\u5668\u89e3\u7801\u5668\u91cd\u5efa\u7684\u6807\u8bc6\u7b26. \u5bf9\u4e8e\u524d\u8005, \u6211\u4eec\u4ece EnCodec (\u975e\u56e0\u679c) \u4e2d\u9009\u62e9\u4e86\u7b2c\u4e00\u5c42\u7f16\u89e3\u7801\u5668\u6807\u8bc6\u7b26 (\u5728\u8868 02 \u4e2d\u79f0\u4e3a Codec Token + LF 0) . \u5bf9\u4e8e\u540e\u8005, \u6211\u4eec\u9075\u5faa\u540c\u4e00\u6a21\u578b\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f16\u89e3\u7801\u5668\u67b6\u6784\u6765\u91cd\u5efa\u6b4c\u5531\u97f3\u9891 (\u5728\u88682\u4e2d\u79f0\u4e3aCodec Decoder) . \u66f4\u591a\u7f16\u89e3\u7801\u5668\u89e3\u7801\u5668\u91cd\u5efa\u7684\u793a\u4f8b\u53ef\u4ee5\u5728\u56fe 03 (e) \u548c (f) \u4e2d\u627e\u5230. \u5b9e\u9a8c\u8868\u660e, \u7f16\u89e3\u7801\u5668\u6807\u8bc6\u7b26\u7684\u6027\u80fd\u4e0d\u5982\u4ece SSL \u6a21\u578b\u83b7\u5f97\u7684\u6807\u8bc6\u7b26. \u6b64\u5916, \u7f16\u89e3\u7801\u5668\u89e3\u7801\u5668\u5728\u97f3\u9ad8\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#34ablation-of-musical-encoder","title":"3.4.Ablation of Musical Encoder.","text":"\u539f\u6587  &gt; Section 2.2 mentions that tokens may also carry some melody information. &gt; Therefore, in addition to introducing a melody predictor, we employ a melody enhancement strategy to assist in token prediction (implementation details in Section 3.1). &gt; The ablation experiments presented in Table 3 demonstrate that both the melody predictor and the melody enhancement strategy enhance the performance of SVS system, particularly in terms of melody expression.   <p>\u7b2c 2.2 \u8282\u63d0\u5230, \u6807\u8bc6\u7b26\u4e5f\u53ef\u80fd\u643a\u5e26\u4e00\u4e9b\u65cb\u5f8b\u4fe1\u606f. \u56e0\u6b64, \u9664\u4e86\u5f15\u5165\u65cb\u5f8b\u9884\u6d4b\u5668\u4e4b\u5916, \u6211\u4eec\u8fd8\u91c7\u7528\u4e86\u4e00\u79cd\u65cb\u5f8b\u589e\u5f3a\u7b56\u7565\u6765\u8f85\u52a9\u6807\u8bc6\u7b26\u9884\u6d4b (\u5b9e\u65bd\u7ec6\u8282\u89c1\u7b2c3.1\u8282) . \u88683\u4e2d\u5c55\u793a\u7684\u6d88\u878d\u5b9e\u9a8c\u8868\u660e, \u65cb\u5f8b\u9884\u6d4b\u5668\u548c\u65cb\u5f8b\u589e\u5f3a\u7b56\u7565\u90fd\u63d0\u9ad8\u4e86SVS\u7cfb\u7edf\u7684\u6027\u80fd, \u7279\u522b\u662f\u5728\u65cb\u5f8b\u8868\u8fbe\u65b9\u9762.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#35ablation-of-token-formulations","title":"3.5.Ablation of Token Formulations","text":"\u539f\u6587  &gt; We compare the performance of single-layer tokens with the three formulations of multi-layer token formulations mentioned in Section 2.1. &gt; The single-layer tokens originate from the 6th layer of WavLM-large. &gt; For multi-layer token formulations, Formulation 1 incorporates tokens from the 23rd layer of the same model; Formulation 2 adds the 6th layer from a different model, HuBERT-large; and Formulation 3 applies a four-layer residual clustering. &gt; Both subjective and objective evaluations indicate that all multi-layer formulations enhance performance to varying degrees. &gt; It is noteworthy that although residual clustering in Formulation 3 improves the performance of the vocoder by encoding more details, it also increases the prediction difficulty in the musical encoder. &gt; We observe a noticeable decrease in prediction accuracy in the later layers of residual clustering.   <p>\u6211\u4eec\u5c06\u5355\u5c42\u6807\u8bc6\u7b26\u7684\u6027\u80fd\u4e0e\u7b2c 2.1 \u8282\u4e2d\u63d0\u5230\u7684\u4e09\u79cd\u591a\u5c42\u6807\u8bc6\u7b26\u6784\u6210\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83. \u5355\u5c42\u6807\u8bc6\u7b26\u6765\u81ea WavLM-large \u7684\u7b2c 6 \u5c42. \u5bf9\u4e8e\u591a\u5c42\u6807\u8bc6\u7b26\u6784\u6210, \u6784\u62101\u6574\u5408\u4e86\u540c\u4e00\u6a21\u578b\u7b2c23\u5c42\u7684\u6807\u8bc6\u7b26\uff1b\u6784\u62102\u6dfb\u52a0\u4e86\u4e0d\u540c\u6a21\u578bHuBERT-large\u7684\u7b2c6\u5c42\u6807\u8bc6\u7b26\uff1b\u6784\u62103\u5e94\u7528\u4e86\u56db\u5c42\u6b8b\u5dee\u805a\u7c7b. \u4e3b\u89c2\u548c\u5ba2\u89c2\u8bc4\u4f30\u90fd\u8868\u660e, \u6240\u6709\u591a\u5c42\u6784\u6210\u65b9\u6cd5\u90fd\u5728\u4e0d\u540c\u7a0b\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd. \u503c\u5f97\u6ce8\u610f\u7684\u662f, \u5c3d\u7ba1\u6784\u62103\u4e2d\u7684\u6b8b\u5dee\u805a\u7c7b\u901a\u8fc7\u7f16\u7801\u66f4\u591a\u7ec6\u8282\u63d0\u9ad8\u4e86\u58f0\u7801\u5668\u7684\u6027\u80fd, \u4f46\u5b83\u4e5f\u589e\u52a0\u4e86\u97f3\u4e50\u7f16\u7801\u5668\u4e2d\u7684\u9884\u6d4b\u96be\u5ea6. \u6211\u4eec\u89c2\u5bdf\u5230\u6b8b\u5dee\u805a\u7c7b\u540e\u671f\u5c42\u7684\u9884\u6d4b\u51c6\u786e\u6027\u6709\u660e\u663e\u4e0b\u964d.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#36transfer-learning","title":"3.6.Transfer Learning","text":"\u539f\u6587  &gt; Experiments in Section 3.3 suggest potential for improving the token-based vocoder, compared to the vocoder trained on SSL model hidden layers. &gt; In Table 5, we explore transfer learning to enhance vocoder, which is first pretrained on [ACE-Opencpop](../../Datasets/ACE-Opencpop.md), and then finetuned on [Opencpop](../../Datasets/Opencpop.md). &gt; The results indicate that transfer learning can further improve vocoder.    <p>\u7b2c 3.3 \u8282\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e, \u4e0e\u5728 SSL \u6a21\u578b\u9690\u85cf\u5c42\u4e0a\u8bad\u7ec3\u7684\u58f0\u7801\u5668\u76f8\u6bd4, \u57fa\u4e8e\u6807\u8bc6\u7b26\u7684\u58f0\u7801\u5668\u6709\u6539\u8fdb\u7684\u6f5c\u529b. \u5728\u8868 05 \u4e2d, \u6211\u4eec\u63a2\u7d22\u4e86\u8fc1\u79fb\u5b66\u4e60\u4ee5\u589e\u5f3a\u58f0\u7801\u5668, \u8be5\u58f0\u7801\u5668\u9996\u5148\u5728 ACE-Opencpop \u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3, \u7136\u540e\u9488\u5bf9 Opencpop \u8fdb\u884c\u5fae\u8c03. \u7ed3\u679c\u8868\u660e, \u8fc1\u79fb\u5b66\u4e60\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u58f0\u7801\u5668\u7684\u6027\u80fd.</p>"},{"location":"TTS/Models/Singing_Voice/2024.06.12_TokSing/#4conclusion","title":"4.Conclusion","text":"\u539f\u6587  &gt; This paper proposes a SVS framework ***TokSing*** based on discrete representations. &gt; A melody enhancement strategy is introduced to compensate for the loss of melody information during the discretization process. &gt; Experiments demonstrate that the discrete tokens are more efficient for singing representation and achieves superior singing synthesis quality with lower spatial overhead compared to traditional continuous representations.   <p>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u8868\u793a\u7684 SVS \u6846\u67b6 TokSing. \u5f15\u5165\u4e86\u4e00\u79cd\u65cb\u5f8b\u589e\u5f3a\u7b56\u7565, \u4ee5\u8865\u507f\u79bb\u6563\u5316\u8fc7\u7a0b\u4e2d\u65cb\u5f8b\u4fe1\u606f\u7684\u635f\u5931. \u5b9e\u9a8c\u8868\u660e, \u79bb\u6563\u6807\u8bc6\u7b26\u5728\u6b4c\u5531\u8868\u793a\u65b9\u9762\u66f4\u9ad8\u6548, \u5e76\u4e14\u4e0e\u4f20\u7edf\u7684\u8fde\u7eed\u8868\u793a\u76f8\u6bd4, \u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6b4c\u5531\u5408\u6210\u8d28\u91cf, \u540c\u65f6\u5177\u6709\u66f4\u4f4e\u7684\u7a7a\u95f4\u5f00\u9500.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/","title":"VALL-E","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers - \u4f5c\u8005:   - 01 [Chengyi Wang](../../Authors/Chengyi_Wang_(\u738b\u7a0b\u4e00).md)   - 02 [Sanyuan Chen](../../Authors/Sanyuan_Chen_(\u9648\u4e09\u5143).md)   - 03 [Yu Wu](../../Authors/Yu_Wu_(\u5434\u4fe3).md)   - 04 [Ziqiang Zhang](../../Authors/Ziqiang_Zhang_(\u5f20\u81ea\u5f3a).md)   - 05 [Long Zhou](../../Authors/Long_Zhou_(\u5468\u9f99).md)   - 06 [Shujie Liu](../../Authors/Shujie_Liu_(\u5218\u6811\u6770).md)   - 07 [Zhuo Chen](../../Authors/Zhuo_Chen_(\u9648\u5353).md)   - 08 [Yanqing Liu](../../Authors/Yanqing_Liu.md)   - 09 [Huaming Wang](../../Authors/Huaming_Wang.md)   - 10 [Jinyu Li](../../Authors/Jinyu_Li_(\u674e\u52b2\u5b87).md)   - 11 [Lei He](../../Authors/Lei_He_(\u4f55\u78ca).md)   - 12 [Sheng Zhao](../../Authors/Sheng_Zhao_(\u8d75\u80dc).md)   - 13 [Furu Wei](../../Authors/Furu_Wei_(\u97e6\u798f\u5982).md) - \u673a\u6784:   - [Microsoft](../../Institutions/Microsoft.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2023.01.05 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.20 - \u53d1\u8868:   - None - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2301.02111)   - [Demo](https://aka.ms/valle)   - [Scholar](https://scholar.google.com/scholar?cluster=) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md)   - [\u8bed\u8a00\u6a21\u578b](../../Tags/LanguageModel.md) - \u9875\u6570: 16 - \u5f15\u7528: 49 - \u88ab\u5f15: 321 - \u6570\u636e:   - [LibriSpeech](../../Datasets/LibriSpeech.md)   - [LibriLight](../../Datasets/2019.12.17_Libri-Light.md)   - [VCTK](../../Datasets/VCTK.md) - \u590d\u73b0:   - 2023.01.27 [lifeiteng/vall-e](https://github.com/lifeiteng/vall-e)   - 2023.12.02 [open-mmlab/Amphion](https://github.com/open-mmlab/Amphion/tree/main/models/tts/valle)   - 2024.06.10 [dukGuo/valle-audiodec](https://github.com/dukGuo/valle-audiodec) \u4ec5\u63a8\u7406"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#abstract","title":"Abstract","text":"\u539f\u6587  &gt; We introduce a language modeling approach for *text-to-speech synthesis (TTS)*. &gt; Specifically, we train a neural codec language model (called ***VALL-E***) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. &gt; During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. &gt; ***VALL-E*** emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. &gt; Experiment results show that ***VALL-E*** significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. &gt; In addition, we find that ***VALL-E*** could preserve the speaker\u2019s emotion and acoustic environment of the acoustic prompt in synthesis. &gt; See https://aka.ms/valle for demos of our work.   <p>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u8bed\u97f3\u5408\u6210\u7684\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u4f7f\u7528\u4ece\u73b0\u6210\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u5bfc\u51fa\u7684\u79bb\u6563\u7f16\u7801\u8bad\u7ec3\u4e86\u4e00\u4e2a\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b (\u79f0\u4e3a VALL-E), \u5e76\u5c06 TTS \u89c6\u4e3a\u4e00\u4e2a\u6761\u4ef6\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1, \u800c\u4e0d\u662f\u50cf\u4ee5\u524d\u7684\u5de5\u4f5c\u90a3\u6837\u89c6\u4e3a\u8fde\u7eed\u4fe1\u53f7\u56de\u5f52\u4efb\u52a1. \u5728\u9884\u8bad\u7ec3\u9636\u6bb5, \u6211\u4eec\u5c06 TTS \u8bad\u7ec3\u6570\u636e\u6269\u5c55\u5230 60K \u5c0f\u65f6\u7684\u82f1\u8bed\u8bed\u97f3, \u8fd9\u6bd4\u73b0\u6709\u7cfb\u7edf\u5927\u6570\u767e\u500d. VALL-E \u5c55\u73b0\u51fa\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b, \u5e76\u53ef\u7528\u4e8e\u4f7f\u7528\u4ec5 3 \u79d2\u7684\u672a\u89c1\u8bf4\u8bdd\u8005\u7684\u8f93\u5165\u5f55\u97f3\u4f5c\u4e3a\u58f0\u5b66\u63d0\u793a\u6765\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u8bed\u97f3. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e VALL-E \u5728\u8bed\u97f3\u81ea\u7136\u5ea6\u548c\u8bf4\u8bdd\u8005\u76f8\u4f3c\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u6b21\u8bed\u8a00\u5408\u6210\u7cfb\u7edf. \u6b64\u5916, \u6211\u4eec\u53d1\u73b0 VALL-E \u53ef\u4ee5\u5728\u5408\u6210\u4e2d\u4fdd\u7559\u58f0\u5b66\u63d0\u793a\u4e2d\u7684\u8bf4\u8bdd\u8005\u7684\u60c5\u611f\u548c\u58f0\u5b66\u73af\u5883 \u8bf7\u8bbf\u95ee https://aka.ms/valle \u67e5\u770b\u672c\u9879\u5de5\u4f5c\u7684\u793a\u4f8b.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#1introduction","title":"1.Introduction","text":"\u539f\u6587  &gt; The last decade has yielded dramatic breakthroughs in speech synthesis through the development of neural networks and end-to-end modeling. &gt; Currently, cascaded *text-to-speech (TTS)* systems ([Tacotron2 (2017)](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md), [FastSpeech (2019)](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md), [Transformer TTS (2018)](../../Models/TTS2_Acoustic/2018.09.19_Transformer_TTS.md)) usually leverage a pipeline with an acoustic model and a vocoder using mel spectrograms as the intermediate representations. &gt; While advanced TTS systems can synthesize high-quality speech from single or multiple speakers ([DelightfulTTS2 (2022)](../../Models/TTS2_Acoustic/2022.07.11_DelightfulTTS2.md), [VITS (2021)](../../Models/E2E/2021.06.11_VITS.md)), it still requires high-quality clean data from the recording studio. &gt; Large-scale data crawled from the Internet cannot meet the requirement, and always lead to performance degradation. &gt; Because the training data is relatively small, current TTS systems still suffer from poor generalization. &gt; Speaker similarity and speech naturalness decline dramatically for unseen speakers in the zero-shot scenario.  &gt; To tackle the zero-shot TTS problem, existing work leverages speaker adaptation [Chen et al., 2019, Wang et al., 2020] and speaker encoding [Arik et al., 2018, [YourTTS (2021)](../../Models/E2E/2021.12.04_YourTTS.md)] methods, requiring additional fine-tuning, complex pre-designed features, or heavy structure engineering.  &gt; Instead of designing a complex and specific network for this problem, the ultimate solution is to train a model with large and diverse data as much as possible, motivated by success in the field of text synthesis [Brown et al., 2020, Chowdhery et al., 2022]. &gt; Recent years have witnessed notable performance improvement for data increase in the text language model, from 16GB of uncompressed text [Devlin et al., 2019], to 160GB [Liu et al., 2019], to 570GB [Brown et al., 2020], and finally, around 1TB [Chowdhery et al., 2022]. &gt; Transferring this success to the field of speech synthesis, we introduce ***VALL-E***, the first language model-based TTS framework leveraging large, diverse, and multi-speaker speech data.   \u539f\u6587  &gt; As shown in [Fig.01](), to synthesize personalized speech (e.g., zero-shot TTS), ***VALL-E*** generates the corresponding acoustic tokens conditioned on the acoustic tokens of the 3-second enrolled recording and the phoneme prompt, which constrain the speaker and content information respectively. &gt; Finally, the generated acoustic tokens are used to synthesize the final waveform with the corresponding neural codec decoder [D\u00e9fossez et al., 2022]. &gt; The discrete acoustic tokens derived from an audio codec model enable us to treat TTS as conditional codec language modeling and advanced prompting-based large-model techniques (as in GPTs [Brown et al., 2020])can be leveraged for the TTS tasks. &gt; The acoustic tokens also allow us to generate diverse synthesized results in TTS by using different sampling strategies during inference.  &gt; We train ***VALL-E*** with LibriLight [Kahn et al., 2020], a corpus consisting of 60K hours of English speech with over 7000 unique speakers. &gt; The original data is audio-only, so we employ a speech recognition model to generate the transcriptions. &gt; Compared to previous TTS training datasets, such as LibriTTS [Zen et al., 2019], our data contain more noisy speech and inaccurate transcriptions but provide diverse speakers and prosodies. &gt; We believe the proposed approach is robust to the noise and generalize well by leveraging large data. &gt; It is worth noting that existing TTS systems are always trained with dozens of hours of single-speaker data or hundreds of hours of multi-speaker data, which is over hundreds of times smaller than ***VALL-E***. &gt; [Tab.01]() summarizes the innovation of ***VALL-E***, a language model approach for TTS, using audio codec codes as intermediate representations, leveraging large and diverse data, leading to strong in-context learning capabilities.   Table 1 Current Systems VALL-E Intermediate Representation Mel Spectrogram Audio Codec Code Objective Function Continuous Signal Regression Language Model Training Data \u2264600 Hours 60K Hours In-Context Language \u00d7 \u221a \u539f\u6587  &gt; We evaluate ***VALL-E*** on LibriSpeech [Panayotov et al., 2015] and VCTK [Veaux et al., 2016]datasets, where all test speakers are unseen in the training corpus. &gt; ***VALL-E*** significantly outperforms the state-of-the-art zero-shot TTS system ([YourTTS (2021)](../../Models/E2E/2021.12.04_YourTTS.md)) in terms of speech naturalness and speaker similarity, with +0.12 comparative mean option score (CMOS) and +0.93 similarity mean option score (SMOS) improvement on LibriSpeech. &gt; ***VALL-E*** also beats the baseline on VCTK with+0.11 SMOS and +0.23 CMOS improvements. &gt; It even achieves a +0.04 CMOS score against ground truth, showing the synthesized speech of unseen speakers is as natural as human recordings on VCTK. &gt; Moreover, the qualitative analysis shows that ***VALL-E*** is able to synthesize diverse outputs with the same text and target speaker, which could benefit pseudo-data creation for the speech recognition task. &gt; We also find that ***VALL-E*** could keep the acoustic environment (e.g., reverberation) and emotion (e.g. anger) of the acoustic prompt.  &gt; In summary, we make the following contributions. &gt; - We propose ***VALL-E***, the first TTS framework with strong in-context learning capabilities as GPT-3, which treats TTS as a language model task with audio codec codes as an intermediate representation to replace the traditional mel spectrogram. &gt; It has in-context learning capability and enables prompt-based approaches for zero-shot TTS, which does not require additional structure engineering, pre-designed acoustic features, and fine-tuning as in previous work. &gt; - We build a generalized TTS system in the speaker dimension by leveraging a huge amount of semi-supervised data, suggesting that simple scaling up semi-supervised data has been underestimated for TTS. &gt; - ***VALL-E*** is able to provide diverse outputs with the same input text and keep the acoustic environment and speaker\u2019s emotion of the acoustic prompt. &gt; - We verify that ***VALL-E*** synthesizes natural speech with high speaker similarity by prompt-ing in the zero-shot scenario. &gt; Evaluation results show that ***VALL-E*** significantly outperforms the state-of-the-art zero-shot TTS system on LibriSpeech and VCTK. &gt;  &gt; We encourage the reader to listen to our samples on the demo page https://aka.ms/valle."},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#2related-work","title":"2.Related Work","text":""},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#21zero-shot-tts","title":"2.1.Zero-Shot TTS","text":"\u539f\u6587  &gt; Current TTS methods can be categorized into cascaded and end-to-end methods. &gt; Cascaded TTS systems ([Tacotron2 (2017)](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md), [FastSpeech (2019)](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md), [Transformer TTS (2018)](../../Models/TTS2_Acoustic/2018.09.19_Transformer_TTS.md)) usually leverage a pipeline with an acoustic model and a vocoder using mel spectrograms as the intermediate representations. &gt; To tackle the drawbacks of the vocoder, end-to-end TTS models ([VITS (2021)](../../Models/E2E/2021.06.11_VITS.md), [DelightfulTTS2 (2022)](../../Models/TTS2_Acoustic/2022.07.11_DelightfulTTS2.md)) are proposed to jointly optimize the acoustic model and vocoder. &gt; In real scenarios, it is highly desirable to customize a TTS system to an arbitrary voice with rare enrolled recordings. &gt; Therefore, there is growing interest in the zero-shot multi-speaker TTS techniques, and most of work is done in the context of cascaded TTS systems. &gt; As the pioneers, Arik et al.2018 proposes speaker adaptation and speaker encoding approaches. &gt; In the line of speaker adaptation, the following work [Chen et al., 2019, Wang et al., 2020, Chen et al., 2021] tries to improve the adaptation efficiency with less target speaker data and speaker-specific parameters. &gt; Huang et al.[2022] applies meta-learning on speaker adaptation, which only requires 5-shot to build a well-performed system. &gt; In parallel, speaker encoding-based methods achieved great progress in recent years. &gt; A speaker encoding based system contains a speaker encoder and a TTS component, where the speaker encoder could be pre-trained on the speaker verification task [Jia et al., 2018]. &gt; In Jia et al.[2018] and Arik et al.[2018], the experiments show that the model is able to generate high-quality outputs with 3 seconds enrolled recordings for in-domain speakers. &gt; To improve the quality of unseen speakers, advanced speaker embedding models [Cai et al., 2018] can be employed, but it is still undesirable according to Tan et al.[2021]. &gt; Another way is to design advanced but complex speaker encoder [Wu et al., 2022].Diffusion model based TTS [Popov et al., 2021, Kim et al., 2022] is also extended to zero-shot TTS [Kang et al., 2022] and achieved good results. &gt; Compared to previous work [[FastSpeech (2019)](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md), Du et al.,2022], our work follows the line of cascaded TTS but first uses audio codec code as intermediate representations. &gt; It is the first one that has strong in-context learning capabilities as GPT-3, which does not require fine-tuning, pre-designed features, or a complex speaker encoder."},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#22spoken-generative-pre-trained-models","title":"2.2.Spoken Generative Pre-Trained Models","text":"\u539f\u6587  &gt; Self-supervised learning is widely investigated in the field of speech understanding [[Wav2Vec2.0 (2020)](../../Models/Speech_Representaion/2020.06.20_Wav2Vec2.0.md), [HuBERT (2021)](../../Models/Speech_Representaion/2021.06.14_HuBERT.md), Chen et al., 2022] and speech-to-speech generation [Lakhotia et al., 2021, [AudioLM (2022)](2022.09.07_AudioLM.md)]. &gt; In the context of speech-to-speech generation, a hot topic is how to synthesize speech in a textless setting. &gt; GSLM [Lakhotia et al.,2021] proposes to synthesize speech based on [HuBERT (2021)](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) codes, and Polyak et al.[2021] improves the performance by combining [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) codes with codes of VQVAE and a speaker encoder. &gt; [AudioLM (2022)](2022.09.07_AudioLM.md) follows a similar way but use audio codecs [Zeghidour et al.,2022] to synthesize speech, together with semantic codes. &gt; It should be noted that AudioLM is able to synthesize speech based on audio codecs without training an additional vocoder such as [HiFi-GAN (2020)](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md). &gt; AudioLM is a speech-to-speech model, whereas ***VALL-E*** is a TTS model, so we can explicitly control the content in speech synthesis. &gt; Another direction is to apply pre-training to the neural TTS. &gt; Chung et al.[2018] pre-trains speech decoder in TTS through autoregressive mel-spectrogram prediction. &gt; In Ao et al.[2022], the authors propose a unified-modal encoder-decoder framework SpeechT5, which can leverage unlabeled speech and text data to pre-train all components of TTS model. &gt; Tjandra et al.[2019] quantizes unlabeled speech into discrete tokens by a VQVAE model [van den Oord et al., 2017], and train a model with the token-to-speech sequence. &gt; They demonstrate that the pre-trained model only requires a small amount of real data for fine-tuning. &gt; Bai et al.[2022] proposes mask and reconstruction on mel spectrogram and showing better performance on speech editing and synthesis. &gt; Previous TTS pre-training work leverages less than 1K hours of data, whereas ***VALL-E*** is pre-trained with 60K hours of data. &gt; Furthermore, ***VALL-E*** is the first to use audio codec codes as intermediate representations, and emerge in-context learning capability in zero-shot TTS."},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#3background-speech-quantization","title":"3.Background: Speech Quantization","text":"\u539f\u6587  &gt; Since audio is typically stored as a sequence of 16-bit integer values, a generative model is required to output $2^{16}=65,536$ probabilities per timestep to synthesize the raw audio. &gt; In addition, the audio sample rate exceeding ten thousand leads to an extraordinarily long sequence length, making it more intractable for raw audio synthesis. &gt; To this end, speech quantization is required to compress integer values and sequence length.  &gt; $\\mu$-law transformation can quantize each timestep to 256 values and reconstruct high-quality raw audio. &gt; It is widely used in speech generative models, such as WaveNet [van den Oord et al., 2016], but the inference speed is still slow since the sequence length is not reduced. &gt; Recently, vector quantization is widely applied in self-supervised speech models for feature extraction, such as vq-wav2vec [Baevski et al., 2020a] and [HuBERT (2021)](../../Models/Speech_Representaion/2021.06.14_HuBERT.md). &gt; The following work [Lakhotia et al., 2021, Du et al., 2022] shows the codes from self-supervised models can also reconstruct content, and the inference speed is faster than WaveNet. &gt; However, the speaker identity has been discarded and the reconstruction quality is low [AudioLM (2022)](2022.09.07_AudioLM.md). &gt; [AudioLM (2022)](2022.09.07_AudioLM.md) trains speech-to-speech language models on both k-means tokens from a self-supervised model and acoustic tokens from a neural codec model, leading to high-quality speech-to-speech generation.  &gt; In this paper, we follow [AudioLM (2022)](2022.09.07_AudioLM.md) to leverage neural codec models to represent speech in discrete tokens. &gt; To compress audio for network transmission, codec models are able to encode waveform into discrete acoustic codes and reconstruct high-quality waveform even if the speaker is unseen in training. &gt; Compared to traditional audio codec approaches, the neural-based codec is significantly better at low bitrates, and we believe the quantized tokens contain sufficient information about the speaker and recording conditions. &gt; Compared to other quantization methods,the audio codec shows the following advantages:  &gt; 1. It contains abundant speaker information and acoustic information, which could maintain speaker identity in reconstruction compared to [HuBERT (2021)](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) codes. &gt; 2. There is an off-the-shelf codec decoder to convert discrete tokens into a waveform, without the additional efforts on vocoder training like VQ-based methods that operated on spectrum [Du et al., 2022]. &gt; 3. It could reduce the length of time steps for efficiency to address the problem in $\\mu$-law transformation [van den Oord et al., 2016].  &gt; We adopt a pre-trained neural audio codec model, EnCodec [D\u00e9fossez et al., 2022], as our tokenizer. &gt; EnCodec is a convolutional encoder-decoder model, whose input and output are both 24 kHz audio across variable bitrates. &gt; The encoder produces embeddings at 75 Hz for input waveforms at 24 kHz,which is a 320-fold reduction in the sampling rate. &gt; Each embedding is modeled by a *residual vector quantization (RVQ)*, in which we choose eight hierarchy quantizers with 1024 entries each as shown in [Fig.02]().   \u539f\u6587  &gt; This configuration corresponds to EnCodec at 6K bitrates for 24 kHz audio reconstruction. &gt; In this setting, given a 10-second waveform, the discrete representation is a matrix with750 \u00d7 8entries, where 750 =24,000\u00d710/320 is the downsampled time step and 8 is the number of quantizers. &gt; It is fine to choose other bitrate settings. &gt; A larger bitrate corresponds to more quantizers and better reconstruction quality. &gt; For example, if we choose EnCodecc at 12K bitrates, there are 16 quantizers are needed and the 10-second waveform corresponds to a matrix with 750\u00d716 entries. &gt; With the discrete codes from all quantizers, the convolutional decoder of EnCodec generates real-valued embeddings and reconstructs the waveform at 24 kHz."},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#4vall-e","title":"4.VALL-E","text":""},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#41problem-formulation-regarding-tts-as-conditional-codec-language-modeling","title":"4.1.Problem Formulation: Regarding TTS as Conditional Codec Language Modeling","text":"\u539f\u6587  &gt; Given a dataset $\\mathcal{D}=\\{\\mathbf{x}_i, \\mathbf{y}_i\\}$, where $\\mathbf{y}$ is an audio sample and $\\mathbf{x} = \\{x_0,x_1, \\cdots x_L\\}$ is its corresponding phoneme transcription, we use a pre-trained neural codec model to encode each audio sample into discrete acoustic codes, denoted as $\\text{Encodec}(\\mathbf{y}) = C^{T\\times 8}$, where $C$ represents the two-dimensional acoustic code matrix, and $T$ is the downsampled utterance length. &gt; The row vector of each acoustic code matrix $c_{t,:}$ represents the eight codes for frametand the column vector of each acoustic code matrix $c_{:,j}$ represents the code sequence from the $j$-th codebook, where $j \\in \\{1,\\cdots 8\\}$. &gt; After quantization, the neural codec decoder is able to reconstruct the waveform, denoted as $\\text{Decodec}(C)\\approx\\hat{\\mathbf{y}}$.  &gt; Zero-shot TTS requires the model to synthesize high-quality speech for unseen speakers. &gt; In this work, we regard zero-shot TTS as a conditional codec language modeling task. &gt; We train a neural language model to generate an acoustic code matrix $C$ conditioned on a phoneme sequence $\\mathbf{x}$ and an acoustic prompt matri $\\tilde{C}^{T'\\times 8}$ with the optimization objective of $\\max p(C|\\mathbf{x},\\tilde{C})$. &gt; Here, $\\tilde{C}$ is obtained by the same neural codec with an enrolled recording as the input. &gt; We expect the neural language model learns to extract the content and speaker information from the phoneme sequence and the acoustic prompt, respectively. &gt; During inference, given a phoneme sequence and a 3-second enrolled recording of the unseen speaker, the acoustic code matrix with corresponding content and speaker\u2019s voice is firstly estimated by the trained language model. &gt; Then the neural codec decoder synthesizes the high-quality speech."},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#42training-conditional-codec-language-modeling","title":"4.2.Training: Conditional Codec Language Modeling","text":"\u539f\u6587  &gt; The neural speech codec model allows us to operate on discrete audio representations. &gt; Due to residual quantization in the neural codec model, the tokens have a hierarchical structure: tokens from previous quantizers recover acoustic properties like speaker identity, while the consecutive quantizers learn fine acoustic details. &gt; Each quantizer is trained to model the residual from the previous quantizers. &gt; Motivated by this, we design two conditional language models in a hierarchical manner.  &gt; For the discrete tokens from the first quantizer $c_{:,1}$, we train an *autoregressive (AR)* decoder-only language model. &gt; It is conditioned on the phoneme sequencexand the acoustic prompt $\\tilde{C}_{:,1}$, formulated as    <p>$$   p(c_{:,1}|\\mathbf{x}, \\tilde{C}{:,1}; \\theta{AR}) =\\prod_{t=0}^T p(c_{t,1}|c_{&lt;t,1},\\tilde{c}{:,1}, \\mathbf{x}; \\theta{AR}) \\tag{1} $$</p> \u539f\u6587 <p>Since VALL-E is a decoder-only LM, the concatenation of $\\tilde{c}{:,1}$ and $c{:,1}$ is a whole sequence, and we do not distinguish them or insert a specific token in training. Only $c_{:,1}$ is predicted while the prefix $\\tilde{c}_{:,1}$ is given during inference.</p> <p> </p> \u539f\u6587  &gt; For the discrete tokens from the second to the last quantizers, $c_{:,j}\\in[2,8]$, we train a *non-autoregressive (NAR)* language model. &gt; Since the tokens can not access each other in a NAR manner, to constrain the speaker identity, the acoustic prompt matrix $\\tilde{C}$ is used as an acoustic prompt. &gt; Thus, the model is conditioned on the phoneme sequencex, the acoustic prompt $\\tilde{C}$ and the predicted acoustic tokens belong to the previous codebooks $C_{:,   $$   p(C_{:,2:8}|\\mathbf{x},\\tilde{C};\\theta_{NAR})=\\prod_{j=2}^{8}p(c_{:,j}|C_{:, \u539f\u6587  &gt; The combination of the AR model and the NAR model provides a good trade-off between speech quality and inference speed. &gt; On the one hand, the rate of the generated speech should be consistent with the enrolled recording, and it is hard to train a length predictor for different speakers since their speaking speed may be very diverse. &gt; In this case, the AR model is a more natural choice with its flexibility for acoustic sequence length prediction. &gt; On the other hand, for the consecutive stages, as the number of output slots follows the sequence length of the first stage, NAR can reduce the time complexity from $\\mathcal{O}(T)$ to $\\mathcal{O}(1)$. &gt; Overall, the prediction of C can be modeled as:   <p></p> <p>$$   p(C|\\mathbf{x},\\tilde{C};\\theta)=p(c_{:,1}|\\tilde{C}{:,1}, \\mathbf{X}; \\theta{AR}) \\prod_{j=2}^{8}p(c_{:,j}|c_{:,&lt;j},\\mathbf{x},\\tilde{C};\\theta_{NAR}) \\tag{3} $$</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#421autoregressive-codec-language-modeling","title":"4.2.1.Autoregressive Codec Language Modeling","text":"\u539f\u6587 <p>The autoregressive language model generates the tokens from the first quantizer. It comprises a phoneme embedding $W_x$, an acoustic embedding $W_a$, a transformer decoder, and a prediction layer. In order to generate speech with specific content, we use the phoneme sequence as the phoneme prompt of the language model. Thus, the model input is the concatenation of $\\mathbf{x}$ and $\\mathbf{c}{:,1}$, and two special <code>&lt;EOS&gt;</code> tokens are appended after each of them. We compute sinuous position embedding separately for prompt and input tokens. For the causal transformer model, each tokenct,1can attend to $(\\mathbf{x}, c{\\leq t,1})$ as illustrated in the left part of Fig.03.</p> <p> </p> <p></p> \u539f\u6587  &gt; The model is optimized to maximize the probability of the next token in the first codebook. &gt; We share the parameters of the output projection layer with the parameters of the acoustic embedding $W_a$.  &gt; In the AR model, we do not explicitly extract an audio clip as the prompt in training. &gt; The training process is pure casual language model training. &gt; In this way, any prefix sequence $c_{ During inference, given an enrolled recording, we should concatenate the phoneme sequence of the enrolled recording and the phoneme sequence for synthesis together. &gt; Meanwhile, the acoustic token sequence of the enrolled recording is used as the prefix in AR decoding, as formulated in [Eq.01](). &gt; We will study the superiority of this setting in the experiment.   <p></p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#422non-autoregressive-codec-language-modeling","title":"4.2.2.Non-Autoregressive Codec Language Modeling","text":"\u539f\u6587  &gt; When we obtain the first quantizer codes by the AR model, we employ a non-autoregressive (NAR) model to generate codes of the other seven quantizers. &gt; The NAR model has a similar architecture to the AR model, except that it contains eight separate acoustic embedding layers. &gt; In each training step, we randomly sample a training stage $i\\in [2, 8]$. &gt; The model is trained to maximize the acoustic tokens from the $i$-th quantizer codebook. &gt; The acoustic tokens from stage $1$ to stage $i\u22121$ are embedded and summed up as model input:   <p>$$ \\begin{align}   e_{c_{t,j}}&amp;=W_a^j\\odot c_{t,j}\\tag{4}\\\\mathbf{e_{c_t}}&amp;=\\sum_{j=1}^{i-1}e_{c_t,j}\\tag{5} \\end{align} $$</p> <p>where $\\odot$ indicates index selection.</p> \u539f\u6587  &gt; The phoneme sequence is also regarded as the prompt of the language model. &gt; Besides, to clone the unique voice of the given speaker, we also use the acoustic tokens from the enrolled speech as the acoustic prompt. &gt; Specifically, we first tokenize the enrolled speech with the neural codec model as $\\tilde{C}^{T\\times 8}$. &gt; The embedded representations from all of the eight codebooks are summed up as the acoustic prompt $e_{\\tilde{c}_t}=\\sum_{j=1}^8 e_{\\tilde{c}_{t,j}}$. &gt; To predict the acoustic tokens from thei-th codebook, the transformer input is the concatenation of $(\\mathbf{e}_{\\mathbf{x}}, \\mathbf{e}_{\\tilde{c}}, \\mathbf{e}_{c_{:, The positional embeddings are also computed separately for prompts and the acoustic sequence. &gt; The current stage $i$ is injected into the network with Adaptive Layer Normalization [Xu et al., 2019] operator, i.e., $\\text{AdaLN}(h, i) = a_i\\text{LayerNorm}(h) + b_i$, where $h$ is the intermediate activations, $a_i$ and $b_i$ are obtained from a linear projection of the stage embedding. &gt; Unlike AR, the NAR model allows each token to attend to all the input tokens in the self-attention layer. &gt; We also share the parameters of the acoustic embedding layer and the output prediction layer, which means the weights of the $j$-th prediction layer are the same as the $(j + 1)$-th acoustic embedding layer.   <p></p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#43inference-in-context-learning-via-prompting","title":"4.3.Inference: In-Context Learning via Prompting","text":"\u539f\u6587  &gt; In-context learning is a surprising ability of the text-based language model, which is able to predict labels for unseen inputs without additional parameter updates. &gt; For TTS, if the model can synthesize high-quality speech for unseen speakers without fine-tuning, the model is believed to have in-context learning capability. &gt; However, the in-context learning capability of existing TTS systems is not strong,because they either require additional fine-tuning or degrade dramatically for unseen speakers.  &gt; For language models, prompting is necessary to enable in-context learning in the zero-shot scenario. &gt; We design prompts and inference as follows. &gt; We first convert the text into a phoneme sequence and encode the enrolled recording into an acoustic matrix, forming the phoneme prompt and acoustic prompt. &gt; Both prompts are used in the AR and NAR models. &gt; For the AR model, we use sampling-based decoding conditioned on the prompts since we observe that beam search may lead the LM into an infinity loop. &gt; Furthermore, the sampling-based method could significantly increase the diversity of the output. &gt; For the NAR model, we use greedy decoding to choose the token with the highest probability. &gt; Finally, we use the neural codec decoder to generate the waveform conditioned on the eight code sequences.  &gt; The acoustic prompt may or may not semantically relate to the speech to be synthesized, resulting in two cases: &gt; - ***VALL-E***:  &gt; Our main interest is to generate given content for unseen speakers. &gt; The model is given a text sentence, a segment of enrolled speech, and its corresponding transcription. &gt; We prepend the transcription phoneme of the enrolled speech to the phoneme sequence of the given sentence as the phoneme prompt, and use the first layer acoustic token of the enrolled speech\u02dcc:,1as an acoustic prefix. &gt; With the phoneme prompt and the acoustic prefix, ***VALL-E*** generates the acoustic tokens for the given text cloning the voice of this speaker. &gt; - **VALL-E-continual**:  &gt; In this setting, we use the whole transcription and the first 3 seconds of the utterance as the phoneme and acoustic prompts respectively, and ask the model to generate the continuations. &gt; The inference process is the same as setting ***VALL-E***, except that the enrolled speech and the generated speech are semantically continuous."},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#5experiment","title":"5.Experiment","text":""},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#51experiment-setup","title":"5.1.Experiment Setup","text":""},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#dataset","title":"Dataset:","text":"<p>We use LibriLight [Kahn et al., 2020] as the training data which contains 60K hours of unlabelled speech from audiobooks in English. The number of distinct speakers is around 7000 in LibriLight. We train a hybrid DNN-HMM ASR model on 960 hours labeled LibriSpeech following Kaldi recipe [Povey et al., 2011]. Once the hybrid model is trained, unlabeled speech data is decoded and transduced to the best phoneme-level alignment paths where the frameshift is 30ms. The EnCodec model [D\u00e9fossez et al., 2022] is used to generate the acoustic code matrix for the 60K hours of data.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#model","title":"Model:","text":"<p>Both the AR model and the NAR model have the same transformer architecture with 12 layers, 16 attention heads, an embedding dimension of 1024, a feed-forward layer dimension of 4096, and a dropout of 0.1. The average length of the waveform in LibriLight is 60 seconds. During training, we randomly crop the waveform to a random length between 10 seconds and 20 seconds. Its corresponding phoneme alignments are used as the phoneme prompt. We remove the consecutive repetitions in the force-aligned phoneme sequence. For the NAR acoustic prompt tokens, we select a random segment waveform of 3 seconds from the same utterance.</p> <p>The models are trained using 16 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustic tokens per GPU for 800k steps. We optimize the models with the AdamW optimizer, warm up the learning rate for the first 32k updates to a peak of 5 \u00d7 10\u22124, and then linear decay it.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#baseline","title":"Baseline:","text":"<p>We choose the SOTA zero-shot TTS model YourTTS [Casanova et al., 2022b] as the baseline, which is trained on a combined dataset of VCTK [Veaux et al., 2016], LibriTTS [Zen et al., 2019], and TTS-Portuguese [Casanova et al., 2022a]. We use their released checkpoint\u2217. </p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#automatic-metrics","title":"Automatic metrics:","text":"<p>We employ the SOTA speaker verification model, WavLM-TDNN [Chen et al., 2022], to evaluate the speaker similarity between prompt (the decompressed enrolled speech) and synthesized speech. WavLM-TDNN achieved the top rank at the VoxSRC Challenge 2021 and 2022 leaderboards. It reached an average Equal Error Rate (EER) of 0.383, 0.480, and 0.986 on Vox1-O, Vox1-E, and Vox1-H respectively. The similarity score predicted by WavLM-TDNN is in the range of [\u22121, 1], where a larger value indicates a higher similarity of input samples.</p> <p>We also evaluate the synthesis robustness of our model. Neural TTS systems suffer from the robustness issue, which sometimes has deletion, insertion, and replacement errors due to wrong attention alignments. We perform ASR on the generated audio and calculate the word error rate (WER) with respect to the original transcriptions. In this experiment, we employ the HuBERT-Large [Hsu et al., 2021] model fine-tuned on LibriSpeech 960h as the ASR model, which is a CTC-based model without language model fusion.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#human-evaluation","title":"Human evaluation:","text":"<p>We calculate the comparative mean option score (CMOS) and similarity mean option score (SMOS) by crowdsourcing, where 12 and 6 native speakers are invited as CMOS and SMOS contributors. The scale of SMOS is from 1 to 5 with 0.5-point increments. CMOS ranges from -3 (the new system is much worse than baseline) to 3 (the new system is much better than baseline) with intervals of 1. CMOS is an indicator of speech naturalness, and SMOS measures whether the speech is similar to the original speaker\u2019s voice.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#52librispeech-evaluation","title":"5.2.LibriSpeech Evaluation","text":"<p>We first use LibriSpeech [Panayotov et al., 2015] for zero-shot TTS evaluation, since there is no speaker overlap between LibriLight training data and LibriSpeech test-clean data. Following Borsos et al. [2022], we use the samples from LibriSpeech test-clean with lengths between 4 and 10 seconds, resulting in a 2.2 hours subset. For each sample synthesis, VALL-E randomly choose another utterance of the same speaker and crop a 3-seconds speech segment as the enrolled speech. Each experiment runs three times and the average score is reported. VALL-E-continual uses the first 3 seconds of the ground-truth speech as enrolled speech.</p> <p>Table 2 shows the objective evaluation results. We first compute the WER score and the speaker similarity score of the ground truth speech as the upper bound. To compare the speaker similarity, we use speech pairs from the same speaker in the test set. Compared with the YourTTS baseline, our model is significantly better in both robustness and speaker similarity, showing that our generated speech is highly faithful to the given text and the given enrolled speech. Furthermore, the word error rate can be further reduced in VALL-E-continual setting, because the acoustic tokens for the first 3 seconds are extracted from the ground truth. We also compare the robustness with other speech-to-speech LM-based generation models, GSLM and AudioLM, which use audio latent codes as input. GSLM uses HuBERT code as input and reconstructs the waveform with the Tacotron2 [Shen et al., 2018] model and the WaveGlow [Prenger et al., 2019] vocoder. We run their open-sourced code using the released model and evaluate the results. Since the HuBERT codes discard the speaker identity, it achieves a poor speaker score. For the AudioLM, we list their WER score reported in their paper, which is obtained by a Conformer Transducer model. The experiment results show that VALL-E is better than other speech-to-speech LM-based generative systems in terms of robustness. One major reason is VALL-E trained with pseudo-phoneme instead of HuBERT/w2v-BERT codes, which enjoys better alignment quality with the input text.</p> <p>We randomly sample one utterance for each speaker in LibriSpeech test-clean for the human evaluation, resulting in 40 test cases. Table 3 shows the human evaluation results. VALL-E is very closed to ground truth in terms of SMOS, indicating the synthesized speech is similar to the given unseen speaker in testing. It significantly outperforms the baseline with +0.93 SMOS, demonstrating the effectiveness of VALL-E in zero-shot scenarios. Regarding naturalness, VALL-E beats the baseline with +0.12 CMOS, indicating the proposed method could synthesize more natural and realistic speech against baselines</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#ablation-study","title":"Ablation Study:","text":"<p>In this section, we perform detailed ablation experiments. We first study the NAR model. We train three NAR models with different numbers of prompts. The settingNAR-no prompt is trained without any prompts. The settingNAR-phn prompt is trained with only phoneme sequence as prompt and the setting NAR-2 prompts uses both phoneme prompt and acoustic token prompt as conditions. In evaluation, we use the ground-truth first-level acoustic tokens as the model input and compute the WER and speaker similarity scores. The results are listed in Table 4. Results show that the model without any prompts performs poorly on both ASR and speaker similarity evaluations, even though the acoustic input token is ground truth. When adding the phoneme prompt, the WER is reduced by a large margin from 19.6 to 3.0. It shows the phoneme prompt mainly contributes to the content of the generation. In theNAR-2 prompts, the model can learn speaker information from the acoustic token prompt and thus improve the speaker evaluation quality.</p> <p>We further conduct the ablation experiments on the AR model. In these experiments, we always use theNAR-2 promptssetting as the NAR model. In Table 5, we can see that when we remove the acoustic prompt (w/o acoustic prompt), it can only obtain a speaker similarity score of 0.236, showing the prompt is extremely crucial for speaker identity. Even if the NAR model could see the prompt, the prompt for the AR model also contributes a lot to speaker similarity.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#53vctk-evaluation","title":"5.3.VCTK Evaluation","text":"<p>We evaluate our model on VCTK consisting of 108 speakers, where none of the speakers are observed during training. Since YourTTS has seen 97 speakers in VCTK as training, we evaluate YourTTS performance on the full 107 speakers and 11 unseen speakers, respectively. For each speaker, we randomly selected three utterances of 3s/5s/10s as the prompts and the text of another utterance as the text prompt.</p> <p>We first evaluate two models with the speaker verification metric as described before. From Table 6, we can see that VALL-E outperforms the baseline even if the baseline has seen 97 speakers in training, indicating our model is able to synthesize speech with higher speaker similarity. When we compare with the baseline in a fair setting (11 speakers), the performance gap becomes larger, especially when only 3s prompts are available. By comparing different lengths of the prompt, we can see our model is able to generate more similar speech when the prompt becomes longer, which is consistent with our intuition.</p> <p>We sample 60 speakers for human evaluation, one utterance for each, where 11 are unseen speakers, and 49 speakers have been seen for YourTTS. VALL-E do not see any of the 60 speakers. During model synthesis, each speaker has a 3-second enrolled recording. Table 7 shows a comparison of our method against baseline and ground truth. The comparison of SMOS shows that VALL-E has better speaker similarity than the baseline, even if the baseline has seen some of the speakers in training. The side-by-side CMOS evaluation shows that VALL-E is +0.23 over YourTTS, indicating a significantly better performance on speaking of naturalness. Furthermore, VALL-E achieves +0.04 CMOS over ground-truth, demonstrating no statistically significant difference from human recordings on this dataset. Compared to the evaluation results on LibriSpeech, VALL-E shows a better CMOS score in the comparison with ground truth, which is mainly because the average sentence length is shorter and some of the ground truth utterances also have noisy environments in VCTK. In terms of speaker similarity, VCTK is more challenging as it contains speakers with various accents while the training data and LibriSpeech test data do not contain various accent speakers.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#54qualitative-analysis","title":"5.4.Qualitative Analysis","text":"<p>Diversity:Previous TTS systems have a strong one-one mapping between input text and output waveform, because mel spectrum generation is based on reconstruction for each step without randomness. Since VALL-E uses the sampling-based method to generate discrete tokens, its output is diverse for the same input text due to the randomness in inference. Given a sentence and an enrolled recording, we run the inference process twice and visualize its waveform in Figure 4. In Figure 4(a), we observe the two samples have different lengths and phrase durations, where the first has a faster speech rate. In Figure 4(b), we observe that the accents of the two samples are different. The second output emphasizes the word \u201cmust\" with a larger amplitude whereas the first output does not. We leave more samples on our demo page.</p> <p>The diversity is important for some downstream scenarios. For example, speech recognition always benefits from diverse inputs with different speakers and acoustic environments, which cannot be met by the previous TTS system. Considering the diversity feature of VALL-E, it is an ideal candidate to generate pseudo-data for speech recognition.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#acoustic-environment-maintenance","title":"Acoustic environment maintenance:","text":"<p>Another interesting finding is the acoustic environment consistency between the acoustic prompt and the generation. When the acoustic prompt has reverberation, VALL-E could synthesize speech with reverberation as well, whereas the baseline outputs clean speech. Our explanation is that VALL-E is trained on a large-scale dataset consisting of more acoustic conditions than the data used by the baseline, so VALL-E could learn the acoustic consistency instead of a clean environment only during training. We show consistency on our demo page.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#speakers-emotion-maintenance","title":"Speaker\u2019s emotion maintenance:","text":"<p>Emotional TTS is a classic subtopic of speech synthesis, which synthesizes speech with a required emotion. Traditional methods [Lei et al., 2021] always train a model on a supervised emotional TTS dataset, where the speech corresponds to a transcription and an emotion label. We find that VALL-E can preserve the emotion in the prompt at a zero-shot setting. We select acoustic prompts from EmoV-DB [Adigwe et al., 2018], a dataset containing speech with five emotions, VALL-E is able to keep the same emotion of the prompt in speech synthesis, even if the model is not fine-tuned on an emotional TTS dataset. We put audio samples on our demo page.</p>"},{"location":"TTS/Models/Speech_LLM/2023.01.05_VALL-E/#6conclusion-limitations-future-work","title":"6.Conclusion, Limitations, Future Work","text":"\u539f\u6587  &gt; We introduced ***VALL-E***, a language model approach for TTS with audio codec codes as intermediate representations.  &gt; We pre-train ***VALL-E*** with 60K hours of speech data, and show the in-context learning capability in zero-shot scenarios.  &gt; We achieve new state-of-the-art zero-shot TTS results on LibriSpeech and VCTK.  &gt; Furthermore, ***VALL-E*** could keep the acoustic environment and speaker\u2019s emotion in synthesis, and provide diverse outputs in different sampling-based decoding processes.  &gt; Despite making significant progress, ***VALL-E*** still suffers from several issues.  &gt; **Synthesis robustness** &gt; We observe that some words may be unclear, missed, or duplicated in speech synthesis.  &gt; It is mainly because the phoneme-to-acoustic language part is an autoregressive model, in which disordered attention alignments exist and no constraints to solving the issue.  &gt; The phenomenon is also observed in vanilla Transformer-based TTS, which was addressed by applying non-autoregressive models or modifying the attention mechanism in modeling.  &gt; In the future, we would like to leverage these techniques to solve the issue.  &gt; **Data coverage** &gt; Even if we use 60K hours of data for training, it still cannot cover everyone\u2019s voice,especially accent speakers.  &gt; The worse result on VCTK than LibriSpeech also implies insufficient coverage of accent speakers. Moreover, the diversity of speaking styles is not enough, as LibriLight is an audiobook dataset, in which most utterances are in reading style.  &gt; In the future, we will further scale up the training data to improve the model performance across prosody, speaking style, and speaker similarity perspectives.  &gt; We believe the zero-shot TTS task could be almost solved through our approach with model and data scale-up.  &gt; **Model Structure** &gt; Now, we use two models to predict codes of different quantizers.  &gt; A promising direction is to predict them with a large universal model.  &gt; Another interesting direction is using full NAR models to speed up model inference in the framework.  &gt; **Broader impacts** &gt; Since ***VALL-E*** could synthesize speech that maintains speaker identity, it may carry potential risks in misuse of the model, such as spoofing voice identification or impersonating a specific speaker.  &gt; To mitigate such risks, it is possible to build a detection model to discriminate whether an audio clip was synthesized by ***VALL-E***.  &gt; We will also put Microsoft AI Principles\u2217into practice when further developing the models."},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/","title":"SoundStorm","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: SoundStorm: Efficient Parallel Audio Generation - \u4f5c\u8005:   - [Zalan Borsos](../../Authors/Zalan_Borsos.md)   - [Matt Sharifi](../../Authors/Matt_Sharifi.md)   - [Damien Vincent](../../Authors/Damien_Vincent.md)   - [Eugene Kharitonov](../../Authors/Eugene_Kharitonov.md)   - [Neil Zeghidour](../../Authors/Neil_Zeghidour.md)   - [Marco Tagliasacchi](../../Authors/Marco_Tagliasacchi.md) - \u673a\u6784:   - [Google Research](../../Institutions/Google.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2023.05.16 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.13 - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2305.09636)   - [DOI]()   - [Github]()   - [Demo](https://google-research.github.io/seanet/soundstorm/examples/) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md) - \u9875\u6570: 9 - \u5f15\u7528: ? - \u88ab\u5f15: 36"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM (2022), and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM (2022), our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers\u2019 voices. Audio samples are available at https://google-research.github.io/seanet/soundstorm/examples/</p> <p>\u6211\u4eec\u63d0\u51fa\u4e86 SoundStorm, \u4e00\u4e2a\u7528\u4e8e\u9ad8\u6548\u975e\u81ea\u56de\u5f52\u97f3\u9891\u751f\u6210\u7684\u6a21\u578b. SoundStorm \u4ee5 AudioLM \u7684\u8bed\u4e49 token \u4f5c\u4e3a\u8f93\u5165, \u5e76\u4f9d\u8d56\u4e8e\u53cc\u5411\u6ce8\u610f\u529b\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5e76\u884c\u89e3\u7801\u6765\u751f\u6210\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684 token. \u4e0e AudioLM \u7684\u81ea\u56de\u5f52\u751f\u6210\u65b9\u6cd5\u76f8\u6bd4, SoundStorm \u80fd\u591f\u751f\u6210\u76f8\u540c\u8d28\u91cf\u7684\u97f3\u9891, \u4e14\u5728\u58f0\u97f3\u548c\u58f0\u5b66\u6761\u4ef6\u4e0b\u5177\u6709\u66f4\u9ad8\u7684\u4e00\u81f4\u6027\u7684\u540c\u65f6, \u901f\u5ea6\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7. \u6211\u4eec\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf, \u81ea\u7136\u5bf9\u8bdd\u7247\u6bb5\u6765\u5c55\u793a\u6a21\u578b\u6269\u5c55\u97f3\u9891\u751f\u6210\u66f4\u957f\u5e8f\u5217\u7684\u80fd\u529b. \u8fd9\u4e9b\u5bf9\u8bdd\u7247\u6bb5\u9700\u8981\u7ed9\u5b9a\u4e00\u4e2a\u5e26\u6709\u8bf4\u8bdd\u8005\u8f6c\u5f55\u811a\u672c\u548c\u4e00\u4e2a\u5e26\u6709\u8bf4\u8bdd\u4eba\u58f0\u97f3\u7684\u7b80\u77ed\u63d0\u793a.</p>"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Modeling discrete representations of audio produced by neural codecs (SoundStream (2021); EnCodec (2022)) makes the task of audio generation amenable to the powerful Transformer-based sequence-to-sequence modeling approaches (Transformer (2017)). Casting unconditional and conditional audio generation as sequence-to-sequence modeling has unlocked rapid progress in speech continuation (AudioLM (2022)), text-to-speech (VALL-E (2023); SPEAR-TTS (2023)), and general audio and music generation (AudioGen (2022); MusicLM (2023)).</p> <p>\u5efa\u6a21\u7531\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u751f\u6210\u7684\u97f3\u9891\u79bb\u6563\u8868\u793a\u4f7f\u5f97\u97f3\u9891\u751f\u6210\u4efb\u52a1\u53ef\u4ee5\u91c7\u7528\u5f3a\u5927\u7684\u57fa\u4e8e Transformer \u5e8f\u5217\u5230\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5. \u5c06\u65e0\u6761\u4ef6\u548c\u6709\u6761\u4ef6\u97f3\u9891\u751f\u6210\u89c6\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u5efa\u6a21\u5df2\u7ecf\u89e3\u9501\u4e86\u5728\u8bed\u97f3\u5ef6\u7eed, \u6587\u672c\u8f6c\u8bed\u97f3, \u4e00\u822c\u97f3\u9891\u548c\u97f3\u4e50\u751f\u6210\u65b9\u9762\u7684\u5feb\u901f\u53d1\u5c55.</p> <p>For generating high-quality audio by modeling the tokens of a neural codec, the rate of the discrete representation must be increased, resulting in either an exponential growth in codebook size or in long token sequences. While the exponential growth of the codebook is prohibitive due to memory limitations, in turn, long token sequences also present computational challenges for autoregressive models. In particular, attention-based models, which are the main focus of this work, will incur quadratic runtime complexity with respect to the sequence length for calculating the self-attention. Thus, addressing the trade-off between perceptual quality and runtime is one of the core challenges for audio generation.</p> <p>\u4e3a\u4e86\u901a\u8fc7\u5efa\u6a21\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u7684 token \u6765\u751f\u6210\u9ad8\u8d28\u91cf\u97f3\u9891, \u79bb\u6563\u8868\u793a\u7684\u6bd4\u7387\u5fc5\u987b\u589e\u52a0, \u5bfc\u81f4\u4ee3\u7801\u672c\u5c3a\u5bf8\u6216\u957f token \u5e8f\u5217\u5448\u6307\u6570\u589e\u957f. \u867d\u7136\u7531\u4e8e\u5185\u5b58\u9650\u5236, codebook \u7684\u6307\u6570\u589e\u957f\u662f\u4e0d\u53ef\u884c\u7684, \u4f46\u53cd\u8fc7\u6765, \u957f token \u5e8f\u5217\u4e5f\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u5e26\u6765\u4e86\u8ba1\u7b97\u6311\u6218. \u7279\u522b\u662f\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u578b, \u548c\u7528\u4e8e\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\u7684\u5e8f\u5217\u957f\u5ea6\u5448\u4e8c\u6b21\u65f6\u95f4\u590d\u6742\u6027. \u56e0\u6b64, \u89e3\u51b3\u611f\u77e5\u8d28\u91cf\u548c\u8fd0\u884c\u65f6\u95f4\u7684\u6743\u8861\u662f\u97f3\u9891\u751f\u6210\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218.</p> <p>The problem of generating long audio token sequences can be addressed by at least three orthogonal approaches or a combination thereof: 1. Efficient attention mechanisms (Reformer (2020); Performers (2021); Nystr\u00f6mformer (2021); Perceiver AR (2022)), 2. Non-autoregressive, parallel decoding schemes (NAT (2017); Mask-Predict (2019); MaskGIT (2022)), 3. Custom architectures adapted to the special structure of the tokens produced by neural audio codecs (AudioGen (2022); VALL-E (2023); RQ-Transformer (2022)).</p> <p>However, in the context of modeling the token sequence of neural audio codecs, either unconditionally or based on weak conditioning such as text, the efficient generation of long, high-quality audio segments remains an open problem.</p> <p>\u751f\u6210\u957f\u97f3\u9891 token \u5e8f\u5217\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u81f3\u5c11\u4e09\u79cd\u6b63\u4ea4\u65b9\u6cd5\u6216\u7ec4\u5408\u6765\u89e3\u51b3: 1. \u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236; 2. \u975e\u81ea\u56de\u5f52\u5e76\u884c\u89e3\u7801\u65b9\u6848; 3. \u9002\u7528\u4e8e\u7531\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u751f\u6210\u7684 token \u7279\u6b8a\u7ed3\u6784\u7684\u81ea\u5b9a\u4e49\u7f51\u7edc\u7ed3\u6784.</p> <p>\u7136\u800c\u5728\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684 token \u5e8f\u5217\u5efa\u6a21\u7684\u80cc\u666f\u4e0b, \u65e0\u8bba\u662f\u5728\u65e0\u6761\u4ef6\u8fd8\u662f\u5f31\u6761\u4ef6 (\u5982\u6587\u672c) \u4e0b, \u9ad8\u6548\u751f\u6210\u957f\u4e14\u9ad8\u8d28\u91cf\u7684\u97f3\u9891\u7247\u6bb5\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898.</p> <p>We believe that it is the special structure of the audio token sequence that holds the most promise for future advances in long-sequence audio modeling. Concretely, both SoundStream (2021) and EnCodec (2022) rely on Residual Vector Quantization (RVQ), where each compressed audio frame is quantized by a series of quantizers, with each quantizer operating on the residual of the previous one, and the number of quantizers control-ling the overall bitrate. This induces a hierarchical token structure, where tokens from finer RVQ levels contribute less to the perceptual quality, allowing for efficient factorizations and approximations of the joint distribution of the token sequence. Hence, the models and decoding schemes should take this special structure of the input into account for efficient training and inference.</p> <p>\u6211\u4eec\u8ba4\u4e3a\uff0c\u97f3\u9891\u6807\u8bb0\u5e8f\u5217\u7684\u7279\u6b8a\u7ed3\u6784\u5bf9\u4e8e\u672a\u6765\u5728\u957f\u5e8f\u5217\u97f3\u9891\u5efa\u6a21\u65b9\u9762\u7684\u8fdb\u6b65\u6700\u6709\u5e0c\u671b\u3002 \u5177\u4f53\u6765\u8bf4\uff0cSoundStream\uff082021\uff09\u548cEnCodec\uff082022\uff09\u90fd\u4f9d\u8d56\u4e8eResidual Vector Quantization\uff08RVQ\uff09\uff0c\u5176\u4e2d\u6bcf\u4e2a\u538b\u7f29\u7684\u97f3\u9891\u5e27\u7531\u4e00\u7cfb\u5217\u91cf\u5316\u5668\u8fdb\u884c\u91cf\u5316\uff0c\u6bcf\u4e2a\u91cf\u5316\u5668\u5728\u4e4b\u524d\u7684\u91cf\u5316\u5668\u7684\u6b8b\u5dee\u4e0a\u64cd\u4f5c\uff0c\u91cf\u5316\u5668\u7684\u6570\u91cf\u63a7\u5236\u603b\u4f53\u6bd4\u7279\u7387\u3002 \u8fd9\u5f15\u5165\u4e86\u5206\u5c42\u7684\u6807\u8bb0\u7ed3\u6784\uff0c\u5176\u4e2d\u6765\u81ea\u66f4\u7cbe\u7ec6RVQ\u7ea7\u522b\u7684\u6807\u8bb0\u5bf9\u611f\u77e5\u8d28\u91cf\u7684\u8d21\u732e\u8f83\u5c0f\uff0c\u5141\u8bb8\u5bf9\u6807\u8bb0\u5e8f\u5217\u7684\u8054\u5408\u5206\u5e03\u8fdb\u884c\u6709\u6548\u7684\u56e0\u5f0f\u5206\u89e3\u548c\u8fd1\u4f3c\u3002 \u56e0\u6b64\uff0c\u6a21\u578b\u548c\u89e3\u7801\u65b9\u6848\u5e94\u8be5\u8003\u8651\u5230\u8f93\u5165\u7684\u8fd9\u79cd\u7279\u6b8a\u7ed3\u6784\uff0c\u4ee5\u4fbf\u8fdb\u884c\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002</p> <p>In this work, we present SoundStorm, a method for efficient and high-quality audio generation. SoundStorm addresses the problem of generating long audio token sequences by relying on: 1. an architecture adapted to the hierarchical structure of the audio tokens, 2. a parallel, non-autoregressive, confidence-based decoding scheme inspired by MaskGIT (2022) for residual vector-quantized token sequences.</p> <p>\u6211\u4eec\u63d0\u51fa\u4e86 SoundStorm, \u4e00\u79cd\u7528\u4e8e\u9ad8\u6548\u9ad8\u8d28\u91cf\u97f3\u9891\u751f\u6210\u7684\u65b9\u6cd5. SoundStorm \u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u89e3\u51b3\u751f\u6210\u957f\u97f3\u9891 token \u5e8f\u5217\u95ee\u9898: 1. \u9002\u7528\u4e8e\u97f3\u9891 token \u5c42\u6b21\u7ed3\u6784\u7684\u67b6\u6784; 2. \u7528\u4e8e\u6b8b\u5dee\u77e2\u91cf\u91cf\u5316 token \u5e8f\u5217\u7684\u5e76\u884c\u975e\u81ea\u56de\u5f52, \u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u89e3\u7801\u65b9\u6848.</p> <p>SoundStorm relies on a bidirectional attention-based Conformer (2020) that is trained to predict masked audio tokens produced by SoundStream given a condition-ing signal such as the semantic tokens of AudioLM (2022). On the input side, it sums up the embeddings of the tokens corresponding to the same SoundStream frame, such that the internal sequence length for the self-attention is identical to the number of SoundStream frames, and independent of the number of quantizers in the RVQ. The output embeddings are then processed by separate heads per RVQ level to predict the masked target tokens. At inference time, given the conditioning signal, SoundStorm starts with all audio tokens masked out, and fills in the masked tokens RVQ level-by-level over several iterations, predicting multiple tokens in parallel during a single iteration within a level. To support this inference scheme, we propose a masking scheme for training that mimics the inference procedure.</p> <p>SoundStorm \u4f9d\u8d56\u4e8e\u4e00\u4e2a\u57fa\u4e8e\u53cc\u5411\u6ce8\u610f\u529b\u7684Conformer\uff082020\uff09, \u8be5 Conformer \u7ecf\u8fc7\u8bad\u7ec3\u4ee5\u9884\u6d4b\u7531 SoundStream \u751f\u6210\u7684\u63a9\u7801\u97f3\u9891\u6807\u8bb0, \u7ed9\u5b9a\u4e00\u4e2a\u6761\u4ef6\u4fe1\u53f7, \u4f8b\u5982 AudioLM\uff082022\uff09\u7684\u8bed\u4e49\u6807\u8bb0. \u5728\u8f93\u5165\u65b9\u9762, \u5b83\u5c06\u5bf9\u5e94\u4e8e\u540c\u4e00 SoundStream \u5e27\u7684\u6807\u8bb0\u7684\u5d4c\u5165\u76f8\u52a0, \u4f7f\u5f97\u81ea\u6ce8\u610f\u529b\u7684\u5185\u90e8\u5e8f\u5217\u957f\u5ea6\u4e0e SoundStream \u5e27\u7684\u6570\u91cf\u76f8\u540c, \u5e76\u4e14\u4e0e RVQ \u4e2d\u7684\u91cf\u5316\u5668\u6570\u91cf\u65e0\u5173. \u7136\u540e\u8f93\u51fa\u5d4c\u5165\u7531\u6bcf\u4e2a RVQ \u7ea7\u522b\u7684\u5355\u72ec\u5934\u5904\u7406\u4ee5\u9884\u6d4b\u63a9\u7801\u76ee\u6807\u6807\u8bb0. \u5728\u63a8\u7406\u65f6\u95f4, \u7ed9\u5b9a\u6761\u4ef6\u4fe1\u53f7, SoundStorm \u4ece\u6240\u6709\u97f3\u9891\u6807\u8bb0\u90fd\u88ab\u63a9\u7801\u5f00\u59cb, \u7136\u540e\u5728\u51e0\u6b21\u8fed\u4ee3\u4e2d\u9010\u7ea7\u586b\u5145\u63a9\u7801\u6807\u8bb0, \u5728\u6bcf\u4e2a\u7ea7\u522b\u7684\u4e00\u6b21\u8fed\u4ee3\u4e2d\u5e76\u884c\u9884\u6d4b\u591a\u4e2a\u6807\u8bb0. \u4e3a\u4e86\u652f\u6301\u8fd9\u79cd\u63a8\u7406\u65b9\u6848, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u63a9\u7801\u65b9\u6848, \u8be5\u65b9\u6848\u6a21\u4eff\u63a8\u7406\u8fc7\u7a0b.</p> <p>We demonstrate that SoundStorm can serve as AudioLM (2022)\u2019s acoustic generator, replacing both AudioLM (2022)\u2019s stage two(coarse acoustic model) and stage three (fine acoustic model). SoundStorm produces audio two orders of magnitude faster than AudioLM (2022)\u2019s hierarchical autoregressive acoustic generator with matching quality and improved consistency in terms of speaker identity and acoustic conditions. Furthermore, we show that SoundStorm, coupled with the text-to-semantic modeling stage of SPEAR-TTS (2023), can synthesize high-quality, natural dialogues, allowing one to control the spoken content(via transcripts), speaker voices (via short voice prompts)and speaker turns (via transcript annotations). When synthesizing dialogues of 30 seconds, we measure a runtime of 2 seconds on a single TPU-v4 (2023).</p> <p>\u6211\u4eec\u5c55\u793a\u4e86 SoundStorm \u53ef\u4ee5\u4f5c\u4e3a AudioLM \u7684\u58f0\u5b66\u751f\u6210\u5668, \u53d6\u4ee3 AudioLM \u7684\u7b2c\u4e8c\u9636\u6bb5 (\u7c97\u58f0\u5b66\u6a21\u578b) \u548c\u7b2c\u4e09\u9636\u6bb5 (\u7ec6\u58f0\u5b66\u6a21\u578b) SoundStorm \u751f\u6210\u7684\u97f3\u9891\u901f\u5ea6\u6bd4 AudioLM \u7684\u5206\u5c42\u81ea\u56de\u5f52\u58f0\u5b66\u751f\u6210\u5668\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7, \u540c\u65f6\u4fdd\u6301\u76f8\u540c\u8d28\u91cf\u548c\u63d0\u9ad8\u7684\u8bed\u97f3\u548c\u58f0\u5b66\u6761\u4ef6\u4e00\u81f4\u6027. \u6b64\u5916, \u6211\u4eec\u5c55\u793a\u4e86 SoundStorm \u4e0e SPEAR-TTS (2023) \u7684\u6587\u672c\u5230\u8bed\u4e49\u5efa\u6a21\u9636\u6bb5\u76f8\u7ed3\u5408, \u53ef\u4ee5\u5408\u6210\u9ad8\u8d28\u91cf\u81ea\u7136\u7684\u5bf9\u8bdd, \u5141\u8bb8\u901a\u8fc7\u811a\u672c\u63a7\u5236\u8bf4\u8bdd\u5185\u5bb9, \u901a\u8fc7\u77ed\u8bed\u97f3\u63d0\u793a\u63a7\u5236\u8bf4\u8bdd\u8005\u58f0\u97f3\u548c\u901a\u8fc7\u811a\u672c\u6ce8\u91ca\u63a7\u5236\u8bf4\u8bdd\u8005\u8f6e\u6b21. \u5728\u5408\u6210 30 \u79d2\u7684\u5bf9\u8bdd\u65f6, \u6211\u4eec\u5728\u5355\u4e2a TPU-v4\uff082023\uff09\u4e0a\u6d4b\u91cf\u7684\u8fd0\u884c\u65f6\u95f4\u4e3a 2 \u79d2.</p>"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#2related-works","title":"2.Related Works\u00b7\u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#modeling-the-tokens-of-neural-audio-codecs","title":"Modeling the Tokens of Neural Audio Codecs","text":"<p>Unsupervised speech embeddings (Wav2Vec2.0; HuBERT; W2V-BERT) have provided a low-framerate representation of the underlying signal which remains rich enough after discretization for language models to generate intelligible speech from a specific speaker as a sequence of tokens (Generative Spoken Language Modeling (2021)). Neural audio codecs (SoundStream (2021); EnCodec (2022)), with their ability of reconstructing high-quality audio at very low bitrates, subsequently allowed for extending discrete modeling to audio signals as diverse as multi-speaker speech and piano (AutoLM (2022); SPEAR-TTS (2023)), music (MusicLM (2023)) or sound effects (AudioGen (2022)). In particular, AudioLM (2022) introduces a hierarchical sequence-to-sequence approach where high-level semantic tokens are generated as an intermediate representation, which is then used as a conditioning signal for predicting tokens of a SoundStream (2021) codec. While this hierarchical approach has demonstrated remarkable results for speech (SPEAR-TTS (2023)) and music modeling (MusicLM (2023); SingSong (2023)), the computational cost of modeling flattened SoundStream tokens with self-attention scales quadratically with the sequence length and thus the bitrate of the neural codec, preventing these models from generating long-form audio with high quality. SoundStorm alleviates this issue by modeling the multi-level tokens of the neural codec in parallel, inducing a two-order of magnitude speed-up over autoregressive modeling and unlocking the ability to scale audio generation abilities both in quality and in sequence length.</p> <p>\u65e0\u76d1\u7763\u8bed\u97f3\u5d4c\u5165\u63d0\u4f9b\u4e86\u57fa\u7840\u4fe1\u53f7\u7684\u4f4e\u5e27\u7387\u8868\u793a, \u5728\u79bb\u6563\u5316\u540e\u4ecd\u7136\u4fdd\u7559\u8db3\u591f\u4e30\u5bcc\u7684\u4fe1\u606f, \u4f7f\u5f97\u8bed\u8a00\u6a21\u578b\u4ece\u7279\u5b9a\u8bf4\u8bdd\u4eba\u5904\u4ee5\u4e00\u7cfb\u5217 token \u7684\u5f62\u5f0f\u751f\u6210\u53ef\u7406\u89e3\u7684\u8bed\u97f3. \u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668, \u7531\u4e8e\u5b83\u4eec\u80fd\u591f\u5728\u975e\u5e38\u4f4e\u6bd4\u7279\u7387\u4e0b\u91cd\u5efa\u9ad8\u8d28\u91cf\u97f3\u9891\u7684\u80fd\u529b, \u5141\u8bb8\u5c06\u79bb\u6563\u5efa\u6a21\u6269\u5c55\u5230\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3, \u94a2\u7434, \u97f3\u4e50\u6216\u58f0\u6548. \u7279\u522b\u662f AutoLM (2022) \u5f15\u5165\u4e86\u4e00\u79cd\u5c42\u6b21\u5e8f\u5217\u5230\u5e8f\u5217\u65b9\u6cd5, \u9ad8\u7ea7\u522b\u8bed\u4e49 token \u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a, \u4e4b\u540e\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u7528\u4e8e\u9884\u6d4b SoundStream \u7f16\u89e3\u7801\u5668\u7684 token. \u867d\u7136\u8fd9\u4e00\u5c42\u6b21\u65b9\u6cd5\u5df2\u7ecf\u5728\u8bed\u97f3\u548c\u97f3\u4e50\u5efa\u6a21\u65b9\u9762\u83b7\u5f97\u4e86\u663e\u8457\u6210\u679c, \u4f46\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u5bf9\u6241\u5e73\u5316 SoundStream token \u8fdb\u884c\u5efa\u6a21\u7684\u8ba1\u7b97\u6210\u672c\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u548c\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u7684\u6bd4\u7279\u7387\u5448\u4e8c\u6b21\u589e\u957f, \u4ece\u800c\u963b\u6b62\u4e86\u8fd9\u4e9b\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u957f\u5f62\u5f0f\u97f3\u9891. SoundStorm \u901a\u8fc7\u5e76\u884c\u5efa\u6a21\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u7684\u591a\u7ea7\u522b token \u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898, \u4e0e\u81ea\u56de\u5f52\u5efa\u6a21\u76f8\u6bd4\u901f\u5ea6\u63d0\u9ad8\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7, \u5e76\u5728\u8d28\u91cf\u548c\u5e8f\u5217\u957f\u5ea6\u65b9\u9762\u6269\u5c55\u97f3\u9891\u751f\u6210\u80fd\u529b.</p>"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#rvq-aware-architectures","title":"RVQ-Aware Architectures","text":"<p>A common design choice for modeling RVQ token sequences is to sum the embeddings corresponding to the same RVQ input embedding (frame) in order to reduce the sequence length. Operating on such sequences, AudioGen (2022) proposes a Transformer with $Q$ separate heads for the different RVQ levels, predicting the tokens for an RVQ frame in parallel. While providing a significant speedup for inference, the authors found that, for text-to-audio generation, this approach has an inferior performance compared to modeling the token sequence of a neural audio codec with similar bitrate and reconstruction quality, but with a single level of quantization.</p> <p>\u5bf9\u4e8e\u5efa\u6a21\u6b8b\u5dee\u77e2\u91cf\u91cf\u5316 token \u5e8f\u5217\u7684\u5e38\u89c1\u8bbe\u8ba1\u662f\u5c06\u5bf9\u5e94\u540c\u4e00\u4e2a RVQ \u8f93\u5165\u5d4c\u5165 (\u5e27) \u7684\u5d4c\u5165\u76f8\u52a0\u4ee5\u51cf\u5c11\u5e8f\u5217\u957f\u5ea6. \u5728\u5904\u7406\u8fd9\u4e00\u5e8f\u5217\u65f6, AudioGen (2022) \u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709 $Q$ \u4e2a\u5206\u522b\u7528\u4e8e\u4e0d\u540c RVQ \u7ea7\u522b\u7684\u72ec\u7acb\u5934\u7684 Transformer, \u5e76\u5e76\u884c\u9884\u6d4b RVQ \u5e27\u7684 tokens. \u867d\u7136\u4e3a\u63a8\u7406\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u52a0\u901f, \u4f46\u4f5c\u8005\u53d1\u73b0\u5bf9\u4e8e\u6587\u672c\u5230\u97f3\u9891\u751f\u6210, \u4e0e\u5177\u6709\u76f8\u4f3c\u6bd4\u7279\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u4f46\u53ea\u6709\u4e00\u7ea7\u91cf\u5316\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684 token \u5e8f\u5217\u5efa\u6a21\u76f8\u6bd4, \u8fd9\u79cd\u65b9\u6cd5\u6027\u80fd\u8f83\u5dee.</p> <p>VALL-E (2023) instead relies on a hybrid approach, where the tokens corresponding to the first RVQ level are predicted autoregressively, and the subsequent levels are produced non-autoregressively. The latter is achieved by a model that sums up the embeddings from the same RVQ input frame, and applies bidirectional self-attention to predict all tokens from RVQ level $q+1$ given all tokens from levels $1,\\cdots,q$, the acoustic prompt, and the phoneme sequence. During inference, tokens starting from the second level of the RVQ are produced iteratively, performing greedy decoding (choosing the most likely tokens) level-by-level. Level-wise greedy decoding represents the baseline for our method.</p> <p>VALL-E (2023) \u5219\u4f9d\u8d56\u4e8e\u6df7\u5408\u65b9\u6cd5, \u5176\u4e2d\u5bf9\u5e94\u7b2c\u4e00\u4e2a RVQ \u7ea7\u522b\u7684 token \u662f\u81ea\u56de\u5f52\u9884\u6d4b\u7684, \u540e\u7eed\u7ea7\u522b\u4ee5\u975e\u81ea\u56de\u5f52\u751f\u6210. \u540e\u8005\u901a\u8fc7\u4e00\u4e2a\u6a21\u578b\u5b9e\u73b0, \u5c06\u6765\u81ea\u540c\u4e00\u4e2a RVQ \u8f93\u5165\u5e27\u7684\u5d4c\u5165\u6c42\u548c, \u4e14\u5e94\u7528\u53cc\u5411\u81ea\u6ce8\u610f\u529b\u5728\u7ed9\u5b9a $1,\\cdots,q$ \u7ea7\u522b, \u58f0\u5b66\u63d0\u793a\u548c\u97f3\u7d20\u5e8f\u5217\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b RVQ $q+1$ \u7ea7\u522b\u7684 token. \u5728\u63a8\u7406\u65f6, \u4ece RVQ \u7684\u7b2c\u4e8c\u7ea7\u5f00\u59cb\u8fed\u4ee3\u5f0f\u751f\u6210 token, \u9010\u7ea7\u6267\u884c\u8d2a\u5fc3\u89e3\u7801 (\u9009\u62e9\u6700\u53ef\u80fd\u7684 token). \u9010\u7ea7\u8d2a\u5fc3\u89e3\u7801\u8868\u793a\u6211\u4eec\u65b9\u6cd5\u7684\u57fa\u7ebf.</p> <p>Modeling sequences produced by RVQ has been also investigated in domains other than audio. For example, the RQ-Transformer (2022) also adds up the embeddings corresponding to the same RVQ input frame, but factorizes the full joint distribution efficiently with a spatial and a depth Transformer, for modeling autoregressively the RVQ frames and tokens within the frames, respectively. While it has not been demonstrated yet, this approach, potentially coupled with parallel decoding schemes, is a promising future avenue for audio generation.</p> <p>\u9664\u4e86\u97f3\u9891\u9886\u57df, RVQ \u4ea7\u751f\u7684\u5e8f\u5217\u5efa\u6a21\u4e5f\u5728\u5176\u4ed6\u9886\u57df\u8fdb\u884c\u4e86\u7814\u7a76. \u4f8b\u5982 RQ-Transformer (2022) \u4e5f\u5c06\u5bf9\u5e94\u4e8e\u540c\u4e00 RVQ \u8f93\u5165\u5e27\u7684\u5d4c\u5165\u76f8\u52a0, \u4f46\u901a\u8fc7\u7a7a\u95f4\u548c\u6df1\u5ea6 Transformer \u6709\u6548\u5730\u5206\u89e3\u4e86\u8054\u5408\u5206\u5e03, \u5206\u522b\u7528\u4e8e\u81ea\u56de\u5f52\u5efa\u6a21 RVQ \u5e27\u548c\u5e27\u5185\u7684 token. \u867d\u7136\u8fd8\u6ca1\u6709\u5c55\u793a, \u4f46\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u4e0e\u5e76\u884c\u89e3\u7801\u65b9\u6848\u7ed3\u5408, \u662f\u97f3\u9891\u751f\u6210\u7684\u6709\u524d\u666f\u7684\u672a\u6765\u65b9\u5411.</p>"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#parallel-decoding","title":"Parallel Decoding","text":"<p>In order to improve the inference time and to allow bidirectional non-causal attention on the input sequence, parallel decoding schemes have been proposed for text (NAT (2017); Mask-Predict (2019)), image (MaskGIT (2022)) and video generation (Phenaki (2022)). Of particular relevance to our work is the parallel, iterative sampling scheme of MaskGIT (2022). During inference time, MaskGIT starts from masked tokens, and in each round, predicts a portion of the tokens based on confidence scores. The portion of the predicted tokens in each round is controlled by a schedule, and usually progressively increases over the iterations - once predicted, the tokens are treated as fixed. Our proposed decoding scheme can be seen as the extension of MaskGIT\u2019s decoding to token sequences produced by residual quantization.</p> <p>\u4e3a\u4e86\u63d0\u9ad8\u63a8\u7406\u65f6\u95f4\u4ee5\u53ca\u5141\u8bb8\u5728\u8f93\u5165\u5e8f\u5217\u4e0a\u8fdb\u884c\u53cc\u5411\u975e\u56e0\u679c\u6ce8\u610f\u529b, \u5df2\u7ecf\u6709\u4e86\u7528\u4e8e\u6587\u672c, \u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u7684\u5e76\u884c\u89e3\u7801\u65b9\u6848. \u548c\u672c\u9879\u5de5\u4f5c\u6700\u76f8\u5173\u7684\u662f MaskGIT (2022) \u7684\u5e76\u884c, \u8fed\u4ee3\u91c7\u6837\u65b9\u6848. \u5728\u63a8\u7406\u65f6, MaskGIT \u4ece\u63a9\u7801\u6807\u8bb0\u5f00\u59cb, \u5728\u6bcf\u4e00\u8f6e\u4e2d, \u6839\u636e\u7f6e\u4fe1\u5ea6\u5206\u6570\u9884\u6d4b\u4e00\u90e8\u5206 token. \u6bcf\u8f6e\u9884\u6d4b\u7684\u90e8\u5206 token \u901a\u8fc7\u4e00\u4e2a\u8ba1\u5212\u8868\u63a7\u5236, \u901a\u5e38\u5728\u8fed\u4ee3\u4e2d\u9010\u6e10\u589e\u52a0 - \u4e00\u65e6\u9884\u6d4b, token \u88ab\u89c6\u4e3a\u56fa\u5b9a. \u6211\u4eec\u63d0\u51fa\u7684\u89e3\u7801\u65b9\u6848\u53ef\u4ee5\u89c6\u4e3a MaskGIT \u7684\u89e3\u7801\u5728\u6b8b\u5dee\u91cf\u5316\u751f\u6210\u7684 token \u5e8f\u5217\u7684\u6269\u5c55.</p>"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#3methodology","title":"3.Methodology\u00b7\u65b9\u6cd5","text":"<p>SoundStorm receives as input a sequence of discrete tokens representing the conditioning signal and produces as output a sequence of SoundStream tokens, which can be decoded back to audio waveforms. We assume that the conditioning signal is time-aligned with the SoundStream frames or can be upsampled to the same rate. Such a conditioning signal is, for example, the semantic token sequence used in AudioLM (2022), SPEAR-TTS (2023), or MusicLM, which makes our method a drop-in replacement for the acoustic generators of these models.</p> <p>We leave the extension to other types of conditioning signals via cross-attention or to unconditional sampling for future work, and focus our presentation of SoundStorm as the acoustic generator within AudioLM (2022), replacing both AudioLM (2022)\u2019s coarse and fine acoustic modeling stages.</p>"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#31","title":"3.1.\u7f51\u7edc\u67b6\u6784","text":"<p>The architecture of the model is illustrated in Figure 1.</p> <p></p> <p>At the input side, we interleave the time-aligned conditioning tokens with the SoundStream tokens at the frame level, embed the resulting sequence, sum the embeddings corresponding to the same frame, including the embedding of the conditioning token, and pass the resulting continuous embeddings to a Conformer. Consequently, the sequence length for bidirectional self-attention in the Conformer is determined by the number of SoundStream frames (typically 50 per second), and thus is independent of the number of RVQ levels $Q$, allowing one to handle audio with length on the order of minutes. At the output side, we use $Q$ dense layers as heads to produce the target SoundStream tokens.</p> <p>\u5728\u8f93\u5165\u4fa7, \u6211\u4eec\u5c06\u65f6\u95f4\u5bf9\u9f50\u7684\u6761\u4ef6 token \u548c SoundStream token \u5728\u5e27\u7ea7\u522b\u4ea4\u9519, \u5d4c\u5165\u7ed3\u679c\u5e8f\u5217, \u5bf9\u5e94\u540c\u4e00\u5e27\u7684\u5d4c\u5165\u6c42\u548c, \u5305\u62ec\u6761\u4ef6 token \u5d4c\u5165, \u5e76\u5c06\u7ed3\u679c\u8fde\u7eed\u5d4c\u5165\u4f20\u9012\u7ed9 Conformer. \u56e0\u6b64, Conformer \u4e2d\u4f7f\u7528\u7684\u53cc\u5411\u81ea\u6ce8\u610f\u529b\u7684\u5e8f\u5217\u957f\u5ea6\u7531 SoundStream \u5e27\u6570\u51b3\u5b9a, \u548c RVG \u7ea7\u522b\u6570\u65e0\u5173, \u4ece\u800c\u53ef\u4ee5\u5904\u7406\u957f\u5ea6\u4e3a\u5206\u949f\u7ea7\u7684\u97f3\u9891. \u5728\u8f93\u51fa\u4fa7, \u6211\u4eec\u4f7f\u7528 $Q$ \u4e2a\u7a20\u5bc6\u5c42\u4f5c\u4e3a\u5934\u6765\u4ea7\u751f\u76ee\u6807 SoundStream token.</p>"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#32masking","title":"3.2.Masking","text":"<p>For designing our masking and decoding, we extend the masking and confidence-based parallel decoding scheme of MaskGIT (2022) to token sequences produced by RVQ. At a high level, our approach can be seen as following the strategy of MaskGIT (2022) per RVQ level in a coarse-to-fine order. The coarse-to-fine ordering is of particular importance, since it not only respects the conditional dependencies between levels of the RVQ hierarchy, but also exploits the conditional independence of tokens from finer levels given all tokens from coarser levels. The tokens of finer levels are responsible for local, fine acoustic details and can thus be sampled in parallel without a loss of audio quality.</p> <p>\u4e3a\u4e86\u8bbe\u8ba1\u6211\u4eec\u7684\u63a9\u7801\u548c\u89e3\u7801, \u6211\u4eec\u5c06 MaskGIT (2022) \u4e2d\u7684\u63a9\u7801\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5e76\u884c\u89e3\u7801\u65b9\u6848\u6269\u5c55\u5230\u7531 RVQ \u751f\u6210\u7684 token \u5e8f\u5217. \u5728\u9ad8\u7ea7\u522b, \u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u89c6\u4e3a\u6309\u7c97\u5230\u7ec6\u987a\u5e8f\u9010\u4e2a RVQ \u7ea7\u522b\u9075\u5faa MashGIT \u7b56\u7565. \u7c97\u5230\u7ec6\u7684\u987a\u5e8f\u7279\u522b\u91cd\u8981, \u56e0\u4e3a\u4e0d\u4ec5\u9075\u5faa\u4e86 RVQ \u5c42\u6b21\u7ed3\u6784\u4e2d\u5404\u4e2a\u7ea7\u522b\u4e4b\u95f4\u7684\u6761\u4ef6\u4f9d\u8d56\u6027, \u8fd8\u5229\u7528\u4e86\u7ed9\u5b9a\u6240\u6709\u66f4\u7c97\u7ea7\u522b token \u65f6\u6765\u81ea\u66f4\u7ec6\u7ea7\u522b token \u7684\u6761\u4ef6\u65e0\u5173\u6027. \u8f83\u7ec6\u7ea7\u522b\u7684 token \u8d1f\u8d23\u5c40\u90e8\u7684, \u7cbe\u7ec6\u7684\u58f0\u5b66\u7ec6\u8282, \u56e0\u6b64\u53ef\u4ee5\u5e76\u884c\u91c7\u6837\u800c\u4e0d\u635f\u5931\u97f3\u9891\u8d28\u91cf.</p> <p>We design our masking scheme for training accordingly. To enable voice prompting, we randomly sample a timestep $t\\in{1,\\cdots,T}$, where $T$ denotes the maximum sequence length, and we do not mask any tokens before this timestep. The conditioning tokens are never masked. Let $Y\\in{1,\\cdots, C}^{T\\times Q}$ denote the SoundStream tokens, where $C$ indicates the codebook size used in each RVQ level out of the $Q$ levels. Our masking scheme proceeds as follows: - Sample the prompt delimiter timestep $t\\sim\\mathcal{U}{0,T-1}$; - Sample the current RVQ level $q\\sim\\mathcal{U}{1,Q}$; - Sample the mask $M \\in {0, 1}^T$ according to a cosine schedule (MaskGIT (2022)) for level $q$, i.e., sample the masking ratio $p = \\cos(u)$ where $u \\sim \\mathcal{U}[0, \\pi/2]$, and sample iid $M_i\\sim \\text{Bernoulli}(p)$. - Mask the selected non-prompt tokens at the current RVQ level $q$ (mask $Y_{t',q}$ if $M_{t'}= 1$ and $t'&gt;t$) and all non-prompt tokens at finer RVQ levels ($Y_{&gt;t,&gt;q}$).</p> <p>\u6211\u4eec\u5bf9\u5e94\u5730\u8bbe\u8ba1\u4e86\u6211\u4eec\u7684\u63a9\u7801\u65b9\u6848. \u4e3a\u4e86\u80fd\u591f\u8bed\u97f3\u63d0\u793a, \u6211\u4eec\u968f\u673a\u91c7\u6837\u4e00\u4e2a\u65f6\u95f4\u6b65\u5e76\u4e0d\u5bf9\u8be5\u65f6\u95f4\u6b65\u4e4b\u524d\u7684 token \u8fdb\u884c\u63a9\u76d6. \u6761\u4ef6 token \u4e0d\u4f1a\u88ab\u63a9\u76d6. \u8bb0 SoundStream token \u4e3a $Y\\in{1,\\cdots, C}^{T\\times Q}$, $C$ \u4e3a\u5728\u6bcf\u4e2a RVQ \u7ea7\u522b\u4e2d\u4f7f\u7528\u7684\u7801\u672c\u5c3a\u5bf8, \u4e00\u5171\u6709 $Q$ \u4e2a\u7ea7\u522b. \u6211\u4eec\u7684\u63a9\u7801\u65b9\u6848\u5982\u4e0b: - \u91c7\u6837\u63d0\u793a\u7684\u5206\u9694\u7b26\u65f6\u95f4\u6b65 $t$; - \u91c7\u6837\u5f53\u524d RVQ \u7ea7\u522b $q$; - \u6839\u636e\u4f59\u5f26\u8c03\u5ea6\u5bf9\u7ea7\u522b $q$ \u91c7\u6837\u63a9\u7801 $M$, \u5373\u5148\u91c7\u6837\u63a9\u7801\u7387, \u518d\u6309\u7167\u4f2f\u52aa\u5229\u5206\u5e03\u91c7\u6837\u63a9\u7801; - \u5f53\u524d RVQ \u7ea7\u522b\u5bf9\u975e\u63d0\u793a token \u8fdb\u884c\u63a9\u7801.</p> <p>Given a masked token sequence, we train the model with cross-entropy loss with the ground-truth tokens as target, where the loss is only calculated on the masked tokens within the $q$-th RVQ level. An example of this masking scheme is illustrated in Figure01, with $T = 4$, $Q = 3$, $t = 0$ and $q = 2$.</p> <p>\u7ed9\u5b9a\u4e00\u4e2a\u63a9\u7801 token \u5e8f\u5217, \u6211\u4eec\u7528\u548c\u76ee\u6807 token \u4e4b\u95f4\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u6765\u8bad\u7ec3\u6a21\u578b, \u5176\u4e2d\u635f\u5931\u53ea\u5728\u7b2c $q$ \u4e2a RVQ \u7ea7\u522b\u7684\u63a9\u7801 token \u4e0a\u8ba1\u7b97. \u63a9\u7801\u65b9\u6848\u7684\u4f8b\u5b50\u5982\u56fe\u4e00\u6240\u793a.</p>"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#33iterative-parallel-decoding","title":"3.3.Iterative Parallel Decoding","text":"<p>Given a conditioning signal, our decoding scheme starts with all SoundStream tokens masked out except for the ones of the prompt (if provided). Then, it proceeds to sampling the tokens RVQ level-wise in a coarse-to-fine order, only proceeding to level $q + 1$ when all tokens for levels $1,\\cdots, q$ have been sampled. Within an RVQ level, we use the confidence-based sampling scheme of MaskGIT (2022). Namely, we perform multiple forward passes, and at each iteration $i$, we sample candidates for the masked positions, retaining $p_i$ of them based on confidence scores, where $p_i$ follows a cosine schedule. Compared to MaskGIT (2022), we use greedy decoding instead of confidence-based sampling for the last iteration within each RVQ level, which we found to improve the perceived audio quality.</p> <p>\u7ed9\u5b9a\u4e00\u4e2a\u6761\u4ef6\u4fe1\u53f7, \u6211\u4eec\u7684\u89e3\u7801\u65b9\u6848\u4ece\u9664\u4e86\u63d0\u793a\u4e4b\u5916\u7684\u6240\u6709 SoundStream token \u5f00\u59cb. \u7136\u540e, \u5b83\u9010\u7ea7\u5728\u7c97\u5230\u7ec6\u987a\u5e8f\u4e2d\u91c7\u6837 tokens. \u5728\u4e00\u4e2a\u7ea7\u522b\u5185, \u6211\u4eec\u4f7f\u7528\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u91c7\u6837\u65b9\u6848. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u6267\u884c\u591a\u6b21\u524d\u5411\u4f20\u9012, \u5728\u7b2c $i$ \u6b21\u8fed\u4ee3\u4e2d\u6211\u4eec\u91c7\u6837\u63a9\u7801\u4f4d\u7f6e\u7684\u5019\u9009, \u6839\u636e\u7f6e\u4fe1\u5ea6\u5206\u6570\u4fdd\u7559 $p_i$ \u4e2a, $p_i$ \u670d\u4ece\u4f59\u5f26\u8c03\u5ea6. \u548c MaskGIT \u76f8\u6bd4, \u6211\u4eec\u5728\u6bcf\u4e2a RVQ \u7ea7\u522b\u7684\u6700\u540e\u4e00\u6b21\u8fed\u4ee3\u4e2d\u4f7f\u7528\u8d2a\u5fc3\u89e3\u7801\u800c\u4e0d\u662f\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u91c7\u6837, \u6211\u4eec\u53d1\u73b0\u8fd9\u6837\u80fd\u591f\u63d0\u9ad8\u611f\u77e5\u5230\u7684\u97f3\u9891\u8d28\u91cf.</p> <p>Performing the decoding RVQ level-wise makes it possible to exploit the conditional independence assumption in finer levels, namely that multiple finer tokens can be sampled in parallel since they represent local, fine acoustic details. This implies that we can decrease the number of forward passes significantly as we progress to finer RVQ levels during decoding.</p> <p>\u9010\u7ea7\u8fdb\u884c\u89e3\u7801\u4f7f\u5f97\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u66f4\u7ec6\u7ea7\u522b\u4e2d\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u5047\u8bbe, \u5373\u591a\u4e2a\u66f4\u7ec6\u7684 token \u53ef\u4ee5\u5e76\u884c\u91c7\u6837, \u56e0\u4e3a\u5b83\u4eec\u8868\u793a\u5c40\u90e8\u4e14\u7cbe\u7ec6\u7684\u58f0\u97f3\u7ec6\u8282. \u8fd9\u610f\u5473\u7740\u968f\u7740\u6211\u4eec\u5728\u89e3\u7801\u4e2d\u8fdb\u884c\u5230\u66f4\u7ec6 RVQ \u7ea7\u522b\u65f6\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u524d\u5411\u4f20\u9012\u7684\u6b21\u6570.</p>"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#4experiments","title":"4.Experiments\u00b7\u5b9e\u9a8c","text":""},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#41","title":"4.1.\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u8bbe\u7f6e","text":""},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#42","title":"4.2.\u8bed\u97f3\u6e05\u6670\u5ea6, \u97f3\u9891\u8d28\u91cf, \u58f0\u97f3\u8868\u793a\u548c\u58f0\u5b66\u4e00\u81f4\u6027","text":""},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#5","title":"5.\u5bf9\u8bdd\u5408\u6210","text":""},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#6conclusions","title":"6.Conclusions\u00b7\u7ed3\u8bba","text":"<p>In this paper we present SoundStorm, a model that can synthesize high-quality audio from discrete conditioning tokens efficiently. When compared to the acoustic generator of AudioLM (2022), SoundStorm is two orders of magnitude faster and achieves higher temporal consistency when generating long audio samples. By combining a text-to-semantic token model similar to SPEAR-TTS (2023) with SoundStorm, we can scale text-to-speech synthesis to longer contexts and generate natural dialogues with multiple speaker turns, controlling both the voices of the speakers and the generated content.</p> <p>\u672c\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86 SoundStorm, \u4e00\u4e2a\u53ef\u4ee5\u9ad8\u6548\u5730\u4ece\u79bb\u6563\u6761\u4ef6 token \u751f\u6210\u9ad8\u8d28\u91cf\u97f3\u9891\u7684\u6a21\u578b. \u548c AudioLM (2022) \u7684\u58f0\u5b66\u751f\u6210\u5668\u76f8\u6bd4, SoundStorm \u8fd0\u884c\u901f\u5ea6\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7, \u4e14\u5728\u751f\u6210\u957f\u97f3\u9891\u6837\u672c\u65f6\u83b7\u5f97\u66f4\u9ad8\u7684\u65f6\u5e8f\u4e00\u81f4\u6027. \u901a\u8fc7\u7ed3\u5408\u7c7b\u4f3c\u4e8e SPEAR-TTS (2023) \u7684\u6587\u672c\u5230\u8bed\u4e49 token \u6a21\u578b, \u80fd\u591f\u5c06\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u6269\u5c55\u5230\u66f4\u957f\u5185\u5bb9\u548c\u751f\u6210\u5177\u6709\u591a\u4e2a\u8bf4\u8bdd\u4eba\u8f6e\u6b21\u7684\u81ea\u7136\u5bf9\u8bdd, \u540c\u65f6\u63a7\u5236\u8bf4\u8bdd\u4eba\u7684\u58f0\u97f3\u548c\u751f\u6210\u7684\u5185\u5bb9.</p>"},{"location":"TTS/Models/Speech_LLM/2023.05.16_SoundStorm/#7broader-impact","title":"7.Broader Impact\u00b7\u66f4\u5e7f\u6cdb\u7684\u5f71\u54cd","text":"<p>SoundStorm is a model for high-quality, efficient generation of neural audio codec-derived representations of audio. In this work, we use it as a replacement for the acoustic generation pipeline of AudioLM (2022) and SPEAR-TTS (2023). We acknowledge that the audio samples produced by the model may be influenced by the biases present in the training data, for instance in terms of represented accents and voice characteristics. In our generated samples, we demonstrate that we can reliably control speaker characteristics via prompting. However, a more thorough analysis of any training data and its limitations would be an area of future work in line with our responsible AI principles.</p> <p>SoundStorm \u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u8d28\u91cf, \u9ad8\u6548\u751f\u6210\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5bfc\u51fa\u97f3\u9891\u8868\u793a\u7684\u6a21\u578b. \u6211\u4eec\u5c06\u5176\u4f5c\u4e3a AudioLM (2022) \u548c SPEAR-TTS (2023) \u7684\u58f0\u5b66\u751f\u6210\u7ba1\u9053\u7684\u66ff\u4ee3\u54c1. \u6211\u4eec\u627f\u8ba4\u6a21\u578b\u751f\u6210\u7684\u97f3\u9891\u6837\u672c\u53ef\u80fd\u4f1a\u53d7\u5230\u8bad\u7ec3\u6570\u636e\u4e2d\u504f\u5dee\u7684\u5f71\u54cd, \u4f8b\u5982\u5728\u4ee3\u8868\u7684\u53e3\u97f3\u548c\u58f0\u97f3\u7279\u5f81\u65b9\u9762. \u5728\u751f\u6210\u6837\u672c\u4e2d, \u6211\u4eec\u5c55\u793a\u4e86\u53ef\u4ee5\u901a\u8fc7\u63d0\u793a\u53ef\u9760\u5730\u63a7\u5236\u8bf4\u8bdd\u4eba\u7279\u5f81. \u7136\u800c\u5bf9\u4e8e\u8bad\u7ec3\u6570\u636e\u53ca\u5176\u5c40\u9650\u6027\u8fdb\u884c\u66f4\u5f7b\u5e95\u5730\u5206\u6790\u5c06\u662f\u672a\u6765\u5de5\u4f5c\u7684\u4e00\u4e2a\u9886\u57df, \u4e0e\u6211\u4eec\u7684\u8d1f\u8d23\u4efb AI \u5143\u7ec4\u4fdd\u6301\u4e00\u81f4.</p> <p>In turn, the ability to mimic a voice can have numerous malicious applications, including bypassing biometric identification and for the purpose of impersonation. Thus, it is crucial to put in place safeguards against potential misuse: to this end, we have verified that the audio generated by SoundStorm remains detectable by a dedicated classifier (98.5%\u3000using the same classifier as AudioLM (2022)). Hence, as a component of a larger system, we believe that SoundStorm would be unlikely to introduce additional risks to those discussed previously by AudioLM (2022) and SPEAR-TTS (2023). At the same time, relaxing the memory and computational requirements of AudioLM (2022) would make research in the domain of audio generation more accessible to a wider community. In the future, we plan to explore other approaches for detecting synthesized speech, e.g., audio watermarking, so that any potential product usage of this technology strictly follows our responsible AI principles.</p> <p>\u53cd\u8fc7\u6765, \u6a21\u4eff\u58f0\u97f3\u7684\u80fd\u529b\u53ef\u80fd\u6709\u8bb8\u591a\u6076\u610f\u5e94\u7528, \u5305\u62ec\u7ed5\u8fc7\u751f\u7269\u8bc6\u522b\u8ba4\u8bc1\u548c\u7528\u4e8e\u5192\u5145. \u56e0\u6b64, \u5fc5\u987b\u91c7\u53d6\u63aa\u65bd\u9632\u6b62\u6f5c\u5728\u7684\u6ee5\u7528: \u4e3a\u6b64\u6211\u4eec\u9a8c\u8bc1\u4e86 SoundStorm \u751f\u6210\u7684\u97f3\u9891\u4ecd\u7136\u53ef\u4ee5\u88ab\u4e13\u7528\u7684\u5206\u7c7b\u5668\u68c0\u6d4b\u5230 (AudioLM (2022) \u76f8\u540c \u7684\u5206\u7c7b\u5668, \u68c0\u6d4b\u7387\u4e3a 98.5%). \u56e0\u6b64\u4f5c\u4e3a\u66f4\u5927\u7cfb\u7edf\u7684\u4e00\u90e8\u5206, \u6211\u4eec\u8ba4\u4e3a SoundStorm \u4e0d\u592a\u53ef\u80fd\u5f15\u5165\u7531 AudioLM (2022) \u548c SPEAR-TTS (2023) \u8ba8\u8bba\u7684\u989d\u5916\u98ce\u9669. \u540c\u65f6, \u653e\u677e AudioLM (2022) \u7684\u5185\u5b58\u548c\u8ba1\u7b97\u8981\u6c42\u5c06\u4f7f\u5f97\u97f3\u9891\u751f\u6210\u9886\u57df\u7684\u7814\u7a76\u5bf9\u66f4\u5e7f\u6cdb\u793e\u533a\u66f4\u53ef\u8bbf\u95ee. \u672a\u6765\u6211\u4eec\u8ba1\u5212\u63a2\u7d22\u68c0\u6d4b\u5408\u6210\u8bed\u97f3\u7684\u5176\u4ed6\u65b9\u6cd5, \u4f8b\u5982\u97f3\u9891\u6c34\u5370, \u4ee5\u4fbf\u4efb\u4f55\u6f5c\u5728\u7684\u4ea7\u54c1\u4f7f\u7528\u8fd9\u79cd\u6280\u672f\u4e25\u683c\u9075\u5faa\u6211\u4eec\u7684\u8d1f\u8d23\u4efb AI \u539f\u5219.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/","title":"Mega-TTS","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias - \u4f5c\u8005:   - 01 [Ziyue Jiang](../../Authors/Ziyue_Jiang.md)   - 02 [Yi Ren](../../Authors/Yi_Ren_(\u4efb\u610f).md)   - 03 [Zhenhui Ye](../../Authors/Zhenhui_Ye.md)   - 04 [Jinglin Liu](../../Authors/Jinglin_Liu.md)   - 05 [Chen Zhang](../../Authors/Chen_Zhang.md)   - 06 [Qian Yang](../../Authors/Qian_Yang.md)   - 07 [Shengpeng Ji](../../Authors/Shengpeng_Ji.md)   - 08 [Rongjie Huang](../../Authors/Rongjie_Huang_(\u9ec4\u878d\u6770).md)   - 09 [Chunfeng Wang](../../Authors/Chunfeng_Wang.md)   - 10 [Xiang Yin](../../Authors/Xiang_Yin.md)   - 11 [Zejun Ma](../../Authors/Zejun_Ma.md)   - 12 [Zhou Zhao](../../Authors/Zhou_Zhao_(\u8d75\u6d32).md) - \u673a\u6784:   - [\u6d59\u6c5f\u5927\u5b66](../../Institutions/ZJU_\u6d59\u6c5f\u5927\u5b66.md)   - [\u5b57\u8282\u8df3\u52a8](../../Institutions/ByteDance.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2023.06.06 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.17 - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2306.03509)   - [DOI]()   - [Github]()   - [Demo](https://mega-tts.github.io/demo-page)   - [Scholar](https://scholar.google.com/scholar?cluster=15692405188854768212) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md)   - [GAN](../../Tags/Model_GAN.md)   - [\u8bed\u8a00\u6a21\u578b](../../Tags/LanguageModel.md) - \u9875\u6570: 20 - \u5f15\u7528: 67 - \u88ab\u5f15: 27 - \u6570\u636e:   - \u8bad\u7ec3: [WeNetSpeech](../../Datasets/WeNetSpeech.md)   - \u8bad\u7ec3: [GigaSpeech](../../Datasets/GigaSpeech.md)   - \u8bc4\u4f30: [VCTK](../../Datasets/VCTK.md)   - \u8bc4\u4f30: [LibriSpeech](../../Datasets/LibriSpeech.md)   - \u8bc4\u4f30: [AISHELL-3](../../Datasets/AISHELL.md)"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#abstract","title":"Abstract: \u6458\u8981","text":"<p>Scaling text-to-speech to a large and wild dataset has been proven to be highly effective in achieving timbre and speech style generalization, particularly in zero-shot TTS. However, previous works usually encode speech into latent using audio codec and use autoregressive language models or diffusion models to generate it, which ignores the intrinsic nature of speech and may lead to inferior or uncontrollable results. We argue that speech can be decomposed into several attributes (e.g., content, timbre, prosody, and phase) and each of them should be modeled using a module with appropriate inductive biases. From this perspective, we carefully design a novel and large zero-shot TTS system called Mega-TTS, which is trained with large-scale wild data and models different attributes in different ways:  1) Instead of using latent encoded by audio codec as the intermediate feature, we still choose spectrogram as it separates the phase and other attributes very well. Phase can be appropriately constructed by the GAN-based vocoder and does not need to be modeled by the language model.  2) We model the timbre using global vectors since timbre is a global attribute that changes slowly over time.  3) We further use a VQGAN-based acoustic model to generate the spectrogram and a latent code language model to fit the distribution of prosody, since prosody changes quickly over time in a sentence, and language models can capture both local and long-range dependencies. We scale Mega-TTS to multi-domain datasets with 20K hours of speech and evaluate its performance on unseen speakers. Experimental results demonstrate that Mega-TTS surpasses state-of-the-art TTS systems on zero-shot TTS, speech editing, and cross-lingual TTS tasks, with superior naturalness, robustness, and speaker similarity due to the proper inductive bias of each module. Audio samples are available at https://mega-tts.github.io/demo-page.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#1introduction","title":"1.Introduction: \u5f15\u8a00","text":"<p>Text-to-speech (TTS) synthesis [53, 2, 49, 35, 48, 45, 29, 66, 43, 28] aims to generate human-like speech from text and has gained significant attention in the field of machine learning. Traditional TTS systems [13, 11, 60, 8, 21] are usually trained on limited datasets, which impairs their models\u2019 ability to produce diverse and generalizable results. In contrast, large-scale TTS systems [58, 67, 27] are trained on tens of thousands of hours of speech data, which significantly improves their zero-shot capability [58, 67]. Current large-scale TTS systems typically encode the speech waveform into latent with neural codec models [14] as the intermediate representation and model it with autoregressive language models (LM) [58] or diffusion models [50].</p> <p>As presented in Table 1, human speech can be decoupled into several attributes: content, timbre, prosody, phase, etc. However, current large-scale TTS systems directly use neural audio codec models to encode the entire speech into latent and ignore the following intrinsic nature of speech:  1) phase is highly dynamic and irrelevant to semantics, which means people are much less sensitive to perceive phase than to prosody and timbre, especially for monaural audio. Therefore, only one reasonable phase is needed for waveform reconstruction, and it is not necessary to model all possible phases. Modeling phase with LM or diffusion model can waste a lot of model parameters since they model the full distribution of phase4.  2) Timbre should remain stable within the sentence as a global vector. Modeling timbre with time-varying latent is costly5.  3) Prosody typically has both local and long-term dependencies and changes rapidly over time with a weak correlation to text, which makes conditional phoneme-level LLMs inherently ideal for generating prosody sequences.  4) Content has monotonic alignment with speech while the autoregressive language model cannot guarantee that, which can lead to repeating or missing word issues [59, 58, 67].</p> <p>To make use of the large and wild training datasets while matching the inductive bias of the model and the intrinsic nature of speech, we propose a zero-shot text-to-speech model called Mega-TTS. Specifically,  1) considering the limitations of neural audio codec models, we select mel-spectrogram as the intermediate representation to separate the phase and other attributes. We adopt a GAN-based vocoder to reconstruct the phase information to improve our model\u2019s efficiency.  2) To model timbre information, we employ global vectors since timbre is a global attribute that changes slowly over time. We extract the global information from a different speech of the same speaker with the global speaker encoder to decompose the timbre and content information.  3) To capture prosody information in a sentence, we adopt a VQGAN-based acoustic model to generate the mel-spectrogram and a latent code language model called P-LLM to fit the distribution of prosody. The P-LLM is capable of capturing both local and long-range dependencies for prosody modeling.</p> <p>To evaluate the zero-shot performance of Mega-TTS, we perform experiments on VCTK [57], AISHELL-3 [51] and LibriSpeech test-clean [42] datasets. All of the test speakers are unseen in the training corpus. Our Mega-TTS surpasses the state-of-the-art zero-shot TTS systems [8, 58] in terms of speaker similarity, speech naturalness, and generation robustness, which demonstrates the superiority of introducing appropriate inductive biases. Moreover, Mega-TTS outperforms state-ofthe-art models on speech editing [52, 3] and cross-lingual TTS [67] tasks. The main contributions of this work are summarized as follows: - We propose Mega-TTS, a zero-shot text-to-speech system that considers intrinsic inductive biases. Instead of using latent encoded by audio codec as the intermediate representation [64, 14, 58], we decompose mel-spectrogram into content, timbre, prosody, and phase attributes and model each of them according to their intrinsic properties. - We train Mega-TTS on a multi-domain and multi-lingual dataset that contains 20k hours of speech data. It is worth noting that existing large-scale TTS systems [58, 50] are typically trained with speech corpora from audiobooks, while our system is trained on multi-domain speech corpora. - We evaluate Mega-TTS on 3 down-stream speech generation tasks (i.e., zero-shot TTS, speech editing, and cross-lingual TTS), demonstrating that Mega-TTS can be applied to various speech generation tasks. We also propose a novel sampling strategy for speech editing via the discrete prosody tokens extracted by Mega-TTS.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#2related-works","title":"2.Related Works: \u76f8\u5173\u5de5\u4f5c","text":"<p>In this section, we briefly overview the background of this work, including zero-shot text-to-speech (TTS) and generative models for speech synthesis.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#zero-shot-text-to-speech","title":"Zero-shot text-to-speech.","text":"<p>Text-to-speech models usually generate mel-spectrogram from text [59, 2, 35, 48, 29, 47, 36, 22] and then synthesize speech waveform from the generated mel-spectrogram using a separately pre-trained vocoder [41, 31, 62, 20], or directly generate waveform from text in an end-to-end manner [45, 15, 30, 37]. For decades, the increasing demand for personalized speech generation in various applications has posed challenges for TTS models [53], especially in zero-shot multi-speaker scenarios regarding domain shifts. Previous approaches can be categorized into speaker adaptation [13, 11, 60, 23] and speaker encoding [25, 1, 26, 61] methods. Traditional works are typically trained on small datasets [11, 23, 21, 8], while some recent works [4, 58, 27, 67] are trained on large-scale datasets and demonstrate the effectiveness in zero-shot scenarios. These systems utilize the neural audio codec models [64, 14] to convert audio waveform into latent and consider it as the intermediate representation for speech generation. Among them, SPEAR-TTS [27] splits the TTS task into two sequence-to-sequence tasks, which enables the training using abundant audio-only data. NaturalSpeech 2 [50] uses a text-conditioned diffusion model to generate the latent vectors of the neural audio codec model. VALL-E [58, 67] proposes the first neural codec language model for text-to-speech, exhibiting strong in-context learning abilities to overcome challenges in zero-shot speech generation. However, these methods ignore the intrinsic property of speech and may lead to inferior or uncontrollable results (e.g., word skipping, repeating, and collapse [58, 67]). Considering the nature of different speech attributes, the autoregressive language model is ideally suitable for prosody modeling. ProsoSpeech [46] has proposed to improve the prosody modeling for TTS with latent prosody vectors predicted by a language model. Nevertheless, it lacks the in-context learning capacity, which restricts its application scenarios.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#generative-models-for-speech-synthesis","title":"Generative models for speech synthesis.","text":"<p>Generative models, like language models [4, 33], VAE [34, 47], GAN [31, 30], Normalizing flow [39, 29], and diffusion model [32, 24, 43, 22], have been applied to speech or audio synthesis for years. Previous works of autoregressive generative model mainly aim at waveform generation [41, 18] and continuous acoustic feature generation [59, 49]. Recently, speech generation systems like AudioLM [4] and VALL-E [58] propose to utilize neural audio codec models [64, 14] to convert audio waveform into discrete codes as the intermediate representation and design LLMs to generate these codes to achieve speech synthesis. Although good reconstruction quality can be achieved by neural audio codec models, they ignore the intrinsic nature of speech [14] and may not be suitable to serve as the generator of intermediate representation for speech generation. The encoded latent contains the phase, content, and timbre attributes and language models are not suitable for predicting these due to the error propagation problem.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#3methodology","title":"3.Methodology: \u65b9\u6cd5","text":"\u539f\u6587  &gt; To introduce proper inductive biases into large-scale TTS systems, we propose ***Mega-TTS***, a zero-shot TTS system for natural and robust speech generation in various scenarios (i.e., zero-shot prompt-based TTS, speech editing, and cross-lingual TTS). &gt; As shown in Figure.01, ***Mega-TTS*** consists of a VQGAN-based [16] TTS model and a prosody large language model (P-LLM). &gt; We carefully model different speech attributes in different ways. &gt; First, we choose the mel-spectrogram as the intermediate representation as it separates the phase from other attributes very well. &gt; Secondly, we extract the global vector from the random previous sentence of the same speaker with the global timbre encoder to disentangle the timbre and content information. &gt; Finally, we further use a VQGAN-based acoustic model to generate the mel-spectrogram and propose a latent code language model called P-LLM to fit the distribution of prosody, since language models are capable of capturing both local and long-range dependency. &gt; During inference, we propose to use the content from the given text sequence, the timbre extracted from the prompt speech, and the prosody predicted by our P-LLM to generate the target speech, which is a novel TTS decoding mechanism called prosody-oriented speech decoding. &gt; Finally, to demonstrate that our model can be applied to various scenarios, we design inference strategies for downstream tasks. &gt; We describe these designs and the training and inference procedures in detail in the following subsections.   <p>\u4e3a\u4e86\u5411\u5927\u89c4\u6a21\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u5f15\u5165\u9002\u5f53\u7684\u5f52\u7eb3\u504f\u7f6e, \u6211\u4eec\u63d0\u51fa\u4e86 Mega-TTS, \u4e00\u4e2a\u7528\u4e8e\u5728\u5404\u79cd\u573a\u666f\u4e0b\u8fdb\u884c\u81ea\u7136\u4e14\u5065\u58ee\u7684\u8bed\u97f3\u5408\u6210\u7684\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf (\u5373\u96f6\u6837\u672c\u57fa\u4e8e\u63d0\u793a\u7684\u6587\u672c\u8f6c\u8bed\u97f3, \u8bed\u97f3\u7f16\u8f91, \u4ee5\u53ca\u8de8\u8bed\u8a00\u6587\u672c\u8f6c\u8bed\u97f3). \u5982\u56fe 01 \u6240\u793a, Mega-TTS \u7531\u57fa\u4e8e VQGAN \u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u548c\u97f5\u5f8b\u5927\u8bed\u8a00\u6a21\u578b (P-LLM) \u7ec4\u6210. \u6211\u4eec\u4ed4\u7ec6\u5730\u7528\u4e0d\u540c\u65b9\u5f0f\u5efa\u6a21\u4e0d\u540c\u8bed\u97f3\u5c5e\u6027. \u9996\u5148, \u6211\u4eec\u9009\u62e9\u6885\u5c14\u9891\u8c31\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a, \u56e0\u4e3a\u5b83\u53ef\u4ee5\u5f88\u597d\u5730\u5206\u79bb\u76f8\u4f4d\u548c\u5176\u4ed6\u5c5e\u6027. \u7136\u540e, \u6211\u4eec\u4ece\u540c\u4e00\u4e2a\u8bf4\u8bdd\u4eba\u968f\u673a\u5148\u524d\u53e5\u5b50\u4e2d\u7528\u5168\u5c40\u97f3\u8272\u7f16\u7801\u5668\u63d0\u53d6\u5168\u5c40\u5411\u91cf\u4ee5\u89e3\u8026\u97f3\u8272\u548c\u5185\u5bb9\u4fe1\u606f. \u6700\u540e, \u6211\u4eec\u8fdb\u4e00\u6b65\u4f7f\u7528\u57fa\u4e8e VQGAN \u7684\u58f0\u5b66\u6a21\u578b\u7528\u4e8e\u751f\u6210\u6885\u5c14\u9891\u8c31, \u7136\u540e\u63d0\u51fa\u4e00\u4e2a\u9690\u7f16\u7801\u8bed\u8a00\u6a21\u578b\u79f0\u4e3a P-LLM \u7528\u4e8e\u62df\u5408\u97f5\u5f8b\u7684\u5206\u5e03, \u56e0\u4e3a\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6355\u83b7\u5c40\u90e8\u548c\u957f\u671f\u4f9d\u8d56\u6027. \u5728\u63a8\u7406\u65f6, \u6211\u4eec\u4f7f\u7528\u4ece\u7ed9\u5b9a\u6587\u672c\u5e8f\u5217\u63d0\u53d6\u7684\u5185\u5bb9, \u4ece\u53c2\u8003\u8bed\u97f3\u63d0\u53d6\u7684\u97f3\u8272, \u4ee5\u53ca\u7531 P-LLM \u9884\u6d4b\u7684\u97f5\u5f8b\u7528\u4e8e\u751f\u6210\u76ee\u6807\u8bed\u97f3, \u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u89e3\u7801\u673a\u5236, \u79f0\u4e3a\u97f5\u5f8b\u5bfc\u5411\u8bed\u97f3\u89e3\u7801. \u6700\u540e, \u4e3a\u4e86\u5c55\u793a\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u5e94\u7528\u4e8e\u5404\u79cd\u573a\u666f, \u6211\u4eec\u4e3a\u4e0b\u6e38\u4efb\u52a1\u8bbe\u8ba1\u4e86\u63a8\u7406\u7b56\u7565. \u6211\u4eec\u5c06\u5728\u540e\u7eed\u5c0f\u8282\u4e2d\u8be6\u7ec6\u63cf\u8ff0\u8fd9\u4e9b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u63a8\u7406\u8fc7\u7a0b.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#31disentangling-speech-into-different-components","title":"3.1.Disentangling Speech into Different Components: \u5c06\u8bed\u97f3\u89e3\u8026\u6210\u4e0d\u540c\u6210\u5206","text":"\u539f\u6587  &gt; To introduce appropriate inductive biases into different speech attributes, we need to separately express these attributes and carefully design different architectures for them. &gt; The overall model architecture of ***Mega-TTS*** is shown in Figure.01. &gt; We use three types of encoders to separately encode content, prosody, and timbre representations. &gt; Then we adopt a GAN-based mel-spectrogram decoder to generate mel-spectrograms with these representations. &gt; We describe the disentangling strategy and detailed design of the proposed encoders as follows.   <p>\u4e3a\u4e86\u7ed9\u4e0d\u540c\u8bed\u97f3\u5c5e\u6027\u5f15\u5165\u9002\u5f53\u7684\u5f52\u7eb3\u504f\u7f6e, \u6211\u4eec\u9700\u8981\u5355\u72ec\u8868\u8fbe\u8fd9\u4e9b\u5c5e\u6027, \u5e76\u4e3a\u5b83\u4eec\u4ed4\u7ec6\u8bbe\u8ba1\u4e0d\u540c\u67b6\u6784. Mega-TTS \u7684\u6574\u4f53\u6a21\u578b\u67b6\u6784\u5982\u56fe 01 \u6240\u793a. \u6211\u4eec\u4f7f\u7528\u4e09\u79cd\u7f16\u7801\u5668\u4ee5\u5206\u522b\u7f16\u7801\u5185\u5bb9, \u97f5\u5f8b\u548c\u97f3\u8272\u8868\u793a. \u7136\u540e\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u6885\u5c14\u9891\u8c31\u89e3\u7801\u5668\u5229\u7528\u8fd9\u4e9b\u8868\u793a\u751f\u6210\u6885\u5c14\u9891\u8c31. \u89e3\u8026\u7b56\u7565\u548c\u7f16\u7801\u5668\u7684\u8be6\u7ec6\u8bbe\u8ba1\u5982\u4e0b\u6240\u793a</p> \u539f\u6587  &gt; #### Disentangling Strategy &gt; We disentangle the mel-spectrogram into content, prosody, and timbre representations with the reconstruction loss of the autoencoder and a carefully designed bottleneck [44]:  &gt; 1) we feed the mel-spectrogram into the prosody encoder, and we also introduce carefully-tuned dimension reduction and phoneme-level downsampling to the prosody encoder to constrain the information flow;  &gt; 2) the content encoder encodes the phoneme sequence into the content representation;  &gt; 3) we feed the reference mel-spectrogram sampled from a different speech of the same speaker to disentangle the timbre and content information and temporally average the output of the timbre encoder to get a one-dimensional global timbre vector. &gt; The correctly-designed bottleneck will learn to remove the content information and the global timbre information from the output of the prosody encoder, which ensures the performance of disentanglement. &gt; Due to the limited page space, we put more details about the hyperparameter selection for the information bottleneck in Appendix D.   <p></p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#_1","title":"\u89e3\u8026\u7b56\u7565","text":"<p>\u6211\u4eec\u5229\u7528\u81ea\u7f16\u7801\u5668\u7684\u91cd\u6784\u635f\u5931\u548c\u4ed4\u7ec6\u8bbe\u8ba1\u7684\u74f6\u9888\u5c06\u6885\u5c14\u9891\u8c31\u89e3\u8026\u6210\u5185\u5bb9, \u97f5\u5f8b\u548c\u97f3\u8272\u8868\u793a: 1. \u5c06\u6885\u5c14\u9891\u8c31\u8f93\u5165\u5230\u97f5\u5f8b\u7f16\u7801\u5668, \u6b64\u5916\u5f15\u5165\u4ed4\u7ec6\u8c03\u6574\u7684\u964d\u7ef4\u548c\u97f3\u7d20\u7ea7\u522b\u4e0b\u91c7\u6837\u5230\u97f5\u5f8b\u7f16\u7801\u5668\u4e2d\u7528\u4e8e\u7ea6\u675f\u4fe1\u606f\u6d41; 2. \u5185\u5bb9\u7f16\u7801\u5668\u5c06\u97f3\u7d20\u5e8f\u5217\u7f16\u7801\u4e3a\u5185\u5bb9\u8868\u793a; 3. \u8f93\u5165\u540c\u4e00\u8bf4\u8bdd\u4eba\u7684\u4e0d\u540c\u8bed\u97f3\u4e2d\u91c7\u6837\u7684\u53c2\u8003\u6885\u5c14\u9891\u8c31\u4ee5\u89e3\u8026\u97f3\u8272\u548c\u5185\u5bb9\u4fe1\u606f, \u5e76\u4e14\u5bf9\u97f3\u8272\u7f16\u7801\u5668\u7684\u8f93\u51fa\u8fdb\u884c\u65f6\u5e8f\u5e73\u5747\u4ee5\u83b7\u5f97\u4e00\u7ef4\u5168\u5c40\u97f3\u8272\u5411\u91cf.</p> <p>\u6b63\u786e\u8bbe\u8ba1\u7684\u74f6\u9888\u5c06\u5b66\u4e60\u4ece\u97f5\u5f8b\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4e2d\u79fb\u9664\u5185\u5bb9\u4fe1\u606f\u548c\u5168\u5c40\u97f3\u8272\u4fe1\u606f, \u786e\u4fdd\u89e3\u8026\u6548\u679c. \u7531\u4e8e\u9875\u9762\u7a7a\u95f4\u6709\u9650, \u6211\u4eec\u5c06\u5728\u9644\u5f55 D \u4e2d\u8be6\u7ec6\u4ecb\u7ecd\u4fe1\u606f\u74f6\u9888\u7684\u8d85\u53c2\u6570\u9009\u62e9.</p> \u539f\u6587  &gt; #### Architecture Design of Encoders.  &gt; 1) The prosody encoder consists of two convolution stacks, a phoneme-level pooling layer, and a vector quantization (VQ) bottleneck. &gt; The first convolution stacks compress mel-spectrograms into phoneme-level hidden states according to the phoneme boundary and the second stacks capture phoneme-level correlations. &gt; The vector quantization layer [54] then utilizes these hidden states to obtain phoneme-level prosody codes $u = \\{u_1, u_2, \\cdots, u_T \\}$ and hidden states $H_{prosody}$. &gt; To ease the difficulty of disentanglement, only the low-frequency band of the mel-spectrogram (the first 20 bins in each mel-spectrogram frame) is used as input, as it contains almost complete prosody and significantly less timbre/content information compared to the full band [46];  &gt; 2) The content encoder is composed of several feed-forward Transformer layers. &gt; To achieve the monotonic alignment between the speech content and generated speech, we adopt the duration predictor and length regulator following common practice in non-autoregressive TTS systems [48, 50]. &gt; Differently, we feed the prosody information extracted by the prosody encoder to the duration predictor in order to ease the one-to-many mapping problem [48, 45];  &gt; 3) The timbre encoder is designed to extract a global vector $H_{timbre}$ that contains the speaker identity of the given speech. &gt; The timbre encoder consists of several stacks of convolution layers. &gt; To ensure the stability of timbre information across the time axis, we temporally average the output of the timbre encoder to get a one-dimensional timbre vector Htimbre.  &gt; To keep good perceptual quality, we introduce a GAN-based mel-spectrogram decoder. &gt; We adopt the multi-length discriminator [10, 63] based on random windows of different lengths as the discriminator. &gt; Overall, the first-stage training loss $Loss$ of ***Mega-TTS*** can be formulated as:  $$   Loss_{VQ} = \\| y_t - y'_t\\|^2 + \\| \\text{sg}[E(y_t)]-z_q\\|^2_2 + \\| \\text{sg}[z_q]-E(y_t)\\|^2_2 $$  $$  Loss = \\mathbb{E}[Loss_{VQ} + Loss_{Adv}] $$  &gt; where $y_t$ is the target speech and $y'_t$ is the generated speech. &gt; $Loss_{rec} = \\|y_t \u2212 y'_t\\|^2$ is the reconstruction loss, $\\text{sg}[\\cdot]$ denotes the stop-gradient operation, and $z_q$ is the temporal collection of codebook entries. &gt; $Loss_{VQ}$ is the VQVAE loss function [54, 16] and $Loss_{Adv}$ is the LSGAN-styled adversarial loss [38] whose objective is to minimize the distribution distance between the predicted mel-spectrograms and the ground truth mel-spectrograms.   <p></p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#_2","title":"\u7f16\u7801\u5668\u67b6\u6784\u8bbe\u8ba1","text":"<ol> <li>\u97f5\u5f8b\u7f16\u7801\u5668\u7531\u4e24\u4e2a\u5377\u79ef\u5806\u6808, \u4e00\u4e2a\u97f3\u7d20\u7ea7\u522b\u6c60\u5316\u5c42, \u5411\u91cf\u91cf\u5316\u74f6\u9888\u7ec4\u6210.   \u7b2c\u4e00\u4e2a\u5377\u79ef\u5806\u6808\u6839\u636e\u97f3\u7d20\u8fb9\u754c\u5c06\u6885\u5c14\u9891\u8c31\u538b\u7f29\u4e3a\u97f3\u7d20\u7ea7\u522b\u7684\u9690\u72b6\u6001;   \u7b2c\u4e8c\u4e2a\u5377\u79ef\u5806\u6808\u6355\u83b7\u97f3\u7d20\u7ea7\u522b\u7684\u76f8\u5173\u6027.   \u5411\u91cf\u91cf\u5316\u5c42\u5229\u7528\u8fd9\u4e9b\u9690\u72b6\u6001\u4ee5\u83b7\u5f97\u97f3\u7d20\u7ea7\u522b\u7684\u97f5\u5f8b\u7f16\u7801 $u = {u_1, u_2, \\cdots, u_T }$ \u548c\u9690\u72b6\u6001 $H_{prosody}$.   \u4e3a\u4e86\u51cf\u8f7b\u89e3\u8026\u7684\u96be\u5ea6, \u53ea\u6709\u6885\u5c14\u9891\u8c31\u7684\u4f4e\u9891\u5e26 (\u6bcf\u4e2a\u6885\u5c14\u9891\u8c31\u5e27\u4e2d\u7684\u524d 20 bin) \u4f5c\u4e3a\u8f93\u5165, \u56e0\u4e3a\u5b83\u5305\u542b\u51e0\u4e4e\u5168\u90e8\u7684\u97f5\u5f8b, \u5e76\u4e14\u76f8\u6bd4\u5168\u9891\u5e26\u643a\u5e26\u66f4\u5c11\u7684\u97f3\u8272\u548c\u5185\u5bb9\u4fe1\u606f.</li> <li>\u5185\u5bb9\u7f16\u7801\u5668\u7531\u6570\u4e2a\u524d\u9988 Transformer \u5c42\u7ec4\u6210.   \u4e3a\u4e86\u5b9e\u73b0\u8bed\u97f3\u5185\u5bb9\u548c\u751f\u6210\u8bed\u97f3\u4e4b\u95f4\u7684\u5355\u8c03\u5bf9\u9f50, \u6211\u4eec\u91c7\u7528\u65f6\u957f\u9884\u6d4b\u5668\u548c\u957f\u5ea6\u8c03\u8282\u5668, \u8fd9\u4e9b\u65b9\u6cd5\u5728\u975e\u81ea\u56de\u5f52\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u4e2d\u666e\u904d\u91c7\u7528.   \u4e0d\u540c\u7684\u662f, \u6211\u4eec\u5c06\u7531\u97f5\u5f8b\u7f16\u7801\u5668\u63d0\u53d6\u7684\u97f5\u5f8b\u4fe1\u606f\u8f93\u5165\u5230\u65f6\u957f\u9884\u6d4b\u5668\u4e2d, \u4ee5\u7f13\u89e3\u4e00\u5bf9\u591a\u6620\u5c04\u95ee\u9898.</li> <li>\u97f3\u8272\u7f16\u7801\u5668\u88ab\u8bbe\u8ba1\u7528\u4e8e\u63d0\u53d6\u5168\u5c40\u5411\u91cf $H_{timbre}$ \u5305\u542b\u7ed9\u5b9a\u8bed\u97f3\u7684\u8bf4\u8bdd\u4eba\u8eab\u4efd.   \u97f3\u8272\u7f16\u7801\u5668\u7531\u591a\u4e2a\u5377\u79ef\u5806\u6808\u7ec4\u6210.   \u4e3a\u4e86\u786e\u4fdd\u65f6\u8f74\u4e0a\u7684\u97f3\u8272\u4fe1\u606f\u7a33\u5b9a, \u6211\u4eec\u5bf9\u97f3\u8272\u7f16\u7801\u5668\u7684\u8f93\u51fa\u8fdb\u884c\u65f6\u5e8f\u5e73\u5747\u4ee5\u83b7\u5f97\u4e00\u7ef4\u97f3\u8272\u5411\u91cf $H_{timbre}$.</li> </ol> <p>\u4e3a\u4e86\u4fdd\u6301\u826f\u597d\u7684\u611f\u77e5\u8d28\u91cf, \u6211\u4eec\u5f15\u5165\u57fa\u4e8e GAN \u7684\u6885\u5c14\u9891\u8c31\u89e3\u7801\u5668. \u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u4e0d\u540c\u957f\u5ea6\u7a97\u53e3\u7684\u591a\u957f\u5ea6\u5224\u522b\u5668\u4f5c\u4e3a\u5224\u522b\u5668.</p> <p>\u603b\u4f53\u6765\u8bf4, Mega-TTS \u7684\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u635f\u5931\u53ef\u4ee5\u8868\u793a\u4e3a:</p> <p>$$   Loss_{VQ} = | y_t - y'_t|^2 + | \\text{sg}[E(y_t)]-z_q|^2_2 + | \\text{sg}[z_q]-E(y_t)|^2_2 $$</p> <p>$$  Loss = \\mathbb{E}[Loss_{VQ} + Loss_{Adv}] $$</p> <p>\u5176\u4e2d: - $y_t$: \u76ee\u6807\u8bed\u97f3; - $y't$: \u751f\u6210\u8bed\u97f3; - $z_q$: \u7801\u672c\u5143\u7d20\u7684\u65f6\u5e8f\u96c6\u5408; - $Loss{rec}$: \u91cd\u6784\u635f\u5931; - $\\text{sg}[\\cdot]$: \u505c\u6b62\u68af\u5ea6\u64cd\u4f5c; - $Loss_{VQ}$: VQVAE \u635f\u5931\u51fd\u6570; - $Loss_{Adv}$: LSGAN \u98ce\u683c\u7684\u5bf9\u6297\u635f\u5931, \u5176\u76ee\u6807\u662f\u6700\u5c0f\u5316\u9884\u6d4b\u6885\u5c14\u9891\u8c31\u548c\u771f\u5b9e\u6885\u5c14\u9891\u8c31\u4e4b\u95f4\u7684\u5206\u5e03\u8ddd\u79bb.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#32p-llm","title":"3.2.P-LLM","text":"\u539f\u6587  &gt; The P-LLM is a latent code language model that captures local and long-range dependency for prosody modeling. &gt; We describe the prosody-oriented speech decoding mechanism and details of the P-LLM as follows.   <p>P-LLM \u662f\u4e00\u4e2a\u9690\u7f16\u7801\u8bed\u8a00\u6a21\u578b, \u7528\u4e8e\u6355\u83b7\u97f5\u5f8b\u5efa\u6a21\u7684\u5c40\u90e8\u548c\u957f\u671f\u4f9d\u8d56. \u6211\u4eec\u5c06\u4ecb\u7ecd\u97f5\u5f8b\u5bfc\u5411\u8bed\u97f3\u89e3\u7801\u673a\u5236\u548c P-LLM \u7684\u8be6\u7ec6\u8bbe\u8ba1.</p> \u539f\u6587  &gt; #### Prosody-Oriented Speech Decoding.  &gt; Denote $(y_p, x_p)$ and $(y_t, x_t)$ as the prompt and target speech-transcription pairs. &gt; Our goal is to synthesize the high-quality target speech $y_t$ given an unseen speech prompt $y_p$. &gt; During inference, the timbre of the target speech $H'_{timbre}$ is expected to be the same as that of the prompt speech. &gt; Therefore, to generate the target speech $y_t$, we only need the prosody information $u'$ of the target speech. &gt; Therefore, the prosody-oriented speech decoding procedure can be formulated as follows:  $$   \\text{Encoder}: u = E_{prosody}(y_p), H_{content} = E_{content}(x_p), H'_{timbre}=E_{timbre}(y_p) $$  $$   \\text{Prosody Prediction}: u' = f(u'|u, H_{content}, H'_{timbre}, H'_{content};\\theta) $$  $$   \\text{Decoder}: y'_t = D(u', H'_{timbre}, H'_{content}) $$  &gt; where $E_{prosody}$, $E_{timbre}$, $E_{content}$, and $D$ denote the prosody encoder, timbre encoder, content encoder, and mel decoder.  &gt; $u$ is the prosody tokens of the prompt speech, $u'$ is the predicted prosody tokens of the target speech, $f$ is the prosody prediction function, and $\\theta$ is the parameter of the P-LLM. &gt; $y'_t$ is the generated speech.   <p></p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#_3","title":"\u97f5\u5f8b\u5bfc\u5411\u8bed\u97f3\u89e3\u7801","text":"<p>\u8bb0 $(y_p, x_p)$ \u548c $(y_t, x_t)$ \u4e3a\u63d0\u793a\u548c\u76ee\u6807\u8bed\u97f3-\u6587\u672c\u5bf9. \u6211\u4eec\u7684\u76ee\u6807\u662f\u6839\u636e\u672a\u89c1\u8fc7\u7684\u63d0\u793a\u8bed\u97f3 $y_p$ \u5408\u6210\u9ad8\u8d28\u91cf\u7684\u76ee\u6807\u8bed\u97f3 $y_t$. \u63a8\u7406\u8fc7\u7a0b\u4e2d, \u76ee\u6807\u8bed\u97f3\u7684\u97f3\u8272 $H'_{timbre}$ \u5e94\u5f53\u4e0e\u63d0\u793a\u8bed\u97f3\u7684\u97f3\u8272\u76f8\u540c. \u56e0\u6b64, \u4e3a\u4e86\u751f\u6210\u76ee\u6807\u8bed\u97f3 $y_t$, \u6211\u4eec\u53ea\u9700\u8981\u76ee\u6807\u8bed\u97f3\u7684\u97f5\u5f8b\u4fe1\u606f $u'$ . \u56e0\u6b64, \u97f5\u5f8b\u5bfc\u5411\u8bed\u97f3\u89e3\u7801\u8fc7\u7a0b\u53ef\u4ee5\u8868\u793a\u4e3a:</p> <p>$$   \\text{\u7f16\u7801\u5668}: u = E_{prosody}(y_p), H_{content} = E_{content}(x_p), H'{timbre}=E{timbre}(y_p) $$</p> <p>$$   \\text{\u97f5\u5f8b\u9884\u6d4b}: u' = f(u'|u, H_{content}, H'{timbre}, H'{content};\\theta) $$</p> <p>$$   \\text{\u89e3\u7801\u5668}: y't = D(u', H'{timbre}, H'_{content}) $$</p> <p>\u5176\u4e2d $E_{prosody}$, $E_{timbre}$, $E_{content}$, \u548c $D$ \u5206\u522b\u4e3a\u97f5\u5f8b\u7f16\u7801\u5668, \u97f3\u8272\u7f16\u7801\u5668, \u5185\u5bb9\u7f16\u7801\u5668, \u548c\u6885\u5c14\u89e3\u7801\u5668. $u$ \u4e3a\u63d0\u793a\u8bed\u97f3\u7684\u97f5\u5f8b\u6807\u8bb0, $u'$ \u4e3a\u76ee\u6807\u8bed\u97f3\u7684\u9884\u6d4b\u97f5\u5f8b\u6807\u8bb0, $f$ \u4e3a\u97f5\u5f8b\u9884\u6d4b\u51fd\u6570, $\\theta$ \u4e3a P-LLM \u7684\u53c2\u6570. $y'_t$ \u4e3a\u751f\u6210\u8bed\u97f3.</p> \u539f\u6587  &gt; #### Generating Prosody Codes.  &gt; The proposed prosody-oriented speech decoding mechanism requires the predicted prosody codes $u'$ of the target speech. &gt; Leveraging the powerful in-context learning capability of LLMs, we design the P-LLM module to predict $u'$. &gt; The P-LLM is a decoder-only transformer-based architecture [7] for prosody modeling, which uses prosody codes $u$ from $y_p$ as the prompt and $H_{content}$, $H'_{content}$, and $H'_{timbre}$ as the condition. &gt; The autoregressive prosody prediction process of P-LLM can be formulated as:  $$   p(u'|u,H_{content}, H'_{timbre}, H'_{content};\\theta) = \\prod_{t=0}^{T} p(u'_t|u'_{ where $\\theta$ is the parameter of our P-LLM. &gt; Since the discrete prosody sequence $u$ is phoneme-level, we directly concatenate it with $H_{content}$, $H'_{content}$, and $H'_{timbre}$ as the input. &gt; The P-LLM is trained in a teacher-forcing mode in the training stage via the cross-entropy loss."},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#_4","title":"\u751f\u6210\u97f5\u5f8b\u7f16\u7801","text":"<p>\u97f5\u5f8b\u5bfc\u5411\u8bed\u97f3\u89e3\u7801\u673a\u5236\u9700\u8981\u76ee\u6807\u97f3\u9891\u7684\u9884\u6d4b\u97f5\u5f8b\u7f16\u7801 $u'$ \u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u529b\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b, \u6211\u4eec\u8bbe\u8ba1\u4e86 P-LLM \u6a21\u5757\u6765\u9884\u6d4b $u'$. P-LLM \u662f\u4ec5\u57fa\u4e8e Transformer \u89e3\u7801\u5668\u67b6\u6784\u7528\u4e8e\u97f5\u5f8b\u5efa\u6a21, \u4ee5\u6765\u81ea $y_p$ \u7684\u97f5\u5f8b\u7f16\u7801 $u$ \u4f5c\u4e3a\u63d0\u793a, $H{content}$, $H'{content}$, \u548c $H'{timbre}$ \u4e3a\u6761\u4ef6. P-LLM \u7684\u81ea\u56de\u5f52\u97f5\u5f8b\u9884\u6d4b\u8fc7\u7a0b\u53ef\u4ee5\u8868\u793a\u4e3a:</p> <p>$$   p(u'|u,H_{content}, H'{timbre}, H'{content};\\theta) = \\prod_{t=0}^{T} p(u't|u'{&lt;t},u,H_{content}, H'{timbre}, H'{content};\\theta) $$</p> <p>\u5176\u4e2d $\\theta$ \u662f P-LLM \u7684\u53c2\u6570. \u7531\u4e8e\u97f5\u5f8b\u5e8f\u5217 $u$ \u662f\u97f3\u7d20\u7ea7\u7684, \u6211\u4eec\u76f4\u63a5\u5c06\u5176\u4e0e $H_{content}$, $H'{content}$, \u548c $H'{timbre}$ \u8fde\u63a5\u4f5c\u4e3a\u8f93\u5165. P-LLM \u5728\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u4ea4\u53c9\u71b5\u635f\u5931\u8fdb\u884c\u6559\u5e08\u5f3a\u8feb\u8bad\u7ec3.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#33speech-prompting-for-inference","title":"3.3.Speech Prompting for Inference: \u7528\u4e8e\u63a8\u7406\u7684\u8bed\u97f3\u63d0\u793a","text":"\u539f\u6587 <p>To facilitate in-context learning for various speech generation tasks, we design different speech prompting mechanisms to encourage Mega-TTS to follow the information in the speech prompt.</p> <p> </p> <p>\u4e3a\u4e86\u4fc3\u8fdb\u5404\u79cd\u8bed\u97f3\u751f\u6210\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60, \u6211\u4eec\u8bbe\u8ba1\u4e0d\u540c\u7684\u8bed\u97f3\u63d0\u793a\u673a\u5236\u4ee5\u9f13\u52b1 Mega-TTS \u9075\u5faa\u8bed\u97f3\u63d0\u793a\u4e2d\u7684\u4fe1\u606f.</p> \u539f\u6587  &gt; #### Inference for TTS.  &gt; For zero-shot TTS, P-LLM uses $u$, $H_{content}$, $H'_{timbre}$, $H'_{content}$ to generate the target prosody codes $u'$ for the target speech according to Equation.04. &gt; We use the top-k random sampling scheme [17] to sample the results since we observe that the sampling-based method could increase the diversity of the generated speech. &gt; Then, we concatenate the content $H'_{content}$, timbre $H'_{timbre}$, and prosody $u'$ information to generate the target speech $y_t$ using the mel decoder. &gt; Leveraging the proper inductive biases and powerful in-context learning capability of our P-LLM, the generated speech can retain not only similar timbre but also the rhythmic habits of the prompt speech. &gt; For cross-lingual TTS, $u$, $H_{content}$, $H'_{timbre}$, $H'_{content}$ are extracted from the prompt speech in a foreign language, and the subsequent procedure keeps the same as that of zero-shot TTS.   <p></p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#_5","title":"\u6587\u672c\u8f6c\u8bed\u97f3\u63a8\u7406","text":"<p>\u5bf9\u4e8e\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3, P-LLM \u4f7f\u7528 $u$, $H_{content}$, $H'{timbre}$, $H'{content}$ \u7b49\u4fe1\u606f\u751f\u6210\u76ee\u6807\u8bed\u97f3\u7684\u97f5\u5f8b\u7f16\u7801 $u'$ \u5e76\u6839\u636e\u516c\u5f0f 04 \u8fdb\u884c\u751f\u6210. \u6211\u4eec\u4f7f\u7528 Top-K \u968f\u673a\u91c7\u6837\u65b9\u6848\u6765\u91c7\u6837\u7ed3\u679c\u56e0\u4e3a\u6211\u4eec\u89c2\u5bdf\u5230\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u80fd\u591f\u589e\u52a0\u751f\u6210\u8bed\u97f3\u7684\u591a\u6837\u6027. \u7136\u540e\u6211\u4eec\u5c06\u5185\u5bb9 $H'{content}$, \u97f3\u8272 $H'{timbre}$, \u4ee5\u53ca\u97f5\u5f8b $u'$ \u4fe1\u606f\u62fc\u63a5\u4f7f\u7528\u6885\u5c14\u89e3\u7801\u5668\u751f\u6210\u76ee\u6807\u8bed\u97f3 $y_t$. \u5229\u7528\u9002\u5f53\u7684\u5f52\u7eb3\u504f\u7f6e\u548c P-LLM \u7684\u5f3a\u529b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b, \u751f\u6210\u7684\u8bed\u97f3\u4e0d\u4ec5\u80fd\u591f\u4fdd\u6301\u76f8\u4f3c\u97f3\u8272\u8fd8\u80fd\u4fdd\u6301\u63d0\u793a\u8bed\u97f3\u7684\u8282\u594f\u4e60\u60ef. \u5bf9\u4e8e\u8de8\u8bed\u8a00\u6587\u672c\u8f6c\u8bed\u97f3, $u$, $H_{content}$, $H'{timbre}$, $H'{content}$ \u4ece\u5176\u4ed6\u8bed\u8a00\u7684\u63d0\u793a\u8bed\u97f3\u4e2d\u63d0\u53d6\u5e76\u4f7f\u7528, \u540e\u7eed\u8fc7\u7a0b\u4e0e\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u76f8\u540c.</p> \u539f\u6587  &gt; #### Inference for Speech Editing.  &gt; In speech editing, the predicted prosody codes should achieve smooth transitions at both the left and right boundaries of the masked region. &gt; Previous works like EditSpeech [52] propose to perform left and right autoregressive inferences separately and concat the mel-spectrogram at the least L2-norm difference fusion point. &gt; However, the L2-norm difference of the mel-spectrogram is far from human perception, leading to poor audio naturalness. &gt; Since the prosody representations in ***Mega-TTS*** is discrete, we can solve the transition problem by operating on discrete prosody representations. &gt; First, we regard the area on the left side of the mask as a prompt to generate N candidate paths with top-k random sampling strategy. &gt; Secondly, the N generated paths are used as new prompts to generate the probability matrix of the area on the right side of the mask and the ground-truth prosody codes are used to obtain the probabilities of each decoding step from the probability matrix. &gt; In the third stage, we sum up the log probabilities of each decoding step for the candidate paths. &gt; Finally, we choose the path that achieves the maximum probability in the second step as the predicted result. &gt; The decoding strategy for speech editing can be formulated as follows:  $$   \\max_{i\\in [1,N]} \\text{Likelihood} = \\max_{i\\in [1,N]} \\prod_{t=L}^{R} p(u_t^i | u_{ where $L$ and $R$ are the left and right boundaries of the mask. &gt; $T$ is the length of the mel-spectrogram.  &gt; $u_t^i$ is the prosody code in the $i$-th candidate path.  &gt; $u^{gt}_t$ is the ground-truth prosody codes. &gt; Since our decoding strategy considers the prosody information of the boundaries on both sides, the edited region can achieve smooth transitions.   <p></p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#_6","title":"\u8bed\u97f3\u7f16\u8f91\u63a8\u7406","text":"<p>\u5728\u8bed\u97f3\u7f16\u8f91\u4e2d, \u9884\u6d4b\u7684\u97f5\u5f8b\u7f16\u7801\u5e94\u8be5\u5728\u63a9\u7801\u533a\u57df\u7684\u5de6\u53f3\u8fb9\u754c\u5904\u5b9e\u73b0\u5e73\u6ed1\u8fc7\u6e21. \u4e4b\u524d\u7684\u5de5\u4f5c\u5982 EditSpeech [52] \u63d0\u51fa\u4e86\u5206\u522b\u8fdb\u884c\u5de6\u53f3\u81ea\u56de\u5f52\u63a8\u7406\u5e76\u5728\u6700\u5c0f L2 \u8303\u6570\u878d\u5408\u70b9\u62fc\u63a5\u6885\u5c14\u9891\u8c31. \u7136\u800c, \u97f3\u9891\u81ea\u7136\u5ea6\u7684 L2 \u8303\u6570\u5dee\u8ddd\u8fdc\u8fdc\u504f\u79bb\u4eba\u7c7b\u7684\u611f\u77e5, \u5bfc\u81f4\u8bed\u97f3\u8d28\u91cf\u8f83\u5dee. \u7531\u4e8e Mega-TTS \u4e2d\u7684\u97f5\u5f8b\u8868\u793a\u662f\u79bb\u6563\u7684, \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u79bb\u6563\u97f5\u5f8b\u8868\u793a\u6765\u89e3\u51b3\u8fc7\u6e21\u95ee\u9898. \u9996\u5148, \u6211\u4eec\u5c06\u63a9\u7801\u533a\u57df\u7684\u5de6\u4fa7\u533a\u57df\u89c6\u4e3a\u63d0\u793a\u751f\u6210 N \u4e2a\u5019\u9009\u8def\u5f84, \u4f7f\u7528 Top-K \u968f\u673a\u91c7\u6837\u7b56\u7565\u751f\u6210. \u5176\u6b21, N \u4e2a\u751f\u6210\u8def\u5f84\u88ab\u7528\u4f5c\u65b0\u7684\u63d0\u793a, \u7528\u4e8e\u751f\u6210\u63a9\u7801\u533a\u57df\u7684\u53f3\u4fa7\u533a\u57df\u7684\u6982\u7387\u77e9\u9635, \u5e76\u4f7f\u7528\u6982\u7387\u77e9\u9635\u4ece\u5019\u9009\u8def\u5f84\u4e2d\u83b7\u5f97\u6bcf\u4e2a\u89e3\u7801\u6b65\u7684\u6982\u7387. \u7b2c\u4e09\u6b65, \u6211\u4eec\u5bf9\u6bcf\u4e2a\u5019\u9009\u8def\u5f84\u7684\u89e3\u7801\u6b65\u7684\u6982\u7387\u6c42\u548c. \u6700\u540e, \u6211\u4eec\u5728\u7b2c\u4e8c\u6b65\u4e2d\u9009\u62e9\u6982\u7387\u6700\u5927\u7684\u8def\u5f84\u4f5c\u4e3a\u9884\u6d4b\u7ed3\u679c. \u8bed\u97f3\u7f16\u8f91\u7684\u89e3\u7801\u7b56\u7565\u53ef\u4ee5\u8868\u793a\u4e3a:</p> <p>$$   \\max_{i\\in [1,N]} \\text{Likelihood} = \\max_{i\\in [1,N]} \\prod_{t=L}^{R} p(u_t^i | u_{&lt;t}^i, H_{content}, H'{timbre}, H'{content};\\theta)\\cdot\\prod_{t=R}^{T} p(u_t^{gt}|u_{&lt;t}^{i}, H_{content}, H'{timbre}, H'{content};\\theta) $$</p> <p>\u5176\u4e2d $L$ \u548c $R$ \u662f\u63a9\u7801\u533a\u57df\u7684\u5de6\u53f3\u8fb9\u754c. $T$ \u662f\u6885\u5c14\u9891\u8c31\u7684\u957f\u5ea6. $u_t^i$ \u662f\u7b2c i \u4e2a\u5019\u9009\u8def\u5f84\u4e2d\u7684\u97f5\u5f8b\u7f16\u7801. $u^{gt}_t$ \u662f\u76ee\u6807\u97f5\u5f8b\u7f16\u7801. \u7531\u4e8e\u6211\u4eec\u7684\u89e3\u7801\u7b56\u7565\u8003\u8651\u4e86\u63a9\u7801\u533a\u57df\u7684\u5de6\u53f3\u8fb9\u754c, \u7f16\u8f91\u533a\u57df\u53ef\u4ee5\u5b9e\u73b0\u5e73\u6ed1\u8fc7\u6e21.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#4experiments","title":"4.Experiments: \u5b9e\u9a8c","text":"<p>In this section, we present the evaluation results of Mega-TTS and the comparison with baselines in terms of the objective and subjective metrics.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#41experimental-setup","title":"4.1.Experimental setup","text":""},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#training-datasets","title":"Training datasets.","text":"<p>We use GigaSpeech [9] and WenetSpeech [65] as the training corpora, which contains 20k hours of multi-domain speeches in English and Chinese in total. Since the speech in GigaSpeech and WenetSpeech does not have speaker identities and multiple speakers may appear in a speech clip, we process the datasets with an open-source automatic speaker diarization model6 [6, 5]. We also extract the phoneme-level alignments with the external alignment tool7. More information can be found in Appendix A.3.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#evaluation-datasets","title":"Evaluation datasets.","text":"<p>We employ two datasets for evaluation:  1) VCTK dataset [57], an English dataset that contains 108 speakers;  2) LibriSpeech [42] test-clean, an English dataset that contains 40 speakers. For each of these datasets, we randomly sample 10 utterances for each of the 40 speakers, resulting in a subset of 400 utterances for evaluation; Specifically, to synthesize each sample, we randomly select a different utterance of the same speaker to form the speech prompt. Note that all speakers in the evaluation datasets are unseen during training.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#model-configuration","title":"Model configuration.","text":"<p>Our Mega-TTS consists of three encoders, a prosody large language model, a mel decoder, and a discriminator. The prosody encoder, timbre encoder, and mel generator consist of 5 convolutional blocks with 320 hidden size, 5 convolution 1D kernel size. The content encoder is a 4-layer Transformer [56] with 2 attention heads, 320 embedding dimensions, 1280 1D convolution filter size, and 5 convolution 1D kernel size. The duration predictor is a 3-layer 1D convolution with ReLU activation and layer normalization, which have 320 hidden size. The discriminator follows the architecture proposed in SyntaSpeech [63]. The P-LLM model is a decoder-only architecture that contains 8 Transformer layers with 8 attention heads, 512 embedding dimensions, 2048 1D convolution filter size, and 5 convolution 1D kernel size. The overall number of model parameters is 222.5M. We add more detailed model configurations in Appendix A.1.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#training-and-inference","title":"Training and inference.","text":"<p>In the training stage, we train Mega-TTS on 8 NVIDIA A100 GPUs, with a batch size of 30 sentences on each GPU. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03f5 = 10\u22129 and follow the same learning rate schedule in [56]. It takes 320k steps for the VQ-GAN TTS model\u2019s training and 100K steps for the P-LLM\u2019s training until convergence. The predicted mel-spectrograms are transformed into audio samples using pre-trained HiFi-GAN V18 [31]. In the inference stage, we use the top-5 random sampling scheme [17] to sample diverse results.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#objective-metrics","title":"Objective metrics.","text":"<p>We evaluate the pitch distance and speaker similarity for zero-shot TTS. In terms of the pitch distance, we compute the average dynamic time warping (DTW) [40] distances between the pitch contours of ground-truth speech and synthesized speech. And for the cosine speaker similarity, we use the WavLM model [12] finetuned for speaker verification9 to compute the cosine speaker similarity score between the ground-truth speech and synthesized speech. The similarity score is in the range of [\u22121, 1], where a larger value indicates a higher similarity of input samples. In addition, we also evaluate the word error rate (WER) for cross-lingual TTS. We use the ASR system from the released HuBERT-Large model [19] to transcribe the generated speech into text. Then, the WER between the transcribed text and the original target text is measured. We use all samples in the test set for the objective evaluation. We put more information in Appendix A.4 and Appendix A.5.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#subjective-metrics","title":"Subjective metrics.","text":"<p>We conduct the MOS (mean opinion score) and CMOS (comparative mean opinion score) evaluation on the test set to measure the audio naturalness via Amazon Mechanical Turk. We keep the text content and prompt speech consistent among different models to exclude other interference factors. We randomly choose 50 samples from the test set of each dataset for the subjective evaluation and each audio is listened to by at least 20 testers. We analyze the MOS in three aspects: MOS-Q (Quality: clarity, high-frequency, and original timbre reconstruction), MOS-P (Prosody: naturalness of pitch, energy, and duration), and MOS-S (Speaker similarity). We also analyze the CMOS in terms of audio quality and speech prosody. We tell the tester to focus on one corresponding aspect and ignore the other aspect when scoring. We put more information about the subjective evaluation in Appendix A.2.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#42results-of-zero-shot-synthesis","title":"4.2.Results of zero-shot synthesis","text":"<p>We compare the zero-shot synthesis performance of Mega-TTS with baseline systems, including:  1)YourTTS [8], a powerful zero-shot TTS model trained on 1k hours of speech dataset. We use the official code and released checkpoint10;  2) VALL-E, a large-scale zero-shot TTS model using the audio codec model to generate discrete speech codes and LLM to generate them. For VALL-E, we directly download the first 16 utterances from the VALL-E demo page. The audio samples consist of 8 samples from LibriSpeech and 8 samples from VCTK11. As shown in Table 2, Mega-TTS significantly outperforms YourTTS in terms of audio quality and speech prosody. And in terms of speaker similarity, Mega-TTS significantly outperforms YourTTS with +0.51 MOS-S on VCTK and +0.68 MOS-S on LibriSpeech, demonstrating the effectiveness of Mega-TTS in zero-shot scenarios. Besides, as shown in Table 3, Mega-TTS outperforms VALL-E in all metrics. It can be seen that Mega-TTS is able to generate more natural speeches than VALL-E, demonstrating the effectiveness of introducing intrinsic inductive biases. To further investigate the performance of disentanglement, we also visualize the distribution of the timbre and prosody representations in Appendix C.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#43results-of-zero-shot-speech-editing","title":"4.3.Results of zero-shot speech editing","text":"<p>We compare the quality of generated audio samples of our Mega-TTS with SOTA speech editing baselines, including 1) EditSpeech [52]; 2) A3T [3]. Since the text content of the generated speech has been edited in the speech editing evaluation, the ground truth is missing. Therefore, we only conduct the subjective evaluation. We manually define modification operations (i.e., insertion, replacement, and deletion) of the test samples. We then conduct the experiments on the VCTK dataset. We evaluate the audio quality, speech prosody, and speaker similarity for each audio sample. The results are presented Table 4. It can be seen that Mega-TTS achieves the highest perceptual quality, prosody, and speaker similarity score, which demonstrates the effectiveness of our proposed speech prompting mechanism for speech editing and the powerful in-context learning capability of Mega-TTS.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#44results-of-zero-shot-cross-lingual-tts","title":"4.4.Results of zero-shot cross-lingual TTS","text":"<p>To compare Mega-TTS with the zero-shot cross-lingual TTS models VALL-E X [67], we directly download the utterances from the VALL-E X demo page, which consists of 6 speech pairs from LibriSpeech, EMIME, and AISHELL-3. Since YourTTS [8] is built only for English TTS, we evaluate the performance of English TTS with Chinese samples as prompts. The results are listed in Table 5. It can be seen that Mega-TTS surpasses VALL-E X in terms of audio quality, speech prosody, and speaker similarity scores, which further demonstrates the superiority of introducing proper inductive biases to different speech attributes. For objective evaluations, we use all of the text samples in the LibriSpeech test-clean set as the target sentences and randomly select one audio from AISHELL-3 as the speech prompt for each target sentence. The results show that Mega-TTS achieves a significantly lower WER than YourTTS, demonstrating the effectiveness of our method.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#45results-of-robustness-evaluation","title":"4.5.Results of robustness evaluation","text":"<p>To further evaluate the robustness of the proposed model, we adopt the 50 particularly hard sentences following FastSpeech [48]. As shown in Table 6, Tacotron [59] and VALL-E [58] show poor robustness on these complicated sentences. As a comparison, our Mega-TTS shows equivalent robustness to the non-autoregressive models, such as FastSpeech [48], without any repeat or skip issues. It can be seen that directly modeling the discrete speech tokens with LLMs like VALL-E [58] would cause robustness issues. As a comparison, Mega-TTS not only leverages the in-context learning capability of LLMs, but also maintains good robustness by introducing the proper inductive bias to each speech component.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#5conclusions","title":"5.Conclusions: \u7ed3\u8bba","text":"<p>In this paper, we proposed Mega-TTS, which aims to introduce proper inductive biases into large-scale zero-shot TTS systems. We disentangle speech into different attributes (i.e., content, timbre, prosody, and phase) and model different attributes in different ways. We train Mega-TTS with 20K hours of multi-domain speech data and evaluate its performance on unseen datasets. Our experimental results on three speech synthesis tasks show that Mega-TTS outperforms state-of-the-art zero-shot TTS models regarding audio quality, speech prosody, speaker similarity, and robustness. Due to limited page space, we discuss the limitations and future works in Appendix F and the broader impacts in Appendix G.</p>"},{"location":"TTS/Models/Speech_LLM/2023.06.06_Mega-TTS/#appendix","title":"Appendix","text":"\u539f\u6587   ### A. #### A.2.Details in Subjective Evaluation  &gt; We perform the audio quality, speech prosody, and speaker similarity evaluations on Amazon Mechanical Turk (MTurk). &gt; For each dataset, we randomly select 50 samples from the test set and use the TTS systems to generate the audio samples. &gt; Each audio has been listened to by at least 20 listeners. &gt; For MOS, each tester is asked to evaluate the subjective score of a sentence on a 1-5 Likert scale. &gt; For CMOS, listeners are asked to compare pairs of audio generated by systems A and B, indicating which of the two audio they prefer, and choose one of the following scores according to the degree of superiority: 0 indicating no difference, 1 indicating source slightly better, 2 indicating source mostly better and 3 indicating source completely better. &gt; For audio quality evaluation (MOS-Q and CMOS-Q), we tell listeners to \u201cPlease focus on the audio quality and ignore other factors\u201d. &gt; For prosody evaluations (MOS-P and CMOS-P), we tell listeners to \u201cPlease focus on the prosody and style, and ignore the differences of grammar, audio quality, or other factors. \u201d. &gt; For speaker similarity evaluations (MOS-S), we tell listeners to \u201c Please focus only on the similarity of the speaker to the reference, and ignore the differences of content, grammar, prosody, audio quality, or other factors.\u201d. &gt; The screenshots of instructions for testers are shown in Figure.03. &gt; We paid $12 to participants hourly and totally spent about $1000 on participant compensation. &gt; We tell the participants that the data will be used in scientific research.  #### A.3.Details of Speaker Diarization Model  &gt; To obtain the speaker information from GigaSpeech and WenetSpeech, we use a released automatic speaker diarization model called pyannote.audio12, which achieves DER=11.24% on the VoxConverse dataset and DER=14.09% on the AISHELL-4 dataset. &gt; We only assign the speaker ID to the audio clip when its activation score is higher than 70% and abandon other audio clips. &gt; We also abandon the audio clips that contain multiple speakers speaking simultaneously.  #### A.4.Details of Speaker Similarity Model  &gt; To measure the speaker similarity, we use the WavLM [12] model finetuned for speaker verification from https://huggingface.co/microsoft/wavlm-base-plus-sv to extract the speaker embedding. &gt; Then the cosine similarity between the synthesized speech\u2019s speaker embedding and the ground-truth speech\u2019s speaker embedding is calculated as the speaker similarity score. &gt; The WavLM model is pretrained on 94,000 hours of speech data and finetuned on the VoxCeleb1 dataset using an X-Vector head with an Additive Margin Softmax loss, which achieves 0.84%, 0.928%, and 1.758% EER (Equal Error Rate) on the Vox1-O, Vox1-E, and Vox1-H trial lists.  #### A.5.Details of ASR Model &gt; To measure the audio quality and speech intelligibility for cross-lingual TTS systems, we evaluate the word error rate (WER) metric. &gt; We use the finetuned HuBERT-Large model to transcribe the synthesized speech into text and calculate the WER between the transcribed text and the original target text. &gt; The finetuned HuBERT-Large model from https://huggingface.co/facebook/ hubert-large-ls960-ft is finetuned on 960h of Librispeech and achieves 1.5%, 3.0%, 1.9%, and 3.3% WER on the dev-clean, dev-other, test-clean, and test-other set of Librispeech.  #### A.6.Error Bars and Random Seeds  &gt; For the subjective evaluations, we report confidence intervals of the results of MOS tests in Table 2, Table 3, Table 4, and Table 5. &gt; For the objective evaluations, we ran the experiments 10 times with 10 different random seeds ([1234, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]) and obtained the averaged results.  ### B.Visualizations of Mel-Spectrograms  &gt; We put more visualizations of mel-spectrograms with different random seeds in Figure.04. &gt; We can see that with different random seeds, ***Mega-TTS*** can generate diverse results that have different prosody and frequency details.  ### C.Visualization of Representations  &gt; To validate the effectiveness of disentanglement for speech components in Section 3.1, we adopt T-SNE [55] to visualize timbre embedding and prosody embedding for unseen speakers on the VCTK dataset. &gt; We randomly select 10 speakers and directly use the encoders proposed in Section 3.1 to extract the timbre and prosody information from their audio samples. &gt; The results are shown in Figure.05 and Figure.06. &gt; It can be seen that the timbre embeddings are ideally located according to the speaker ID. &gt; However, the prosody embeddings of different speakers have similar distributions. &gt; It shows that our proposed prosody and timbre encoders are able to disentangle the corresponding representations from the mel-spectrograms, which further ensures the effectiveness of our P-LLM.  ### D.Hyperparameter Selection for the Information Bottleneck  &gt; In this section, we describe the details of the hyperparameter selection for the information bottleneck proposed in Section 3.1. &gt; The information bottleneck of ***Mega-TTS*** mainly contains two key hyperparameters: the channel size and the embedding size of the vector quantization (VQ) layer. &gt; When the channel size and the embedding size are too small or large, the performance of disentanglement will be poor. &gt; Therefore, we should carefully select these hyperparameters. &gt; We train the VQGAN-based TTS models with different VQ hyperparameters and evaluate their pitch distance and speaker similarity following Section 4. &gt; Differently, we use the proposed encoders to extract the timbre, content, and prosody embeddings of the test samples. &gt; Then, we randomly shuffle the timbre embedding sequence and reconstruct the mel-spectrogram with the original content, original prosody, and shuffled timbre information. &gt; We calculate the pitch distance between the ground-truth speech and the generated speech, but we calculate the speaker similarity between the shuffled ground-truth speech and the generated speech. &gt; As shown in Table 8, when the channel size is 256 and the embedding size is 2048, the VQGAN-based TTS model shows the best pitch accuracy and speaker similarity, i.e., the disentanglement performance is the best.  ### E.Ablation Studies of Dataset Size and Model Size  &gt; In this section, we evaluate the influences of the training dataset size and model size on the zero-shot TTS task for ***Mega-TTS***. &gt; We evaluate the pitch distance, speaker similarity, and the average absolute duration error in milliseconds on the LibriSpeech test-clean set. &gt; As shown in Table 9, when the dataset size grows, the zero-shot performance of ***Mega-TTS*** is significantly improved. &gt; Moreover, from Table 10, we can see that when the hidden size of P-LLM grows, the pitch distance significantly drops, demonstrating that the in-context learning capability of P-LLM can be greatly improved by the size of the model.  ### F.Limitations and Future Works  &gt; Although achieving superior performance on various zero-shot speech synthesis tasks, ***Mega-TTS*** still suffers from two main limitations.  &gt; Data coverage. &gt; Although we use 20K hours of multi-domain data for training, our model still cannot cover everyone\u2019s voice. &gt; Especially for some speakers with extremely heavy accents, our model cannot imitate their speaking style very well. &gt; In the future, we will further scale up the training data to 200K hours to further improve the performance of the model. &gt; Reconstruction Robustness. &gt; Although the reconstruction quality of the proposed VQGAN-based TTS model is satisfying on the clean dataset, it will be influenced by the background music or the extremely loud reverberation. &gt; In future work, we will explore a new model structure that is more robust against the acoustic environment noises.  ### G.Broader Impacts  &gt; ***Mega-TTS*** improves the quality and efficiency of zero-shot speech synthesis, which makes it easier for people to synthesize personalized speeches. &gt; In most cases, people will utilize this technique to facilitate movies, games, podcasts, and other services only. &gt; However, it may carry potential risks in misuse of the model, such as spoofing voice or other deepfake-related usages. &gt; To handle this, potential solutions like building a corresponding deepfake detection model should be considered. &gt; We also plan to include restrictions in the open-source license of the ***Mega-TTS*** project to prevent the misuse of the model."},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/","title":"Mega-TTS 2","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis - \u4f5c\u8005:   - 01 [Ziyue Jiang](../../Authors/Ziyue_Jiang.md)   - 02 [Jinglin Liu](../../Authors/Jinglin_Liu.md)   - 03 [Yi Ren](../../Authors/Yi_Ren_(\u4efb\u610f).md)   - 04 [Jinzheng He](../../Authors/Jinzheng_He.md)   - 05 [Zhenhui Ye](../../Authors/Zhenhui_Ye.md)   - 06 [Shengpeng Ji](../../Authors/Shengpeng_Ji.md)   - 07 [Qian Yang](../../Authors/Qian_Yang.md)   - 08 [Chen Zhang](../../Authors/Chen_Zhang.md)   - 09 [Pengfei Wei](../../Authors/Pengfei_Wei.md)   - 10 [Chunfeng Wang](../../Authors/Chunfeng_Wang.md)   - 11 [Xiang Yin](../../Authors/Xiang_Yin.md)   - 12 [Zejun Ma](../../Authors/Zejun_Ma.md)   - 13 [Zhou Zhao](../../Authors/Zhou_Zhao_(\u8d75\u6d32).md) - \u673a\u6784:   - \u673a\u6784  - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2023.07.14 ArXiv v1 (Zero-Shot Text-to-Speech with Arbitrary Length Speech Prompts)   - \u9884\u5370\u65f6\u95f4: 2023.09.28 ArXiv v2   - \u9884\u5370\u65f6\u95f4: 2024.03.18 ArXiv v3 (Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis)    - \u53d1\u8868\u65f6\u95f4: 2024.01.16 ICLR2024   - \u9884\u5370\u65f6\u95f4: 2024.04.10 ArXiv v4   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.17 - \u53d1\u8868:   - [ICLR 2024](../../Publications/ICLR.md) - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2307.07218)   - [DOI]()   - [Github]()   - [Demo](https://boostprompt.github.io/boostprompt/)   - [Scholar](https://scholar.google.com/scholar?cluster=16735322993503076322) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md)   - [\u8bed\u8a00\u6a21\u578b](../../Tags/LanguageModel.md) - \u9875\u6570: 21 - \u5f15\u7528: ? - \u88ab\u5f15: 14 - \u6570\u636e:"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#abstract","title":"Abstract: \u6458\u8981","text":"<p>Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech prompts, which significantly reduces the data and computation requirements for voice cloning by skipping the fine-tuning process. However, the prompting mechanisms of zero-shot TTS still face challenges in the following aspects: </p> <ol> <li>previous works of zero-shot TTS are typically trained with single-sentence prompts, which significantly restricts their performance when the data is relatively sufficient during the inference stage.</li> <li>The prosodic information in prompts is highly coupled with timbre, making it untransferable to each other.</li> </ol> <p>This paper introduces Mega-TTS 2, a generic prompting mechanism for zero-shot TTS, to tackle the aforementioned challenges. Specifically, we design a powerful acoustic autoencoder that separately encodes the prosody and timbre information into the compressed la tent space while providing high-quality reconstructions. Then, we propose a multi-reference timbre encoder and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts. We further leverage the probabilities derived from multiple P-LLM outputs to produce transferable and controllable prosody. Experimental results demonstrate that Mega-TTS 2 could not only synthesize identity-preserving speech with a short prompt of an unseen speaker from arbitrary sources but consistently outperform the fine-tuning method when the volume of data ranges from 10 seconds to 5 minutes. Furthermore, our method enables to transfer various speaking styles to the target timbre in a fine-grained and controlled manner. Audio samples can be found in https://boostprompt.github.io/boostprompt/.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#1introduction","title":"1.Introduction","text":"<p>In recent years, there has been remarkable progress in the development of text-to-speech (TTS) technology (Shen et al., 2018; Jia et al., 2018; Li et al., 2019; Kim et al., 2020; Ren et al., 2019; 2020; Kim et al., 2021; 2022a). Among them, adaptive TTS systems (Chen et al., 2021; Min et al., 2021; Kim et al., 2022b) are capable of cloning personalized voices given a few minutes of speech data. However, the performance of these systems relies heavily on the quality and quantity of the data utilized during the fine-tuning phases (Tan et al., 2021). Insufficient data during the fine-tuning stages can lead to diminished audio naturalness or speech intelligibility (Kang et al., 2023). Moreover, the computational demands also constrain its application for cloning everyone\u2019s voice.</p> <p>To reduce such a reliance, existing works leverage generative models to perform zero-shot TTS (Cooper et al., 2020a; Casanova et al., 2022; Huang et al., 2022a; Kang et al., 2023; Kharitonov et al., 2023; Wang et al., 2023; NaturalSpeech 2; Matthew et al., 2023). These powerful models can effectively synthesize speech given only a single speech prompt, eliminating the need for data preparation and the computational requirements for fine-tuning methods. However, the prompting mechanisms of current solutions still face two primary challenges: </p> <ul> <li>Lack of multi-sentence prompting strategies. Previous works of zero-shot TTS typically employ single-sentence speech prompts during training (Wang et al., 2023; NaturalSpeech 2; Matthew et al., 2023). In inference, the information in the single-sentence speech prompt is insufficient to guide the zero-shot TTS systems to imitate the voice variability of a natural person perfectly.1From another perspective, the performance of fine-tuning methods can be further improved by increasing the amount of data, while zero-shot TTS systems lack an appropriate strategy to extract useful information from multi-sentence speech prompts.</li> <li>Lack of specialized prompting mechanism for prosodic information. Current solutions for zero-shot TTS primarily concentrate on improving the similarity of timbre and prosody between the generated speech and the prompts. However, they neglect to express various unseen prosodic styles in a controlled manner while also preserving the unique timbre of the given one-sentence prompt. In order to control the prosodic styles, it is necessary to disentangle the prosody information from speech prompts.</li> </ul> <p>We address the above challenges by decomposing speech into content, timbre, and prosody. Intuitively, representing speeches for numerous speakers requires a substantial number of codebook entries for timbre modeling (Defossez et al., 2022; Yang et al., 2023). Through the decoupling of prosody information, a highly compact codebook for prosody modeling can be obtained, which enables our model to effectively handle extremely long prompts and have flexible control over prosodic styles. Therefore, this work proposes Mega-TTS 2, a generic framework that boosts the prompting mechanisms for zero-shot TTS systems. Specifically, we begin by designing an acoustic autoencoder that can effectively decompose speech into prosody and timbre representations and represent them in a compact latent space. Then, we design a Multi-Reference Timbre Encoder (MRTE) and a prosody latent language model (P-LLM) to extract useful information from multi-sentence prompts. In addition to the multi-sentence prompting mechanism, we propose a prosody interpolation technique to control the generation process of prosody codes by utilizing prosody prompts from multiple speakers while maintaining the target speaker\u2019s timbre. By utilizing the probabilities derived from both the prosodic prompts of the target speaker and the auxiliary speaker, the prosodic styles of speech can be generated in a controlled manner. Experiments on LibriSpeech test-clean (Panayotov et al., 2015) and ESD (Zhou et al., 2021) datasets show that Mega-TTS 2 outperforms other state-of-the-art fine-tuning and zero-shot TTS models in terms of speaker similarity and speech naturalness. Notably, when the length of the prompt is further extended, our method surpasses the fine-tuning baseline model in the objective and subjective evaluations. The extensive studies on adaptive prosody transfer further highlight the superiority of our proposed prompting mechanisms. The main contributions of this work are summarized as follows:  - We design an acoustic autoencoder that separately compresses the prosody and timbre information into the latent space, which allows our model to process prompts of up to 300 seconds in length effectively. - We propose a multi-reference timbre encoder and an auto-regressive prosody language model to extract fine-grained information from multiple reference speeches, which bridges the speaker similarity gap between zero-shot methods and fine-tuning methods. - Experimental results also reveal that the performance of Mega-TTS 2 surpasses the powerful fine-tuning baseline when we have 10 seconds to 5 minutes of data for each unseen speaker, indicating the superiority of our proposed prompting mechanisms. - The proposed prosody interpolation technique ensures the controllability of prosody and is capable of transferring various speaking styles to the desired timbre. For instance, we can transform a voice with a sad tone into a happier one with the auxiliary prosody prompt from another speaker.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#2related-works","title":"2.Related Works: \u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#adaptive-tts","title":"Adaptive TTS: \u81ea\u9002\u5e94\u6587\u672c\u8f6c\u8bed\u97f3","text":"<p>Adaptive TTS (Arik et al., 2018; Kons et al., 2019; Moss et al., 2020; Chien et al., 2021) focuses on synthesizing personalized voice for any user with few data. During the adaptation process, a TTS model pre-trained on a multi-speaker speech dataset is typically fine-tuned with few adaptation data for the target voice (Tan et al., 2021). Chen et al. (2018) design independent learned embeddings for each speaker, which requires few data at deployment time to adapt to new speakers rapidly. AdaSpeech (Chen et al., 2021) proposes an acoustic-condition modeling method for high-quality and efficient customization of new voices. There are also some works leveraging the meta-learning approach (Chen et al., 2018; Min et al., 2021; Huang et al., 2022b) and data augmentation (Cooper et al., 2020b; Yang &amp; He, 2020) for speaker adaptation. However, although some works are data-efficient (Min et al., 2021; Huang et al., 2022b) and parameter-efficient (Chen et al., 2021), these systems still suffer from audio quality issues when data size is small, as well as computational cost issues due to hundreds of fine-tuning steps.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#zero-shot-tts","title":"Zero-Shot TTS: \u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3","text":"<p>Zero-shot adaptation (Jia et al., 2018; Arik et al., 2018; Cooper et al., 2020a; Casanova et al., 2021; Wu et al., 2022; Huang et al., 2022b;a; Casanova et al., 2022) aims to synthesize unseen voices with a speaker encoder that extracts speaker embeddings from the reference audio. This scenario is highly attractive because it does not require any adaptation data or parameters (Kang et al., 2022). The attention-based adaptation method (Choi et al., 2020; Zhou et al., 2022; Yin et al., 2022; Lin et al., 2021) utilizes attention mechanisms to extract fine-grained speech features from reference audios. Among them, Attentron (Choi et al., 2020) proposes to extracts useful style information from arbitrary number of reference audios. However, they do not separately model the timbre and prosody information, lacking controllability over timbre and prosody. Most recently, some works (Kharitonov et al., 2023; Zhang et al., 2023) are proposed to use in-context learning methods (Dong et al., 2022) to efficiently extract speaker information from acoustic prompts and have achieved remarkable results in zero-shot TTS. VALL-E (2023) proposes the neural codec language model that exhibits strong in-context learning capability for zero-shot speech generation. NaturalSpeech 2 introduces in-context learning to latent diffusion model (Rombach et al., 2022), which is achieved by partitioning a speech clip into the prompt and target regions. VoiceBox (Matthew et al., 2023) solves a text-guided speech-infilling task with large-scale data to learn from context information. However, these methods are trained with single-sentence prompts, lacking an appropriate strategy to extract fine-grained information from multi-sentence speech prompts.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#prosody-transfer-for-speech-synthesis","title":"Prosody Transfer for Speech Synthesis: \u8bed\u97f3\u5408\u6210\u7684\u97f5\u5f8b\u8fc1\u79fb","text":"<p>Prosody transfer (Lee &amp; Kim, 2019; Klimkov et al., 2019; Gururani et al., 2019; Pan &amp; He, 2021; Karlapati et al., 2022) aims to transfer the prosody from a reference utterance to the synthesized target speech, which is essential for producing natural and expressive speech in a controlled manner (Wagner &amp; Watson, 2010). Skerry-Ryan et al. (2018) first integrate a prosody reference encoder into a TTS system based on Tacotron (Wang et al., 2017), which is capable of performing similar-text prosody transfer. Recent works try to transfer prosody in different-text and different-speaker settings (Karlapati et al., 2020; Za\u0131di et al., 2021) with the bottleneck of the prosody encoder. Among them, Daft-Exprt (Za\u0131di et al., 2021) uses a gradient reversal layer to penalize the prosody encoder if its output contains information about the speaker identity from the reference utterance, which enhances the target speaker fidelity for cross-speaker prosody transfer. However, as pointed out by Sigurgeirsson &amp; King (2023), current solutions do not learn a transferable representation of prosody, but rather an utterance-level representation that is relatively dependent on both the reference speaker and reference text.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#3methodology","title":"3.Methodology: \u65b9\u6cd5","text":"<p>This section introduces Mega-TTS 2. To begin with, we provide an intuitive illustration of how Mega-TTS 2 decomposes the timbre and prosody information from speech. Next, we provide detailed explanations of our prompting mechanisms and the two-stage training process of the proposed model.</p> <p>\u672c\u8282\u4ecb\u7ecd Mega-TTS 2. \u9996\u5148\u6211\u4eec\u63d0\u4f9b Mega-TTS 2 \u89e3\u6784\u8bed\u97f3\u7684\u97f3\u8272\u548c\u97f5\u5f8b\u4fe1\u606f\u7684\u76f4\u89c2\u63d2\u56fe. \u7136\u540e\u6211\u4eec\u63d0\u4f9b\u6211\u4eec\u63d0\u793a\u673a\u5236\u7684\u8be6\u7ec6\u89e3\u91ca\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#31decomposition-for-prosody-and-timbre","title":"3.1.Decomposition for Prosody and Timbre: \u97f5\u5f8b\u548c\u97f3\u8272\u7684\u5206\u89e3","text":""},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#problem-formulation","title":"Problem Formulation: \u95ee\u9898\u5b9a\u4e49","text":"<p>Denote $H(X)$ as the Shannon entropy of $X$ and Denote $I(Y;X)$ as the mutual information.  We assume that the mel-spectrogram $y$ can be reconstructed through the following generative process: $y=D(z_c,z_{pd},z_t,g)$, where $z_c$ and $z_t$ denote the fine-grained content and timbre hidden states. $g$ denotes the global style information that contains timbre and prosody. We assume that $z_{pd}=(z_p,z_d)$ contains the fine-grained prosodic style information of pitch and energy $z_p$ and duration $z_d$. $z_d=Aligner(y)$ can be obtained by the external alignment tools (McAuliffe et al., 2017) and disentangled from $z_{pd}$. Denote $D$ as the mel-spectrogram decoder. Our goal is to construct an autoencoder-based model to disentangle speech components.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#decomposition-via-corpus-partition","title":"Decomposition via Corpus Partition: \u8bed\u6599\u5e93\u5206\u5272","text":"<p>Decomposition via Corpus Partition Denote Y = {y1, \u00b7 \u00b7 \u00b7 , yn} as the speech corpus for a certain speaker S. In training, we partition Y into the target mel-spectrogram yt and the other mel-spectrograms \u02dcy. Here, we make an important assumption that the mutual information between yt and \u02dcy only contains timbre information H(zt) and global style information H(g) of yt, i.e., First, based on the assumption, zt and g can be extracted through Et(\u02dcy), and there is no way for Et(\u02dcy) to obtain the zp and zc. Second, if we only feed phoneme sequence to Ec, Ec can only pass all the content information zc. Third, since the information zc and zt are available now, the prosody encoder Ep will prioritize removing the fine-grained content and timbre information if it is forced to lose some information by information bottleneck B(\u00b7) (Qian et al., 2019). The bottleneck forces Ep(yt) to pass only the fine-grained prosodic style zp that other encoders cannot supply, hence achieving the decomposition. We provide a detailed explanation of how we ensure the validity of Equation 1 in Appendix A.8. After the decomposition, we describe the detailed designs of our prompting mechanisms in the following subsections.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#32compressive-acoustic-autoencoder","title":"3.2.Compressive Acoustic Autoencoder: \u538b\u7f29\u6027\u58f0\u5b66\u81ea\u7f16\u7801\u5668","text":"<p>Note that to store timbre information for thousands of speakers, we need a large number of codebook entries. However, since the prosody and timbre have been decomposed, the prosodic information zp can be compressed into a highly compact codebook, and the timbre information zt can be extracted via a powerful speaker encoder. The decomposition strategy not only allows our model to accommodate extremely long prosody prompts but also enables our model to control the prosodic styles of generated speeches. As shown in Figure 1, we design the vector quantised (VQ) encoder as Ep, the multireference timbre encoder as Et, and the content encoder as Ec. Since Ep mainly captures the prosodic variance information, a GAN-based mel-spectrogram decoder D is adopted to model the highfrequency details in spectrograms, which ensures perceptually high-quality reconstructions. Overall, the first-stage training loss can be formulated as L = Lrec + LVQ + LAdv, where Lrec = \u2225yt \u2212 \u02c6yt\u22252 is the reconstruction loss, LVQ is the VQ codebook loss (Van Den Oord et al., 2017), and LAdv is the LSGAN-styled adversarial loss (Mao et al., 2017) whose objective is to minimize the distribution distance between the predicted mel-spectrograms and the ground truth mel-spectrograms. Among the proposed three encoders, the content encoder is composed of several feed-forward Transformer layers following common practice in non-autoregressive TTS systems (Ren et al., 2019). In the following paragraphs, we describe the details of the prosody and timbre encoders, respectively.</p> <p>Vector Quantised Encoder The vector quantised encoder Ep consists of two convolution stacks and a vector quantization bottleneck. The first convolution stacks compress mel-spectrograms into hidden states by a factor of r in length, and the second stacks capture the correlation of features. After that, the vector quantization layer utilizes these hidden states to obtain prosody codes u = {u1, u2, ..., un} and hidden states zp. The information bottleneck B(\u00b7) of the VQ encoder is composed of the temporal compression and the vector quantization layer. The detailed instructions for ensuring an appropriate information bottleneck B(\u00b7) can be found in Appendix F. Multi-Reference Timbre Encoder Our objective is to extract fine-grained timbre information from multi-sentence speech prompts. Since speakers can change their timbre by using different speaking techniques according to their speaking habits or desired semantic meanings (McAdams, 2013), the timbre encoder needs to extract fine-grained timbre information from multiple prompts that can represent the speakers\u2019 habits. Here, we introduce a multi-reference timbre encoder (MRTE) to achieve this objective. First, we concatenate the reference mel-spectrograms \u02dcy that belong to the target speaker but are different from the target mel-spectrogram. The mel encoder then compresses the concatenated mel-spectrogram into acoustic hidden states zt by a factor of d in length. Subsequently, to extract semantically relevant timbre information from speech prompts, we introduce a timbre-tocontent attention module. This module takes zc as the query and zt as both the key and the value. Finally, we upsample the output of the timbre-to-content attention module to match the length of the target mel-spectrogram using the length regulator (Ren et al., 2019).</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#33prosody-latent-language-model","title":"3.3.Prosody Latent Language Model: \u97f5\u5f8b\u9690\u8bed\u8a00\u6a21\u578b","text":"<p>Unlike previous models that are trained with single-sentence prompts, our prosody latent language model (P-LLM) aims to capture the speaker\u2019s prosodic patterns from multi-sentence prompts effectively. During the second-stage training process, we first extract the compressed prosody hidden states {zp1, zp2, \u00b7 \u00b7 \u00b7 , zpn} and the content hidden states {zc1, zc2, \u00b7 \u00b7 \u00b7 , zcn} from multiple speech clips {s1, s2, \u00b7 \u00b7 \u00b7 , sn} of the target speaker using the proposed compressive acoustic autoencoder. We then concatenate them along the time axis to construct z\u2032 p = Concat(zp1, zp2, \u00b7 \u00b7 \u00b7 , zpn) and z\u2032 c = Concat(zc1, zc2, \u00b7 \u00b7 \u00b7 , zcn). In order to match the lengths of z\u2032 c in the temporal dimension, we expand z\u2032 c to the frame level with duration information zd and compress it r times with a max pooling layer. After that, we transform z\u2032 c into the P-LLM, which predicts the prosody code in an auto-regressive manner: p to prosody code u\u2032 and then feed u\u2032 and z\u2032 p and z\u2032 where \u03b8 is the parameters of P-LLM and L is the length of the concatenated prosody code u\u2032. In training, we set batch size as 1 to increase the maximum number m of prosody codes in each batch as much as possible. If the total number of speech frames from a single speaker is less than m \u00d7 r, we will include speech samples from other speakers in this batch and incorporate speaker-level attention masks into P-LLM. We do not specifically define the speech prompt; instead, we train the language model directly using the concatenated speech samples through the teacher-forcing technique with the cross-entropy loss. To avoid the transition area problems caused by directly concatenating the prompts, we assign the start token and end token to each sentence, which guides P-LLM to continue writing the current sentence and extract useful information from previous sentences. This training strategy enables the model to capture the useful prosody-level information contained in the multi-sentence prompts. Therefore, in the inference stage, users can flexibly improve the generation quality by extending the length of prompts by concatenating the reference speech clips. For duration modeling, we propose a phoneme-level auto-regressive duration model. This model enhances the duration modeling by leveraging the powerful in-context learning capabilities of auto-regressive models. The overall architecture of the auto-regressive duration model remains the same as P-LLM, but we use mean squared error (MSE) loss instead.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#34prosody-interpolation","title":"3.4.Prosody Interpolation: \u97f5\u5f8b\u63d2\u503c","text":"<p>Here, we propose a prosody interpolation technique to control or replace the prosodic style of the target speaker in the discrete space while ensuring the quality of timbre reconstruction. We achieve this objective by interpolating the probabilities from multiple P-LLM outputs, which come from multiple speakers. For example, our target speaker has a relatively sad speaking tone, but we want to generate speeches that sound happier for him while preserving his timbre. The solution is to 1) extract prosody latent ua from speeches in a happy tone of other speakers and the sad prosody latent ub from the target speech prompt; 2) utilize two language models to separately decode the target prosody code \u02c6u with the prosodic prompt ua and ub. These language models share the same parameters. In every step t of the decoding process, the probability distributions of the two language models are interpolated with the weight \u03b3, which can be formulated as follows: where zcb and zca are the content information from speech clips sb and sa. \u02c6zc is the content information of the target sentence. With our prosody interpolation technique, users can freely control the prosodic style of the generated speech in the inference stage. Moreover, the proposed prosody interpolation algorithm utilizes the autoregressive probability distribution of the language model for prosody transfer. Compared with directly substituting the time-averaged prosody representation ub with ua (Karlapati et al., 2020; Za\u0131di et al., 2021), the prosody latent language model is able to mix ua and ub in a soft and fine-grained manner in the autoregressive generation process.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#4experiments","title":"4.Experiments","text":""},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#41experimental-setup","title":"4.1.Experimental Setup","text":"<p>Training Datasets. We train Mega-TTS 2 and all baselines on LibriLight (Kahn et al., 2020), which contains 60K hours of unlabelled speech derived from LibriVox audiobooks. The sample rate is 16KHz for all speech data. We transcribe the speech with the hybrid DNN-HMM ASR model pre-trained on 960 hours labeled LibriSpeech following VALL-E (Wang et al., 2023). We align the phoneme sequence with speech using the external alignment tool (McAuliffe et al., 2017). Model Configuration. We provide model configuration in Appendix A.4 and detailed hyperparameter settings in Table 5. Training and Inference. In the first training stage, we train the first-stage model on 4 NVIDIA A100 GPUs, with a batch size of 48 sentences on each GPU. In the second stage, we train the P-LLM and duration model on 8 NVIDIA A100 GPUs, with a batch size of 4,000 tokens on each GPU. It means that our model supports 4,000 \u00d7 8 frames of prompts theoretically. We use the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.999, \u03f5 = 10\u22129 and follow the same learning rate schedule in Vaswani et al. (2017). It takes 600k steps for the first stage model\u2019s training and 300K steps for the second stage model\u2019s training until convergence. The predicted mel-spectrograms are transformed into audio samples using pre-trained HiFi-GAN V1 (Kong et al., 2020). Objective Metrics. For zero-shot TTS, we evaluate the word error rate (WER), speaker similarity (SIM), and average dynamic time warping (DTW) (M\u00a8uller, 2007) distance of the pitch for the groundtruth speech and synthesized speech. In terms of the cosine speaker similarity, we use the WavLM model (Chen et al., 2022) fine-tuned for speaker verification2 to compute the cosine speaker similarity score between the ground-truth speech and the synthesized speech. The similarity score is in the range of [\u22121, 1], where a larger value indicates a higher similarity of input samples. We also evaluate the word error rate (WER) for cross-lingual TTS. We use the released HuBERT-Large model (Hsu et al., 2021) fine-tuned on the LibriSpeech 960h dataset to transcribe the generated speech into text. Then, the WER between the transcribed text and the original target text is measured. We use all samples in</p> <p>the test set for the objective evaluation. For prosody transfer, we evaluate the WER, SIM, duration error (DE), and the moments (standard deviation (\u03c3), skewness (\u03b3) and kurtosis (\u03ba)) (Andreeva et al., 2014; Niebuhr &amp; Skarnitzl, 2019) of the pitch distribution. Subjective Metrics. We conduct the MOS (mean opinion score) evaluation on the test set to measure the audio naturalness via Amazon Mechanical Turk. We keep the text content and prompt speech consistent among different models to exclude other interference factors. We randomly choose 50 samples from the test set of each dataset for the subjective evaluation, and each audio is listened to by at least 20 testers. We analyze the MOS in two aspects: QMOS (Quality, clarity, naturalness, and high-frequency details) and SMOS (Speaker similarity in terms of timbre reconstruction and prosodic pattern). We also analyze the CMOS in terms of audio quality and speaker similarity. We tell the testers to focus on one corresponding aspect and ignore the other aspect when scoring.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#42results-of-zero-shot-speech-synthesis","title":"4.2.Results of Zero-Shot Speech Synthesis","text":"<p>In this subsection, we evaluate our model with various lengths of speech prompts and compare our model with zero-shot and fine-tuning baselines to demonstrate the effectiveness of the multi-sentence prompting mechanism. We randomly choose 20 speakers from the LibriSpeech test-clean set and randomly choose 400 seconds of speeches for each of them. We split the 400 seconds of speech into a 300-second prompt set and a 100-second target set. We keep the prompts consistent among different models to exclude other interference factors. We compare the zero-shot speech synthesis performance of Mega-TTS 2 with two systems, including: 1) VALL-E (zero-shot) (Wang et al., 2023), a large-scale zero-shot TTS model using large language models to generate discrete speech codes. Since VALL-E has not been open-sourced yet, we carefully implement it for optimal performance; 2) Baseline (fine-tune), a model that incorporates the GAN used in our Mega-TTS 2 to the FastSpeech 2 backbone (Ren et al., 2020). To make the baseline support adaptive scenarios, we use the powerful speaker encoder from Meta-StyleSpeech (Min et al., 2021) to extract timbre information. We carefully fine-tune the baseline system for 2,000 steps to reach an optimal balance between WER and SIM. Note that all of the systems in this experiment are pre-trained on the LibriLight dataset. We provide further explanation for the selection of the baseline systems in Appendix A.7. Analysis As shown in Table 1, as the amount of adaptation data increases, the performance of MegaTTS 2 continues to improve. Although the performance of VALL-E improves as the data volume increases from 3 seconds to 10 seconds, the performance significantly drops in the 20-second setting due to the single-sentence prompting mechanisms in training. Moreover, since the compression rate of the Encodec model restricts the length of prompts, VALL-E fails to generate reasonable speeches with prompts longer than 20 seconds in our experiments. From another perspective, when we have 10 seconds or 60 seconds of speeches for each speaker, our Mega-TTS 2 surpasses the fine-tuning baseline in terms of speech naturalness and speaker similarity. Additionally, when we have 300</p> <p>seconds of speeches per speaker, Mega-TTS 2 still outperforms the baseline system in terms of WER and achieves comparable performance with it in terms of speaker similarity. We also visualize the WER and SIM in the fine-tuning process and compare the baseline system with Mega-TTS 2 in Figure 3. Our approach can enhance speaker similarity by utilizing more data like fine-tuning baseline, while maintaining a relatively low word error rate.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#43results-of-prosody-transfer","title":"4.3.Results of Prosody Transfer","text":"<p>In this subsection, we evaluate the prosody transfer performance of our model by transferring the emotional styles from the ESD dataset (Zhou et al., 2021) to speakers in the LibriSpeech test-clean dataset. We randomly choose 20 speakers from the LibriSpeech test-clean set and choose 50 sentences for each of them. Then, we randomly select an emotional speech clip from the ESD dataset for each of the sentences in the LibriSpeech test-clean set and use the selected emotional speech as the prosodic reference. We keep the reference speeches consistent among different models to exclude other interference factors. We compare the prosody transfer performance of Mega-TTS 2 with two systems, including:  (1) CopyCat (Karlapati et al., 2020), a model that utilizes a reference encoder architecture capable of capturing temporal prosodic representations;  (2) Daft-Exprt (Za\u0131di et al., 2021), a model disentangles identity and prosodic information through an adversarial training strategy that enables accurate prosody transfer across speakers. </p> <p>To make fair comparisons, we incorporate the techniques for prosody transfer from CopyCat and Daft-Exprt to the baseline system proposed in the previous subsection and scale up the model capacity to ensure that all models have a comparable number of parameters. All of the systems in this experiment are pre-trained on the LibriLight dataset.</p> <p>Analysis Table 2 demonstrates that compared with CopyCat and Daft-Exprt, the moments (\u03c3, \u03b3, and \u03ba) of the generated speeches of Megs-TTS are closer to the ground-truth audio and the DE is lower than other methods, demonstrating the effectiveness of the proposed prosody interpolation techniques. Besides, we observe that our method can efficiently preserve the original timbre and maintain a high audio quality. We also visualize the prosody distribution before and after the prosody transfer process and compare the baseline system with Mega-TTS 2 in Figure 4.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#44ablation-studies","title":"4.4.Ablation Studies","text":"<p>Prosody and Timbre Prompts We evaluate different lengths of prompts for the MRTE and P-LLM separately. In Table 3, the SIM score and the speech quality increase with longer timbre prompts while the DTW distance almost remains unchanged. When we increase the length of prosody prompts, the DTW distance decreases while the speaker similarity remains at the same level. It can be seen that the proposed timbre and prosody prompting mechanisms boost the subjective speaker similarity in terms of timbre and prosody modeling separately.</p> <p>VQ Encoder and MRTE We test the following four settings:  (1) w/o MRTE, which removes the MRTE from our model and does not disentangle the prosody and timbre;  (2) w/ VAE, which uses VAE to perform generative prosody modeling;  (3) w/ VAE+LDM, which uses VAE and latent diffusion model (LDM) (Rombach et al., 2022) to perform generative prosody modeling. </p> <p>The architecture and prompting mechanism of LDM is based on NaturalSpeech 2. All baselines use 10 seconds of prompts. The results are shown in Table 4. For setting 1), it can be observed that the removal of MRTE significantly affects both the audio quality and speaker similarity. This is because the timbre information is absorbed by the VQ codebook and puts great pressure on the P-LLM, which demonstrates the effectiveness of decomposing timbre and prosody information. For setting 3), substituting the VQ encoder and P-LLM with VAE and LDM results in similar performance compared to Ours-10s. However, the performance of w/ VAE+LM is still much inferior to Ours-300s, indicating the superiority of the proposed multi-sentence prompting mechanism.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#5conclusions","title":"5.Conclusions","text":"<p>In this paper, we present Mega-TTS 2, a framework that boosts the prompting mechanisms for zero shot TTS systems. With the proposed multi-sentence prompting strategy, our approach outperforms the fine-tuning baseline when 10 seconds to 5 minutes of adaptation data is available for each speaker. Furthermore, our method utilizes a prosody interpolation technique to successfully transfer various prosodic styles to the target speaker while preserving the target speaker\u2019s timbre. Experimental results demonstrate that our method exhibits superior performance in terms of audio naturalness and speaker similarity. Due to space limitations, we include additional discussions in the appendix.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#appendix","title":"Appendix","text":""},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#adetailed-experimental-settings","title":"A.Detailed Experimental Settings","text":""},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#a1details-in-objective-evaluations","title":"A.1.Details in Objective Evaluations","text":"<p>Here, we provide details of the model used in objective evaluations. Speaker Similarity Model To measure the speaker similarity, we use the WavLM (Chen et al., 2022) model fine-tuned for speaker verification from https://huggingface.co/microsoft/ wavlm-base-plus-sv to extract the speaker embedding. Then the cosine similarity between the synthesized speech\u2019s speaker embedding and the ground-truth speech\u2019s speaker embedding is calculated as the speaker similarity score. The WavLM model is pre-trained on 94,000 hours of speech data and fine-tuned on the VoxCeleb1 dataset using an X-Vector head with an Additive Margin Softmax loss, which achieves 0.84%, 0.928%, and 1.758% EER (Equal Error Rate) on the Vox1-O, Vox1-E, and Vox1-H trial lists. ASR Model To measure the audio quality and speech intelligibility, we evaluate the word error rate (WER) metric. We use the fine-tuned HuBERT-Large model to transcribe the synthesized speech into text and calculate the WER between the transcribed text and the original target text. The HuBERTLarge model from https://huggingface.co/facebook/hubert-large-ls960-ft is fine-tuned on 960h of Librispeech and achieves 1.5%, 3.0%, 1.9%, and 3.3% WER on the dev-clean, dev-other, test-clean, and test-other set of Librispeech.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#a2details-in-subjective-evaluations","title":"A.2.Details in Subjective Evaluations","text":"<p>We perform the audio quality and speaker similarity evaluations on Amazon Mechanical Turk (MTurk). For each dataset, we randomly select 50 samples from the test set and use the TTS systems to generate the audio samples. Each audio has been listened to by at least 20 listeners. For MOS, each tester is asked to evaluate the subjective score of a sentence on a 1-5 Likert scale. For CMOS, listeners are asked to compare pairs of audio generated by systems A and B following Loizou (2011), indicating which of the two audio they prefer. For audio quality evaluation (QMOS and CMOS-Q), we tell listeners to \u201cPlease focus on the speech quality in terms of clarity, naturalness, and high-frequency details, and ignore other factors\u201d. For speaker similarity evaluations (MOS-S), we tell listeners to \u201cPlease focus only on the similarity of the speaker to the reference one in terms of the timbre and prosodic patterns, and ignore the differences of content, grammar, audio quality, or other factors.\u201d. We paid $15 to participants hourly and totally spent about $1200 on participant compensation. We tell the participants that the data will be used in scientific research.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#a3details-in-network-structure","title":"A.3.Details in Network Structure","text":"<p>MRTE As shown in Figure 5, the proposed MRTE is composed of two convolution stacks and a downsampling block. To reduce the computational requirements while maintaining the quality of timbre reconstruction, we downsample the timbre hidden states by a factor of d = 16 in length. In training, we randomly sample 2,000 frames from \u02dcy for training efficiency. VQ Encoder The bottleneck of our VQ Encoder is composed of a max pooling layer with a stride of 8 and a vector quantised layer. In our experiments, we found that compressing the mel-spectrograms with a compression rate of r = 8 yields superior results compared to phoneme-level compression. We have tried different compression rates (2, 4, 8, 16, 32) and found that r = 8 reached an optimal balance between the reconstruction performance and compression. On the other hand, in the training process, we also found that the vanilla VQ-VAE suffers from codebook collapse (Takida et al., 2022), which means only a small portion of codebook vectors are optimized. It restricts the expressive capacity of the codebook and affects the convergence of the training process. To solve the codebook collapse issue, we adopt a dynamical initialization strategy based on CVQ-VAE (Zheng &amp; Vedaldi, 2023) during training, which ensures the code vectors that are less-used or unused to be modified more than frequently used ones.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#a4model-configuration","title":"A.4.MODEL CONFIGURATION","text":"<p>Our Mega-TTS 2 consists of three encoders, a prosody latent language model, a mel decoder, and a discriminator. The prosody encoder, timbre encoder, and decoder consist of 5 convolutional blocks with 512 hidden size and 5 kernel size. The content encoder is an 8-layer Transformer (Vaswani et al., 2017; Shen et al., 2023a) with 512 hidden size. The GAN discriminator follows the architecture of ML-GAN proposed in Chen et al. (2020). The P-LLM model is a decoder-only architecture that contains 12 Transformer layers with 1024 hidden size, which has 151M parameters. The duration predictor is an 8-layer decoder-only Transformer model with 512 hidden size. The codebook embedding size is 1024, and the hidden size of the codebook vector is 256. The compression rate r and d is set as 8 and 16, respectively. For prosody transfer experiments, \u03b3 is set as 0.8. We provide detailed hyper-parameter settings about the model configuration in Table 5.</p>"},{"location":"TTS/Models/Speech_LLM/2023.07.14_Mega-TTS2/#a5error-bars-and-random-seeds","title":"A.5.ERROR BARS AND RANDOM SEEDS","text":"<p>For the subjective evaluations, we report confidence intervals of the results of MOS tests. For the objective evaluations, we ran the experiments 10 times with 10 different random seeds ([1234, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]) and obtained the averaged results.</p> <p>A.6 SAMPLING STRATEGY FOR P-LLM In all of our experiments, we utilize the top-k sampling strategy for P-LLM, where k is set to 10. The sampling-based method, when used with an appropriate k, enhances the output diversity compared to greedy decoding. A.7 ABOUT THE SELECTION OF BASELINES VALL-E (Wang et al., 2023), NaturalSpeech 2 (Leng et al., 2023), and VoiceBox (Matthew et al., 2023) are the state-of-the-art zero-shot TTS models. In the experiments of zero-shot TTS, we have tried to carefully reproduce their works but failed to reproduce NaturalSpeech 2 and VoiceBox. Since all of them do not provide the pre-trained models and source code, we only compare Mega-TTS 2 with VALL-E in our experiments. A.8 DETAILED DECOMPOSITION STRATEGY The prosody encoder Ep aims to capture fine-grained and local prosodic style zp. For local prosodic style zp, we assume that psd(\u00b7) is a perfect local prosody extractor, and we can obtain the following equation: I(psd(yt), psd(\u02dcy)) = 0. The content information zc is also local and fine-grained like zp. On the other hand, the global prosodic information like the averaged volume and pitch can not be captured by Ec, intuitively. And since we have designed an information bottleneck B(\u00b7) for Ep, the global prosodic information will be prioritized by the timbre encoder Et and stored in H(zt). Now that both the local and global prosodic information is appropriately extracted, the validity of Equation 1 and our disentanglement strategy can be ensured. B ABOUT SCALING UP DATASET SIZE Scaling up dataset size is crucial for the practical application of zero-shot TTS. Therefore, we crawled 200K hours of audiobook recordings from YouTube and novelfm3. The crawled corpus contains both labelled and unlabelled speeches, and most of them do not have speaker information. To transcribe the unlabelled speech in the wild, we use a powerful ASR model called WhisperX (Bain et al., 2023). And to obtain the speaker information, we use a released automatic speaker diarization model called pyannote.audio4, which achieves DER=11.24% on the VoxConverse dataset and DER=14.09% on the AISHELL-4 dataset. In this experiment, we do not change the hyperparameter settings of our model. The results are shown in Table 6. It can be seen that increasing the dataset size can improve the speaker similarity of the generated speeches. C ABOUT THE DEFINITION OF ADAPTIVE TTS The concept of adaptive TTS encompasses many aspects like the adaption for different voices, languages, styles, and domains (Tan et al., 2021). It is also known as various terms in academia and industry, such as voice adaptation (Chen et al., 2018), voice cloning (Arik et al., 2018), custom voice (Chen et al., 2021), etc. In this paper, we primarily focus on adaptive TTS for different voices.</p> <p>D VISUALIZATION OF ATTENTION MATRICES To further verify the proposed P-LLM and multi-sentence prompting mechanism, we visualize the attention matrices averaged across all layers of P-LLM in Figure 6. In this experiment, we separately conduct short-sentence generation and long-sentence generation. For short-sentence generation, we randomly selected two sentences that are shorter than 3 seconds from speaker \u201c908\u201d in the LibriSpeech test-clean set and concatenated them together. The target texts for Figure 6 (a) and (b) are both about 15 words in length. For long-sentence generation, we randomly selected two sentences that are longer than 15 seconds from speaker \u201c908\u201d in the LibriSpeech test-clean set and concatenated them together. The target texts for Figure 6 (c) and (d) are both about 100 words in length. It can be seen that our P-LLM can capture both short-term and long-term information, demonstrating the effectiveness of the P-LLM\u2019s training strategy and the multi-sentence prompting mechanism. E LIMITATIONS AND ETHICS IMPACTS In this section, we begin by discussing the limitations of the proposed method and outlining our strategies for addressing them in future research. Subsequently, we discuss the ethical impacts that might be brought by zero-shot TTS and our measures to address these concerns. E.1 LIMITATIONS AND FUTURE WORK Firstly, our model is trained on an English dataset and does not support multilingual TTS. We plan to address this problem by introducing more multilingual training data. Secondly, the speech quality can be improved by introducing more high-fidelity training data. Thirdly, a well-designed attention window may further enhance the in-context-learning capability of our P-LLM. E.2 ETHICS IMPACTS Mega-TTS 2 improves the quality and efficiency of zero-shot speech synthesis, which makes it easier for people to synthesize personalized speeches. Under appropriate and legal usage, this technique could facilitate applications like movies, games, podcasts, and other services, making human life more convenient. However, zero-shot TTS may be misused in deepfake-related usages, such as spoofing voices. To handle this, potential solutions like building a corresponding deepfake detection model should be considered. We also plan to add watermarks to the synthesized speeches so that the public can easily tell whether the speeches are synthesized or not. Additionally, restrictions will be included in the license of our project to prevent the misuse of the model. F DESCRIPTION OF INFORMATION BOTTLENECK The settings of the information bottleneck B(\u00b7) are crucial for the performance of disentanglement of our proposed method. Intuitively, there are four crucial variables for ensuring an appropriate information bottleneck: the number of codebook embedding, the codebook channel size, the compression</p> <p>rate r of the VQ Encoder, and the downsampling rate d of the MRTE. However, the search space of these hyperparameters is too large. Since the settings of r and d also influence the reconstruction quality, the burden of P-LLM, and the computational requirements, we first consider r and d, fix them, and find the best setting for the hyperparameters of the codebook. The Compression Rate r We have conducted evaluations for different compression rates r of the VQ Encoder. In the experiments, we found that a lower compression rate would result in better reconstruction performance for the compressive acoustic autoencoder, but it would impose a heavier burden on P-LLM since the token sequence is longer. As shown in Table 9, although r = 2 achieves the highest objective similarity score, the subjective speech quality and similarity significantly decrease, which means the final quality of generation is affected. Therefore, we use r = 8 to reach an optimal balance between the reconstruction performance and compression. The Downsampling Rate d We have conducted evaluations for different downsampling rates d of the MRTE. The results are shown in Figure 10. It can be seen that when the downsampling rate d of the MRTE is low, the mel-spectrogram sequence can provide more information to the timbre encoder, resulting in better reconstruction. However, a low downsampling ratio puts a significant computational burden on the attention operation in MRTE. To reduce the computational requirements while maintaining the quality of timbre reconstruction, we choose d = 16 for our Mega-TTS 2. The Information Bottleneck with Different Amount of Data Intuitively, the performance of the information bottleneck might be very sensitive to the size of the dataset. Therefore, we conduct experiments analyzing the relationship between dataset size and the hyperparameters. The results are presented in Appendix G. Although the hyperparameters do not change across these experiments, we find that the model consistently performs well in scenarios with varying amounts of available data. G SCALING WITH DIFFERENT SIZES OF TRAINING DATA Here we evaluated the performance of our Mega-TTS 2 scale with varying amounts of available data. In this experiment, all of the systems use 3 seconds of speech prompts. The results are shown in the following table. We can see that Mega-TTS 2 performs well with different sizes of training data, while VALL-E fails to obtain satisfying results when the data is insufficient. We also scale our Mega-TTS 2 with 200K hours of speeches and the results can be found in Appendix B. H THE STRATEGY OF PROSODY MODELING In this section, we conduct experiments to verify the performance of the phoneme-level, word-level, and stride-8-level prosody modeling. Stride-8 means that the stride of the pooling layer inside the VQ encoder is set to 8. It is worth noting that ProsoSpeech (Ren et al., 2022) utilizes word-level prosody modeling. Both of us use the auto-regressive Transformer for prosody modeling. However, ProsoSPeech aims to improve the naturalness of prosody modeling. Compared with it, our P-LLM aims at improving the similarity of speaker-relevant prosodic patterns, which extracts fine-grained prosodic information from latent prosodic prompts by leveraging the powerful in-context learning capability of LLM. The experimental results are shown in the following table. It can be seen that the stride-8-level prosody modeling achieves the best performance. Intuitively speaking, the phonemelevel prosody modeling provides finer-grained information for better reconstruction while word-level prosody modeling provides more semantic information. Both of these methods would be easily</p> <p>afftected by the alignment accuracy of the speeches in training and inference stages. In order to enhance the stability and performance of the proposed model, we use stride-8-level prosody modeling. I SPECIAL CASES FOR ASSUMPTION 1 In practical scenarios, there is a special case for Assumption 1: the timbre of a speaker may vary significantly over different time periods. To address this special case, we select \u02dcy randomly from regions near yt as much as possible, ensuring that the timbre information of \u02dcy is close to that of yt. J DIFFERENT LENGTHS OF CONTEXT DURING TRAINING Here we make ablation studies for different lengths of context during our model\u2019s training process. We separately train P-LLM with different numbers of the contextual VQ code tokens and train our compressive acoustic autoencoder with different numbers of the contextual mel-spectrogram frames for MRTE. The results are shown in Figure 7. It can be seen that when we increase the length of context, the performance of the model during training significantly improves, demonstrating the effectiveness of our multi-reference training strategy. K EXPLANATIONS ABOUT MORE CASES OF ROBUST SPEECH SYNTHESIS In our Mega-TTS 2, we employ a language model only for prosody modeling, enabling our model to benefit from the advantages of in-context learning provided by the LLM model. This approach also helps to address the robustness issues (word skipping or repeating) associated with the autoregressive TTS model. Therefore, we make explanations about more cases of robust speech synthesis, to demonstrate our method\u2019s necessity. In commercial scenarios, news reporting, and other formal scenarios, robustness is a crucial factor. Just a few repeating or skipping words can have significant negative impacts. These situations are better suited for models with duration models that ensure robustness, such as FastSpeech [4], Glow-TTS [5], and our Mega-TTS 2. However, for models like tacotron [6], word omissions or repetitions can significantly affect the listening experience. On the other hand, in some scenarios, robustness is relatively less important. For example, occasional missing words or repetitions in dialogue scenes can also be natural. L RESULTS WITH NOISY REFERENCE PROMPTS To verify our model\u2019s robustness against noisy reference prompts, we conduct experiments on LibriSpeech test-other set. The experimental setup for this experiment is consistent with the one described in Section 4.2. The results are shown in Table 11. It can be seen that Mega-TTS 2 maintains excellent performance with noisy reference prompts.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/","title":"ELLA-V","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering - \u4f5c\u8005:   - [Yakun Song](../../Authors/Yakun_Song.md)   - [Zhuo Chen](../../Authors/Zhuo_Chen_(\u9648\u5353).md)   - [Xiaofei Wang](../../Authors/Xiaofei_Wang_(\u738b\u6653\u98de).md)   - [Ziyang Ma](../../Authors/Ziyang_Ma_(\u9a6c\u5b50\u9633).md)   - [Chen Xie](../../Authors/Xie_Chen_(\u9648\u8c10).md) - \u673a\u6784:   - [\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66](../../Institutions/SJTU_\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66.md)   - [Microsoft](../../Institutions/Microsoft.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.01.14 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.06 - \u53d1\u8868:   -  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2401.07333)   - [Demo](https://ereboas.github.io/ELLAV/) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md) - \u9875\u6570: 12 - \u5f15\u7528: ? - \u88ab\u5f15: 4"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V1, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups2. Audio samples are available at https://ereboas.github.io/ELLAV/. </p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Recently, deep generative AI has achieved remarkable results in various tasks, leading to the emergence of many transformative real-world applications (Brown et al., 2020; Ramesh et al., 2022; Ho et al., 2020; Rombach et al., 2022; Borsos et al., 2023; Kim et al., 2021; Chiang et al., 2019). With the advancement of generative models, there have been rapid developments in the field of speech synthesis as well. In particular, zero-shot TTS technology has gained increasing attention because it can synthesize high-quality target voices without the need of specified speaker\u2019s training data. As a state-of-the-art generative model family, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song and Ermon, 2020) progressively add noise to the training data and then learn the reverse process to generate samples. By leveraging diffusion models and their variants (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song and Ermon, 2020; Song et al., 2021; Lipman et al., 2023), many works have successfully applied them to the audio domain (Popov et al., 2021; Huang et al., 2022, 2023; Shen et al., 2023). Another major class of generative models is language modeling based on Transformer (Vaswani et al., 2017a). Devlin et al. (2019); Raffel et al. (2020); Lewis et al. (2020) utilize encoder-only or encoder-decoder architectures to build masked language models so that they selectively focus on relevant segments and effectively model relationships in long sequences. However, masked language model often requires fine-tuning to adapt to specific tasks, which can be inconvenient for practical usage and deployment. On the other hand, AR language models use a decoder-only architecture to predict the next token in a sequence as the training objective, which has demonstrated extremely powerful few-shot and zero-shot capabilities in many generative tasks (Brown et al., 2020; Thoppilan et al., 2022; Chowdhery et al., 2023). In light of this, VALL-E (Wang et al., 2023a) and subsequent works (Kharitonov et al., 2023; Rubenstein et al., 2023; Wang et al., 2023b) have successfully employed decoder-only language model for zero-shot TTS. These approaches first quantize the speech signal into a series of discrete acoustic tokens. Subsequently, they employ an AR language model to predict coarse-grained acoustic tokens, eliminating the necessity for explicit duration predictors or speaker encoders. Once trained on a large-scale corpus, such as LibriLight (Kahn et al., 2020), these approaches are capable of synthesizing speech with competitive fidelity and naturalness in a zero-shot manner.</p> <p>While VALL-E and its variants have achieved numerous impressive milestones, they still possess certain limitations that impact practical deployment. For instance, existing methods (Wang et al., 2023a; Kharitonov et al., 2023) directly concatenate phoneme tokens and acoustic tokens as a whole sequence to train language models. In this way, the alignment between audio and phoneme sequences is completely learned through the self-attention in the transformer, making it potentially unstable as self-attention does not explicitly capture the monotonic alignment between audio and phoneme. Additionally, the decoder-only language model architecture can lead to potential attention degradation issues (Fu et al., 2023), where the alignment quality between the target audio sequence and the source phoneme sequence deteriorates as the generated sequence increases, resulting in inaccurate or low-quality speech outputs. Another limitation stems from the nature of AR language modeling. Specifically, given a sequence x, the standard AR language model factorizes the likelihood p(x)over the dimensions of x via the chain rule p(x) = QTt=0p(xt|x&lt;t). AR models predict the current tokens solely based on the historical tokens without users\u2019 control in the inference process, and sometimes generate semantic repetitions or incoherence in the generated output (Yang et al., 2019; Brown et al., 2020). In the TTS task, correspondingly, VALL-E cannot directly determine which segment of the output audio corresponds to which prompt phoneme, thus there is no trivial way to promptly detect and prevent issues occurring in the generation process. These drawbacks can manifest as meaningless phoneme repetitions, transpositions, omissions, or even catastrophic infinite silence, i.e., during the process of generation, the model anomalously outputs silence or noise tokens for an extended period of time without stopping. Specifically, Tab.01 demonstrates the word error rate (WER) and the probability of the infinite silence in VALL-E samples at different threshold top-p for nuclear sampling (Holtzman et al., 2019). The detailed experimental setup is described in Section 4. Notably, a shift in the decoding strategy of VALL-E from fully sampling-based to fully greedy-based leads to a marked decline in sample quality. It should be emphasized that while sampling-based stochastic decoding strategies have advantages in terms of synthesis diversity, deterministic decoding strategies (e.g., beam search and its variants) are more suitable for cases where there is less tolerance for synthesis errors and more emphasis on fluency and coherence (Ippolito et al., 2019).</p> <p>Faced with the pros and cons of the existing methods, we introduce ELLA-V, a simple but effective language model approach for zero-shot TTS. ELLA-V proposes a generalized AR (GAR) language model to generate the first layer of residual vector quantizer (RVQ) codes of a neural codec model. Then as with VALL-E, ELLA-V employs a non-autoregressive (NAR) language model to obtain codes of the other RVQs. Our core innovation lies in 3 fold: - Firstly, ELLA-V inserts phone tokens into the corresponding positions of the acoustic sequence. Unlike existing methods, Connecting phoneme tokens with their corresponding acoustic tokens can help the language model capture the alignment between phoneme and acoustic modalities in local dependencies. - Secondly, instead of maximizing the expected log-likelihood of the hybrid sequence under a conventional casual mask or a prefix mask like VALL-E and UniLM (Bao et al., 2020), ELLA-V computes loss only on acoustic tokens and special tokensEndOfPhone( EOP ) andEndOfSentence(EOS). This training objective not only reduces the redundant computation of cross-modal alignment in the output based on experimental results, but also provides a natural way to have fine-grained control in inference: the model predicts EOP , and then the user provides the next phone token. Meanwhile, ELLA-V\u2019s GAR model always maintains awareness of the phoneme it is currently synthesizing, allowing it to promptly detect and truncate any abnormal phoneme to avoid any possible infinite silence issue. - Thirdly, we further propose an improvement to the input sequence. We introduce local advance, which involves shifting the EOP token and the next-word phoneme token a few frames ahead. Intuitively, the pronunciation of a phoneme, especially its ending, is not only influenced by the context in history but also by the upcoming phonemes. By advancing these special tokens, the GAR model can better utilize local dependencies to predict the pronunciation of the current phoneme.</p> <p>Experimental results, using comparable model configurations and 960 hours of speech data from LibriSpeech (Panayotov et al., 2015) as a training set, demonstrate the superiority of ELLA-V. Compared to the state-of-the-art zero-shot TTS system VALL-E, ELLA-V significantly improves the accuracy of synthesized speech, and demonstrates comparable or superior speaker similarity and speech naturalness on a series of subjective and objective experiments. ELLA-V achieves a WER of 2.28% on the test-clean set of LibriSpeech. Notably, ELLA-V works well on a wide spectrum of decoding strategies \u2013 even greedy decoding, and still has a substantially better speech accuracy than the best of VALL-E. We further conducted ablation experiments to investigate the effects of our proposed modifications. The results indicate that the global advance in ELLA-V significantly improves the model\u2019s performance, while the local advance enhances the stability of the generated output.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#2related-work","title":"2.Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#language-modeling","title":"Language Modeling\u00b7\u8bed\u8a00\u5efa\u6a21","text":"<p>Recently, language models have garnered increasing interest in both the academic and industrial communities. Compared to models that are confined to specific tasks, language models have been proven to possess the capability to solve a wide array of tasks, shining across various domains such as text (Brown et al., 2020; Chowdhery et al., 2023; Rae et al., 2021; Yu et al., 2022), images (Alayrac et al., 2022; Tsimpoukelli et al., 2021), and videos (Yang et al., 2022; Wang et al., 2022). In the audio domain, AudioLM (Borsos et al., 2023) trains language models on discretized audio tokens, achieving speech synthesis tasks through hierarchical prediction of these tokens. AudioGen (Kreuk et al., 2023) employs an auto-encoding approach to extract discrete encodings of raw audio, and trains a language model conditioned on textual features for controlled audio generation. LM-VC (Wang et al., 2023d) employs three language models\u2014a masked prefix language model, an external LM, and a prefix LM\u2014to achieve zero-shot voice conversion.Kakouros et al. (2023) investigates the role of word surprisal, extracted from language models, in influencing the prosody of speech synthesized by TTS systems. For zero-shot TTS, Wang et al. (2023a) approaches TTS as a conditional language modeling task rather than a continuous signal regression. By employing discrete audio codes obtained from pre-trained neural codec, it trains a discrete audio language model, achieving improved naturalness in speech and preservation of speaker characteristics. VALL-E-X (Zhang et al., 2023) extends VALL-E by utilizing source language speech and target language text as prompts when predicting the acoustic marker sequence of the target language speech. This approach supports high-quality zero-shot cross-lingual voice synthesis. These methods require only a single utterance of an unknown speaker as a prompt to generate high-quality, specified speech.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#speech-synthesis","title":"Speech Synthesis\u00b7\u8bed\u97f3\u5408\u6210","text":"<p>Speech synthesis has long been a significant topic in the fields of artificial intelligence, natural language processing, and speech processing. Early methods were based on Statistical Parametric Speech Synthesis (SPSS) (Zen et al., 2009), typically involving complex components such as text analysis models, acoustic models, and vocoders (e.g., hidden Markov model(HMM) (Yoshimura et al., 1999) based). While cost-effective in terms of data, the generated speech of SPSS still exhibited noticeable differences from natural human speech. With the advancement of modern neural networks, some work initially replaced HMMs with recurrent neural networks (RNNs) but still followed the SPSS paradigm (Fan et al., 2014; Zen and Sak, 2015; Valentini-Botinhao et al., 2016). Later, end-to-end neural TTS models were introduced, which synthesize Mel spectrograms and employ a vocoder (Oord et al., 2017; Prenger et al., 2019) for speech synthesis (Wang et al., 2017; Ar\u0131k et al., 2017; Ren et al., 2019). Some methods, utilizing techniques such as VAE (Hsu et al., 2019; Lee et al., 2022), flow (Miao et al., 2020; Kim et al., 2020), diffusion (Jeong et al., 2021; Kim et al., 2022; Popov et al., 2021), and others (Wu and Shi, 2022), have achieved promising performance in end-to-end speech synthesis.On the other hand, models like VALL-E (Wang et al., 2023a) and AudioLM (Borsos et al., 2023) utilize autoregressive Transformers to model discrete audio tokens, achieving great in-context learning performance.When it comes to zero-shot speech synthesis, autoregressive Transformer-based models can predict and generate audio without the need for an additional duration model, which strikes a favorable balance between efficiency and performance, and has been garnering increasing attention.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#3method","title":"3.Method\u00b7\u65b9\u6cd5","text":""},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#31overview","title":"3.1.Overview\u00b7\u6982\u89c8","text":"<p>Fig.01 demonstrates the overall architecture of ELLA-V. ELLA-V primarily follows a two-stage framework similar to VALL-E, considering zero-shot TTS as a conditional codec language modeling task. ELLA-V maps input text prompts and speech prompts into a unified vocabulary space with a text encoder and a neural codec, respectively. Different from VALL-E, an additional sequence order rearranging step is performed to the text-audio token sequence, after which, ELLA-V utilizes a decoder-only language model to learn to perform conditional generation on the hybrid sequences of phoneme and audio tokens. Detailed information about the language model will be presented in Section 3.2. To obtain discrete audio representations, we employ a pre-trained neural audio codec model, EnCodec (D\u00e9fossez et al., 2023), following VALL-E (Wang et al., 2023a). EnCodec transforms 24 kHz raw waveforms into 75 Hz discrete tokens usingLRVQ layers. The discrete acoustic tokens have a hierarchical structure, where the first layer quantizer contains semantic information and coarse-grained acoustic contours, while subsequent L \u2212 1quantizers learn fine-grained acoustic details. In our experiments, we use the same settings as VALL-E, withL = 8. For each quantizer, we set the codebook size to 1024. In this setting, each second of the waveform is represented by75 \u00d7 8 discrete tokens from RVQ.</p> <p>To obtain phoneme sequences, we apply the Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) to the input audio and text transcriptions. Notably, MFA not only serves as a text tokenizer but also extracts alignment relationships between phonemes and the corresponding speech. The forced alignment information is essential for ELLA-V to change sequence order.In Section 3.2, we will provide a detailed explanation of how this information is used to construct the target sequence.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#32training-codec-language-model","title":"3.2.Training: Codec Language Model\u00b7\u8bad\u7ec3\uff1a\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b","text":"<p>ELLA-V employs a Generalized Autoregressive Codec language model for the prediction of the first quantization layer in the EnCodec, which corresponds to capturing semantic information and coarse-grained acoustic profiles. Subsequently, a non-autoregressive language model is utilized to generate codes for the subsequent quantization layers, aimed at reconstructing fine-grained acoustic details. Specifically, given a speech corpus D = {xi, yi}, wherexrepresents an audio sample, andyis its text transcription. We utilize the EnCodec to extract the discrete representation ofx, formulated as  whereCrepresents the two-dimensional acoustic code matrix, andTis the downsampled utterance length. We employ MFA to obtain the phoneme sequenceP1:ncorresponding to the transcriptiony, while also extracting forced alignment information between the audio x and the transcription y:  wherenis the number of phonemes of the audio samplex, andlidenotes the length of thei-th phoneme of the discrete audio sequence. MFA treats silence also as a kind of phoneme, so that the original audio sequence is partitioned intonconsecutive intervals corresponding tonphonemes. Specifically, let\u27e8Ci\u27e9li\u00d78represent the audio sequence corresponding to the i-th phoneme:</p> <p>After quantization, we utilize the EnCodec decoder to reconstruct the audio waveform from the discrete acoustic sequenceC, formulated as \u02c6x \u2248 DeCodec(C). For the zero-shot TTS task, the optimization objective is max p(C|P, \u02c6C), where\u02c6Cis the acoustic prompt of the unseen speaker. We use language modeling to generate acoustic tokens for the unseen speaker, by learning on the mixed sequence composed of phonemes and codec codes, consistent with previous works (Wang et al., 2023a; Rubenstein et al., 2023). Unlike existing approaches, ELLA-V does not concatenate phoneme tokens and acoustic tokens directly to form the target sequence for training the language model. Instead, ELLA-V interleaves phoneme and acoustic tokens in order to make it easier for language models to learn the alignment between audio and text. Specifically, we insert each phoneme tokenPi(except the silence phoneme) into the corresponding position of the audio sequence, so that each phoneme\u2019s audio\u27e8Ci\u27e9 is sandwiched between Pi and EOP tokens. We also prepend the phoneme sequence to the beginning of the mixed sequence, which is referred to as global advance. In Section 3.4, we further propose a variant sequence order with higher generation stability, named local advance, which moves the non-acoustic tokens of the sequence several frames forward.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#321","title":"3.2.1.","text":"<p>Generalized Autoregressive Codec (GAR) Codec Language Model\u00b7\u901a\u7528\u81ea\u56de\u5f52\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b</p> <p>As shown in Fig.02, ELLA-V first constructs a hybrid sequenceH:,1of acoustic and phoneme tokens, structured as:</p> <p>It is worth noting that the MFA (Montreal Forced Aligner) treats silence as a distinct phoneme, whereas our phoneme sequencePexclusively comprises phonemes other than silence. To clarify, we retain the acoustic component associated with silence but do not sandwich it with an EOP and a specific silence phoneme, nor do we use a silence phoneme in the global advance part. We design a GAR language model to learn the continuation task on the aforementioned hybrid sequence, to generate the discrete acoustic code sequenceC:,1. The GAR model consists of multiple Transformer decoder layers (Vaswani et al., 2017b). After training, it can generate discrete audio codes for a specified text prompt and acoustic prompt. GAR is also responsible for predicting EOP and EOSto indicate the conclusion of a phoneme and the entire sentence, respectively. The optimization of GAR is achieved by maximizing the likelihood of the acoustic partC:,1of the hybrid sequenceH:,1, as well as the special EOP andEOStokens. Under forward factorization, this process is formulated as:</p> <p>where H has a size ofTH\u00d7 8,{P}denotes the phoneme set,\ufffd\u02dcCi\ufffdis the concatenation of \u27e8Ci\u27e9 along with its broadcast trailing EOP and/or EOStokens,\u02dcCis then the concatenation of\u27e8Ci\u27e9, and \u03b8 GAR represents neural network parameters of GAR model.The factorization of the training objective naturally encapsulates the core intuition of the GAR model: GAR generates the audio sequence phoneme-by-phoneme. GAR produces maximum likelihood predictions for each phoneme token successively, indicating the end of generating a specified phoneme by predicting EOP . Through global advancement, GAR can directly infer the next phoneme to be generated without relying on network predictions. After the prediction for the last phoneme is completed, GAR stops the generation process by predictingEOS. The generated sequence by GAR is self-aligned, as it can instantly know the corresponding position of any generated acoustic token in relation to the phoneme prompt. During training, we apply a bidirectional mask to the phoneme sequence before the BOS in the hybrid sequence, while a unidirectional mask is used for the part after BOS . We frame the training as a next-token-prediction language modeling task on the hybrid sequence. However, it\u2019s important to note that the model does not predict phonemes (or BOS). In other words, as shown in Fig.02, we only compute loss when the token to be predicted is not a phoneme (or BOS). During inference, whenever the model predicts an EOP for a phoneme, the next phoneme token is directly appended to the end of the sequence, which will be further discussed in Section 4.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#322non-autoregressive-nar-codec-language-model","title":"3.2.2.Non-Autoregressive (NAR) Codec Language Model\u00b7\u975e\u81ea\u56de\u5f52\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b","text":"<p>In the second stage, the NAR language model is employed to predict the codes from the second to the last quantization layers in parallel. The input-output sequence construction of the NAR model follows the same pattern as used in the GAR model discussed in Section 3.2.1. Specifically, the i-th columnH:,iof the hybrid sequence matrixHis structured as:</p> <p>And in practice if Pi represents the silence,C:,i will not be sandwiched by Pi and EOP . The NAR model takes the previously generated hybrid sequence of the previous j \u2212 1layers as input and predicts the codes of the j-th layer in parallel, formulated as:</p> <p>where {C:,j} denotes the acoustic token set of the j-th quantizer. In this formulation, The embeddings of tokens from the previous j \u2212 1quantizers are summed up to feed the NAR model to predict the j-th layer. Intuitively, both the GAR and NAR model of ELLA-V compute the loss on the acoustic tokens of the target sequence, and GAR additionally computes loss for EOP and EOS.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#33inference","title":"3.3.Inference\u00b7\u63a8\u7406","text":"<p>ELLA-V can use a short clip of speech from an unseen speaker as an acoustic prompt to synthesize speech for a specified text prompt. Fig.03 illustrates the inference process of the GAR model. While VALL-E may get stuck in an infinite loop during inference, resulting in the synthesis of either infinite silence or repetitive pronunciation (Wang et al., 2023a), ELLA-V is capable of generating EOP and promptly truncating abnormally long phonemes. Following an EOP , we can directly append the next phoneme token to the end of the generated sequence, ensuring the proper generation of speech without abnormal pauses or repetitions. For the GAR model, we employ a sampling-based decoding strategy, whereas for the NAR model, we use a greedy decoding approach to strike a balance between efficiency and performance.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#34local-advance","title":"3.4.Local Advance\u00b7\u5c40\u90e8\u8fdb\u6b65","text":"<p>One intuition is that the pronunciation of a phoneme is strongly related to the pronunciation of the phonemes just before and after it. However, due to the autoregressive nature of the GAR model, an acoustic token cannot attend to the following phoneme tokens, even though we can leverage the transformer\u2019s ability to model long-term dependencies through global advance to provide complete context for the acoustic token generation. To further harness the powerful capability of the transformer in modeling local dependencies, ELLA-V introduces an additional change in the sequence order based on Section 3.2. Specifically, we move the phoneme token and the EOP token ahead by a few frames, referred to as local advance.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#4experiment","title":"4.Experiment\u00b7\u5b9e\u9a8c","text":""},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#41experi-mental-setup","title":"4.1.Experi-mental Setup\u00b7\u5b9e\u9a8c\u8bbe\u7f6e","text":""},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#data-tasks","title":"Data &amp; Tasks\u00b7\u6570\u636e\u4e0e\u4efb\u52a1","text":"<p>We trained ELLA-V using the Librispeech (Panayotov et al., 2015) 960h training dataset. We utilized Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) to obtain forced alignment information for the audio-transcription pairs. Sentences with unrecognized or unknown phones by MFA were excluded. The open-source 24kHz checkpoint3of EnCodec(D\u00e9fossez et al., 2023) was used as the codec to generate discrete acoustic tokens. The LibriSpeech training data was upsampled to 24 kHz before feeding it into EnCodec. In evaluating the model, two zero-shot TTS tasks were considered. For the zero-shot TTS continuation task, we adhered to methodologies established by previous works (Wang et al., 2023a; Le et al., 2023; Wang et al., 2023c), selecting examples ranging from 4 seconds to 10 seconds from the LibriSpeech test-clean dataset as our test set. In this task, we used the complete phoneme transcription as the text prompt and the first 3 seconds of the test audio sample as the acoustic prompt. The model was required to generate continuations. For the zero-shot TTS cross-speaker task, we designed a hard case set comprising 100 hard sentences, as outlined in the demo page . These sentences included challenging phonetic patterns, alliteration, and unusual (abnormal) combinations of words that might pose difficulties for a TTS system to generate natural-sounding speech. In this case, we randomly picked 3-second sentences from the LibriSpeech test-clean subset as the acoustic prompt. We then concatenated the transcription of this segment and the target phoneme sequence in the hard case set to form the text prompt. The model was tasked with cloning the voice of the speaker to say the specified target text in the hard case set.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#training-configuration","title":"Training Configuration\u00b7\u8bad\u7ec3\u914d\u7f6e","text":"<p>For both GAR and NAR models, we stacked 12 Transformer decoder layers with an embedding dimension of 1024, a hidden state dimension of 1024, and a feed-forward layer dimension of 4096. All models were trained in parallel using 8 NVIDIA Tesla V100 GPUs with a batch size of 16384 tokens for GAR and 12288 tokens for NAR per GPU, respectively, learning a total of 320k steps. We used the AdamW optimizer with \u03b21= 0.9,\u03b22= 0.999,\u03f5 = 10\u22129. We employed an inverse-sqrt learning rate scheduler with warm-up. For the first32000updates, we linearly increased the learning rate from10\u22127to a peak of 5 \u00d7 10\u22124. The weight decay was 0.01.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#baseline","title":"Baseline\u00b7\u57fa\u7ebf","text":"<p>In our research, we benchmarked the performance of zero-shot speech synthesis against VALL-E (Wang et al., 2023a). This system was originally trained on a substantial 60k hours of audio from the Librilight dataset (Kahn et al., 2020). To ensure a rigorous evaluation, we reproduced the VALL-E model and adapted it to train on the LibriSpeech 960h dataset. We also adjusted the model dimensions and the number of layers to match the parameter settings of ELLA-V and VALL-E. Both GAR (or AR) and NAR models of VALL-E and ELLA-V have 154.3M parameters. Moreover, to mitigate any potential bias introduced by the audio codec, we pre-processed the authentic speech samples using EnCodec\u2019s encoder and decoder. We also include the result for Encodec reconstructed speech for reference, denoted as Ground-Truth Encodec.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#evaluation-metrics","title":"Evaluation Metrics\u00b7\u8bc4\u4f30\u6307\u6807","text":"<p>We evaluated our system with several objective metrics. Speaker similarity (SPK) and WER served as our primary measures. SPK was assessed using the fine-tuned WavLMTDNN model4(Chen et al., 2022), scoring similarity on a scale of -1 to 1, with values above 0.86 indicate the same speaker identity (This value comes from the release model card page). The WER was determined by comparing the synthesized speech to the original text using the Conformer-Transducer model5(Gulati et al., 2020). In addition to these standard metrics, we introduced two novel measures: INF% and CUT%. INF% quantified the frequency of generating infinitely long audio, indicative of a failure in synthesis. It is used to measure the likelihood of the model falling into abnormal repetition (such as infinite silence). A higher INF% indicates poorer stability in the generated output of the model. In the practical implementation, INF% referred to the proportion of sentences for which generation was not stopped when the length of the generated audio reached twice the original, serving as a proxy for infinite generation. On the other hand, as discussed in the previous session, the design of ELLA-V enables the control of the duration for each phoneme during inference, thus avoiding the synthesis failure. In our experiments, we forcibly truncate the synthesis of phonemes with a length greater than 0.4 seconds. CUT% is used to measure the frequency of forced cuts of phonemes in synthesis by ELLA-V. For each objective metric, we reported average values over three experimental runs with different random seeds. For subjective analysis, we relied on the mean opinion score (MOS). 30 test samples were chosen for this purpose, with each sample being evaluated by at least 15 listeners for aspects like naturalness and speaker similarity. The comparative mean option score (CMOS) and the similarity mean option score (SMOS) were the key subjective metrics used. SMOS was rated on a 1 to 5 scale, in 0.5point increments, to gauge speaker similarity, while CMOS, ranging from -1 to 1, assessed the overall naturalness and quality of the synthesized speech against the baseline.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#42results","title":"4.2.Results\u00b7\u7ed3\u679c","text":""},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#zero-shot-tts-continuation-tasktts","title":"Zero-Shot TTS Continuation Task\u00b7\u96f6\u6837\u672cTTS\u7eed\u5199\u4efb\u52a1","text":"<p>We present the evaluation results in Tab.02, where a comparison between ELLA-V and VALL-E is shown. First, regarding speaker similarity, both subjective (SMOS) and objective (SPK) results indicate that ELLA-V and VALL-E performed similarly, which can be attributed to their shared backbone approach, combining (G)AR and NAR. Meanwhile, CMOS testing shows that ELLA-V achieved a +0.10 score, demonstrating a higher generation quality (i.e., naturalness) compared to VALL-E. Additionally, WERs calculated between the recognized text of synthesized audio and the ground-truth text show that ELLA-V is significantly better than VALL-E (2.28 versus 5.00). This underscores ELLA-V\u2019s enhanced capability in synthesizing higher-quality and more robust speech. Overall, ELLA-V substantially improved the synthesis accuracy and robustness of the language model-based TTS framework without affecting the naturalness and speaker similarity. This conclusion is not only corroborated by this easy continuation task, but also validated via the challenging synthesis sets in the subsequent section.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#zero-shot-tts-cross-speaker-task-on-hard-casestts","title":"Zero-Shot TTS Cross-Speaker Task on hard cases\u00b7\u96f6\u6837\u672cTTS\u8de8\u8bf4\u8bdd\u4efb\u52a1(\u56f0\u96be\u6848\u4f8b)","text":"<p>VALL-E utilized a traditional AR model that frequently resulted in alignment errors, including repetitions, transpositions, and omissions, particularly in more challenging synthesis cases (see Section 4.1 for details of the challenging synthesis set). Tab.03 presents the WER comparison of VALL-E and ELLA-V on the 100 particularly hard synthesis sentences. In contrast to VALL-E, ELLA-V demonstrates markedly lower WER, signifying its enhanced robustness. This substantial reduction in errors translates to more accurate and reliable voice synthesis applications, significantly improving user experience in real-world scenarios. Regarding VALL-E\u2019s tendency to fall into infinite silence, an intuitive explanation is that the silence patterns in the training data are relatively simple and many of them are repetitive. In this case, a traditional language model is prone to overfitting to these patterns. During testing, when the model encounters silence, it assigns a high probability to silence. This leads to issues such as beam search, which is based on maximum likelihood, getting stuck in a loop. However, ELLA-V does not face this problem.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#analysis-of-decoding-strategies","title":"Analysis of Decoding Strategies\u00b7\u89e3\u7801\u7b56\u7565\u5206\u6790","text":"<p>To demonstrate the stability of ELLA-V under different decoding strategies, we conducted an ablation study, testing the decoding performance with different top-p values for nuclear sampling, by varyingp \u2208 {1, 0.99, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0(greedy)}.The results are shown in Fig.05. We can observe that as top_p decreases, the accuracy of VALL-E\u2019s synthesized speech significantly decreases. At this point, VALL-E is more prone to generating a large number of overfit silence tokens, leading to a significant increase in INF%. And compared to VALL-E, the audio synthesized by ELLA-V is less sensitive to rate changes in the top_p sampling strategy, whose WER consistently outperforms VALL-E. When the local advance is set to 5 or 10 tokens, the generated audio exhibits significant stronger robustness. On the other hand, as shown in Fig.05 (right), as top_p decreases, VALL-E tends to get stuck in infinite loops of failed generation, while the generation of ELLA-V remains significantly stable. Moreover, ELLA-V can promptly handle (truncate) the synthesis of exceptional phonemes, resulting in significantly higher robustness.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#ablation-study","title":"Ablation Study\u00b7\u6d88\u878d\u5b9e\u9a8c","text":"<p>In this paragraph, we conduct ablation experiments. (1) To investigate the impact of global phoneme information on synthesized speech, we removed the global phoneme sequence at the beginning of the trained sequence (abbr. ELLA-V-noglobal). (2) To investigate whether it is necessary to provide the specific phoneme token before its corresponding acoustic tokens during both training and inference, rather than just using the EOP separator, we removed all phoneme tokens following BOS in the mixed sequence (abbr. ELLA-V-nophn). The experimental results are shown in Tab.04. It is observed that the accuracy of synthesized speech significantly deteriorated either when global phoneme tokens were not used or when local phoneme tokens were disabled within the hybrid sequence. It is also notable that even in the absence of global advance (i.e., in the ELLA-V-no global configuration), the SPK and WER of the synthesized audio were comparable to those of VALL-E. These findings indicate the importance of both local and global information in achieving more accurate synthesized audios, meanwhile, combining both of them potentially leads to further enhancements in accuracy.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.14_ELLA-V/#5conclusion","title":"5.Conclusion\u00b7\u7ed3\u8bba","text":"<p>In this paper, we introduce ELLA-V, a simple and efficient two-stage zero-shot TTS framework based on language modeling. By learning interleaved sequences of acoustic and text tokens, our proposed GAR model can provide fine-grained control over synthesized audio at the phoneme level and can better leverage local dependencies to predict the pronunciation of the current phoneme. Experimental results demonstrate that ELLA-V achieves higher accuracy and more stable results under different threshold top-p for nuclear sampling. We aspire for this work to advance research in enhancing the robustness of speech generation.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/","title":"VALL-T","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech - \u4f5c\u8005:   - [Chenpeng Du](../../Authors/Chenpeng_Du.md)   - [Yiwei Guo](../../Authors/Yiwei_Guo.md)   - [Hankun Wang](../../Authors/Hankun_Wang.md)   - [Yifan Yang](../../Authors/Yifan_Yang.md)   - [Zhikang Niu](../../Authors/Zhikang_Niu.md)   - [Shuai Wang](../../Authors/Shuai_Wang.md)   - [Hui Zhang](../../Authors/Hui_Zhang.md)   - [Xie Chen](../../Authors/Xie_Chen_(\u9648\u8c10).md)   - [Kai Yu](../../Authors/Kai_Yu_(\u4fde\u51ef).md) - \u673a\u6784:   - [\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66](../../Institutions/SJTU_\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.01.25 ArXiv v1   - \u9884\u5370\u65f6\u95f4: 2024.01.30 ArXiv v4   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.06 - \u53d1\u8868:   -  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2401.14321)   - [DOI]()   - [Github](https://github.com/cpdu/vallt)   - [Demo](http://cpdu.github.io/vallt) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md) - \u9875\u6570: 13 - \u5f15\u7528: ? - \u88ab\u5f15: 1"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>Recent TTS models with decoder-only Transformer architecture, such as SPEAR-TTS (2023) and VALL-E (2023), achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt. However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and repeating. To address this limitation, we propose VALL-T, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer. Consequently, VALL-T retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3% in the word error rate. Furthermore, the controllability of alignment in VALL-T during decoding facilitates the use of untranscribed speech prompts, even in unknown languages. It also enables the synthesis of lengthy speech by utilizing an aligned context window. The audio samples are available at http://cpdu.github.io/vallt.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Text-to-speech (TTS) synthesis is a monotonic sequence-to-sequence task, maintaining a strict order between the input phoneme sequence and the output speech sequence. Moreover, the output speech sequence is at frame-level and one phoneme may correspond to multiple frames of speech, so the output sequence is significantly longer than its corresponding input phoneme sequence. Mainstream neural text-to-speech models, such as FastSpeech2, GradTTS (Popov et al., 2021) and VoiceFlow (Guo et al., 2024), integrate a duration prediction module. Prior to training, the target duration is conventionally derived using the Viterbi forced alignment algorithm. During training, this module is optimized by minimizing the mean square error (MSE) between predicted and target durations. In the inference phase, the duration predictor module predicts the duration for each input phoneme, establishing the alignment between the input and output sequences accordingly. The encoded input phoneme sequence is then expanded to the frame level based on the predicted duration and is subsequently passed to the speech decoder. This mechanism enforces monotonic alignment constraints on the sequence-to-sequence process, ensuring robustness in the synthesis of speech.</p> <p>Over the past two years, utilizing discrete speech tokens for speech generation is proposed in GSLM (Lakhotia et al., 2021) and VQTTS (Du et al., 2022), paving the way for integrating cutting-edge language modeling techniques into TTS systems. Inspired by exceptional strides in natural language processing driven by decoder-only large Transformer models like GPT 3 (Brown et al., 2020) and the LLaMA2 (2023), Tortoise-TTS (2023), SPEAR-TTS (2023), VALL-E (2023) and LauraGPT (Wang et al., 2023b) adopted the decoder-only architecture for TTS, achieving remarkable naturalness. SPEAR-TTS (2023) and VALL-E (2023) also have the ability to perform zero-shot speaker adaptation through Auto-Regressive (AR) continuation from a given speech prompt. Furthermore, these decoder-only TTS models, unlike traditional neural TTS model, circumvent explicit duration modeling and the requirement for phoneme durations obtained prior to training. This characteristic offers convenience and simplifies training process, especially when training on large scale datasets. However, the implicit duration modeling within these systems lacks the monotonic alignment constraints, often leading to hallucination issues like mispronunciation, word skipping and repeating.</p> <p>In fact, we do have a training scheme named Transducer (Graves, 2012) designed specifically for monotonic sequence-to-sequence task and has demonstrated success in automatic speech recognition (ASR) (He et al., 2019). It adopts a modularized architecture, composed of an encoder, a prediction network and a joint network. However, such modularized architecture of Transducer is specifically designed for ASR as a classification task, making it less suited for TTS as a generation task. Further insights into this matter will be discussed in Chapter 3.</p> <p>To achieve the best of both worlds, we propose VALL-T, a generative Transducer model that utilizes the decoder-only Transformer architecture. Specifically, alongside the conventional absolute position embedding, we incorporate additional relative position embeddings into the input phoneme sequence. Here, a relative position of 0 specifies the current phoneme under synthesis, allowing us to explicitly guide the monotonic generation process through shifting the relative positions from left to right. To the best of our knowledge, this is the first work that implements Transducer with a decoder-only Transformer architecture. VALL-T presents several advantages compared to previous TTS models: - VALL-T introduces monotonic alignment constraints without altering the decoder-only architecture, leading to a better robustness against hallucination. - VALL-T utilizes implicit duration modeling, removing the necessity for acquiring phoneme durations before training. - The alignment controllability of VALL-T during decoding enables the utilization of untranscribed speech prompts, even in unknown languages.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#2related-work","title":"2.Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#21decoder-only-zero-shot-tts-with-speech-promptstts","title":"2.1.Decoder-Only Zero-Shot TTS with Speech Prompts\u00b7\u4f7f\u7528\u8bed\u97f3\u63d0\u793a\u7684\u4ec5\u89e3\u7801\u5668\u7684\u96f6\u6837\u672cTTS","text":"<p>Zero-shot TTS refers to the ability to generate speech in the voice of an unseen speaker given only a short sample of that speaker\u2019s speech. Decoder-only TTS models, such as VALL-E (2023), are able to perform zero-shot speaker adaptation through auto-regressive continuation from the target speaker\u2019s sample. Therefore, the speech sample of the target speaker is also named speech prompt.</p> <p>Specifically, in the training process, illustrated in Fig.01(a), the phoneme and speech sequences are concatenated along the time axis and fed into a decoder-only Transformer model. It is assumed that the speaker\u2019s voice remains constant within each training utterance. In the inference phase, as shown in Fig.01(b), a speech prompt yp is required to determine the voice of the generated speech. The phoneme transcription of the speech prompt xp and the speech prompt itself yp are positioned at the beginning of the input and output sequences respectively, followed by the input phonemes to be generated and their corresponding output speech tokens. The process of auto-regressive continuation from the speech prompt is believed to preserve the speaker\u2019s voice in the generated output.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#22transducer","title":"2.2.Transducer\u00b7\u8f6c\u5f55\u5668","text":"<p>The Transducer model (Graves, 2012), also known as RNN- T, is designed for monotonic sequence-to-sequence tasks and comprises three components: an encoder, a prediction network, and a joint network. Here, the prediction network is an auto-regressive network, such as RNN and LSTM. Transducer model also introduces a special output token called blank, denoted as \\varnothing , which signifies the alignment boundary between output and input sequence. We define Yas the vocabulary of output tokens and \\bar{Y} = Y \u222a { \\varnothing }as the extended vocabulary. Also, we denote the lengths of the input sequence x and output sequence y as T and U and the size of the extended vocabulary \\bar{Y} as \u00afV .</p> <p>In the training phase, as shown in Fig.02(a), the encoder and prediction network encode the two sequences x and y respectively, yielding encoded hidden sequences f and g. Subsequently, we slice the hidden vectors ft and gu at position st and u respectively, then send them to the joint network to calculate the probability pt,u= Pr(\\bar{y}t+u|ft, gu) for the next token prediction, where \\bar{y}t+u\u2208 \\bar{Y}. We iterate over all possible sliced hidden vectors of the two sequences, from f0tofT \u22121and from g0togU, generating a matrix p of shapeT \u00d7(U +1)whose entry at(t, u)is pt,u. Each path \\bar{y} from the bottom left corner to the top right corner represents an alignment between x and y, with a length ofT +U. Fig.02(b) demonstrates an example of the alignment path where \\bar{y} = [y1, y2,  \\varnothing , y3,  \\varnothing , y4, y5,  \\varnothing , y6,  \\varnothing ]. The training criterion of Transducer model is to maximize the probability ofPr(y|x), which is the summation of the probabilities of all possible alignment paths \\bar{y}, that is where fti and gui are sliced hidden vectors at corresponding positions specified by the alignment path $\\bar{y}$. In practice, this probability can be effectively calculated with dynamic programming. In the inference phase, the prediction network auto-regressively predicts the next token, conditioning on the sliced input hidden vectors that slide from f0tofT \u22121whenever the blank token $\\varnothing$ emerges. The Transducer model has demonstrated remarkable success in ASR. However, its modularized architecture is not suitable enough for generation tasks. Recently, some literatures have explored the application of Transducer to TTS (Chen et al., 2021; Kim et al., 2023), but they still rely on the typical modularized architecture and consequently result in limited performance. Different from the previous works, we propose for the first time to implement Transducer with a decoder-only architecture that achieves better performance.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#3vall-t-decoder-only-generative-transducervall-t","title":"3.VALL-T: Decoder-Only Generative Transducer\u00b7VALL-T: \u4ec5\u89e3\u7801\u5668\u7684\u751f\u6210\u8f6c\u5f55\u5668","text":"<p>Current modularized Transducer model has demonstrated significant success in ASR. Nevertheless, its suitability for generation tasks is limited. Typically, the joint network is a small network, comprising only one or a few linear projection layers, and the prediction network is LSTM or Transformer blocks. This architecture introduces a limitation wherein the input condition x is not incorporated into the generation process until it reaches the joint network. Worse still, the joint network is too small to effectively integrate input conditions into the generation process. Moreover, the modularized Transducer model utilizes slicing to denote specific positions. Consequently, the joint network is unable to explicitly perceive the input context, further making difficulties in achieving satisfactory performance for conditional generation tasks.</p> <p>To address the above issues, we propose VALL-T that integrates the encoder, the prediction network and the joint network into one single decoder-only Transformer architecture and leverages relative position embedding to denote the corresponding positions. We discuss the training and inference details below.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#31training","title":"3.1.Training\u00b7\u8bad\u7ec3","text":"<p>We use a decoder-only architecture for VALL-T. Similar to the approach in the previous work VALL-E (2023), we concatenate the input phoneme and output speech tokens along the time axis and present them to the model as a unified sequence. Unlike traditional RNN and LSTM architectures, the Transformer lacks a specific time order for input tokens, relying instead on position embeddings to indicate their positions. The position indices for the input sequence range from0to T \u2212 1and are converted into position embeddings through a sinusoidal function (Vaswani et al., 2017). Similarly, the output sequence adopts position indices from0toU, including an additional <code>&lt;sos&gt;</code> token at the beginning. Following VALL-E (2023), we utilize a triangular attention mask for the output sequence, facilitating auto-regressive generation. This mask ensures that each speech token attends to only previously generated tokens, maintaining a proper sequential order in the output.</p> <p>Beyond the typical absolute position indices starting from 0, we introduce additional relative position indices in VALL-T for input tokens. The relative position index0specifies the current phoneme under synthesis. The phonemes to its left are assigned negative position indices starting from \u22121, while those to its right are assigned positive position indices starting from1. These relative position indices are converted to relative position embeddings with a same sinusoidal function as the absolute position indices. The resulting absolute and relative position embeddings are added to the input phoneme embeddings and subsequently presented to the decoder-only Transformer. In adopting this approach, the model gains awareness of the phoneme presently undergoing synthesis, specifically the one assigned a relative position of0, and the phonemes serving as its preceding and subsequent contexts.</p> <p>To eliminate the need for explicit duration modeling, we introduce a special output token called blank, which serves as a marker denoting the end of each phoneme\u2019s generation. Consequently, the output projection following the decoder-only Transformer projects the hidden sequence into a size of\u00afV. The projected hidden sequence, with a length of U + 1, undergoes a Softmax function to yield a sequence representing the output distribution. Illustrated in Fig.03, we iteratively assign relative position0to each of theT phonemes and subsequently stack every output sequence, each of lengthU + 1. This stacking process results in a matrix p of shapeT \u00d7 (U + 1). The optimization of VALL-T utilizes the Transducer loss, calculated using this matrix and the ground-truth speech tokens, to maximize the probability of p(y|x) following Equation (1).</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#32monotonic-auto-regressive-inference","title":"3.2.Monotonic Auto-Regressive Inference\u00b7\u5355\u8c03\u81ea\u56de\u5f52\u63a8\u7406","text":"<p>Let us first consider the auto-regressive inference process without a speech prompt. Initially, the relative position 0is designated to the first phoneme, starting the speech generation from the <code>&lt;sos&gt;</code> token. The model then auto-regressively produces speech tokens based on the input phoneme tokens and previously generated speech tokens until the blank token \\varnothing emerges. The emergence of \\varnothing denotes the completion of the first phoneme\u2019s generation and triggers a shift in relative positions. We iteratively conduct the above process until the appearance of \\varnothing for the last phoneme, indicating the conclusion of the entire generation process for the input phoneme sequence. Since the model is encouraged to generate speech tokens for the phoneme assigned relative position0by Transducer loss during training, the step-by-step shifting operation during decoding facilitates the monotonic generation process and consequently enhance the robustness against hallucination.</p> <p>Next, we consider the integration of the speech prompt for zero-shot speaker adaptation. Following the approach used in VALL-E (2023), the phoneme transcription of the speech prompt is placed at the start of the input sequence, while the speech prompt itself is positioned at the beginning of the output sequence. The two sequences are followed by the input phonemes to be generated and their corresponding output speech tokens respectively. Given that the speech prompt are provided, we assign the relative position 0 to the first phoneme right after the prompt transcription, as shown in Fig.03, and perform speech continuation. Likewise, the relative positions undergo a shift each time \\varnothing emerges, repeating until the generation for the final phoneme is completed.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#33pseudo-prompt-transcription-for-untranscribed-speech-prompt","title":"3.3.Pseudo Prompt Transcription for Untranscribed Speech Prompt\u00b7\u672a\u8f6c\u5f55\u8bed\u97f3\u63d0\u793a\u7684\u4f2a\u63d0\u793a\u8f6c\u5f55","text":"<p>In previous decoder-only TTS models, the alignment is learned implicitly with self-attentions. These models have to discern which phoneme is currently being synthesized at each time step solely based on the self-attentions between the input tokens and the preceding output tokens. Therefore, they rely on correct transcription of the speech prompt to get correct alignment and start the generation accordingly. However, in practice, it is inconvenient to obtain transcribed speech prompt, so we hope to leverage speech prompt directly and eliminate the need of its transcription.</p> <p>In VALL-T, it is evident that the alignment is controllable during inference, allowing us to manipulate the generation process by assigning position0to the phoneme we intend to synthesize without relying on a paired speech prompt and its transcription. Accordingly, we can perform zero-shot adaptation with untranscribed speech prompts. Specifically, given an untranscribed speech prompt, we use the phoneme sequence of a random utterance, referred to as pseudo prompt transcription, as its transcription and place it at the beginning of the input sequence. Then the generation can start correctly by leveraging exactly the same algorithm as described in section 3.2. The reason for using a pseudo prompt transcription rather than no prompt transcription lies in the presence of absolute position embeddings in the input sequence. We need to avoid unseen alignment pattern in the view of absolute position embeddings. Moreover, since there is no necessity for transcribing the speech prompt, the utilization of untranscribed speech prompts can be expanded to include prompts in unknown languages. This enables cross-lingual zero-shot adaptive speech synthesis.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#34aligned-context-window-for-lengthy-speech-synthesis","title":"3.4.Aligned Context Window for Lengthy Speech Synthesis\u00b7\u957f\u8bed\u97f3\u5408\u6210\u7684\u4e0a\u4e0b\u6587\u5bf9\u9f50\u7a97\u53e3","text":"<p>Decoder-only Transformer models have very limited ability to generalize to unseen position embeddings. That means if we are synthesizing lengthy speech that exceeds the maximum length encountered during training, the performance would be degraded.</p> <p>Fortunately, in VALL-T, the alignment is available during inference, allowing us to employ aligned context window that constrains both the input and output sequence length simultaneously. Specifically, at each decoding step, we retain only n phonemes that precede the current phoneme and m phonemes that follow it, creating a constrained sliding context window on input phonemes. Also, we preserve only the speech tokens corresponding to the n preceding phonemes given the alignment and discard more distant history, forming a context window on the output sequence as well. Hence, by leveraging aligned context window, VALL-T consistently maintains a limited context on both input and output sequence, allowing it to generate speech of any lengths.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#4experiments-and-results","title":"4.Experiments and Results\u00b7\u5b9e\u9a8c\u4e0e\u7ed3\u679c","text":""},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#41setup","title":"4.1.Setup\u00b7\u8bbe\u7f6e","text":"<p>In our experiments, we leverage our Encodec (2022) (Defossez et al., 2022) speech tokenizer whose frame shift is 20ms and the sampling rate of output waveforms is 16k. It comprises 8 residual vector quantization (RVQ) indices for each frame. To ensure a fair comparison between VALL-E (2023) and our proposed model VALL-T, we follow the approach introduced in VALL-E (2023) that predicts the sequence of the first RVQ index with the auto-regressive models and then predicts the remaining 7 RVQ indices conditioned on the first RVQ index with a separate non-auto-regressive (NAR) model. Both the input and output sequences are encoded with BPE (Sennrich et al., 2016) algorithm to shorten sequence lengths and diminish GPU memory consumption. VALL-T adopts an identical architecture to VALL-E (2023), containing 12 layers of Transformer blocks. Each block comprises 12 attention heads and has a hidden dimension of 1024.</p> <p>We use LibriTTS (Zen et al., 2019) dataset in our experiments, which is a multi-speaker transcribed English speech dataset. Its training set consists of approximately 580 hours of speech data from 2,306 speakers. We train our model for 40 epochs using a ScaledAdam (Yao et al., 2023) optimizer. The learning rate scheduler is Eden (Yao et al., 2023) with a base learning rate of0.05, an epoch scheduling factor of 4 and a step scheduling factor of 5000.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#42alignment-analysis","title":"4.2.Alignment Analysis\u00b7\u5bf9\u9f50\u5206\u6790","text":"<p>We first do alignment analysis to check if relative position embedding in VALL-T indicates the alignment as expected. Given the speech y and its transcription x, we iterate over all relative positions and calculate the matrix p of output distributions in the shape ofT \u00d7(U +1). Then we calculate the forward variables, backward variables and posterior probabilities accordingly. The concepts of forward variable, backward variables, and posterior probabilities were initially introduced in Hidden Markov Models (Young et al., 2002) and were also introduced in Transducer (Graves, 2012). The definitions and calculation for these values are elaborated in Appendix A.</p> <p>In Fig.04, we illustrate an example of the forward variable, backward variable, and posterior probability for VALL-T, with darker colors indicating lower values. The values are plotted on a logarithmic scale. In Fig.04(a) and 4(b), we can see a faint bright line on the diagonal of the two graphs.</p> <p>Pixel-wise summing the values from Fig.04(a) and Fig.04(b) produces Fig.04(c), which represents the posterior probability. The diagonal line becomes much clearer in this composite figure, indicating that VALL-T correctly models the alignment between the input and output sequences with relative position embeddings. Accordingly, VALL-T is capable of forced alignment, where the most probable path from the bottom-left corner to the top-right corner in the posterior probability map serves as the alignment path. The alignment path for this example is depicted in Fig.04(d). Since ground-truth labels for alignment are unavailable, our alignment analysis here only focuses on qualitative aspects.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#43evaluation-on-zero-shot-ttstts","title":"4.3.Evaluation on Zero-Shot TTS\u00b7\u96f6\u6837\u672cTTS\u7684\u8bc4\u4f30","text":"<p>In this section, we conduct an evaluation of our models on zero-shot TTS task. The task refers to synthesizing speech in the voices of unseen speakers given speech prompts and their corresponding transcriptions. Our test set uses a same test set as in (Du et al., 2024), containing 500 utterances and involving 37 speakers from the LibriTTS test set. Each speaker is assigned a specific speech prompt. Before assessing the performance of our models, we conduct speech resynthesis using our Encodec (2022) to evaluate the speech tokenizer. We also do an experiment named \u201cNAR resynthesis\u201d. In this experiment, we send the ground-truth first RVQ index to the NAR model for predicting the remaining 7 RVQ indices. Then, we convert all the 8 RVQ indices to waveform using the Encodec (2022) decoder. The purpose of the NAR resynthesis experiment is to demonstrate the performance degradation introduced by the NAR model, so we can better analyze the results of the entire pipelines, where the AR models are the primary focus of our paper.</p> <p>The baselines of this experiment include two models. One is the popular decoder-only TTS model VALL-E (2023) and another is the recently proposed TTS model with a modularized Transducer architecture called \u201cTransduce and Speak\u201d (Kim et al., 2023). The main evaluation metric in this paper is the word error rate (WER). In our evaluation process, we first synthesize speech for the test set, and then perform speech recognition using a well-known ASR model, Whisper1(Radford et al., 2023). The transcriptions obtained from the ASR model are then compared to the ground-truth input text to calculate the word error rate. Tab.01 shows that VALL-T attains significant lower WER than baselines, which is a 28.3% relative reduction when compared to VALL-E (2023) and is only 0.41 higher than NAR resynthesis, suggesting the robustness of VALL-T.</p> <p>Additionally, we present the mel-cepstral distortion (MCD) in the table, serving as a metric for quantifying the distance between the generated speech and the corresponding ground-truth recordings. VALL-T also achieves the lowest MCD across all models. Further evaluations extend to Mean Opinion Score (MOS) listening tests for naturalness and speaker similarity. 15 listeners were tasked with rating each utterance on a scale from 1 to 5, with higher scores indicating better naturalness and similarity. Note that the speaker similarity is evaluated between the generated speech and the provided speech prompt, not the corresponding ground-truth speech. This distinction arises from the variability in a speaker\u2019s timbre across different utterances, and the goal is to emulate solely the timbre of the given prompt. In the listening tests, VALL-T achieves a naturalness score comparable to VALL-E (2023), with a slightly better speaker similarity. Finally, the evaluation extends to the calculation of Speaker Embedding Cosine Similarity (SECS), measured using a pretrained speaker verification model2. This metric measures the speaker similarity by assessing the cosine similarity between the speaker embeddings of the generated speech and the provided speech prompt. While VALL-T exhibits a marginally lower SECS value than VALL-E (2023), it still surpasses other models and does not detrimentally affect human perception according to the results of subjective listening tests on similarity.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#44leveraging-untranscribed-speech-prompts","title":"4.4.Leveraging Untranscribed Speech Prompts\u00b7\u5229\u7528\u672a\u8f6c\u5f55\u8bed\u97f3\u63d0\u793a","text":"<p>The alignment controllability of VALL-T allow us to leverage untranscribed speech prompts for zero-shot TTS. In this experiment, we still use a same test set as in the previous section, excluding the transcription of the speech prompts to simulate a scenario where prompt transcriptions are unavailable. One utterance is randomly chosen from the LibriTTS test set, and its phoneme transcription serves as the pseudo prompt transcription for generating all utterances in the test set. We compare the proposed approach with three baselines. The first baseline is generating with VALL-T but do not use any prompt transcription. The remaining two baselines use VALL-E (2023), one utilizing pseudo prompt transcriptions and the other using no prompt transcription.</p> <p>The results are presented in Tab.02. We find VALL-E (2023) consistently fails to perform continuation in the absence of the correct prompt transcription, regardless of whether pseudo prompt transcriptions are provided or not. Although VALL- T exhibits improved robustness, it still fails in continuation tasks when no prompt transcription is used. This failure is caused by the unseen alignment pattern in the view of absolute position embeddings. When provided with pseudo prompt transcriptions, VALL-T successfully accomplishes the continuation from the speech prompt. The WER is significantly lower than the three baselines and even lower than both the results obtained using real prompt transcription and using NAR resynthesis in Tab.01. This improvement may be attributed to the reduced noise in the fixed pseudo prompt transcription compared to the diverse real prompt transcriptions. This result further demonstrate the robustness of VALL-T.</p> <p>Similarly, we observe a lower MCD compared with other baselines with the proposed approach. We do not conduct listening tests on the three baselines since it makes no sense to assess the naturalness and similarity for entirely incorrect generated audio samples. The naturalness of the proposed approach is almost the same as that observed when using real prompt transcriptions while its speaker similarity is slightly lower. We can also observe that in SECS evaluation.</p> <p>Next, we extend the utilization of untranscribed speech prompts to those spoken in unknown languages. Specifically, we continue to use the same test set as in the previous experiments, but leverage speech prompts from 10 German and 10 Spanish speakers randomly selected from the Multilingual Librispeech dataset (Pratap et al., 2020), simulating the speech prompt in unknown languages. Employing the same English pseudo prompt transcription as in the previous experiment for both VALL-T and the baseline VALL-E (2023), we generate continuations from the speech prompts in German and Spanish. The results are posted in Tab.03. VALL-E (2023) continues to fail in the generation due to the unknown prompt transcription. On the contrary, VALL-T still successfully performs the zero-shot TTS from the speech prompts in German and Spanish, achieving a WER of 4.22. Note that the similarity MOS and SECS in this experiment cannot be directly compared with the corresponding results in Tab.01 and 2 since the speakers of the speech prompts differ. We do not have corresponding ground-truth speech that speaks the utterances in the test set in the voice of German and Spanish speakers, so we also do not calculate the MCD in this experiment.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#45evaluation-on-lengthy-speech-generation","title":"4.5.Evaluation on Lengthy Speech Generation\u00b7\u957f\u8bed\u97f3\u751f\u6210\u7684\u8bc4\u4f30","text":"<p>We also evaluate our model on lengthy speech synthesis that exceeds the maximum length encountered during training. Due to the limitation of GPU memory, the maximum duration of training utterances is approximately 15 seconds. The test set for this experiment consists of 85 utterances, each formed by concatenating five utterances from the previous test set to simulate lengthy utterance. The generated speech in this test set exceeds 20 seconds. We use n = 50and m = 15 as the context window size.</p> <p>Examining the results in Tab.04, we observe that VALL-T exhibits superior generalization to long speech compared to VALL-E (2023), attributed to its utilization of relative position embedding, even in the absence of an aligned context window. In contrast, VALL-E (2023) often starts mumbling after generating approximately 20 seconds of speech and frequently terminates prematurely without completing the generation. Upon applying the aligned context window, the WER of VALL-T further decreases and approaches the result of generating normal utterances. Additionally, the gap in MOS scores for naturalness and speaker similarity between generated speech and ground-truth is also comparable to the result of synthesizing normal utterances.</p>"},{"location":"TTS/Models/Speech_LLM/2024.01.25_VALL-T/#5conclusion","title":"5.Conclusion\u00b7\u7ed3\u8bba","text":"<p>In this research, we present VALL-T, a decoder-only generative Transducer model designed to improve the robustness and controllability of TTS models. VALL-T incorporates monotonic alignment constraints into the decoder-only TTS framework, enabling implicit modeling of phoneme durations. Therefore, this model eliminates the need for acquiring phoneme durations before training. VALL-T supports forced alignment given input phonemes and the corresponding output speech by searching the best path on the posterior probability map. This alignment is controllable during inference, facilitating zero-shot synthesis with untranscribed speech prompts even in unknown languages. Additionally, VALL-T exhibits the capability of streaming generation, coupled with an aligned context window for synthesizing lengthy speech. These features make VALL-T a powerful model for TTS applications.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/","title":"CLaM-TTS","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: CLaM-TTS: Improving Neural Codec Language Modeling for Zero-Shot Text-to-Speech - \u4f5c\u8005:   - [Jaehyeon Kim](../../Authors/Jaehyeon_Kim.md)   - [Keon Lee](../../Authors/Keon_Lee.md)   - [Seungjun Chung](../../Authors/Seungjun_Chung.md)   - [Jaewoong Cho](../../Authors/Jaewoong_Cho.md) - \u673a\u6784:   - [KRAFTON](../../Institutions/KRAFTON.AI.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.04.03 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.06 - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2404.02781)   - [DOI]()   - [Github]()   - [Demo](https://clam-tts.github.io) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u8bed\u8a00\u6a21\u578b](../../Tags/LanguageModel.md)   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md) - \u9875\u6570: 24 - \u5f15\u7528: ? - \u88ab\u5f15: ?"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#abstract","title":"Abstract","text":"<p>With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis.  Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences.  To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams.  Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed.  In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#1-introduction","title":"1. Introduction","text":"<p>Large language models (LLMs), characterized by a considerable number of model parameters and trained on massive text data, have demonstrated remarkable zero-shot learning capabilities (Brown et al., 2020; Chung et al., 2022; Kaplan et al., 2020).  While scaling paradigm affects not only the natural language processing domain but also other fields such as image generation (Ramesh et al., 2021; Saharia et al., 2022), image recognition (Radford et al., 2021), and speech recognition (Baevski et al., 2020b; Radford et al., 2023), significant challenges in their efficient training and inference simultaneously arise.  In the realm of image processing, discretizing image representation (Razavi et al., 2019; Ramesh et al., 2021; Esser et al., 2021) has been shown to mitigate these issues by effectively reducing the input length to a manageable size. </p> <p>Language modeling in the speech domain has become feasible with the emergence of neural audio codecs (Zeghidour et al., 2021; D\u00b4 efossez et al., 2023) that enable high-fidelity audio tokenization.  For Text-to-Speech (TTS) synthesis, there have been several attempts to adopt the LLMs for zero-shot TTS, which namely synthesize the diverse speech of any human voice (Zhang et al., 2023; Wang et al., 2023; Kharitonov et al., 2023; Rubenstein et al., 2023).  These attempts move away from the previous research direction to train models on curated high-quality recording datasets and produce human-like voices on benchmark datasets (Li et al., 2019; VITS (2021); Tan et al., 2024; YourTTS (2021)).  It is demonstrated that, by training LLMs on tens of thousands of hours of diverse audio data, zero-shot adaptation can be accomplished with just a few seconds of audio input. </p> <p>Despite the significant advancements in TTS at scale, it still poses challenges to further scale up the models.  Neural audio codecs typically generate multiple sequences of audio tokens.  For instance, Encodec (D\u00b4 efossez et al., 2023) encodes a 5-second speech into 8 sequences of 375 audio tokens.  Several work (Kharitonov et al., 2023; SoundStorm (2023)) employ the semantic tokens from self-supervised speech representation learning (Chung et al., 2021) as an intermediary between text and audio tokens.  Although semantic tokens compress information more concisely than audio tokens, a 5-second speech segment still demands 125 semantic tokens, presenting a hurdle even setting aside the further complexities of audio token modeling from them. </p> <p>In this work, we aim to bring the capability of efficient training and inference of large-language models within the TTS domain.  To this end, we propose an improved Codec Language Model-based TTS (CLaM-TTS) system that encodes speech into multiple token sequences similar to existingmethods but in a more concise way.  With CLaM-TTS, all multiple tokens at each time step in these sequences are generated through a single autoregressive step of a language model, eliminating the need for iterative generative modeling along the number of sequences.  The core of our method lies in the probabilistic discrete representation learning, ensuring that all discrete latent codes participate in the training process, resulting in a high-quality autoencoder for speech.  Furthermore, we provide a principled framework enabling a latent language model to efficiently generate a stack of tokens at once; the latent language model produces a continuous latent audio representation and converts it to a discrete representation with the proposed probabilistic quantization method.  We scale up the training dataset to 100K hours.  Our experimental findings indicate that CLaM-TTS either surpasses or is on par with leading zero-shot neural codec-based TTS models in aspects such as naturalness, intelligibility, speaker similarity, and inference speed.  Furthermore, we investigate how the depth of pretraining in the language models and their methods of text tokenization influence TTS outcomes.  Our generated samples are available on our demo page.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#2-related-work","title":"2. Related Work","text":"<p>Neural audio codec The neural discrete representation learning within a variational autoencoder(VAE) framework, called the vector-quantized VAE (VQ-VAE), has been proven effective in encoding raw-waveforms into discrete tokens (Baevski et al., 2020a), employed as a speech codec (Oord et al., 2017; G\u02c6 arbacea et al., 2019).  Similar to VQ-VAE, the neural audio codec methods usually use a framework that jointly trains an encoder, a decoder, and a quantizer (Li et al.; Zeghidour et al., 2021; Jiang et al., 2022; Jayashankar et al., 2022; D\u00b4 efossez et al., 2023; Kumar et al., 2023; Wu et al., 2023).  Zeghidour et al.(2021) pioneers using residual vector quantization (RVQ) (Gray, 1984; Vasuki &amp; Vanathi, 2006; Lee et al., 2022) in a neural audio codec model.  It operates efficiently on clean and noisy speech and music, even at low bit-rates.  EnCodec (D\u00b4 efossez et al., 2023) employs a similar model structure with improved training efficiency and stability to achieve a downsampling rate of 320 for input waveforms.  Kumar et al.(2023) identify the issue of codebook under-utilization in EnCodec and improve the codebook utilization with the techniques introduced in Yu et al.(2021) resulting in state-of-the-art performance as a neural audio codec. </p> <p>Building on these advancements, we focus more on the discrete representation learning of speech rather than general audio and optimize the compression level to be suitable for the TTS task.  In other words, we compress mel-spectrograms rather than raw waveforms, delegating the task of converting mel-spectrograms back into raw waveforms to standard vocoders. </p> <p>Large-scale TTS AudioLM (2022) is a language model directly trained on audio tokens.  In AudioLM (2022), semantic tokens are first generated.  These tokens originate from self-supervised discrete speech representation methods (HuBERT (2021), W2V-BERT (2021)) that have previously been utilized for speech resynthesis or generation without text (Lakhotia et al., 2021; Polyak et al., 2021; Nguyen et al., 2023).  Following this, the model produces acoustic tokens of neural audio codes given the semantic tokens.  Wang et al. propose the first neural codec language model, VALL-E (2023), for text-to-speech that utilizes a pre-trained neural audio codec, EnCodec (D\u00b4 efossez et al., 2023).  In a different approach, following AudioLM (2022), text-to-speech has been realized by applying language modeling to generate the semantic tokens from text, as demonstrated by SPEAR-TTS (2023).  A shared characteristic among these neural codec language models is their two-stage pipeline; they autoregressively generate coarse-grained audio tokens and decode them into fine-grained representations.  Recent work in music generation hints at an efficient way to eliminate the second-stage modeling by interleaving audio tokens in a delayed pattern (Copet et al., 2023), but its application in TTS remains unexplored. </p> <p>Given the complexities in modeling long audio sequences, several studies have incorporated phonemes and durations to alleviate the need for speech synthesizers to predict speech rates (Shen et al., 2023; Le et al., 2023; Jiang et al., 2023).  Some work shows that non-autoregressive generative models, such as a diffusion model and flow-matching (Ho et al., 2020; Lipman et al., 2023), can produce diverse and natural-sounding speech with large-scale training.  A hybrid method is utilized in another approach, employing non-autoregressive architecture except prosody modeling (Jiang et al., 2023).  This method aligns with previous work that applies VQ-VAEs to capture fine-grained speech features so that the prosody is controllable by them (Sun et al., 2020; Ren et al., 2022). </p> <p>To address the challenges associated with neural codec language models while not relying on the phoneme and its duration that requires significant domain expertise, we design a language model that generates from coarse to fine-grained tokens without needing a two-stage pipeline.  Our approach is similar to recent work that utilizes pre-trained language models, Spectron (Nachmani et al., 2023) and SoundStorm (2023).  While Spectron employs pre-trained transformer decoders to directly model the mel-spectrogram and then fine-tunes it, our method preserves the pre-trained text encoder and decodes speech tokens that are shorter than the mel-spectrogram using a latent transformer decoder.  SoundStorm freezes a pre-trained text encoder similar to ours, but it generates semantic tokens and subsequently decodes acoustic tokens using an iterative generative model. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#3-preliminaries","title":"3. Preliminaries","text":"<p>Building upon the approach proposed by VALL-E (2023), we explore zero-shot TTS through the lens of neural codec language modeling task.  We consider a setting that includes two types of data: (i) text data and (ii) mel-spectrogram representation of its corresponding speech data, denoted by $\\pmb{x}$ and $\\pmb{y}$, respectively.  We model a sequence of $T$ discrete codes $\\pmb{c}{1:T}={\\pmb{c}_1,\\cdots,\\pmb{c}_T}$ from latent representations $\\pmb{z}{1:T}$ of a mel-spectrogram $\\pmb{y}$, using the framework of a Variational Autoencoder (VAE) with Residual Vector Quantization (RVQ).  Here, $\\pmb{c_t}$ represents the $D$-depth of quantized, discrete codes.  We interchangeably use $\\pmb{c}{t,1:D}$ with $\\pmb{c}_t$.  Subsequently, a neural language model $p\\theta(\\pmb{c}{1:T}|\\pmb{x})$ is employed, aiming to predict $\\pmb{c}{1:T}$ from the text transcript $\\pmb{x}$.  During the inference phase, the language model generates $\\pmb{c}_{1:T}$ for a given text $\\pmb{x}$, which is subsequently transformed into speech through the VAE decoder and a pre-trained vocoder. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#31-residual-quantized-variational-autoencoder-rq-vae","title":"3.1. Residual Quantized Variational Autoencoder (RQ-VAE)","text":"<p>An RQ-VAE (2022) is a neural network architecture representing data as discrete codes using residual vector quantization.  It comprises of three components:  (1) an encoder parameterized by $\\phi$ that maps data $\\pmb{y}$ into a sequence of latent representations $\\pmb{z}{1:T}$;  (2) a residual vector quantizer $RQ\\psi(\\cdot)$, converting the latent vector $\\pmb{z}t$ at each time $t$ into the discrete code representation $\\pmb{c}{t,1:D}=RQ_\\psi(\\pmb{z}t)$, or the corresponding quantized embedding $\\hat{\\pmb{z}}_t$; (3) a decoder parameterized by $\\omega$ reconstructs the data $\\hat{y}$ from a sequence of the quantized latent representations $\\hat{z}{1:T}$ Here $\\pmb{c}{1,1:D}$ represents the set ${c{t,1},\\cdots,c_{t,D}}$ with $D$ indicating the total depth of the quantizer. The latent representation from the encoder is quantized through the multi-stage nearest-neighbor lookup over the codebook embeddings, of which the vocab size is $V$. The process is defined as finding the optimal code from the codebook, which minimizes the residual error at each depth $d$:</p> <p>$$ c_{t,d} = \\arg\\min_{c'\\in{1,\\cdots,V}}|\\pmb{r}{t,d-1}-e{\\psi}(c';d)|^2\\ \\pmb{r}{t,d} =\\pmb{r}{t,d-1}-e_{\\psi}(c_{t,d};d) $$</p> <p>for all $d\\in [1,D]$, where $\\pmb{r}{t,0}=\\pmb{z}_t$ and $e{\\psi}(c;d)$ corresponds to the $c$-th embedding vector in the codebook at depth $d$. The sum of embeddings $\\sum_{d=1}^D e_{\\psi}(c_{t,d};d)$ becomes the quantized latent representation $\\hat{\\pmb{z}}_t$, which is converted back to the input space through the decoder.  The codebook embeddings are updated with the clustered latents by the exponential moving average updates (VQ-VAE-2 (2019)). </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#32-mean-field-variational-inference","title":"3.2. Mean-Field Variational Inference","text":"<p>Consider a latent variable model characterized by a joint distribution $p_\\psi(\\pmb{z}t,\\pmb{c}{t,1:D})$ parameterized by $\\psi$.  Here, $\\pmb{z}t$ denotes an observed random variable, $\\pmb{c}{t,1:D}$ indicates a set of latent random variable ${c_{t,1},\\cdots,c_{t,D}}$. In this model, variational inference is a method to approximate the intractable distribution $p_\\psi(\\pmb{z}t,\\pmb{c}{t,1:D})$ by solving an optimization problem with respect to parameters of approximate distribution $q(\\pmb{c}{t,1:D}|\\pmb{z}_t)$. We can derive a lower bound on the marginal log-likelihood $p\\psi(\\pmb{z}_t)$ known as the evidence lower bound (ELBO) (Blei et al., 2017): $$</p> <p>$$</p> <p>Mean-field variational inference (Koller &amp; Friedman, 2009; Blei et al., 2017) is a specific approach of variational inference that assumes the independence among the latent variables conditioned on the observed variable: $ $. We can show that each optimal variationalposterior distribution $ $, which maximizes the ELBO, satisfies:  $$</p> <p>$$</p> <p>where $ $ denotes the latent variables of all depth $ $ except $ $.  An iterative coordinate ascentalgorithm based on Eq.2 can be used to update the distribution  (Bishop &amp; Nasrabadi, 2006), andthe complexity of the algorithm mainly lies on the computation of the expectation over $ $.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#4-method","title":"4. Method","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#41-mel-vae","title":"4.1. Mel-VAE","text":"<p>We aim to develop a neural codec that can generate discrete speech codes within a short sequence length to make speech audios suitable for language model utilization.  To achieve this, we employ a RQ-VAE that compresses mel-spectrograms of speech audios (see Fig.  1a).  We introduce a variational inference based method for learning residual codewords to address the codeword collapse issue found in conventional vector quantization methods (Kaiser et al., 2018; Roy et al., 2018; Zeghidour et al., 2021; Kumar et al., 2023).  We illustrate Mel-VAE similar to RQ-VAE following most of notations from Sec.3.1.  The encoder maps a mel-spectrogram $ $ into a sequence of latent representations   , and a residual vectorquantizer , converting the latent vector   at each time   into the discrete code representation  , or its corresponding quantized embedding  .  The decoder reconstructs themel-spectrogram  from a sequence of quantized latent representations  With the assumptions that  is uniformly dis-tributed, mean-field variational inference yields the condition of such distribution as the following (see Eq.2): where the latents follows a normal distribution: However, the mutual interdependence of codes at every depth in the latter equation makes it difficult to solve it without an iterative approach.  Instead of using an iterative coordinate update approach, we approximate  pointwisely as   for all  .  The posterior then has a form:  We independently optimize the codebook embeddings at each depth , in a variational inferenceframework: The other modules of Mel-VAE, including the encoder parameters  and the decoder parameters  are trained with commitment loss, reconstruction loss, and adversarial losses. </p> <p>corresponds to coefficients of the reconstruction loss, commitment loss, and ad-versarial losses, respectively.  For adversarial training, we adopt the multi-length discriminator (Chen et al., 2020) that distinguishes mel-spectrograms at different lengths and a modified multi-resolution spectrogram discriminator (Lee et al., 2023) that accepts mel-spectrogram rather than linear spectrogram.  We combine the least squares GAN objective (Mao et al., 2017) and the  feature matchingloss (Kumar et al., 2023) into a loss function denoted by</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#42-latent-language-modeling","title":"4.2. Latent Language Modeling","text":"<p>We propose a conditional speech code language model given text $\\pmb{x}$ aimed at enhancing the efficiency of the model.  This improvement stems from the insight that speech codes are generated through vector quantization.  Our approach leverages this by predicting the vector itself, which can then be converted into multiple tokens at each layer via residual vector quantization.  This is a departure from previous methods that predict speech code tokens sequentially. </p> <p>Specifically, rather than directly predicting tokens from text, we consider a continuous latent representation $\\pmb{z}_t$ of speech that can be converted into a speech code using residual vector quantization:  $$ $$</p> <p>where $ $ indicate $ $, and we employ $ $, the probabilistic quantizer distribution learnedtogether with the Mel-VAE model (see Sec.4.1), as a replacement of $ $.  Here, wedefine the conditional distribution $ $ as a Gaussian mixture model: </p> <p>In this model, we can derive the following variational lower bound on the log-likelihood:  )) + log  for any  . </p> <p>The derivation of the lower bound and the definition of   are providedin Appendix A.  Here, we set With the second loss , which is associated with training a binary classifier to identify theend of speech ( ), the total loss for training the latent language model is the sum of the twolosses above: As shown in Fig.  1, we implement an autoregressive latent model that yields three distinctive outputs:the mixture weights and the means of the Gaussian mixture distribution as well as the probability of concluding the generation.  Specifically, it incorporates a transformer decoder followed three parallel modules, comprising (1) a prediction layer with softmax activation for ; (2) a predictionlayer for ; (3) a binary classifier layer for   prediction.  Additionally, we use thepre-trained quantizer  of Mel-VAE.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#43-model-architecture-and-inference","title":"4.3. Model Architecture and Inference","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#model-architecture","title":"Model Architecture","text":"<p>For the Mel-VAE, we adopt a causal 1d convolutional U-Net, a variant of themodel used in Ho et al.  (2020).  We remove the skip connections and attention layers and append 1-d ConvNeXt (Liu et al., 2022) blocks used in Siuzdak (2023) to the final layer of the decoder.  We employ 32-stage residual vector quantization with a codebook size of 1,024 for each depth.  For the text-to-code latent language model, we adopt a transformer-based encoder-decoder LM, especially a pre-trained ByT5-large (Xue et al., 2021a) similar to SoundStorm (2023).  We keep the text encoder frozen.  Please refer to Appendix C for more detailed model configuration. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#inference","title":"Inference","text":"<p>The text-to-code generation unfolds in three steps:  1. at time step $t$, we randomly select $k$ from the distribution $p_\\theta(k|\\pmb{x},\\pmb{c}{&lt;t})$; 2. following this, randomly sample the latent vector $\\pmb{z}_t$.  Consequently, at time step $t$, the discrete code is obtained through the learned quantizer, $\\pmb{c}_t=RQ\\psi(\\pmb{z}_t)$;  3. if the probability of EOS exceeeds 0.5, conclude the generation, or proceed to step, otherwise.  Subsequently, the generated codes are decoded to mel-spectrograms using the decoder of Mel-VAE, then converted to raw-waveforms through an off-the-shelf pre-trained vocoder, BigVGAN (Lee et al., 2023). </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#5-experimental-setup","title":"5. Experimental Setup","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#training-dataset","title":"Training Dataset","text":"<p>We employ 100K hours of over 12K distinct speakers\u2019 speech-transcript dataset spanning 11 languages: English, Korean, Chinese, Japanese, German, Dutch, French, Spanish, Italian, Portuguese, and Polish.  We train two models: (1) CLaM-en: an English-only model on 55K hour English dataset and (2) CLaM-multi: a multilingual model trained on 11-language dataset.  We provide details of dataset for each language in Appendix B.1, and data pre-processing in Appendix B.2 and B.3. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#training","title":"Training","text":"<ol> <li>Mel-VAE: We train the model on 4 NVIDIA A100 40GB GPUs for around 2Msteps.  Each GPU processes a size one minibatch containing concatenated mel-spectrograms of several audios.  We trim the trailing end to have it precisely 32,768 frames long.  We use Adam optimizer (Kingma &amp; Ba, 2015) with a constant learning rate of 0.0002 throughout the training. </li> <li>Text-to-code: We train only the decoder and use a learned codebook from Mel-VAE.  The model is trained on 4 NVIDIA A100 40GB GPUs for around 4M steps with dynamic batching while keeping a maximum code size of 2,560.  We use AdamW optimizer (Loshchilov &amp; Hutter, 2019), and the learning rate is fixed to 0.0002 throughout the training.  Throughout all our experiments, during the model inference, we sample $k$ using top $p$ sampling (Holtzman et al., 2020) with 0.5 and $z$ is sampled with temperature (Kingma &amp; Dhariwal, 2018) of 2.6, which matches the empirical standard deviation in our validation dataset. </li> </ol>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#baselines","title":"Baselines","text":"<p>We compare the proposed model with the following baselines:  (1) YourTTS (2021), a zero-shot TTS built on VITS (2021) which is flow-based end-to-end TTS (representing Conventional TTS),  (2) Vall-E (2023) and (3) SPEAR-TTS (2023) (representingNeural Codec LM),  (4) VoiceBox (2023), a flow-matching-based TTS model trained on large-scale training data (representing Non-Autoregressive Model with Phoneme Input and Duration).</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#metrics","title":"Metrics","text":"<ol> <li>Intelligibility and Robustness:  We measure these attributes by character error rate (CER) and word error rate (WER) of the synthesized transcription from generated speech concerning the input text.  We follow the procedure in VALL-E (2023).  In English-only Evaluation, we synthesize the transcription by using the automatic speech recognition (ASR) model, the CTC-based HuBERT (2021)-Large model pre-trained on Libri-Light (2020) and then fine-tuned on LibriSpeech (2015).  In the Multilingual Evaluation, we use OpenAI\u2019s Whisper (2023) model.  We adopt NVIDIA\u2019s NeMo-text-processing (Zhang et al., 2021; Bakhturina et al., 2022) for text normalization; </li> <li>Speak Similarity:  We assess the speaker similarity of two separate speech audio clips by following the same procedure outlined in VALL-E (2023).  We employ WavLM-TDCNN (Chen et al., 2022) which outputs the embedding vector representing the speaker\u2019s voice attribute.  We measure the cosine similarity between the two embedding vectors to get a score in $[-1,1]$, where a higher score indicates a higher speaker similarity of the audios.  We borrow the definition of SIM-o and SIM-r from Le et al. (2023).  SIM-o measures the similarity between the generated and the original target speeches, while SIM-r measures the similarity concerning the target speech reconstructed from the original speech by MelVAE and the pre-trained vocoder; </li> <li>Subjective Speech Quality:  We measure the quality of the generated speech from human evaluations via three types of Mean Opinion Score (MOS) (Ribeiro et al., 2011): </li> <li>Quality MOS (QMOS) for an overall audio assessment, </li> <li>Similarity MOS (SMOS) to measure speaker similarity between the prompt and the generated speech,</li> <li>Comparative MOS (CMOS) to compare our model with available baselines. </li> </ol> <p>Detailed settings of subjective tests are described in Sec.B.5. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#tasks","title":"Tasks","text":"<p>We measure the performances of the proposed model under two different tasks: 1. continuations: Given a text and corresponding initial 3 seconds of the Ground Truth speech as a prompt, thetask is to seamlessly synthesize the subsequent portion of the speech,  2. cross-sentence: The model is given a text, a 3-second speech segment, and its corresponding transcript (the transcript is different from the text).  The task is to synthesize a speech reading the text in the style of the provided 3-second speech. </p> <p>We include our samples across the tasks discussed above, covering speaker diversity, text prompting, and other aspects, on our demo page. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#6-experimental-results","title":"6. Experimental Results","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#61-english-only-evaluations","title":"6.1. English-Only Evaluations","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#evaluation-methodology","title":"Evaluation Methodology","text":"<p>We evaluate performances of CLaM-en across continuation and cross-sentence tasks.  Following the evaluation setting in VALL-E (2023), we employ a subset of the LibriSpeech test-clean dataset.  This subset comprises speech clips ranging from 4 to 10 seconds, each with a corresponding transcript.  Note that YourTTS (2021) has official checkpoints, VALL-E (2023) has an unofficial checkpoint, and others do not have checkpoints.  We use checkpoints of YourTTS (2021) and VALL-E (2023) for evaluations.  We compare the other baselines with ours via the performances reported in their papers (VALL-E (2023); SPEAR-TTS (2023); [VoiceBox (2023)](2023.06.23_Voice../../Models/Speech_LLM/2023.06.23_VoiceBox.md Since SPEAR-TTS (2023) and [VoiceBox (2023)](2023.06.../../Models/Speech_LLM/2023.06.23_VoiceBox.mdvaluate using the same approach with VALL-E (2023), they can be directly compared with our model as well.  Details of evaluation are provided in Appendix B.4. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#analysis","title":"Analysis","text":"<p>Tab. 1 and 2 show the results of continuation and cross-sentence task, respectively.  Ours offers great performances for all measures, ranking either first or second, except SIM-r in crosssentence task.</p> <p>It is worth noting that VoiceBox (2023), a phoneme-based duration model, shows better performances than ours.  However, it requires both phoneme and duration for speech synthesis, whereas our model directly employs a pretrained language model.  This allows for the seamless integration of LMs, which are trained across a broad spectrum of texts and tasks, enabling a plug-and-play methodology.  Experimental results of training several T5 variants is shown in Appendix D.2, illustrating the trade-off between leveraging the inherent capacity of LMs and ensuring robustness.  We also compare the end-to-end inference time for a 10-second utterance.  Our method is faster than the generation speed of Vall-E reported in VoiceBox (2023). While ours is faster than VoiceBox (2023) with 64 decoding steps, VoiceBox (2023) can use fewer iterations of decoding steps.  Tab.  3 presents the subjective audio evaluations.  CLaM-en significantly outperforms the baseline, YourTTS (2021), in quality and intelligibility, as indicated by QMOS.  Our adherence to the prompt audio surpasses that of the baseline, as measured by the SMOS.  The comparative scores (CMOS) highlight CLaM-en\u2019s proximity to the Ground Truth regarding naturalness, clarity, and comprehensibility.  Overall, CLaM-en\u2019s generated speech naturalness, quality, intelligibility, and similarity exceed the baseline. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#62-multilingual-evaluations","title":"6.2. Multilingual Evaluations","text":"<p>We evaluate our model, CLaM-multi trained on the multilingual dataset.  On the test set, we measure WER, CER, and SIM-o which are defined in Sec.5.  Here, we only consider continuation task inthis experiment since we cannot get full alignmnets between audio and text for all languages and datasets.  Tab.4 shows the partial results of the multilingual continuation task.  We sample a hundred random samples from the test set of each dataset, ranging from 4 to 20 seconds, and average the scores of three trials. Refer to Tab. 10 for evaluation on other languages and datasets.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#63-ablation-study","title":"6.3. Ablation Study","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#effectiveness-of-proposed-rvq","title":"Effectiveness of Proposed RVQ","text":"<p>To demonstrate the effect of the proposed RVQ on Mel-VAE,we conduct an ablation study by assessing speech reconstruction capability.  We train two MelVAEs: one with the proposed RVQ and the other with the baseline RVQ (D\u00b4 efossez et al., 2023).  We train both for 500k steps on the same training dataset described in Sec.5.  The generated speech is compared to the Ground Truth speech using two metrics: Perceptual Evaluation of Speech Quality (PESQ) (Rix et al., 2001) and Virtual Speech Quality Objective Listener (ViSQOL) (Chinen et al., 2020) in speech mode.  For evaluation, we randomly select 1,800 samples from the test set of each dataset, proportional to the size of each dataset, each being at least 3 seconds long.  The scores of these samples are then averaged for comparison.  Tab.5a shows that ours is more effective than the baseline RVQ.  See Fig.2 to verify the superior codebook usage of our approach.  We also compare the fully trained Mel-VAE with Encodec at 6K bitrates (D\u00b4 efossez et al., 2023), which is widely employed in neural codec language models (Wang et al., 2023; Zhang et al., 2023).  Tab.5b confirms that ours outperforms Encodec in speech reconstruction performance across both measures. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#comparision-of-pre-trained-lm-and-input-variants","title":"Comparision of Pre-trained LM and Input Variants","text":"<p>Our language model is based on T5 (Raffelet al., 2020).  We conduct an ablation studty to compare T5, its variants and a phoneme encoder of comparable size.  The results indicate that ByT5 surpasses other T5 variants with the sole exception of the phoneme model.  This suggests that: 1) the more the pretraining phase is leveraged, the greater the potential increase in TTS performance, and 2) in moderate-sized language modeling, phonemes remain an effective input representation.  For the experimental results and a comprehensive analysis, refer to Appendix D.2.  In addition to the ablation studies presented, we have conducted further experiments detailed in the appendix, which explore the effects of codeword emitting rate on speech codec quality and language modeling as well as the scale of training data on model efficacy.  For comprehensive results and discussions, refer to Appendix D.3 and D.4. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#7-discussion","title":"7. Discussion","text":"<p>Choice of Codeword Rate  Our approach enjoys a 10Hz codeword rate for efficient modeling.We set the codeword rate following the average phoneme rate in English speech (Roach, 2009) since phoneme is the minimum spoken unit.  Nevertheless, we conjecture this may have to be adjusted depending on the language or speaker.  A more compressed codeword rate, for example, 5Hz, might lead to more significant information loss than their efficiency.  There exists an efficiencyperformance tradeoff for rates above 10Hz, which can be optimized as needed. </p> <p>Robustness  We have noticed some words can be muddled, omitted, or repeated, which predom-inantly stems from autoregressive modeling.  We will address it by employing non-autoregressive architecture or improving the attention mechanism in future work. </p> <p>Expressiveness  100K hours of training data may not ensure a complete representation of all voicetypes, especially accentuated ones.  Our datasets predominantly capture audiobook reading styles, leading to limited diversity in speaking styles.  We believe that increasing the model and data size can significantly tackle the expressiveness challenges in zero-shot TTS. </p> <p>Instruction Prompting  We suggest various ways to use the full knowledge of the language model.One can incorporate speaker metadata into each transcript to perform various intriguing tasks.  Such tasks might include synthesizing speech or even conversations characterized by specific genders, voice ages, or accents.  We leave the other tasks for future work. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.03_CLaM-TTS/#8-conclusion","title":"8. Conclusion","text":"<p>We introduce CLaM-TTS, which leverages mean-field variational inference based probabilistic residual vector quantization (1) achieving significant compression in token length, and (2) allowing a latent language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams.  We scale up the training dataset to 100K hours.  We empirically show that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/","title":"RALL-E","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis - \u4f5c\u8005:   - [Detai Xin](../../Authors/Detai_Xin_(\u8f9b\u5fb7\u6cf0).md)   - [Xu Tan](../../Authors/Xu_Tan_(\u8c2d\u65ed).md)   - [Kai Shen](../../Authors/Kai_Shen_(\u6c88\u9534).md)   - [Zeqian Ju](../../Authors/Zeqian_Ju_(\u741a\u6cfd\u8c26).md)   - [Dongchao Yang](../../Authors/Dongchao_Yang_(\u6768\u4e1c\u8d85).md)   - [Yuancheng Wang](../../Authors/Yuancheng_Wang_(\u738b\u8fdc\u7a0b).md)   - [Shinnosuke Takamichi](../../Authors/Shinnosuke_Takamichi_(\u9ad8\u9053\u614e\u4e4b\u4ecb).md)   - [Hiroshi Saruwatari](../../Authors/Hiroshi_Saruwatari_(\u733f\u6e21\u6d0b).md)   - [Shujie Liu](../../Authors/Shujie_Liu_(\u5218\u6811\u6770).md)   - [Jinyu Li](../../Authors/Jinyu_Li_(\u674e\u52b2\u5b87).md)   - [Sheng Zhao](../../Authors/Sheng_Zhao_(\u8d75\u80dc).md) - \u673a\u6784:   - [Microsoft](../../Institutions/Microsoft.md)   - [\u4e1c\u4eac\u5927\u5b66](../../Institutions/UTokyo_\u65e5\u672c\u4e1c\u4eac\u5927\u5b66.md)   - [\u6d59\u6c5f\u5927\u5b66](../../Institutions/ZJU_\u6d59\u6c5f\u5927\u5b66.md)   - [\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66](../../Institutions/USTC_\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66.md)   - [\u9999\u6e2f\u4e2d\u6587\u5927\u5b66](../../Institutions/CUHK_\u9999\u6e2f\u4e2d\u6587\u5927\u5b66.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.04.04 ArXiv v1   - \u9884\u5370\u65f6\u95f4: 2024.04.06 ArXiv v2   - \u66f4\u65b0\u7b14\u8bb0: 20??.??.?? - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2404.03204)   - [DOI]()   - [Github]()   - [Demo](https://ralle-demo.github.io/RALL-E) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u8bed\u8a00\u6a21\u578b](../../Tags/LanguageModel.md)   - [\u601d\u7ef4\u94fe](../../Tags/Trick_CoT.md) - \u9875\u6570: 24 - \u5f15\u7528: ? - \u88ab\u5f15: 0"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#abstract","title":"Abstract","text":"<p>We present ==RALL-E==, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind ==RALL-E== is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, ==RALL-E== first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, ==RALL-E== utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E (2023), ==RALL-E== significantly improves the WER of zero-shot TTS from 6.3% (without reranking) and 2.1% (with reranking) to 2.8% and 1.0%, respectively. Furthermore, we demonstrate that ==RALL-E== correctly synthesizes sentences that are hard for VALL-E (2023) and reduces the error rate from 68% to 4%.</p> <p>\u6211\u4eec\u63d0\u51fa\u4e86 ==RALL-E==, \u4e00\u79cd\u9c81\u68d2\u7684\u7528\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u7684\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5. \u4e4b\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b (Large Language Models, LLMs) \u7684\u5de5\u4f5c\u5728\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272, \u4f46\u540c\u65f6\u4e5f\u5b58\u5728\u4e00\u4e9b\u4e0d\u7a33\u5b9a\u6027, \u5982\u4e0d\u7a33\u5b9a\u97f5\u5f8b (\u5947\u602a\u7684\u97f3\u9ad8\u548c\u8282\u594f/\u65f6\u957f) \u548c\u9ad8\u8bcd\u9519\u8bef\u7387 (Word Error Error, WER), \u8fd9\u4e9b\u90fd\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u56de\u5f52\u9884\u6d4b\u65b9\u5f0f\u6709\u5173. ==RALL-E== \u7684\u6838\u5fc3\u601d\u60f3\u662f\u601d\u7ef4\u94fe (Chain-of-Thought, CoT) \u63d0\u793a, \u5b83\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u7684\u6b65\u9aa4, \u4ee5\u63d0\u5347\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u7684\u9c81\u68d2\u6027. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u601d\u60f3, ==RALL-E== \u9996\u5148\u9884\u6d4b\u8f93\u5165\u6587\u672c\u7684\u97f5\u5f8b\u7279\u5f81 (\u97f3\u9ad8\u548c\u65f6\u957f), \u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a\u4e2d\u95f4\u6761\u4ef6\u7528\u4e8e\u9884\u6d4b\u601d\u7ef4\u94fe\u5f62\u5f0f\u7684\u8bed\u97f3 Tokens. \u5176\u6b21, ==RALL-E== \u5229\u7528\u9884\u6d4b\u7684\u65f6\u957f\u63d0\u793a\u6765\u5f15\u5bfc Transformer \u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u6743\u91cd\u8ba1\u7b97, \u4ee5\u5f3a\u5236\u6a21\u578b\u5728\u9884\u6d4b\u8bed\u97f3 Token \u65f6\u4e13\u6ce8\u4e8e\u76f8\u5e94\u7684\u97f3\u7d20\u548c\u97f5\u5f8b\u7279\u5f81. \u7efc\u5408\u7684\u5ba2\u89c2\u8bc4\u4f30\u548c\u4e3b\u89c2\u8bc4\u4f30\u8bf4\u660e, \u76f8\u6bd4 VALL-E (2023), ==RALL-E== \u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u7684\u8bcd\u9519\u8bef\u7387\u8868\u73b0, \u4ece 6.3% (\u4e0d\u4f7f\u7528 reranking) \u548c 2.1% (\u4f7f\u7528 reranking) \u5206\u522b\u4e0b\u964d\u5230 2.8% \u548c 1.0%. \u6b64\u5916, \u6211\u4eec\u8fd8\u5c55\u793a\u4e86 ==RALL-E== \u80fd\u591f\u6b63\u786e\u5408\u6210 VALL-E (2023) \u96be\u4ee5\u5408\u6210\u7684\u53e5\u5b50, \u5e76\u5c06\u9519\u8bef\u7387\u4ece 68% \u4e0b\u964d\u5230 4%.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#1introduction","title":"1.Introduction","text":"<p>Large language models (LLMs) have demonstrated great progress in natural language generation [1, 20]. With a sufficient model size LLMs emerge powerful in-context learning abilities that can handle unseen tasks with a text instruction (usually called prompt) in a zero-shot or few-shot manner [31]. Moreover, the simple yet effective next-token prediction task of LLMs makes it easy to apply LLMs on other fields, such as vision [5] and speech synthesis [29], as long as the data can be converted to discrete speech tokens. This work focuses on the language modeling of text-to-speech (TTS) synthesis. Recent work [14,29] have shown that TTS can be modeled by a decoder-only language model by using a neural codec [4,34] to convert continuous waveforms into discrete tokens. These methods, typically leverage tens of thousands of hours of speech data, emerge in-context learning ability that can clone a speaker\u2019s voice by providing a short audio prompt to the language model, and thus have impressive performance on zero-shot TTS. However, due to the sequential generation property of language models, such codec language models suffer from poor robustness. Although the AutoRegressive (AR) prediction style of language models enables the model to generate speech with diverse prosody patterns, they can also cause bad cases with unnatural prosody. Moreover, since there is no strict alignment between text and speech, the models can omit or repeat words in the input text. This is quite different from TTS methods based on Non-AutoRegressive (NAR) generative models [12,16,25], which predicts all tokens at the same time, thus have high robustness but relatively low diversity. As suggested by previous work [12,33], LLM-based TTS have a higher Word Error Rate (WER) than NAR TTS even if they have similar performance on other metrics. To alleviate this problem, a simple but effective method is to sample the same input text multiple times and select the best one [14,33]. However, such a reranking method further increases the inference time. In this paper, we present ==RALL-E== (the abbreviation of Robust VALL-E), a method to improve the robustness of LLM-based TTS. The core idea of ==RALL-E== is inspired from the Chain-of-Thought (CoT) prompting [32]. In CoT prompting, the LLM is instructed to generate an intermediate result that is used as a condition for the prediction of the final result. The CoT prompting breaks a complex task into several simpler steps, so that can improve the robustness of LLMs, especially on hard tasks like arithmetic [32]. To adapt CoT prompting to LLM-based TTS, ==RALL-E== predicts prosody tokens (pitch and duration) before predicting speech tokens to stabilize the prosody. Given an input sentence, ==RALL-E== first predicts phoneme-level pitch and duration of the input, then predicts speech tokens conditioning on both the input phonemes and the predicted prosody tokens. Furthermore, ==RALL-E== utilizes the predicted duration to mask irrelevant phonemes and prosody tokens when computing self-attention weights, so that the codec language model is enforced to concentrate on tokens around the phoneme and prosody token the speech token corresponds to. We use VALL-E (2023) [29], a recent powerful LLM-based TTS method, as the baseline, and conduct experiments on a large dataset with 44K hours speech data. Results of comprehensive objective and subjective evaluations demonstrate that RALL significantly improves the robustness of LLM-based TTS by reducing the WER on the LibriSpeech [18] test-clean set from6.3%(w/o reranking) and2.1%(with reranking) to2.8%and 1.0%, respectively. Furthermore, we evaluate the performance of ==RALL-E== on 50 particularly hard sentences. As demonstrated in Tab.01, compared to VALL-E (2023), ==RALL-E== significantly reduces WER from68%to4%by eliminating almost all types of error, which demonstrates the superior robustness of ==RALL-E== (see Section 4.4 for more details). The contributions of this work are summarized as follows:  - We present ==RALL-E==, a robust codec language modeling method with chain-of-thought prompting for TTS. ==RALL-E== improves the robustness of LLM-based TTS by (1) incorporating prosody tokens as chain-of-thought prompting to stabilize the generation of speech tokens and (2) using duration-guided masking to enhance the alignment between phoneme and speech tokens. - We conduct comprehensive objective and subjective evaluations. Experimental results demonstrate that ==RALL-E== obtains significantly better robustness than the baseline VALL-E (2023) and two previous works. - We further evaluate ==RALL-E== on sentences that are particularly hard to synthesize for LLM-based TTS. The results demonstrate that ==RALL-E== correctly synthesizes hard sentences and reduces the error rate from68%to4%compared to VALL-E (2023), which closely approaches the performance of non-autoregressive TTS. Audio samples can be found at https://ralle-demo.github.io/RALL-E.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#2related-work","title":"2.Related Work","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#llm-based-tts","title":"LLM-Based TTS","text":"<p>Inspired by the success of LLMs [1,20], several recent works adopt language models to model TTS (SPEAR-TTS (2023), VALL-E (2023), UniAudio (2023)) and begin to use decoder-only architecture based on Transformer. In such models, text and speech tokens are concatenated together and fed to a single transformer. The whole model is trained on a next-token prediction task like a language model. The LLM-based TTS systems are typically trained on tens of thousands of hours of speech data and have hundreds of millions of parameters, hence can leverage the emergent abilities of LLMs like in-context learning [31] to enable zero-shot TTS (VALL-E (2023)). Besides, recent works (AudioPalM (2023); VioLA (2023); UniAudio (2023)) have shown the decoder-only architecture can be used to learn multiple tasks, as the input and output are processed jointly by a language model, and the model can be signaled to generate results for different tasks by inputting pre-defined special tokens. ==RALL-E== focuses on the robustness problem of LLM-based TTS.</p> <p>\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u83b7\u5f97\u7075\u611f, \u8fd1\u671f\u7684\u4e00\u4e9b\u5de5\u4f5c\u5c06\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u5efa\u6a21\u8bed\u97f3\u5408\u6210\u5e76\u5f00\u59cb\u4f7f\u7528\u4ec5\u6709 Transformer \u89e3\u7801\u5668\u7684\u67b6\u6784. \u8fd9\u4e9b\u6a21\u578b\u5c06\u6587\u672c\u548c\u8bed\u97f3\u6807\u8bb0\u4e32\u8054\u8d77\u6765, \u8f93\u5165\u5230\u5355\u4e2a Transformer \u4e2d. \u6574\u4e2a\u6a21\u578b\u5728\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3, \u5982\u8bed\u8a00\u6a21\u578b\u4e00\u6837. \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u901a\u5e38\u662f\u5728\u6570\u5343\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\u4e0a\u8bad\u7ec3\u7684, \u5177\u6709\u6570\u767e\u4e07\u4e2a\u53c2\u6570, \u56e0\u6b64\u53ef\u4ee5\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u80fd\u529b, \u5982\u4e0a\u4e0b\u6587\u5b66\u4e60, \u5b9e\u73b0\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210. \u9664\u6b64\u4e4b\u5916, \u8fd1\u671f\u7684\u5de5\u4f5c [22,30,33] \u8868\u660e, \u4ec5\u6709\u89e3\u7801\u5668\u67b6\u6784\u7684\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u591a\u4e2a\u4efb\u52a1, \u56e0\u4e3a\u8f93\u5165\u548c\u8f93\u51fa\u662f\u7531\u8bed\u8a00\u6a21\u578b\u4e00\u8d77\u5904\u7406\u7684, \u5e76\u4e14\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u9884\u5b9a\u4e49\u7684\u7279\u6b8a\u6807\u8bb0\u4fe1\u53f7\u751f\u6210\u4e0d\u540c\u4efb\u52a1\u7684\u7ed3\u679c. ==RALL-E== \u7740\u91cd\u4e8e\u8bed\u8a00\u6a21\u578b\u5efa\u6a21\u8bed\u97f3\u5408\u6210\u7684\u5065\u58ee\u6027\u95ee\u9898.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#robust-autoregressive-tts","title":"Robust Autoregressive TTS","text":"<p>The robustness of AR TTS is a popular topic in the literature. For encoder-decoder AR TTS, several previous works enforce the attention weights to be monotonic [2,9, 36] that can effectively improve the robustness. In addition, Shen et al.[24] proposed a non-attentive Tacotron, in which the attention module was replaced by a duration predictor to determine the alignment path before decoding. For decoder-only TTS, a key difference is that the attention weights are computed on text and context at the same time, hence the whole attention weights should not be monotonic. Song et al. proposed ELLA-V (2024) that interleaves the speech tokens with phonemes by inserting a phoneme token and a special End Of Phone(EOP) token at the beginning and end of the speech tokens corresponding to the phoneme, respectively. While the inserted phoneme and the EOP token indicate the duration of each phoneme, such an implicit way entangles the prediction of speech tokens and duration together. ==RALL-E== disentangles the predictions of duration and speech tokens by predicting the duration of all phonemes before the speech tokens, hence has higher controllability over the generation process. Du et al. proposed VALL-T (2024) that uses an unsupervised transducer loss [7] to implicitly model the duration of phonemes. Compared to RALL-E, although VALL-T doesn\u2019t rely on external alignment tools during training, its training time is considerably decelerated since the transducer loss requires the model to perform a forward process for every phoneme. Besides, like ELLA-V (2024), VALL-T (2024) also entangles the predictions of duration and speech tokens, thus has weaker controllability than RALL-E.</p> <p>\u81ea\u56de\u5f52\u8bed\u97f3\u5408\u6210\u7684\u5065\u58ee\u6027\u662f\u4e00\u4e2a\u70ed\u95e8\u8bdd\u9898. \u5bf9\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u81ea\u56de\u5f52\u8bed\u97f3\u5408\u6210, \u4e4b\u524d\u7684\u5de5\u4f5c\u5df2\u7ecf\u63d0\u51fa\u4e86\u5f3a\u5316\u6ce8\u610f\u529b\u6743\u91cd\u4e3a\u5355\u8c03\u7684\u7b56\u7565 [2,9, 36], \u80fd\u591f\u6709\u6548\u63d0\u5347\u8bed\u97f3\u5408\u6210\u7684\u5065\u58ee\u6027. \u6b64\u5916, Shen \u7b49\u4eba\u63d0\u51fa\u4e86\u4e00\u4e2a\u975e\u6ce8\u610f\u529b\u7684 Tacotron, \u5176\u4e2d\u66ff\u6362\u4e86\u6ce8\u610f\u529b\u6a21\u5757, \u7528\u4e00\u4e2a\u65f6\u957f\u9884\u6d4b\u5668\u5728\u89e3\u7801\u524d\u786e\u5b9a\u5bf9\u9f50\u8def\u5f84.</p> <p>\u5bf9\u4e8e\u4ec5\u6709\u89e3\u7801\u5668\u7684\u8bed\u97f3\u5408\u6210, \u5173\u952e\u533a\u522b\u662f\u6ce8\u610f\u529b\u6743\u91cd\u662f\u540c\u65f6\u8ba1\u7b97\u6587\u672c\u548c\u4e0a\u4e0b\u6587\u7684, \u56e0\u6b64\u6ce8\u610f\u529b\u6743\u91cd\u4e0d\u5e94\u8be5\u662f\u5355\u8c03\u7684. Song \u7b49\u4eba\u63d0\u51fa ELLA-V (2024), \u5176\u4e2d\u63d2\u5165\u4e86\u4e00\u4e2a\u97f3\u7d20\u6807\u8bb0\u548c\u4e00\u4e2a\u7279\u6b8a\u7684\u7ed3\u675f\u97f3\u7d20\u6807\u8bb0, \u524d\u8005\u5bf9\u5e94\u4e8e\u97f3\u7d20\u7684\u65f6\u957f, \u540e\u8005\u5bf9\u5e94\u4e8e\u97f3\u7d20\u7684\u7ed3\u675f. \u8fd9\u79cd\u9690\u5f0f\u7684\u65b9\u5f0f\u5c06\u8bed\u97f3\u6807\u8bb0\u548c\u65f6\u957f\u9884\u6d4b\u8054\u7cfb\u5728\u4e00\u8d77, \u5bfc\u81f4\u6a21\u578b\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u63a7\u5236\u529b\u8f83\u5f31. ==RALL-E== \u901a\u8fc7\u9884\u6d4b\u6240\u6709\u97f3\u7d20\u7684\u65f6\u957f, \u89e3\u8026\u4e86\u65f6\u957f\u548c\u8bed\u97f3\u6807\u8bb0\u7684\u9884\u6d4b, \u56e0\u6b64\u53ef\u4ee5\u66f4\u597d\u5730\u63a7\u5236\u751f\u6210\u8fc7\u7a0b. Du \u7b49\u4eba\u63d0\u51fa VALL-T (2024), \u4f7f\u7528\u65e0\u76d1\u7763\u7684\u8f6c\u6362\u5668\u635f\u5931 [7] \u6a21\u578b\u97f3\u7d20\u7684\u65f6\u957f. \u4e0e ==RALL-E== \u76f8\u6bd4, VALL-T \u867d\u7136\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u4f9d\u8d56\u5916\u90e8\u5bf9\u9f50\u5de5\u5177, \u4f46\u8bad\u7ec3\u65f6\u95f4\u663e\u8457\u51cf\u5c11, \u56e0\u4e3a\u8f6c\u6362\u5668\u635f\u5931\u9700\u8981\u6a21\u578b\u5bf9\u6bcf\u4e2a\u97f3\u7d20\u8fdb\u884c\u4e00\u6b21\u524d\u5411\u8ba1\u7b97. \u6b64\u5916, \u4e0e ELLA-V (2024) \u4e00\u6837, VALL-T (2024) \u4e5f\u5c06\u65f6\u957f\u548c\u8bed\u97f3\u6807\u8bb0\u8054\u7cfb\u5728\u4e00\u8d77, \u56e0\u6b64\u63a7\u5236\u529b\u8f83\u5f31.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#3rall-e","title":"3.RALL-E","text":"<p>The overview of ==RALL-E== is illustrated in Fig.01.</p> <p></p> <p>The core idea of ==RALL-E== is CoT prompting that generates intermediate results to assist and stabilize the generation of speech tokens and improve the robustness of LLM-based TTS. To accomplish this idea, we first propose to predict two kinds of phoneme-level prosody tokens: pitch and duration before predicting the speech tokens. The distributions of the prosody tokens are modeled together with speech tokens by a single Transformer so that they can influence the duration and pitch of the predicted speech tokens. To further utilize the predicted duration to guide the generation and improve the robustness, we propose duration-guided masking to enhance the alignment between speech tokens, phonemes, and prosody tokens learned by the language model. At each decoding step of the speech tokens, ==RALL-E== masks phonemes and prosody tokens that are irrelevant to the synthesis of the current speech token based on the duration information. In the following sections, we first briefly introduce VALL-E (2023) since we apply the proposed method to it in the experiments. We then formulate and introduce ==RALL-E== in detail. It should be stressed that, though we use VALL-E (2023) to implement ==RALL-E==, the proposed method can be applied in any decoder-only AR TTS model.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#31preliminary-vall-e","title":"3.1.Preliminary: VALL-E","text":"<p>We inherit most symbols from the original paper of VALL-E (2023) for ease of reading. Readers are recommended to refer to the original paper for more details.</p> <p>Generally, VALL-E (2023) is a decoder-only LLM-based TTS system that uses two Transformers [28] to predict speech tokens from the text. The speech tokens here are extracted from EnCodec (2022), a neural audio codec based on residual vector quantization (RVQ) [34] that can convert continuous speech signal into discrete tokens. After predicting the discrete tokens, the waveforms can be reconstructed by feeding the tokens into the decoder of EnCodec. An RVQ typically contains $N$ quantization layers ($N = 8$ in VALL-E (2023)), hence at each time step the encoded speech has $N$ tokens.  Formally, given speech $\\mathbf{y}$ and its transcription $\\mathbf{x}$, the discrete speech token matrix $\\mathbf{C}$ encoded by the codec has a shape of $T \\times N$, where $T$ is the total time step. In addition tox, to clone a speaker\u2019s voice and utilize the in-context learning ability of LLMs, VALL-E (2023) receives a short prompt $\\tilde{\\mathbf{C}}^{T'\\times N}$ as input before predicting $\\mathbf{C}$. Hence, VALL-E (2023) models and maximizes the following distribution:</p> <p>$$   \\mathbb{P}(\\mathbf{C}|\\mathbf{x},\\tilde{\\mathbf{C}}).\\tag{1} $$</p> <p>VALL-E (2023) predicts speech tokens hierarchically where the speech tokens of the 1st layer of RVQ are first predicted by an AR Transformer, and the tokens of the rest layers are predicted by a NAR Transformer. This is because RVQ uses a residual quantization method, i.e. higher layers encode the information that is not encoded by the lower layers, hence tokens of the1st layer contain most information of the waveforms, and the information encoded by the rest layers gradually decreases. The AR Transformer takes the phoneme sequence $\\mathbf{x}$, and speech tokens of the1st layer of the prompt $\\tilde{c}{:,1}$ as input to predict the target speech tokens of the1st layer $\\mathbf{c}{:,1}$ sequentially, i.e. maximizes the following distribution:</p> <p>$$   \\mathbb{P}(\\mathbf{c}{:,1}|\\mathbf{x},\\tilde{\\mathbf{c}}{:,1};\\theta_{AR})=\\prod_{t=1}^T\\mathbb{P}(\\mathbf{c}{t,1}|\\mathbf{x},\\mathbf{c}{&lt;t,1},\\tilde{\\mathbf{c}}{:,1};\\theta{AR}),\\tag{2} $$</p> <p>where $\\theta_{\\text{AR}}$ is the trainable parameters of the AR Transformer. The NAR Transformer predicts all target speech tokens $\\mathbf{c}{:,j}$ of the $j$-th layer at the same time with the phoneme sequence $\\mathbf{x}$, the prompt $\\tilde{\\mathbf{C}}$, and target speech tokens $\\mathbf{c}{:,&lt;j}$ of all layers less than $j$ as the conditions, i.e. maximizes the following distribution:</p> <p>$$   \\mathbb{P}(\\mathbf{c}{:,2:N}|\\mathbf{x},\\tilde{\\mathbf{C}};\\theta{NAR})=\\prod_{j=2}^N\\mathbb{P}(\\mathbf{c}{:,j}|\\mathbf{x},\\mathbf{c}{:,&lt;j},\\tilde{\\mathbf{C}};\\theta_{NAR}),\\tag{3} $$</p> <p>where $\\theta_{\\text{NAR}}$ is the trainable parameters of the NAR Transformer. By combining Eq.2 and Eq.3, VALL-E (2023) breaks Eq.1 into the following form:</p> <p>$$   \\mathbb{P}(\\mathbf{C}|\\mathbf{x},\\tilde{\\mathbf{C}})=\\mathbb{P}(\\mathbf{c}{:,1}|\\mathbf{x},\\tilde{\\mathbf{c}}{:,1};\\theta_{AR})\\mathbb{P}(\\mathbf{c}{:,2:N}|\\mathbf{x},\\tilde{\\mathbf{C}};\\theta{NAR}).\\tag{4} $$</p> <p>It is noteworthy that in practice the two Transformers have the same architecture but have different attention masks during computation. Specifically, both the two Transformers use a bidirectional mask for the phoneme sequence $\\mathbf{x}$, which means every phoneme $x_i$ can attend to all other phonemes $x_{\\neq i}$. However, for the speech tokens, the AR Transformers uses a unidirectional mask so that $\\mathbf{c}{t,1}$ can only attend to previous tokens $\\mathbf{c}{&lt;t,1}$, while the NAR Transformer still uses a bidirectional mask.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#32prosody-tokens-as-chain-of-thought-prompts","title":"3.2.Prosody Tokens as Chain-of-Thought Prompts","text":"<p>One of the problems of LLM-based TTS is that it directly generates speech from phonemes with no restriction on the prosody, e.g. pitch, duration, etc, which usually results in speech with unstable prosody. A similar problem is also observed in Wei et al.[32] where the authors find LLMs cannot directly answer a complex question like arithmetic and propose CoT prompting to solve this problem. The idea of CoT prompting is breaking a complex task into several simpler tasks so that LLMs can utilize the intermediate results to reach the final answers. As shown in Wei et al.[32], by CoT prompting the correct rate of LLMs on complex tasks can be significantly improved. This motivates us to adapt CoT prompting to LLM-based TTS by generating intermediate prosody tokens before generating speech tokens to alleviate the robustness problem of LLM-based TTS.</p> <p>\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7684\u95ee\u9898\u4e4b\u4e00\u662f\u5b83\u76f4\u63a5\u4ece\u97f3\u7d20\u751f\u6210\u8bed\u97f3, \u800c\u5bf9\u97f5\u5f8b\u6ca1\u6709\u9650\u5236, \u5982\u97f3\u9ad8, \u65f6\u957f\u7b49, \u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u751f\u6210\u97f5\u5f8b\u4e0d\u7a33\u5b9a\u7684\u8bed\u97f3. Wei \u7b49\u4eba\u7684\u7814\u7a76\u4e2d\u4e5f\u80fd\u89c2\u5bdf\u5230\u76f8\u4f3c\u95ee\u9898, \u4ed6\u4eec\u53d1\u73b0\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u4e0d\u80fd\u76f4\u63a5\u56de\u7b54\u590d\u6742\u7684\u95ee\u9898, \u5982\u7b97\u672f\u7b49, \u4ece\u800c\u63d0\u51fa\u4e86\u601d\u7ef4\u94fe\u63d0\u793a\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898. \u601d\u7ef4\u94fe\u63d0\u793a\u7684\u601d\u60f3\u662f\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u82e5\u5e72\u4e2a\u7b80\u5355\u4efb\u52a1\u4f7f\u5f97\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5229\u7528\u4e2d\u95f4\u7ed3\u679c\u6765\u83b7\u5f97\u6700\u7ec8\u7b54\u6848. \u5982 Wei \u7b49\u4eba\u6240\u5c55\u793a\u7684, \u901a\u8fc7\u601d\u7ef4\u94fe\u63d0\u793a, \u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6b63\u786e\u7387. \u8fd9\u6fc0\u52b1\u6211\u4eec\u5c06\u601d\u7ef4\u94fe\u63d0\u793a\u5e94\u7528\u4e8e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210, \u901a\u8fc7\u5728\u751f\u6210\u8bed\u97f3\u4e4b\u524d\u751f\u6210\u4e2d\u95f4\u97f5\u5f8b\u6807\u8bb0\u6765\u7f13\u89e3\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u7684\u5065\u58ee\u6027\u95ee\u9898.</p> <p>To incorporate pitch and duration in the AR Transformer of VALL-E (2023), we first get the alignment between phonemes and speech tokens and extract the pitch value for each speech token. We then compute phoneme-level pitch value based on the duration and linearly quantize it to $M_p$ buckets. We define a maximal duration value $M_d$, and all duration values that exceed $M_d$ will be truncated to the maximum. ==RALL-E== predicts the two prosody tokens before the speech tokens in a CoT style.</p> <p>\u4e3a\u4e86\u5728 VALL-E (2023) \u7684\u81ea\u56de\u5f52 Transformer \u4e2d\u6574\u5408\u97f3\u9ad8\u548c\u65f6\u957f, \u6211\u4eec\u9996\u5148\u83b7\u5f97\u97f3\u7d20\u548c\u8bed\u97f3 token \u4e4b\u95f4\u7684\u5bf9\u9f50, \u5e76\u63d0\u53d6\u6bcf\u4e2a\u8bed\u97f3 token \u7684\u97f3\u9ad8\u503c. \u7136\u540e\u6211\u4eec\u57fa\u4e8e\u65f6\u957f\u8ba1\u7b97\u97f3\u7d20\u7ea7\u522b\u7684\u97f3\u9ad8\u503c, \u5e76\u5c06\u5176\u7ebf\u6027\u91cf\u5316\u5230 $M_p$ \u4e2a\u6876\u4e2d. \u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u6700\u5927\u65f6\u957f\u503c $M_d$, \u8d85\u51fa\u8be5\u503c\u7684\u65f6\u957f\u503c\u5c06\u88ab\u622a\u65ad. ==RALL-E== \u4ee5 CoT \u98ce\u683c\u5728\u8bed\u97f3 tokens \u524d\u9884\u6d4b\u4e24\u4e2a\u97f5\u5f8b tokens.</p> <p>Formally, assume $p$, $d$ are the discrete pitch and duration sequences of the target speech tokens $\\mathbf{C}$, $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$ are the ones of the prompt $\\tilde{\\mathbf{C}}$, we model and maximize the following distribution:</p> <p>\u5f62\u5f0f\u4e0a, \u5047\u8bbe $p$, $d$ \u662f\u76ee\u6807\u8bed\u97f3 tokens $\\mathbf{C}$ \u7684\u79bb\u6563\u97f3\u9ad8\u548c\u65f6\u957f\u5e8f\u5217, $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$ \u662f\u63d0\u793a $\\tilde{\\mathbf{C}}$ \u7684\u97f3\u9ad8\u548c\u65f6\u957f\u5e8f\u5217, \u6211\u4eec\u53ef\u4ee5\u5efa\u6a21\u5e76\u6700\u5927\u5316\u4e0b\u5217\u5206\u5e03:</p> <p>$$   \\mathbb{P}(\\mathbf{p},\\mathbf{d}|\\mathbf{x},\\tilde{\\mathbf{p}},\\tilde{\\mathbf{d}};\\theta_{AR})=\\prod_{t=1}^L\\mathbb{P}(p_t,d_t|\\mathbf{x},\\mathbf{p}{&lt;t},\\mathbf{d}{&lt;t},\\tilde{\\mathbf{p}},\\tilde{\\mathbf{d}};\\theta_{AR}),\\tag{5} $$ </p> <p>where $L$ is the length of $\\mathbf{x}$. </p> <p>\u5176\u4e2d $L$ \u4e3a $\\mathbf{x}$ \u7684\u957f\u5ea6.</p> <p>In practice, the model predicts $p_t$ and $d_t$ with two separate heads, and their embeddings are summed up and fed to the model for the prediction of the next step. </p> <p>\u5b9e\u9645\u4e0a, \u6a21\u578b\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acb\u7684\u5934\u9884\u6d4b $p_t$ \u548c $d_t$, \u4ed6\u4eec\u7684\u5d4c\u5165\u5c06\u88ab\u6c42\u548c\u540e\u8f93\u5165\u6a21\u578b\u4ee5\u8fdb\u884c\u4e0b\u4e00\u6b65\u7684\u9884\u6d4b.</p> <p>==RALL-E== then predicts the speech tokens with $p$ and $d$ as a new condition, which makes Eq.2 becomes:</p> <p>==RALL-E== \u7136\u540e\u4ee5 $p$ \u548c $d$ \u4f5c\u4e3a\u65b0\u7684\u6761\u4ef6, \u4f7f\u5f97\u65b9\u7a0b 2 \u53d8\u4e3a:</p> <p>$$ \\mathbb{P}(\\mathbf{c}{:,1}|\\mathbf{x},\\tilde{\\mathbf{c}}{:,1},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta_{AR})=\\prod_{t=1}^T\\mathbb{P}(\\mathbf{c}{t,1}|\\mathbf{x},\\mathbf{c}{&lt;t,1},\\tilde{\\mathbf{c}}{:,1},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta{AR}).\\tag{6} $$</p> <p>The above two equations can be jointly optimized by the AR Transformer.  Although the proposed method adds additional $L$ decoding steps, since $L&lt;&lt;T$, it intuitively has little influence on the efficiency.</p> <p>\u4e0a\u9762\u7684\u4e24\u4e2a\u65b9\u7a0b\u53ef\u4ee5\u901a\u8fc7 AR Transformer \u8054\u5408\u4f18\u5316.  \u5c3d\u7ba1\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u589e\u52a0\u4e86 $L$ \u4e2a\u89e3\u7801\u6b65\u9aa4, \u4f46\u7531\u4e8e $L&lt;&lt;T$, \u5176\u5b9e\u9645\u5f71\u54cd\u5f88\u5c0f.</p> <p>For the NAR Transformer, we simply sum the embeddings of the phoneme, pitch, and duration together as the input. This makes Eq.3 becomes:</p> <p>\u5bf9\u4e8e\u975e\u81ea\u56de\u5f52 Transformer, \u6211\u4eec\u53ea\u9700\u5c06\u97f3\u7d20, \u97f3\u9ad8, \u65f6\u957f\u7684\u5d4c\u5165\u5411\u91cf\u76f8\u52a0\u4f5c\u4e3a\u8f93\u5165. \u8fd9\u4f7f\u5f97\u65b9\u7a0b 3 \u53d8\u4e3a: $$ \\begin{aligned}\\mathbb{P}(\\mathbf{c}{:,2:N}|\\mathbf{x},\\tilde{\\mathbf{C}},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta{NAR})&amp;=\\prod_{j=2}^N\\mathbb{P}(\\mathbf{c}{:,j}|\\mathbf{x},\\mathbf{c}{:,&lt;j},\\tilde{\\mathbf{C}},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta_{NAR}).\\end{aligned}\\tag{7} $$ </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#33enhancing-alignment-with-duration-guided-masking","title":"3.3.Enhancing Alignment with Duration-Guided Masking","text":"<p>As the left side of Fig.02 illustrates, since the speech token attends to all phonemes in the AR Transformer of VALL-E (2023), the alignment between the phonemes and the speech tokens is implicitly modeled by the self-attention of VALL-E (2023). This can be imprecise and causes errors like word omissions or hallucinations. Though ==RALL-E== introduces prosody CoT prompting to guide and stabilize the generation, we still find the model can fail to align in the experiments. We thus propose duration guided masking to fully utilize the intermediate duration results and boost the robustness.</p> <p>\u5982\u56fe 2 \u5de6\u4fa7\u6240\u793a, \u7531\u4e8e VALL-E (2023) \u7684\u81ea\u56de\u5f52 Transformer \u4e2d\u7684\u8bed\u97f3 token \u80fd\u591f\u5173\u6ce8\u5230\u6240\u6709\u97f3\u7d20, \u56e0\u6b64\u97f3\u7d20\u548c\u8bed\u97f3 token \u7684\u5bf9\u9f50\u662f\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u9690\u5f0f\u5efa\u6a21\u7684. \u7136\u800c, \u8fd9\u79cd\u5bf9\u9f50\u53ef\u80fd\u4e0d\u7cbe\u786e, \u5bfc\u81f4\u8bf8\u5982\u8bcd\u6c47\u9057\u6f0f\u6216\u5e7b\u89c9\u7b49\u9519\u8bef. \u5c3d\u7ba1 ==RALL-E== \u63d0\u51fa\u4e86\u97f5\u5f8b CoT \u63d0\u793a\u6765\u5f15\u5bfc\u548c\u7a33\u5b9a\u751f\u6210, \u4f46\u6211\u4eec\u4ecd\u7136\u53d1\u73b0\u6a21\u578b\u5728\u5b9e\u9a8c\u4e2d\u53ef\u80fd\u65e0\u6cd5\u5bf9\u9f50. \u56e0\u6b64, \u6211\u4eec\u63d0\u51fa\u4e86\u65f6\u957f\u5bfc\u5411\u63a9\u7801 (duration-guided masking) \u6765\u5145\u5206\u5229\u7528\u4e2d\u95f4\u65f6\u957f\u7ed3\u679c\u5e76\u63d0\u5347\u9c81\u68d2\u6027.</p> <p>As the right side of Fig.02 illustrates, in the proposed duration-guided masking, the speech token is restricted to only attend on a phoneme (prosody token) window centered at the phoneme (prosody token) it corresponds to. We define the window size ask, thus each speech token can attend on $2k + 1$ phonemes and $2k + 1$ prosody tokens. All phonemes and prosody tokens at other positions will be masked out, hence their attention weights are always zero. When $k = 0$ the speech token strictly attends to the phoneme it corresponds to. If the alignment is perfect this should be enough. However, in the experiments, we found that the alignment results obtained by our alignment tool usually have errors. We thus loosen the restriction by also allowing the speech token to attend at the near phonemes of the corresponding phoneme. Another reason for this design is that the pronunciation of a phoneme is usually dependent on near phonemes. As one will see in Section.4.3 and Appendix.A, the experimental results verify the effectiveness of this design.</p> <p>\u5982\u56fe 2 \u53f3\u4fa7\u6240\u793a, \u5728\u6240\u63d0\u51fa\u7684\u65f6\u957f\u5bfc\u5411\u63a9\u7801\u4e2d, \u8bed\u97f3 token \u53ea\u80fd\u5173\u6ce8\u5230\u4e0e\u5176\u5bf9\u5e94\u7684\u97f3\u7d20 (\u97f5\u5f8b token) \u5904\u4e8e\u4e2d\u5fc3\u4f4d\u7f6e\u7684\u7a97\u53e3\u5185\u7684\u97f3\u7d20 (\u97f5\u5f8b token). \u6211\u4eec\u5b9a\u4e49\u7a97\u53e3\u5927\u5c0f\u4e3a $k$, \u56e0\u6b64\u6bcf\u4e2a\u8bed\u97f3 token \u53ef\u4ee5\u5173\u6ce8\u5230 $2k + 1$ \u4e2a\u97f3\u7d20\u548c $2k + 1$ \u4e2a\u97f5\u5f8b token. \u6240\u6709\u5176\u4ed6\u4f4d\u7f6e\u7684\u97f3\u7d20\u548c\u97f5\u5f8b token \u90fd\u5c06\u88ab\u63a9\u7801\u6389, \u56e0\u6b64\u5b83\u4eec\u7684\u6ce8\u610f\u529b\u6743\u91cd\u603b\u662f\u4e3a\u96f6. \u5f53 $k = 0$ \u65f6, \u8bed\u97f3 token \u4e25\u683c\u5173\u6ce8\u5230\u5b83\u5bf9\u5e94\u7684\u97f3\u7d20. \u7136\u800c, \u5728\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u53d1\u73b0\u6240\u83b7\u5f97\u7684\u5bf9\u9f50\u7ed3\u679c\u7ecf\u5e38\u5b58\u5728\u9519\u8bef. \u56e0\u6b64, \u6211\u4eec\u901a\u8fc7\u5141\u8bb8\u8bed\u97f3 token \u5173\u6ce8\u5230\u4e0e\u5176\u5bf9\u5e94\u7684\u97f3\u7d20\u76f8\u90bb\u7684\u97f3\u7d20\u6765\u653e\u677e\u9650\u5236. \u53e6\u4e00\u4e2a\u539f\u56e0\u662f, \u4e00\u4e2a\u97f3\u7d20\u7684\u53d1\u97f3\u901a\u5e38\u4f9d\u8d56\u4e8e\u9644\u8fd1\u7684\u97f3\u7d20. \u5982 \u7b2c 4.3 \u8282 \u548c \u9644\u5f55 A \u6240\u793a, \u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8fd9\u79cd\u8bbe\u8ba1\u7684\u6709\u6548\u6027.</p> <p>For the NAR Transformer, we obtained almost no gain when applying the proposed masking strategy to it in our preliminary experiments. Thus we only apply the masking strategy on the AR Transformer.</p> <p>\u5bf9\u4e8e\u975e\u81ea\u56de\u5f52 Transformer, \u6211\u4eec\u5728\u521d\u6b65\u5b9e\u9a8c\u4e2d\u5e76\u672a\u83b7\u5f97\u6240\u63d0\u51fa\u7684\u63a9\u7801\u7b56\u7565\u7684\u660e\u663e\u6536\u76ca. \u56e0\u6b64, \u6211\u4eec\u53ea\u5728\u81ea\u56de\u5f52 Transformer \u4e0a\u5e94\u7528\u63a9\u7801\u7b56\u7565.</p> <p>The general inference procedure follows VALL-E (2023) with two differences. First, before sampling the speech tokens $\\mathbf{c}{:,1}$ the prosody tokens $\\mathbf{p}$ and $\\mathbf{d}$ are sampled conditioning on the phoneme sequence $\\mathbf{x}$ and acoustic prompt $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$. Second, although normal language models depend on a special token <code>&lt;eos&gt;</code> as the stop condition, since we know the total duration $D = \\sum^L{t=1}d_t$, we propose a duration guided inference method that forces the inference to stop at the $D$-th step. This method ensures no phoneme is omitted or repeated as it continues the inference if the <code>&lt;eos&gt;</code> token is predicted before the $D$-th step and stops at the right step as guided by the predicted duration..</p> <p>\u6574\u4f53\u63a8\u7406\u8fc7\u7a0b\u9075\u5faa VALL-E (2023) \u7684\u4e00\u822c\u8fc7\u7a0b, \u4f46\u6709\u4e24\u4e2a\u5dee\u522b. \u9996\u5148, \u5728\u91c7\u6837\u8bed\u97f3 tokens $\\mathbf{c}{:,1}$ \u4e4b\u524d, \u5148\u91c7\u6837\u97f5\u5f8b tokens $\\mathbf{p}$ \u548c $\\mathbf{d}$ \u4f5c\u4e3a\u6761\u4ef6, \u6761\u4ef6\u662f\u97f3\u7d20\u5e8f\u5217 $\\mathbf{x}$ \u548c\u58f0\u5b66\u63d0\u793a $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$. \u5176\u6b21, \u867d\u7136\u666e\u901a\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u4e8e\u7279\u6b8a\u7b26\u53f7 <code>&lt;eos&gt;</code> \u4f5c\u4e3a\u505c\u6b62\u6761\u4ef6, \u4f46\u7531\u4e8e\u6211\u4eec\u77e5\u9053\u603b\u65f6\u957f $D = \\sum^L{t=1}d_t$, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65f6\u957f\u5bfc\u5411\u63a8\u7406\u65b9\u6cd5, \u5f3a\u5236\u63a8\u7406\u505c\u6b62\u4e8e $D$-th \u6b65. \u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u786e\u4fdd\u4e0d\u4f1a\u9057\u6f0f\u6216\u91cd\u590d\u4efb\u4f55\u97f3\u7d20, \u56e0\u4e3a\u5982\u679c\u5728 $D$-th \u6b65\u9884\u6d4b\u5230 <code>&lt;eos&gt;</code> \u7b26\u53f7, \u63a8\u7406\u5c31\u4f1a\u7ee7\u7eed, \u76f4\u5230\u9884\u6d4b\u7684\u65f6\u957f\u8fbe\u5230\u8981\u6c42\u7684\u65f6\u957f.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#4experiments","title":"4.Experiments","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#41setup","title":"4.1.Setup","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#data","title":"Data","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#model-configuration","title":"Model Configuration","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#training-and-inference","title":"Training and Inference","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#baseline-methods","title":"Baseline Methods","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#objective-metrics","title":"Objective Metrics","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#word-error-rate-wer","title":"Word Error Rate (WER)","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#reranked-wer-wer-r","title":"Reranked WER (WER-R)","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#substitution-sub-deletion-del-insertion-ins","title":"Substitution (Sub), Deletion (Del), Insertion (Ins)","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#utmos","title":"UTMOS","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#speaker-similarity-sim","title":"Speaker Similarity (SIM)","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#subjective-metrics","title":"Subjective Metrics","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#42main-results","title":"4.2.Main Results","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#43ablation-study","title":"4.3.Ablation Study","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#44evaluation-on-hard-sentences","title":"4.4.Evaluation on Hard Sentences","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#5conclusion","title":"5.Conclusion","text":"<p>This paper presents ==RALL-E==, a robust codec language modeling method with CoT prompting for TTS. To address the robustness problem of LLM-based TTS, ==RALL-E== (1) incorporates prosody features (pitch and duration) in the LLM as a CoT prompting to assist and stabilize the generation of speech tokens, and (2) proposes duration-guided masking that enforces the model to attend on relevant phonemes (prosody features) corresponding to each speech token. We conduct comprehensive objective and subjective evaluations and demonstrate that ==RALL-E== can significantly improve the robustness of LLM-based TTS compared to the baseline VALL-E (2023) and two previous works. Furthermore, we show that ==RALL-E== can correctly synthesize sentences that are particularly hard to synthesize for VALL-E (2023) with a 4% error rate that even approaches the performance of non-autoregressive TTS.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#awindow-size-study","title":"A.Window Size Study","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.04_RALL-E/#btranscript-of-the-50-hard-sentences","title":"B.Transcript of the 50 hard sentences","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/","title":"LLaMA-VITS","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: LLaMA-VITS: Enhancing TTS Synthesis with Semantic Awareness - \u4f5c\u8005:   - [Xincan Feng](../../Authors/Xincan_Feng.md)   - [Akifumi Yoshimoto](../../Authors/Akifumi_Yoshimoto.md) - \u673a\u6784:   - [\u5948\u826f\u5148\u7aef\u79d1\u5b66\u6280\u672f\u5927\u5b66\u9662\u5927\u5b66](../../Institutions/NAIST_\u65e5\u672c\u5948\u826f\u5148\u7aef\u79d1\u5b66\u6280\u672f\u5927\u5b66\u9662\u5927\u5b66.md)   - [CyberAgent](../../Institutions/CyberAgent.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.04.10 ArXiv v1   - \u9884\u5370\u65f6\u95f4: 2024.04.12 ArXiv v2   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.13 - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2404.06714)   - [DOI]()   - [Github](https://github.com/xincanfeng/vitsgpt.git)   - [Demo]() - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u8bed\u8a00\u6a21\u578b](../../Tags/LanguageModel.md)   - [\u5f00\u6e90](../../Tags/OpenSource.md) - \u9875\u6570: 15 - \u5f15\u7528: ? - \u88ab\u5f15: 0"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#abstract","title":"Abstract","text":"<p>Recent advancements in Natural Language Processing (NLP) have seen Large-scale Language Models (LLMs) excel at producing high-quality text for various purposes. Notably, in Text-To-Speech (TTS) systems, the integration of BERT for semantic token generation has underscored the importance of semantic content in producing coherent speech outputs. Despite this, the specific utility of LLMs in enhancing TTS synthesis remains considerably limited. This research introduces an innovative approach, LLaMA-VITS, which enhances TTS synthesis by enriching the semantic content of text using LLM. LLaMA-VITS integrates semantic embeddings from LLaMA2 (2023) with the VITS, a leading end-to-end TTS framework. By leveraging LLaMA2 (2023) for the primary speech synthesis process, our experiments demonstrate that LLaMA-VITS matches the naturalness of the original VITS (ORI-VITS) and those incorporate BERT (BERT-VITS), on the LJSpeech dataset, a substantial collection of neutral, clear speech. Moreover, our method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem dataset, a curated selection of emotionally consistent speech from the EmoV_DB dataset, highlighting its potential to generate emotive speech.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#1introduction","title":"1.Introduction","text":"<p>Text-to-Speech (TTS) synthesis is a technology that transforms written text into its spoken equivalent, thereby enhancing content accessibility. This technology finds application in the production of audiobooks (Chen et al., 2022) and virtual assistants (Wu et al., 2023). However, traditional TTS models, which primarily focus on the acoustic features, often fall short in comprehending the semantic and emotional information embedded within the text. With the significant advancements in Natural Language Processing (NLP) technologies, particularly through Language Models (LMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018; Brown et al., 2020), which have demonstrated formidable capabilities in understanding and generating natural language, researchers have proposed various BERT-based TTS models (Mukherjee et al., 2022; Abbas et al., 2022; Li et al., 2023; Guo et al., 2022) to improve the expressiveness of synthesized speech. Nonetheless, the effectiveness and flexibility of BERT-based TTS models in diverse applications are limited due to the smaller parameter size of BERT models and the necessity for designing specific fine-tuning tasks to enhance their capabilities. On the other hand, Large-scale Language Models (LLMs), such as LLaMA2 (2023), not only require decreasing computational re sources and achieve higher levels of text generation but also possess excellent zero-shot learning capabilities. Moreover, they can achieve improvements comparable to fine-tuning by adjusting only a minimal number of parameters through prompt tuning (Liu et al., 2022; Tu et al., 2022). However, the potential of these LLMs for TTS tasks has not been fully explored. In light of this context, we introduce LLaMA-VITS, a model that leverages semantic representations extracted from LLaMA2 (2023) on top of a state-of-the-art TTS model, VITS, enabling the generated speech to retain acoustic information while understanding and expressing semantics and emotions. Through comprehensive objective and subjective evaluations, LLaMA-VITS has been verified to surpass TTS baselines without semantic input or those integrated with BERT. The main contributions encapsulate:  - We propose LLaMA-VITS model that utilizes the semantic understanding and expression capabilities of LLaMA2 (2023), offering equal or superior acoustic performance compared to baseline models, along with a significantly enhanced ability to understand and express semantics and emotions. - Through empirical analysis, we demonstrate that global tokens in LLaMA-VITS provide more significant improvements than sequential to kens, contrasting with observations in BERT-based TTS models. - We quantitatively verified our findings using both subjective and objective metrics. Our code, models, audio demos, and the filtered single female speaker emotional dataset EmoV_DB_bea_sem are available at https://github.com/xincanfeng/vitsgpt.git.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#2related-works","title":"2.Related Works","text":"<p>TTS technology has significantly advanced in learning acoustic features through structural evolution. However, comprehending and conveying semantics remain challenging. Since BERT-like LMs have demonstrated profound capabilities in understanding semantics through extensive pre-training on vast text corpora, some studies have integrated BERT-like LMs with TTS technology to enhance synthesized speech. Nonetheless, research on incorporating GPT-like LMs within TTS technology is notably scarce.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#21text-to-speech-models","title":"2.1.Text-to-Speech Models","text":"<p>TTS task aims to generate natural, fluent, and easily comprehensible speech. Traditional TTS systems, e.g., a Statistical Parametric Speech Synthesis (SPSS) system (Taylor, 2009), usually comprise multiple distinct components. These include a frontend module that converts text into linguistic features (such as duration and pitch), an acoustic model that maps these linguistic features to acoustic features, and a vocoder responsible for generating speech waveforms from the acoustic features. Over the past decades, the complexity of traditional models has been notable, attributed to their reliance on manually engineered features and the intricate communication between modules.</p> <p>Transitioning from Hidden Markov Models (HMM) based models (Black et al., 2007), through Deep Neural Networks (DNN) models (Zen et al., 2013), to Generative Adversarial Networks (GAN) based models (Saito et al., 2017), there has been a no table enhancement in voice quality, yet the architectural complexity remains significant.</p> <p>The advent of end-to-end TTS models marks a significant milestone, increasingly reducing the distinction between synthesized speech and human voice. End-to-end models are capable of trans forming raw text directly into final speech output, which not only streamlines the structural complexity of TTS systems and facilitates easier deployment but also significantly reduces the dependency on manual feature engineering, simplifying the training process. Moreover, they notably enhance the naturalness and intelligibility of the speech, thereby be coming the predominant architecture in TTS models. For instance, Char2Wav (2017) introduces an attentive encoder-decoder frame work for direct speech synthesis from text input. Tacotron (2017) undertakes training from the ground up and directly predicts linear spectrograms. Furthermore, the speech produced by Tacotron2 (2017) closely mirrors the natural human voice.</p> <p>In the realm of end-to-end TTS models, many have adopted a non-autoregressive architecture. This architecture enables parallel data processing, where the model\u2019s output generation does not depend on the output of the previous time step, thereby enhancing processing speed. It also circumvents the error accumulation issue inherent in traditional autoregressive models, which significantly boosts TTS performance. FastSpeech (2019) and its variants exemplify this trend. FastSpeech (2019) employs a transformer-based architecture to generate mel-spectrograms in parallel. Building on FastSpeech (2019), FastPitch (2020) predicts pitch contours during inference, enabling the production of more expressive and high quality speech. FastSpeech2 (2020) further incorporates explicit duration prediction and introduces pitch and energy as conditional inputs.</p> <p>Previous non-autoregressive approaches typically involve distinct training phases for acoustic models and vocoders. VITS (2021) introduces a more natural-sounding output compared to these two-stage systems through its one-stage parallel end-to-end architecture. Innovatively, VITS (2021) incorporates variational inference combined with normalizing flows and employs an adversarial training methodology. Due to VITS (2021)\u2019s exemplary performance across multiple benchmarks, we select it as the foundational TTS model for our system.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#22-fine-tuning-bert-like-lms-for-tts","title":"2.2. Fine-tuning BERT-like LMs for TTS","text":"<p>While TTS models have increasingly advanced in replicating acoustic features, insufficient training data can hinder the model\u2019s ability to learn the semantic nuances of the same input across different contexts, thus limiting its expressiveness. Consequently, researchers have turned to leveraging the transfer learning capabilities of BERT-like LMs. Ultimately, TTS systems that incorporate pre-trained and fine-tuned BERT-like LMs have achieved better understandings of semantics and enhanced generated speech, marking a significant advancement.</p> <p>Hayashi et al. (2019) utilized a pre-trained BERT model as an auxiliary input to enhance a Tacotron2 based TTS system, resulting in improved speech naturalness. Similarly, Yang et al. (2019) applied a pre-trained BERT model to achieve enhanced front end accuracy. Kenter et al. (2020) demonstrated that integrating a BERT model, pre-trained on extensive unlabeled data and fine-tuned for speech, into an RNN-based TTS system enhances prosody. Kenter et al. (2020) specifically suggest updating the BERT\u2019s parameters during the training of their RNN-based speech synthesis model, emphasizing the critical role of fine-tuning the BERT component for optimal outcomes. As prompt tuning draws wide attention in guiding text or image generation, PromptTTS (2022) takes a prompt representation with both style and content descriptions from a BERT model as input to generate speech with precise style control and high speech quality.</p> <p>In particular, Mukherjee et al. (2022) utilized a pre-trained BERT model to develop a text emotion classification model, employing the final hidden states of the initial <code>[CLS]</code> token as a comprehensive representation of the text. Researchers such as Kenter et al. (2020); Li et al. (2021); Abbas et al. (2022) have applied word-level BERT to capture the semantic and syntactic structure of sentences, thereby aiding TTS synthesis. Li et al. (2023) introduced a phoneme-level BERT, designed with a preliminary task of predicting corresponding graphemes in addition to regular masked phoneme predictions, to enhance the naturalness of speech synthesized from out-of-distribution (OOD) texts.</p> <p>However, despite BERT\u2019s acknowledged capacity to provide detailed word importance, syn tactic and semantic insights, and general knowledge (Hayashi et al., 2019; Kenter et al., 2020), its effectiveness is constrained by the particularities of fine-tuning approaches. Furthermore, BERT\u2019s inherent non-generative nature might limit its ability to account for information outside the immediate sentence context.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#23integrating-gpt-like-lms-for-tts","title":"2.3.Integrating GPT-like LMs for TTS","text":"<p>Considering semantic understanding and expression capabilities, BERT is primarily utilized for com prehension tasks. In comparison, GPT excels not only in understanding text but also in generating natural and coherent text. Moreover, with the larger model parameters, GPT is particularly adept at zero-shot or few-shot learning, enabling its direct application to various tasks with little to no need for fine-tuning or structural modifications.</p> <p>However, research on leveraging GPT-like models to aid TTS systems is very limited. Stephenson et al. (2021) explores the potential of improving speech synthesis naturalness by text input lookahead with GPT prediction. Such an approach potentially restricts TTS applications, as altering the input is often undesirable. Furthermore, the findings were not verified by human subjective evaluation. Saito et al. (2023) suggest employing ChatGPT to aid in empathetic dialogue speech synthesis by extracting the context of conversations. They particularly instruct ChatGPT to produce three key words that encapsulate the intention, emotion, and speaking Style of speech observed in the dialogue history. These keywords are subsequently utilized to train a speech synthesis model. However, due to the inaccessibility of ChatGPT to the public, the re searchers resort to processing ChatGPT\u2019s outputs with BERT to extract embeddings. This approach essentially positions ChatGPT as an alternative to manual annotation, yet it does not delve into investigating ChatGPT\u2019s internal representations and their potential impact on speech-related tasks.</p> <p>In our study, we selected LLaMA2 (2023), a GPT-like LM, for integration into our TTS system, motivated by its technological advancements and potential for di verse applications. LLaMA2 (2023) stands out as one of the largest publicly accessible LMs, rivaling proprietary models such as GPT3.5 (OpenAI et al., 2024) and PaLM (540B) (Chowdhery et al., 2022), and sur passes other open-source alternatives like MPT and Falcon (Almazrouei et al., 2023) in benchmark evaluations. Additionally, the novel architecture of LLaMA2 (2023) not only ensures enhanced security but also facilitates the extension of various down stream tasks (Touvron et al., 2023).</p> <p>Related research that employs LLaMA2 (2023) in speech and other multimodal tasks (Radhakrishnan et al., 2023; Zhang et al., 2023), coupled with the ongoing efforts to reduce computing costs associated with LLaMA2, underscores the model\u2019s significant research interest and its promising prospects in multimodal applications.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#3methodology","title":"3.Methodology","text":"<p>We propose leveraging semantic embeddings de rived from a GPT-like LM to improve TTS synthesis. In our work, LLaMA2 (2023) is employed as the GPT-like model, as elaborated in Sec.2.3, and [VIT../E2E/2021.06_VITS.md06_VITS.md) is utilized as the TTS model for generating audio from phoneme embeddings, as detailed in Sec.2.1. In essence, we extract semantic embeddings $E_{s}$ from the final hidden layer of LLaMA2 (2023) and integrate them with the original acoustic text embeddings $E_{a}$ of [VIT../E2E/2021.06_VITS.md06_VITS.md), forming enhanced text embeddings $E_{as}$ for speech synthesis. Specifically, either a global token or a sequence of tokens is used to encapsulate the semantic attributes of an input sentence for varying objectives. The distinctions between these two token types are further explicated in Sec.3.1.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#31semantic-embeddings-derived-from-llama2","title":"3.1.Semantic Embeddings Derived from LLaMA2","text":"<p>For each input sentence $s$, we extract information from the final hidden layer before the output of LLaMA2 (2023). Different strategies are employed to cre ate various tokens that serve as the semantic em bedding for the sentence.</p> <p>Let $E_{s}$ denote the semantic embedding of sentence $s$, and $H_{LLaMA}^F(s)$ represent the output of the LLaMA2 (2023) model for sentence $s$ at the final hidden layer $F$. Therefore, $E_{s}$ can be expressed as: </p> <p>$$     E_{s}=  H_{LLaMA}^F(s)\\tag{1} $$ </p> <p>Here, $H_{LLaMA}^F(s)$ is a vector that encapsulates the semantic representation of sentence $s$ after pro cessing through all layers of the LLaMA2 (2023), culminating in the final layer.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#formulation-for-global-tokens","title":"Formulation for Global Tokens","text":"<p>We explored five types of global tokens to represent the over arching semantic features of an input sentence, namely <code>[AVE]</code>, <code>[PCA]</code>, <code>[LAST]</code>, <code>[EIS_Word]</code>, and <code>[EIS_Sentence]</code>, with each strategy employing a single token.</p> <p>In the <code>[AVE]</code> strategy, the semantic token is de rived by calculating the average of all tokens\u2019 out put vectors for sentence $s$, formulated as: </p> <p>$$     E_{s}^{AVE}= \\dfrac{1}{n}\\sum_{i=1}^n H_{LLaMA}^F(s,i)\\tag{2}  $$</p> <p>Here, $E_{s}^{AVE}$ denotes the semantic token obtained using the <code>[AVE]</code> strategy, and $H_{LLaMA}^F(s,i)$ represents the output of the $i$ th token of sentence $s$ at the final hidden layer $F$ of LLaMA2, with $s$ comprising $n$ tokens.</p> <p>For the <code>[PCA]</code> strategy, we apply Principal Component Analysis to the output vectors of sentence sto extract principal components and rescale the mean of the PCA results according to the original data\u2019s value range. This rescaling ensures that the PCA-processed data maintains a scale consistent with the original data, preserving the relative importance of semantic information numerically. Formulated as: </p> <p>$$     E_{s}^{PCA}= \\text{PCArescale}(H_{LLaMA}^F(s)) \\tag{3} $$</p> <p>In the <code>[LAST]</code> strategy, the semantic token is obtained by selecting the last token from the output vector of sentence s, as shown in the formula: </p> <p>$$     E_{s}^{LAST}= H_{LLaMA}^F(s, n)\\tag{4}  $$</p> <p>where $H_{LLaMA}^F(s, n)$ refers to the representation of the last token of sentence $s$ after processing through all layers of LLaMA2 (2023) at the final layer.</p> <p>In the <code>[EIS_Word]</code> and <code>[EIS_Sentence]</code> strategies, unlike the above approaches that utilize the sentence itself for representation, we derive the semantic representation of sentence $s$ based on LLaMA2 (2023)\u2019s comprehension $u$. Adapted from Saito et al. (2023)\u2019s practice, we employ prompts as illustrated in 2a and 2b, respectively, to obtain LLaMA2 (2023)\u2019s understanding of sentence $s$ in terms of Emotion, Intention, and speaking Style, denoted as $u$, and calculate the average of this understanding\u2019s representation to serve as the semantic embedding.</p> <p>In the <code>[EIS_Word]</code> strategy, LLaMA2 (2023) is prompted to describe Emotion, Intention, and speaking Style with three separate words, resulting in the following formula for the final semantic token: </p> <p>$$     E_{s}^{\\text{EISWord}} = \\dfrac{1}{m} [\\sum_{i} H_{LLaMA}^{F}(u_E, i) + \\sum_j H_{LLaMA}^{F}(u_I, j) +\\sum_k H_{LLaMA}^{F}(u_S, k)] \\tag{5}  $$</p> <p>where $u_E$, $u_I$, $u_S$ are the representations of LLaMA2 (2023)\u2019s output expressing the sentence\u2019s Emotion, Intention, and speaking Style at the final hid den layer, respectively, with $i$, $j$, $k$ indicating the tokens of each output word, and m being the total number of these tokens.</p> <p>In the <code>[EIS_Sentence]</code> strategy, LLaMA2 (2023) is guided to describe its understanding of the input sentence\u2019s Emotion, Intention, and speaking Style with an easy-to-understand sentence, leading to the fol lowing formula for the final semantic token: </p> <p>$$     E_s^{\\text{EISSentence}} = \\dfrac{1}{m}\\sum_{i=1}^m H_{LLaMA}^{F}(u_{EIS}, i)\\tag{6}  $$</p> <p>where $u_{EIS}$ is the representation of LLaMA2 (2023)\u2019s output expressing the understanding of the original sentence at the final hidden layer, and $m$ is the total number of tokens in this sentence representation.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#formulation-for-sequential-tokens","title":"Formulation for Sequential Tokens","text":"<p>In the implementation of sequential tokens strategies, we focus on utilizing the complete set of tokens from the input sentence to represent its semantic in formation. Unlike the global token approaches, sequential tokens strategies encompass representations based on either text or phonemes, aiming to better align with the TTS model\u2019s potential em phasis on acoustic features. The mathematical representations for these two strategies are as follows:  Under the <code>[TEX]</code> strategy, we directly employ all tokens from the textual form of sentence $s$ to represent its semantic information. If the output of sentence $s$ at the final hidden layer $F$ of LLaMA2 (2023) consists of $n$ tokens, then the semantic token $T_{s}^{TEX}$ is represented as a sequence: </p> <p>$$     E_s^{TEX}= {H_{LLaMA}^F(s,1), H_{LLaMA}^F(s, 2),\\cdots, H_{LLaMA}^F(s, n)} \\tag{7}  $$</p> <p>In the <code>[PHO]</code> strategy, we consider the complete set of tokens from the phonemic form. Here, $s_{pho}$ denotes the phonemic representation of sentence $s$. If the output of $s_{pho}$ at the final hidden layerF of LLaMA2 (2023) comprises $m$ tokens, then the semantic token $T_{s}^{PHO}$ is represented as a sequence: </p> <p>$$     E_{s}^{PHO}={H_{LLaMA}^F(s_{pho}, 1),H_{LLaMA}^F(s_{pho}, 2),\\cdots,H_{LLaMA}^F(s_{pho}, m)}\\tag{8}  $$</p> <p>In both strategies, $H_{LLaMA}^F(s, i)$ and $H_{LLaMA}^F(s_{pho}, i)$ respectively represent the outputs of the $i$ th token of sentence $s$ in its textual and phonemic forms at the final hidden layer $F$ of LLaMA2 (2023). This representation allows the TTS model to leverage the complete semantic information of a sentence, whether based on text or phonemes.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#32fusing-semantic-embeddings-with-acoustic-embeddings","title":"3.2.Fusing Semantic Embeddings with Acoustic Embeddings","text":"<p>To align the dimensions of semantic embedding extracted from LLaMA2 (2023), denoted as $E_{s}$, with the acoustic embeddings from [VIT../E2E/2021.06_VITS.md06_VITS.md), denoted as $E_{a}$, we employ a linear projection. The original dimension of $E_{s}$, $d_{LLaMA}$, is projected to match the dimension of VITS (2021) acoustic embedding, $d_{VITS}$, using a linear transformation matrix $W$ of dimensions $d_{VITS}\\times d_{LLaMA}$. The projected semantic embedding, $E_s'$, is calculated as follows: </p> <p>$$     E_s'= W \\cdot E_{s} \\tag{9}  $$ </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#fusing-global-embedding-with-acoustic-embedding","title":"Fusing Global Embedding with Acoustic Embedding","text":"<p>To obtain an embedding $E_{as}$ that integrates both semantic and acoustic information, for global tokens, we simply add the dimensionally unified global embedding to VITS (2021)\u2019s acoustic em bedding, as shown in the equation: </p> <p>$$     E_{as} = E_{a} + E_s\u2032\\tag{10} $$ </p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#fusing-sequential-embeddings-to-enhance-text-embeddings","title":"Fusing Sequential Embeddings to Enhance Text Embeddings","text":"<p>We utilize the Scaled Dot Product Attention mechanism to merge sequential embeddings with VITS (2021)\u2019s original acoustic embedding to gain enhanced embedding $E_{as}$ , which can be described by the following mathematical formulas:  First, calculate the attention scores $A$:</p> <p>$$     A = \\dfrac{q\\cdot k^{\\mathsf{T}}}{\\gamma} \\tag{11} $$</p> <p>where $q$ is the acoustic embedding $E_{a}$ in VITS (2021) with dimensions $[b, t, d]$;  $k$ and $v$ denotes the semantic embedding $E_s'$ from LLaMA2 (2023), also with dimensions $[b, t, d]$;  $b$ is the batch size,tis the sequence length, and $d$ is the embedding dimension; $\\gamma$ is temperature for scaling. $k^{\\mathsf{T}}$ denotes the transpose of $k$, transforming $k$ from $[b, t, d]$ to $[b, d, t]$ for matrix multiplication. The resulting $A$ has dimensions $[b, t, t]$.</p> <p>If a source mask or target mask is present, a masking operation is applied, setting the attention scores at masked positions to a very low value (e.g.,\u22126e4) to nearly eliminate their weight contribution in the subsequent softmax step.</p> <p>Next, apply the softmax function and dropout to the attention scores, obtaining the final attention weights $W_{attn}$: </p> <p>$$     W_{attn}= \\text{Dropout}(\\text{Softmax}(A))\\tag{12}  $$</p> <p>Finally, the output $E_{as}$ is calculated by weighting $v$ with the attention weights: </p> <p>$$     E_{as} = W_{attn}\\cdot v $$ </p> <p>The output $E_{as}$ , viewed as text embedding fused with semantic information, has dimensions $[b, t, d]$ that match those of $q$.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#4experiments","title":"4.Experiments","text":""},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#41experimental-settings","title":"4.1.Experimental Settings","text":"<p>We propose LLaMA-VITS which uses semantic to kens derived from LLaMA2 (2023) to enhance acoustic embedding in [VIT../E2E/2021.06_VITS.md06_VITS.md) for better TTS performance. To show the effectiveness of our method, we experimented with two baseline models. In the ORI-VITS baseline, we use the original VITS (2021) without external semantic information. In the BERT-VITS baseline, we extract various semantic tokens according to for mer research introduced in Section \u00a7 2.2. Specifically, we use the <code>[CLS]</code> token of BERT as the global token. To form the baseline of the sequential token in BERT, we use all the tokens in the sentence trained by text or phoneme, named <code>[BERT_TEX]</code> and <code>[BERT_PHO]</code>, respectively. In our proposed LLaMA-VITS, we derive global token <code>[AVE]</code>, <code>[LAST]</code>, <code>[PCA]</code>, <code>[EIS_Word]</code>, and <code>[EIS_Sentence]</code>, and sequential tokens <code>[TEX]</code> and <code>[PHO]</code> from LLaMA2 (2023), cor responding to those in BERT-VITS. We use LLaMA2 (2023) (13b) to generate semantic embeddings of dimension 5120. <code>[CLS]</code> and <code>[BERT_TEX]</code> tokens are extracted from BERT-base-uncased model which has a parameter size of 110M that generates token embedding of 768 dimensions. <code>[BERT_PHO]</code> token is extracted from BERT-x-phone-base model whose parameter size is 88M to generate token embedding of 768 dimensions.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#global-token-extraction","title":"Global Token Extraction","text":"<p>In our proposed LLaMA-VITS, global strategy <code>[LAST]</code> only uses the last to ken in the final hidden layer of LLaMA2 (2023) for each sentence. <code>[AVE]</code> uses the average of all tokens for each sentence. <code>[PCA]</code> uses the concatenation of all tokens whose dimension was reduced by Principal Component Analysis (PCA). <code>[EIS_Word]</code> and <code>[EIS_Sentence]</code> use the average of tokens for an answer, which is formed in three words or a sentence by prompts shown in Fig.02, to describe the Emotion, Intention, and speaking Style of the transcript. In BERT-VITS baseline, global strategy <code>[CLS]</code> only uses the first token from the BERT-base-uncased model for each input sentence.</p> <p></p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#sequential-token-extraction","title":"Sequential Token Extraction","text":"<p>In our proposed LLaMA-VITS, sequential strategy <code>[TEX]</code> concatenates the sequence of tokens in a sentence generated by LLaMA2 (2023) using text input. <code>[PHO]</code> concatenates the sequence of tokens of a sentence generated by LLaMA2 (2023) using phonemic input. In the baseline BERT-VITS, sequential strategy <code>[BERT_TEX]</code> concatenates all the tokens in a sentence extracted from BERT-base-uncased model. <code>[BERT_PHO]</code> concatenates all the tokens in a sentence extracted from BERT-x-phone-base model.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#datasets","title":"Datasets","text":"<p>We utilized full LJSpeech, 1-hour LJSpeech, and EmoV_DB_bea_sem dataset for experimental verification. LJSpeech4comprises 24 hours recorded of English speech by sin gle female speaker, where we evaluate how the embeddings extracted from LLaMA2 (2023) can help improve the speech naturalness.Besides full LJSpeech dataset, we also randomly filtered 1 hour LJSpeech which contains only 1-hour records as an ablation study to show how dataset size influences. EmoV_DB5(Adigwe et al., 2018) is a database of emotional speech that contains data for male and female actors in English and French. EmoV_DB covers 5 emotion classes, amused, an gry, disgusted, neutral, and sleepy. To factor out the effect of different speakers, we filtered the original EmoV_DB dataset into the speech of a specific female English speaker, bea. Then we use LLaMA2 (2023) to predict the emotion label of the transcript cho sen from the above 5 emotion classes, and select the audio samples which has the same predicted emotion. The filtered dataset contains 22.8-min records for training. We named the filtered dataset EmoV_DB_bea_sem and investigated how the semantic embeddings from LLaMA2 (2023) behave in naturalness and expressiveness on it. Please refer to Appendix A 12 for more dataset statistics.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#implementation-hyper-parameters-training","title":"Implementation, Hyper-parameters, Training","text":"<p>Our LLaMA-VITS system was built on the VITS (2021) framework using its original implementation, augmented with semantic embeddings de rived from LLaMA2 (2023) (Touvron et al., 2023) using its original implementation7. For training LJSpeech, we use the public configs in the original implementation of VITS (2021). For EmoV_DB_bea_sem, we use the same config as LJSpeech but changed batch size from 64 to 16 since this dataset is much smaller. Besides implementing our proposed LLaMA-VITS, we extracted corresponding semantic tokens <code>[CLS]</code>, <code>[BERT_TEX]</code> from BERT-uncased-base model and <code>[BERT_PHO]</code> from BERT pre-trained on phoneme for comparison. In comparing the experimental results, we choose 100k-step results on both full LJSpeech and 1-hour LJSpeech datasets since they are rather large. On EmoV_DB_bea_sem, we used the pre-trained checkpoint of LJSpeech on 100k step and compare the fine-tuning results on EmoV_DB_bea_sem at 150k-step since it is rather small.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Both subjective and objective metrics are implemented for a comprehensive evaluation. In subjective evaluation, we con duct Emotion Similarity Mean Opinion Score (ES MOS) (Zhu et al., 2023) experiments to evaluate emotion similarity for EmoV_DB_bea_sem.In the subjective evaluation, we compared <code>[AVE]</code>, <code>[TEX]</code> and <code>[PHO]</code> strategies in our LLaMA-VITS with the corresponding token <code>[CLS]</code>, <code>[BERT_TEX]</code> and <code>[BERT_PHO]</code> extracted from different BERT models and the baseline ORI-VITS who does not con tain semantic tokens, with the ground truth samples GT.</p> <p>In evaluating ESMOS, we randomly chose 5 samples from the total 51 test samples proportionally divided by us and received 100 test results from different speakers on Amazon Mechanical Turk. The result significance level is thus 500. Each participant is asked to give a score on emotion similarity compared with ground truth in a 5-scale: Excellent Match 5, Good Match 4, Fair Match 3, Poor Match 2, Bad Match 1.</p> <p>In objective evaluation,we utilize UTokyo-SaruLab Mean Opinion Score (UTMOS) (Saeki et al., 2022), Mel-Cepstral Distortion (MCD), and speech recognition performance measured by Character Error Rate (CER) and Word Error Rate (WER). UTMOS is a MOS prediction network using speech samples from previous Blizzard Challenges and Voice Conversion Challenges, which has reached the best performance in VoiceMOS Challenge 2022. We evaluate objective intelligibility by using Whisper-large (Radford et al., 2022). For calculating UTMOS, we use the implementation in SpeechMOS. For calculating MCD and ASR, we use the evaluation implementation of ESPnet (Hayashi et al., 2020, 2021).</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#5experiment-results","title":"5.Experiment Results","text":"<p>We evaluated our proposed LLaMA-VITS along with baselines ORI-VITS and BERT-VITS models on three distinct datasets: the full LJSpeech, the 1 hour LJSpeech, and EmoV_DB_bea_sem. The experimental outcomes provide a comprehensive understanding of the model performance and the impact of semantic tokens selection. A summary of these results is articulated below and can be referenced in Table 1.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#51results-on-full-ljspeech","title":"5.1.Results on full LJSpeech","text":"<p>The ORI-VITS baseline, achieving a UTMOS of 4.19 \u00b1 0.05, an MCD of7.32 \u00b1 0.61, a CER of6.2, and a WER of 16.5. Enhancements were observed with the BERT-VITS baseline.Specifically, BERT-VITS with <code>[BERT_TEX]</code> semantic tokens demonstrated superior performance in UTMOS (4.22\u00b10.05) and MCD (7.27 \u00b1 0.61), indicating improved speech quality and reduced mel-cepstral distortion. Additionally, a reduced CER of5.9and WER of15.9were noted, highlighting enhanced automatic speech recognition accuracy. Our proposed LLaMA-VITS, integrating various global and sequential semantic tokens, displayed competitive performance.The <code>[PCA]</code> strategy stood out, achieving an MCD of7.23 \u00b1 0.61, indicating optimal mel-cepstral distortion.The <code>[EIS_Sentence]</code>, <code>[AVE]</code>, and <code>[LAST]</code> tokens yielded a top-tier UTMOS of4.21\u00b10.04/0.05, underscoring their effectiveness in enhancing perceived speech quality.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#52results-on-1-hour-ljspeech","title":"5.2.Results on 1-hour LJSpeech","text":"<p>In the more challenging 1-hour LJSpeech dataset, all models experienced a slight performance de crease, an expected outcome given the reduced training data size. BERT-VITS baseline with <code>[CLS]</code> tokens exhibited notable MCD performance (7.39 \u00b1 0.62), while the <code>[BERT_PHO]</code> excelled in UTMOS (4.05 \u00b1 0.07), reflecting enhanced speech naturalness and reduced mel-cepstral distortion. LLaMA-VITS with <code>[AVE]</code> tokens achieved the high est UTMOS (4.10 \u00b1 0.07), while <code>[EIS_Sentence]</code> tokens resulted in the most favorable MCD (7.36 \u00b1 0.59), illustrating the model\u2019s versatility and efficacy in different token configurations.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#53results-on-emov_db_bea_sem","title":"5.3.Results on EmoV_DB_bea_sem","text":"<p>On this even more challenging dataset, a small improvement observed in BERT-VITS only exists in the <code>[BERT_TEX]</code> with a CER of 4.4. While our proposed LLaMA-VITS displayed no table enhancements. The <code>[TEX]</code> strategy achieves an ESMOS of3.22 \u00b1 0.07, indicating much more emotiveness. The <code>[LAST]</code> yielded the best performance on CER of4.3and WER of17.4, other strategies also perform better than or comparable to BERT-VITS, underscoring its effectiveness in enhancing perceived speech expressiveness.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#54analysis","title":"5.4.Analysis","text":"<p>Speaking of the strengths of different tokens, BERT based tokens generally contribute to improving MCD and ASR scores, indicating the enriched semantic understanding translated to speech qual ity. Tokens of LLaMA-VITS exhibited a balanced performance across all metrics, with specific to ken configurations excelling in particular aspects. For instance, <code>[PCA]</code> token emerged as a strong contender in reducing MCD, <code>[AVE]</code> enhanced the UTMOS scores, <code>[TEX]</code> had superior performance to improve ESMOS score. In individual comparisons, LLaMA-VITS\u2019s five global tokens generally outperformed BERT-VITS on the UTMOS metric for naturalness. In the ESMOS metric for emotional expression, LLaMA-VITS\u2019s two sequential tokens also generally sur passed BERT-VITS, particularly the <code>[TEX]</code> token. Therefore, we can infer that GPT-like LMs may have greater potential for TTS tasks than BERT like models. Further, our results reflect different patterns of gains from GPT-like and BERT-like models in TTS tasks. For instance, in the UTMOS naturalness metric, LLaMA-VITS\u2019s global tokens often outperformed sequential tokens, which is the opposite for BERT-VITS; in the ESMOS emotion metric, LLaMA-VITS\u2019s sequential token <code>[TEX]</code> significantly outperformed other tokens, while for BERT-VITS, global tokens performed better. Overall, LLaMA-VITS showed a different pattern in UTMOS compared to BERT-VITS, and superior performance in ESMOS. These results highlight the potential for further exploration of semantic to ken types and fusion methods to achieve more significant enhancements in speech synthesis, particularly in scenarios constrained by limited and complex training data.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#6discussion","title":"6.Discussion","text":"<p>In this section, we discuss factors influencing current outcomes.Based on this discussion, we also point out the directions for future work in Appendix 13.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#61gpt-like-vs-bert-like","title":"6.1.GPT-like vs BERT-like","text":"<p>Initial observations from our experiments indicate that, even without any fine-tuning of LLaMA2 (2023), LLaMA-VITS significantly outperforms both BERT-VITS and ORI-VITS in terms of emotional expressive ness. This finding opens up avenues for future research into emotive TTS tasks. Furthermore, a comparison between BERT-VITS and LLaMA-VITS highlights their distinct performance traits. BERT-VITS, leveraging deep con textual embeddings, provides profound semantic insights yet encounters challenges in customization and adaptability across a range of TTS tasks. Conversely, LLaMA-VITS can provide a more versa tile and adaptable approach, with its array of token types demonstrating particular advantages across various evaluation metrics.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#62semantic-token-strategy","title":"6.2.Semantic Token Strategy","text":"<p>The varying effectiveness of distinct semantic to kens underscores the importance of careful selection and integration tailored to the particular goals of TTS systems. Optimizing the type of token and method of fusion can be instrumental in enhancing aspects such as speech naturalness, emotional expressiveness, Mel Cepstral Distortion (MCD), or Automatic Speech Recognition (ASR) performance.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#7conclusion","title":"7.Conclusion","text":"<p>In summary, this study exemplifies a significant stride towards optimized TTS synthesis by integrating semantic tokens, leveraging the strengths of LLaMA-VITS. Our findings, validated by comprehensive experiments on the LJSpeech and EmoV_DB_bea_sem datasets, underscore the pivotal role of semantic embeddings in enhancing speech quality, naturalness, and emotiveness. The adaptability and efficacy of LLaMA-VITS, especially, open new vistas for customized and context sensitive TTS applications.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.10_LLaMA-VITS/#8limitations","title":"8.Limitations","text":"<p>Compared with our baseline which uses different BERT models, we only tested our method using LLaMA2 (2023). As Kenter et al. (2020) indicate for their BERT-based TTS model, small BERT models work better than big ones, but the parameter size of our proposed GPT-based TTS influence is yet stud ied by our research. Although BERT-based TTS models are normally finetuned on speech tasks to provide more explicit acoustic information for TTS, we didn\u2019t try designing prompts to generate acoustic features and only studied how general semantic information can help. Our experiments were conducted only on clean datasets with limited size, and the effect on more complex datasets is to be further explored. The integration of LLaMA2 (2023)\u2019s embeddings introduces additional computational costs, potentially limiting real-time applications.</p> <p>\u76f8\u6bd4\u4e8e\u57fa\u7ebf\u6a21\u578b\u4f7f\u7528\u4e86\u4e0d\u540c\u7684 BERT \u6a21\u578b, \u6211\u4eec\u7684\u65b9\u6cd5\u4ec5\u4f7f\u7528\u4e86 LLaMA2.</p>"},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/","title":"FlashSpeech: Efficient Zero-Shot Speech Synthesis","text":"<p>@import \"../../style.less\"</p> \u57fa\u672c\u4fe1\u606f  - \u6807\u9898: \"FlashSpeech: Efficient Zero-Shot Speech Synthesis\" - \u4f5c\u8005:   - [Zhen Ye](../../Authors/Zhen_Ye.md)   - [\u741a\u6cfd\u8c26](../../Authors/\u741a\u6cfd\u8c26_(Zeqian_Ju).md)   - [Haohe Liu](../../Authors/Haohe_Liu.md)   - [\u8c2d\u65ed](../../Authors/\u8c2d\u65ed_(Xu_Tan).md)   - [Jianyi Chen](../../Authors/Jianyi_Chen.md)   - [Yiwen Lu](../../Authors/Yiwen_Lu.md)   - [Peiwen Sun](../../Authors/Peiwen_Sun.md)   - [Jiahao Pan](../../Authors/Jiahao_Pan.md)   - [Weizhen Bian](../../Authors/Weizhen_Bian.md)   - [Shulin He](../../Authors/Shulin_He.md)   - [\u67f3\u5d0e\u5cf0](../../Authors/\u67f3\u5d0e\u5cf0_(Qifeng_Liu).md)   - [\u90ed\u6bc5\u53ef](../../Authors/\u90ed\u6bc5\u53ef_(Yike_Guo).md)   - [\u96ea\u5dcd](../../Authors/\u96ea\u5dcd_(Wei_Xue).md) - \u673a\u6784:   - [\u9999\u6e2f\u79d1\u6280\u5927\u5b66](../../Institutions/HKUST.md)   - [\u5fae\u8f6f](../../Institutions/Microsoft.md)   - [\u8428\u91cc\u5927\u5b66](../../Institutions/University_of_Surrey.md)   - [\u5185\u8499\u53e4\u5927\u5b66](../../Institutions/Inner_Mongolia_University.md)   - [\u65b0\u52a0\u5761\u56fd\u7acb\u5927\u5b66](../../Institutions/National_University_of_Singapore.md) - \u65f6\u95f4:   - 2024.04.23 v1   - 2024.04.25 v3 - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2404.14700)   - [Demo](https://flashspeech.github.io/) - \u6807\u7b7e:   - \u8bed\u97f3\u5408\u6210   - \u96f6\u6837\u672c","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"\u539f\u6587  &gt; Recent progress in large-scale zero-shot speech synthesis has been significantly advanced by language models and diffusion models. &gt; However, the generation process of both methods is slow and computationally intensive. &gt; Efficient speech synthesis using a lower computing budget to achieve quality on par with previous work remains a significant challenge. &gt; In this paper, we present FlashSpeech, a large-scale zero-shot speech synthesis system with approximately 5\\% of the inference time compared with previous work. &gt; FlashSpeech is built on the latent consistency model and applies a novel adversarial consistency training approach that can train from scratch without the need for a pre-trained diffusion model as the teacher. &gt; Furthermore, a new prosody generator module enhances the diversity of prosody, making the rhythm of the speech sound more natural. &gt; The generation processes of FlashSpeech can be achieved efficiently with one or two sampling steps while maintaining high audio quality and high similarity to the audio prompt for zero-shot speech generation. &gt; Our experimental results demonstrate the superior performance of FlashSpeech. &gt; Notably, FlashSpeech can be about 20 times faster than other zero-shot speech synthesis systems while maintaining comparable performance in terms of voice quality and similarity. &gt; Furthermore, FlashSpeech demonstrates its versatility by efficiently performing tasks like voice conversion, speech editing, and diverse speech sampling. &gt; Audio samples can be found in\u00a0[this https URL](https://flashspeech.github.io/).   <p>\u8fd1\u671f, \u5927\u578b\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u5728\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7684\u63a8\u52a8\u4e0b\u53d6\u5f97\u663e\u8457\u8fdb\u5c55. \u7136\u800c, \u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u751f\u6210\u8fc7\u7a0b\u7f13\u6162\u4e14\u8ba1\u7b97\u5bc6\u96c6. \u5728\u8f83\u4f4e\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u548c\u4e4b\u524d\u5de5\u4f5c\u76f8\u5ab2\u7f8e\u7684\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218. \u672c\u5de5\u4f5c\u4ecb\u7ecd\u4e86 FlashSpeech, \u4e00\u4e2a\u4ec5\u9700\u5148\u524d\u5de5\u4f5c\u63a8\u7406\u65f6\u95f4 5% \u7684\u5927\u578b\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7cfb\u7edf. FlashSpeech \u5efa\u7acb\u5728\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b\u4e4b\u4e0a, \u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u4e00\u81f4\u6027\u8bad\u7ec3\u65b9\u6cd5, \u80fd\u591f\u4ece\u96f6\u8bad\u7ec3\u800c\u65e0\u9700\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08. \u6b64\u5916, \u4e00\u4e2a\u65b0\u7684\u97f5\u5f8b\u751f\u6210\u5668\u6a21\u5757\u589e\u5f3a\u4e86\u97f5\u5f8b\u591a\u6837\u6027, \u4f7f\u5f97\u8bed\u97f3\u7684\u8282\u594f\u542c\u8d77\u6765\u66f4\u81ea\u7136. FlashSpeech \u7684\u751f\u6210\u8fc7\u7a0b\u80fd\u591f\u901a\u8fc7\u4e00\u4e24\u4e2a\u91c7\u6837\u6b65\u9aa4\u9ad8\u6548\u5b9e\u73b0, \u540c\u65f6\u4fdd\u6301\u9ad8\u97f3\u8d28\u4e0e\u96f6\u6837\u672c\u8bed\u97f3\u751f\u6210\u65f6\u53c2\u8003\u97f3\u9891\u7684\u9ad8\u76f8\u4f3c\u6027. \u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bf4\u660e\u4e86 FlashSpeech \u7684\u4f18\u8d8a\u6027. \u503c\u5f97\u6ce8\u610f\u7684\u662f, FlashSpeech \u76f8\u6bd4\u5176\u4ed6\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u5feb 20 \u500d, \u4f46\u5728\u8bed\u97f3\u8d28\u91cf\u548c\u76f8\u4f3c\u5ea6\u65b9\u9762\u4fdd\u6301\u4e86\u4e0e\u4e4b\u524d\u5de5\u4f5c\u76f8\u5f53\u7684\u6027\u80fd. \u6b64\u5916, FlashSpeech \u7684\u591a\u6837\u6027\u4f7f\u5176\u80fd\u591f\u9ad8\u6548\u5b8c\u6210\u8bf8\u5982\u8bed\u97f3\u8f6c\u6362\u3001\u8bed\u97f3\u7f16\u8f91\u548c\u591a\u6837\u5316\u8bed\u97f3\u91c7\u6837\u7b49\u4efb\u52a1. \u97f3\u9891\u793a\u4f8b\u53ef\u4ee5\u5728\u6b64\u94fe\u63a5\u4e2d\u627e\u5230.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>In recent years, the landscape of speech synthesis has been transformed by the advent of large-scale generative models. Consequently, the latest research efforts have achieved notable advancements in zero-shot speech synthesis systems by significantly increasing the size of both datasets and models. Zero-shot speech synthesis, such as Text-to-Speech (TTS), Voice Conversion (VC) and Editing, aims to generate speech that incorporates unseen speaker characteristics from a reference audio segment during inference, without the need for additional training. Current advanced zero-shot speech synthesis systems typically leverage Language Models (LMs) for in-context speech generation on the large-scale dataset.</p> <p>\u8fd1\u5e74\u6765, \u968f\u7740\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u51fa\u73b0, \u8bed\u97f3\u5408\u6210\u9886\u57df\u53d1\u751f\u4e86\u5de8\u5927\u8f6c\u53d8. \u56e0\u6b64, \u6700\u65b0\u7684\u7814\u7a76\u5de5\u4f5c\u901a\u8fc7\u663e\u8457\u589e\u52a0\u6570\u636e\u96c6\u548c\u6a21\u578b\u5927\u5c0f, \u5728\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55. \u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210, \u5982\u6587\u672c\u8f6c\u8bed\u97f3, \u58f0\u97f3\u8f6c\u6362, \u7f16\u8f91, \u65e8\u5728\u5728\u63a8\u7406\u65f6\u5c06\u672a\u89c1\u8fc7\u7684\u53d1\u8a00\u4eba\u7279\u5f81\u5d4c\u5165\u5230\u8bed\u97f3\u4e2d, \u800c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3. \u5f53\u524d\u5148\u8fdb\u7684\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u7c7b\u6a21\u578b\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e0a\u4e0b\u6587\u6587\u672c\u751f\u6210.</p> <p>However, the generation process of these methods needs a long-time iteration. For example, VALL-E (2023) builds on the language model to predict 75 audio token sequences for a 1-second speech, in its first-stage AutoRegressive (AR) token sequence generation. When using a Non-AutoRegressive (NAR) Latent Diffusion Model based framework, NaturalSpeech2 (2023) still requires 150 sampling steps. As a result, although these methods can produce human-like speech, they require significant computational time and cost.</p> <p>\u7136\u800c, \u8fd9\u4e9b\u65b9\u6cd5\u7684\u751f\u6210\u8fc7\u7a0b\u9700\u8981\u5f88\u957f\u65f6\u95f4\u7684\u8fed\u4ee3. \u4f8b\u5982, VALL-E (2023) \u5728\u7b2c\u4e00\u9636\u6bb5\u7684\u81ea\u56de\u5f52 Token \u5e8f\u5217\u751f\u6210\u8fc7\u7a0b\u4e2d, \u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u9884\u6d4b 1 \u79d2\u8bed\u97f3\u5bf9\u5e94\u7684 75 \u4e2a\u97f3\u9891 token \u5e8f\u5217. \u5f53\u4f7f\u7528\u57fa\u4e8e\u975e\u81ea\u56de\u5f52\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\u65f6, NaturalSpeech2 (2023) \u4ecd\u7136\u9700\u8981 150 \u4e2a\u91c7\u6837\u6b65\u9aa4. \u56e0\u6b64, \u5c3d\u7ba1\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u8bed\u97f3, \u4f46\u5b83\u4eec\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u65f6\u95f4\u548c\u6210\u672c.</p> <p>Some efforts have been made to accelerate the generation process. Voicebox (2023) adopts Flow-Matching (2022) so that fewer sampling steps (Number of function evaluations (NFE): 64) can be achieved because of the optimal transport path. ClaM-TTS (2024) proposes a mel-codec with a superior compression rate and a latent language model that generates a stack of tokens at once. Although the slow generation speed issue has been somewhat alleviated, the inference speed is still far from satisfactory for practical applications. Moreover, the substantial computational time of these approaches leads to significant computational cost overheads, presenting another challenge.</p> <p>\u5df2\u7ecf\u6709\u4e00\u4e9b\u5de5\u4f5c\u5c1d\u8bd5\u52a0\u901f\u751f\u6210\u8fc7\u7a0b. - Voicebox (2023) \u91c7\u7528\u6d41\u5339\u914d (Flow Matching, FM), \u7531\u4e8e\u6700\u4f18\u4f20\u8f93\u8def\u5f84\u53ef\u4ee5\u5b9e\u73b0\u8f83\u5c11\u7684\u91c7\u6837\u6b65\u9aa4. - ClaM-TTS (2024) \u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u4f18\u538b\u7f29\u7387\u7684 mel-codec, \u4ee5\u53ca\u4e00\u4e2a\u6f5c\u5728\u8bed\u8a00\u6a21\u578b, \u4e00\u6b21\u751f\u6210\u4e00\u7cfb\u5217 token. \u5c3d\u7ba1\u6162\u901f\u751f\u6210\u901f\u5ea6\u95ee\u9898\u6709\u6240\u7f13\u89e3, \u4f46\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u6765\u8bf4, \u63a8\u7406\u901f\u5ea6\u4ecd\u7136\u4e0d\u8db3. \u6b64\u5916, \u8fd9\u4e9b\u65b9\u6cd5\u7684\u5927\u91cf\u8ba1\u7b97\u65f6\u95f4\u5bfc\u81f4\u4e86\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500, \u8fd9\u53c8\u662f\u4e00\u4e2a\u6311\u6218.</p> <p>The fundamental limitation of speech generation stems from the intrinsic mechanisms of language models and diffusion models, which require considerable time either auto-regressively or through a large number of denoising steps. Hence, the primary objective of this work is to accelerate inference speed and reduce computational costs while preserving generation quality at levels comparable to the prior research.</p> <p>\u8bed\u97f3\u751f\u6210\u7684\u6839\u672c\u9650\u5236\u5728\u4e8e\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7684\u5185\u5728\u673a\u5236, \u4ee5\u81ea\u56de\u5f52\u65b9\u5f0f\u6216\u9700\u8981\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\u4ece\u800c\u9700\u8981\u76f8\u5f53\u7684\u65f6\u95f4. \u56e0\u6b64, \u672c\u9879\u5de5\u4f5c\u7684\u4e3b\u8981\u76ee\u6807\u662f\u52a0\u901f\u63a8\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c, \u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u4e0e\u4e4b\u524d\u7814\u7a76\u6c34\u5e73\u76f8\u5f53.</p> <p>In this paper, we propose FlashSpeech as the next step towards efficient zero-shot speech synthesis. To address the challenge of slow generation speed, we leverage the Latent Consistency Model (LCM) (2023), a recent advancement in generative models. Building upon the previous non-autoregressive TTS system (NaturalSpeech2 (2023)), we adopt the encoder of a neural audio codec to convert speech waveforms into latent vectors as the training target for our LCM. To train this model, we propose a novel technique called adversarial consistency training, which utilizes the capabilities of pre-trained speech language models (WavLM; HuBERT (2021); Wav2Vec2.0 (2020)) as discriminators. This facilitates the transfer of knowledge from large pre-trained speech language models to speech generation tasks, efficiently integrating adversarial and consistency training to improve performance. The LCM is conditioned on prior vectors obtained from a phoneme encoder, a prompt encoder, and a prosody generator. Furthermore, we demonstrate that our proposed prosody generator leads to more diverse expressions and prosody while preserving stability.</p> <p>\u672c\u6587\u63d0\u51fa\u4e86 FlashSpeech \u4f5c\u4e3a\u9ad8\u6548\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7684\u4e0b\u4e00\u6b65. \u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u901f\u5ea6\u6162\u7684\u95ee\u9898, \u6211\u4eec\u91c7\u7528\u4e86\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b (Latent Consistency Model, LCM), \u8fd9\u662f\u8fd1\u5e74\u6765\u751f\u6210\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55. \u57fa\u4e8e\u5148\u524d\u975e\u81ea\u56de\u5f52\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf NaturalSpeech2 (2023), \u6211\u4eec\u91c7\u7528\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684\u7f16\u7801\u5668, \u5c06\u8bed\u97f3\u6ce2\u5f62\u8f6c\u6362\u4e3a\u9690\u5411\u91cf, \u4f5c\u4e3a LCM \u7684\u8bad\u7ec3\u76ee\u6807. \u4e3a\u4e86\u8bad\u7ec3\u8be5\u6a21\u578b, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u540d\u4e3a\u5bf9\u6297\u4e00\u81f4\u6027\u8bad\u7ec3 (Adversarial Consistency Training) \u7684\u6280\u672f, \u8be5\u6280\u672f\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u4f5c\u4e3a\u5224\u522b\u5668. \u8fd9\u6709\u52a9\u4e8e\u5c06\u77e5\u8bc6\u4ece\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u8fc1\u79fb\u5230\u8bed\u97f3\u751f\u6210\u4efb\u52a1\u4e2d, \u6709\u6548\u5730\u96c6\u6210\u5bf9\u6297\u6027\u548c\u4e00\u81f4\u6027\u8bad\u7ec3, \u63d0\u9ad8\u6027\u80fd. LCM \u5c06\u97f3\u7d20\u7f16\u7801\u5668, \u63d0\u793a\u7f16\u7801\u5668\u548c\u97f5\u5f8b\u751f\u6210\u5668\u83b7\u5f97\u7684\u5148\u9a8c\u5411\u91cf\u4f5c\u4e3a\u6761\u4ef6. \u6b64\u5916, \u6211\u4eec\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u97f5\u5f8b\u751f\u6210\u5668\u80fd\u591f\u4ea7\u751f\u66f4\u5177\u591a\u6837\u6027\u7684\u8868\u8fbe\u548c\u97f5\u5f8b, \u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027.</p> <p>Our contributions can be summarized as follows: - We propose FlashSpeech, an efficient zero-shot speech synthesis system that generates voice with high audio quality and speaker similarity in zero-shot scenarios. - We introduce adversarial consistency training, a novel combination of consistency and adversarial training leveraging pre-trained speech language models, for training the latent consistency model from scratch, achieving speech generation in one or two steps. - We propose a prosody generator module that enhances the diversity of prosody while maintaining stability. - FlashSpeech significantly outperforms strong baselines in audio quality and matches them in speaker similarity. Remarkably, it achieves this at a speed approximately 20 times faster than comparable systems, demonstrating unprecedented efficiency.</p> <p>\u6211\u4eec\u7684\u8d21\u732e\u53ef\u4ee5\u603b\u7ed3\u5982\u4e0b: - \u6211\u4eec\u63d0\u51fa FlashSpeech, \u4e00\u79cd\u9ad8\u6548\u7684\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7cfb\u7edf, \u80fd\u591f\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u751f\u6210\u9ad8\u97f3\u8d28\u548c\u53d1\u8a00\u4eba\u76f8\u4f3c\u5ea6\u7684\u8bed\u97f3. - \u6211\u4eec\u5f15\u5165\u5bf9\u6297\u4e00\u81f4\u6027\u8bad\u7ec3, \u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6297\u6027\u8bad\u7ec3\u548c\u4e00\u81f4\u6027\u8bad\u7ec3\u7684\u7ed3\u5408, </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#2related-work","title":"2.Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#21larger-scale-speech-synthesis","title":"2.1.Larger-Scale Speech Synthesis","text":"<p>Motivated by the success of the large language model, the speech research community has recently shown increasing interest in scaling the sizes of model and training data to bolster generalization capabilities, producing natural speech with diverse speaker identities and prosody under zero-shot settings. The pioneering work is VALL-E (2023), which adopts the EnCodec (2022) to discretize the audio waveform into tokens. Therefore, a language model can be trained via in-context learning that can generate the target utterance where the style is consistent with prompt utterance. However, generating audio in such an autoregressive manner (LM-VC (2023); VoiceCraft (2024) lead to unstable prosody, word skipping, and repeating issues (FastSpeech2; Tan et al.(2021); NaturalSpeech2 (2023) To ensure the robustness of the system, non-autoregressive methods such as NaturalSpeech2 (2023) and Voicebox (2023) utilize diffusion-style model (VP-diffusion Song et al.(2020) or Flow-Matching (2022)) to learn the distribution of a continuous intermediate vector such as mel-spectrogram or latent vector of codec. Both LM-based methods Zhao et al.(2023) and diffusion-based methods show superior performance in speech generation tasks. However, their generation is slow due to the iterative computation. Considering that many speech generation scenarios require real-time inference and low computational costs, we employ the latent consistency model for large-scale speech generation that inference with one or two steps while maintaining high audio quality.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#22acceleration-of-speech-synthesis","title":"2.2.Acceleration of Speech Synthesis","text":"<p>Since early neural speech generation models Tan et al.(2021) use autoregressive models such as Tacotron Wang et al.(2017) and TransformerTTS Li et al.(2019), causing slow inference speed, with $\\mathcal{O}(n)$ computation, whereNis the sequence length. To address the slow inference speed, FastSpeech (2019), FastSpeech2 proposes to generate a mel-spectrogram in a non-autoregressive manner. However, these models Ren et al.(2022) result in blurred and over-smoothed mel-spectrograms due to the regression loss they used and the capability of modeling methods. To further enhance the speech quality, diffusion models are utilized Popov et al.(2021a); Jeong et al.(2021); Popov et al.(2021b) which increase the computation to $\\mathcal{O}(T)$, where $T$ is the diffusion steps. Therefore, distillation techniques Luo (2023) for diffusion-based methods such as CoMoSpeech Ye et al.(2023), CoMoSVC Lu et al.(2024) and Reflow-TTS Guan et al.(2023) emerge to reduce the sampling steps back to $\\mathcal{O}(1)$, but require additional pre-trained diffusion as the teacher model. Unlike previous distillation techniques, which require extra training for the diffusion model as a teacher and are limited by its performance, our proposed adversarial consistency training technique can directly train from scratch, significantly reducing training costs. In addition, previous acceleration methods only validate speaker-limited recording-studio datasets with limited data diversity. To the best of our knowledge, FlashSpeech is the first work that reduces the computation of a large-scale speech generation system back to $\\mathcal{O}(1)$.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#23consistency-model","title":"2.3.Consistency Model","text":"<p>The consistency model is proposed in Song et al.(2023); Song and Dhariwal (2023) to generate high-quality samples by directly mapping noise to data. Furthermore, many variants Kong et al. (2023); Lu et al.(2023); Sauer et al.(2023); Kim et al.(2023a) are proposed to further increase the generation quality of images. The Latent Consistency Model (2023) is proposed by Luo et al. which can directly predict the solution of PF-ODE in latent space. However, the original LCM employs consistency distillation on the pre-trained Latent Diffusion Model (LDM) which leverages large-scale off-the-shelf image diffusion models Rombach et al.(2022). Since there are no pre-trained large-scale TTS models in the speech community, and inspired by the techniques Song and Dhariwal (2023); Kim et al.(2023a); Lu et al.(2023); Sauer et al.(2023); Kong et al.(2023), we propose the novel adversarial consistency training method which can directly train the large-scale latent consistency model from scratch utilizing the large pre-trained speech language model Chen et al.(2022b); HuBERT (2021); Baevski et al. (2020) such as WavLM for speech generation.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#3method","title":"3.Method\u00b7\u65b9\u6cd5","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#31overview","title":"3.1.Overview\u00b7\u603b\u89c8","text":"<p>Our work is dedicated to advancing the speech synthesis efficiency, achieving $\\mathcal{O}(1)$ computation cost while maintaining comparable performance to prior studies that require $\\mathcal{O}(T)$ or $\\mathcal{O}(N)$ computations.  The framework of the proposed method, FlashSpeech, is illustrated in Fig.02. FlashSpeech integrates a neural codec, an encoder for phonemes and prompts, a prosody generator, and an LCM, which are utilized during both the training and inference stages. Exclusively during training, a conditional discriminator is employed. FlashSpeech adopts the in-context learning paradigm VALL-E (2023), initially segmenting the latent vector $\\mathbf{z}$, extracted from the codec, into $\\mathbf{z}{target}$ and $\\mathbf{z}{prompt}$. Subsequently, the phoneme and $\\mathbf{z}_{prompt}$ are processed through the encoder to produce the hidden feature. A prosody generator then predicts pitch and duration based on the hidden feature. The pitch and duration embeddings are combined with the hidden feature and inputted into the LCM as the conditional feature. The LCM model is trained from scratch using adversarial consistency training. After training, FlashSpeech can achieve efficient generation within one or two sampling steps.</p> <p>\u672c\u9879\u5de5\u4f5c\u4e13\u6ce8\u4e8e\u63d0\u9ad8\u8bed\u97f3\u5408\u6210\u6548\u7387, \u5728\u4fdd\u6301\u548c\u5148\u524d\u5de5\u4f5c\u76f8\u8fd1\u6027\u80fd\u7684\u540c\u65f6, \u5c06 $\\mathcal{O}(T)$ \u6216 $\\mathcal{O}(N)$ \u8ba1\u7b97\u91cf\u964d\u4f4e\u5230 $\\mathcal{O}(1)$. \u56fe 02 \u5c55\u793a\u4e86\u6240\u63d0\u65b9\u6cd5 FlashSpeech \u7684\u6846\u67b6.</p> <p></p> <p>FlashSpeech \u7ee7\u627f\u4e86\u795e\u7ecf\u7f16\u89e3\u7801\u5668, \u97f3\u7d20\u548c\u63d0\u793a\u7684\u7f16\u7801\u5668, \u97f5\u5f8b\u751f\u6210\u5668, \u548c\u4e00\u4e2a\u6f5c\u5728\u4e00\u81f4\u6027\u6a21\u578b (Latent Consistency Model, LCM), \u5e76\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u4f7f\u7528\u5b83\u4eec. \u4ec5\u5728\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u6761\u4ef6\u5224\u522b\u5668 (Conditional Discriminator). FlashSpeech \u91c7\u7528\u4e86 VALL-E (2023) \u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u8303\u5f0f, \u9996\u5148\u5c06\u4ece\u7f16\u89e3\u7801\u5668\u63d0\u53d6\u51fa\u7684\u9690\u5411\u91cf $\\mathbf{z}$ \u5206\u5272\u4e3a $\\mathbf{z}{target}$ \u548c $\\mathbf{z}{prompt}$. \u7136\u540e, \u97f3\u7d20\u548c $\\mathbf{z}_{prompt}$ \u901a\u8fc7\u7f16\u7801\u5668\u4ea7\u751f\u9690\u7279\u5f81. \u97f5\u5f8b\u751f\u6210\u5668\u57fa\u4e8e\u9690\u7279\u5f81\u9884\u6d4b\u97f3\u9ad8\u548c\u65f6\u957f. \u97f3\u9ad8\u548c\u65f6\u957f\u5d4c\u5165\u548c\u9690\u7279\u5f81\u7ed3\u5408\u540e\u4f5c\u4e3a\u6761\u4ef6\u7279\u5f81\u8f93\u5165\u5230 LCM \u79cd. LCM \u6a21\u578b\u4f7f\u7528\u5bf9\u6297\u4e00\u81f4\u8bad\u7ec3\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3. \u8bad\u7ec3\u5b8c\u6210\u540e, FlashSpeech \u53ef\u4ee5\u5728\u4e00\u5230\u4e24\u4e2a\u91c7\u6837\u6b65\u5185\u5b9e\u73b0\u9ad8\u6548\u751f\u6210.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#32latent-consistency-model","title":"3.2.Latent Consistency Model","text":"<p>The consistency model (Song et al.(2023)) is a new family of generative models that enables one-step or few-step generation. </p> <p>\u4e00\u81f4\u6027\u6a21\u578b\u662f\u4e00\u7c7b\u65b0\u7684\u751f\u6210\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u4e00\u6b65\u6216\u51e0\u6b65\u751f\u6210.</p> <p>Let us denote the data distribution by $p_{data}(\\mathbf{x})$.  The core idea of the consistency model is to learn the function that maps any points on a trajectory of the PF-ODE to that trajectory\u2019s origin, which can be formulated as:</p> <p>\u6570\u636e\u5206\u5e03\u8bb0\u4e3a $p_{data}(\\mathbf{x})$. \u4e00\u81f4\u6027\u6a21\u578b\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5b66\u4e60\u4e00\u4e2a\u6620\u5c04\u51fd\u6570, \u5c06 PF-ODE \u8f68\u8ff9\u4e0a\u7684\u4efb\u610f\u70b9\u6620\u5c04\u5230\u8be5\u8f68\u8ff9\u7684\u8d77\u70b9, \u53ef\u4ee5\u8868\u793a\u4e3a:</p> <p>$$   f(\\mathbf{x}\\sigma, \\sigma) = \\mathbf{x}{\\sigma_{\\min}},\\tag{01} $$</p> <p>where $f(\\cdot,\\cdot)$ is the consistency function and $\\mathbf{x}{\\sigma}$ represents the data $\\mathbf{x}$ perturbed by adding zero-mean Gaussian noise with standard deviation $\\sigma$. $\\sigma{\\min}$ is a fixed small positive number.</p> <p>\u5176\u4e2d $f(\\cdot, \\cdot)$ \u662f\u4e00\u81f4\u6027\u51fd\u6570, $\\mathbf{x}{\\sigma}$ \u8868\u793a\u6570\u636e $\\mathbf{x}$ \u6dfb\u52a0\u96f6\u5747\u503c $\\sigma$ \u6807\u51c6\u5dee\u7684\u9ad8\u65af\u566a\u58f0\u6270\u52a8\u540e\u7684\u6570\u636e. $\\sigma{\\min}$ \u662f\u56fa\u5b9a\u7684\u5c0f\u6b63\u6570.</p> <p>Then $\\mathbf{x}{\\sigma{\\min}}$ can then be viewed as an approximation sample from the data distribution $p_{data}(\\mathbf{x})$. To satisfy property in Eq.01, following Song et al.(2023), we parameterize the consistency model as</p> <p>$\\mathbf{x}{\\sigma{\\min}}$ \u53ef\u4ee5\u89c6\u4e3a\u6570\u636e\u5206\u5e03 $p_{data}(\\mathbf{x})$ \u7684\u8fd1\u4f3c\u91c7\u6837. \u4e3a\u4e86\u6ee1\u8db3\u5982\u65b9\u7a0b 01 \u6240\u793a\u7684\u6027\u8d28, \u9075\u5faa Song et al., \u6211\u4eec\u5c06\u4e00\u81f4\u6027\u6a21\u578b\u53c2\u6570\u5316\u4e3a</p> <p>$$   f_{\\theta}(\\mathbf{x}\\sigma, \\sigma) = c{skip}(\\sigma)\\mathbf{x} + c_{out}(\\sigma) F_{\\theta}(\\mathbf{x}_{\\sigma}, \\sigma)\\tag{02} $$</p> <p>where $f_{\\theta}$ is to estimate consistency function $f$ by learning from data, $F_{\\theta}$ is a deep neural network with parameter $\\theta$, $c_{skip}(\\sigma)$ and $c_{out}(\\sigma)$ are differentiable functions with $c_{skip}(\\sigma_{\\min})=1$ and $c_{out}(\\sigma_{\\min})=0$ to ensure boundary condition.</p> <p>\u5176\u4e2d  - $f_{\\theta}$ \u901a\u8fc7\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5f97\u5230, \u7528\u4e8e\u4f30\u8ba1\u4e00\u81f4\u6027\u51fd\u6570 $f$, - $F_{\\theta}$ \u662f\u5177\u6709\u53c2\u6570 $\\theta$ \u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, - $c_{skip}(\\sigma)$ \u548c $c_{out}(\\sigma)$ \u662f\u53ef\u5fae\u51fd\u6570, \u4e14 $c_{skip}(\\sigma_{\\min})=1$ \u548c $c_{out}(\\sigma_{\\min})=0$ \u7528\u4e8e\u786e\u4fdd\u8fb9\u754c\u6761\u4ef6.</p> <p>A valid consistency model should satisfy the self-consistency property (Song et al.(2023))</p> <p>\u4e00\u4e2a\u6709\u6548\u7684\u4e00\u81f4\u6027\u6a21\u578b\u9700\u8981\u6ee1\u8db3\u81ea\u4e00\u81f4\u6027:</p> <p>$$   f_{\\theta}(\\mathbf{x}\\sigma, \\sigma) = f{\\theta}(\\mathbf{x}{\\sigma'}, \\sigma'),\\quad \\forall \\sigma, \\sigma' \\in [\\sigma{\\min}, \\sigma_{\\max}].\\tag{03} $$</p> <p>where $\\sigma_{\\max}=80$ and $\\sigma_{\\min}=0.002$ following Karras et al. (2022); Song et al. (2023); Song and Dhariwal (2023). Then the model can generate samples in one step by evaluating </p> <p>$$   \\mathbf{x}{\\sigma{\\min}} = f_{\\theta}(\\mathbf{x}{\\sigma{\\max}}, \\sigma_{\\max})\\tag{04} $$</p> <p>from distribution $\\mathbf{x}{\\sigma{\\max}}\\sim\\mathcal{N}(0,\\sigma_{\\max}^2 \\mathbf{I})$.</p> <p>As we apply a consistency model on the latent space of audio, we use the latent feature $\\mathbf{z}$ which are extracted prior to the residual quantization layer of the codec.</p> <p>$$   \\mathbf{z} = CodecEncoder(\\mathbf{y})\\tag{05} $$</p> <p>where $\\mathbf{y}$ is the speech waveform.</p> <p>Futhermore, we add the feature from the prosody generator and encoder as the conditional feature $c$, our objective has changed to achieve</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#33adversarial-consistency-training","title":"3.3.Adversarial Consistency Training","text":"<p>A major drawback of the LCM is that it needs to pre-train a diffusion-based teacher model in the first stage, and then perform distillation to produce the final model. This would make the training process complicated, and the performance would be limited as a result of the distillation. To eliminate the reliance on the teacher model training, in this paper, we propose a novel adversarial consistency training method to train LCM from scratch. Our training procedure is outlined in Fig.03, which has three parts:</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#331consistency-training","title":"3.3.1.Consistency Training","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#332adversarial-training","title":"3.3.2.Adversarial Training","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#333combined-together","title":"3.3.3.Combined Together","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#34prosody-generator","title":"3.4.Prosody Generator","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#341analysis-of-prosody-prediction","title":"3.4.1.Analysis of Prosody Prediction","text":"<p>Previous regression methods for prosody prediction (FastSpeech2; NaturalSpeech2 (2023)), due to their deterministic mappings and assumptions of unimodal distribution, often fail to capture the inherent diversity and expressiveness of human speech prosody. This leads to predictions that lack variation and can appear over-smoothed. On the other hand, diffusion methods (Voicebox (2023); Li et al.(2023)) for prosody prediction offer a promising alternative by providing greater prosody diversity. However, they come with challenges regarding stability, and the potential for unnatural prosody. Additionally, the iterative inference process in DMs requires a significant number of sampling steps that may also hinder real-time appli cation. Meanwhile, LM-based methods (Mega-TTS2; VALL-E (2023)) also need a long time for inference. To alleviate these issues, our prosody generator consists of a prosody regression module and a prosody refinement module to enhance the diversity of prosody regression results with efficient one-step consistency model sampling.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#342prosody-refinement-via-consistency-model","title":"3.4.2.Prosody Refinement via Consistency Model","text":"<p>As shown in 4, our prosody generator consists of two parts which are prosody regression and prosody refinement. We first train the prosody regression module to get a deterministic output. Next, we freeze the parameters of the prosody regression module and use the residual of ground truth prosody and deterministic predicted prosody as the training target for prosody refinement. We adopt a consistency model as a prosody refinement module. The conditional feature of the consistency model is the feature from prosody regression before the final projection layer. Thus, the residual from a stochastic sampler refines the output of a deterministic prosody regression and produces a diverse set of plausible prosody under the same transcription and audio prompt. One option for the final prosody output p final can be represented as: p final= pres+ p init,(18) where p final denotes the final prosody output, p res represents the residual output from the prosody refinement module, capturing the variations between the ground truth prosody and the deterministic prediction, p init is the initial deterministic prosody prediction from the prosody regression module. However, this formulation may negatively affect prosody stability, a similar observation is found in (Audiobox; Voicebox (2023)). More specifically, higher diversity may cause less stability and sometimes produce unnatural prosody. To address this, we introduce a control factor  $\\alpha$  that finely tunes the balance between stability and diversity in the prosodic output: p final=  $\\alpha$  p res+ p init(19) where  $\\alpha$  is a scalar value ranging between 0 and 1. This adjustment allows for controlled incorporation of variability into the prosody, mitigating issues related to stability while still benefiting from the diversity offered by the prosody refinement module.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#35applications","title":"3.5.Applications","text":"<p>This section elaborates on the practical applications of FlashSpeech. We delve into its deployment across various tasks such as zero-shot TTS, speech editing, voice conversion, and diverse speech sampling. All the sample audios of applications are available on the demo page.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#351zero-shot-tts","title":"3.5.1.Zero-Shot TTS","text":"<p>Given a target text and reference audio, we first convert the text to phoneme using g2p (grapheme-to-phoneme conversion). Then we use the codec encoder to convert the reference audio into $z_{prompt}$. Speech can be synthesized efficiently through FlashSpeech with the phoneme input and $z_{prompt}$, achieving high-quality text-to-speech results without requiring pre-training on the specific voice.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#352voice-conversion","title":"3.5.2.Voice Conversion","text":"<p>Voice conversion aims to convert the source audio into the target audio using the speaker\u2019s voice of the reference audio. Following NaturalSpeech2 (2023); Preechakul et al.(2022), we first apply the reverse of ODE to diffuse the source audio into a starting point that still maintains some information in the source audio. After that, we run the sampling process from this starting point with the reference audio as z prompt and condition c. The condition c uses the phoneme and duration from the source audio and the pitch is predicted by the prosody generator. This method allows for zero-shot voice conversion while preserving the linguistic content of the source audio, and achieving the same timbre as the reference audio.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#353speech-editing","title":"3.5.3.Speech Editing","text":"<p>Given the speech, the original transcription, and the new transcription, we first use MFA (Montreal Forced Aligner) to align the speech and the original transcription to get the duration of each word. Then we remove the part that needs to be edited to construct the reference audio. Next, we use the new transcription and reference to synthesize new speech. Since this task is consistent with the in-context learning, we can concatenate the remaining part of the raw speech and the synthesized part as the final speech, thus enabling precise and seamless speech editing.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#354diverse-speech-sampling","title":"3.5.4.Diverse Speech Sampling","text":"<p>FlashSpeech leverages its inherent stochastic it y to generate a variety of speech outputs under the same conditions. By employing stochastic sampling in its prosody generation and LCM, FlashSpeech can produce diverse variations in pitch, duration, and overall audio characteristics from the same phoneme input and audio prompt. This feature is particularly useful for generating a wide range of speech expressions and styles from a single input, enhancing applications like voice acting, synthetic voice variation for virtual assistants, and more personalized speech synthesis. In addition, the synthetic data via speech sampling can also benefit other tasks such as ASR (Rossenbach et al. (2020)).</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#4experiment","title":"4.Experiment","text":"<p>In the experimental section, we begin by introducing the datasets and the configurations for training in our experiments. Following this, we show the evaluation metrics and demonstrate the comparative results against various zero-shot TTS models. Subsequently, ablation studies are conducted to test the effectiveness of several design choices. Finally, we also validate the effectiveness of other tasks such as voice conversion. We show our speech editing and diverse speech sampling results on our demo page.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#41experimental-settings","title":"4.1.Experimental Settings","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#411data-and-preprocessing","title":"4.1.1.Data and Preprocessing","text":"<p>We use the English subset of Multilingual LibriSpeech (MLS) Pratap et al.(2020), including 44.5k hours of transcribed audiobook data and it contains 5490 distinct speakers. The audio data is resampled at a frequency of 16kHz. The input text is transformed into a sequence of phonemes through grapheme-to-phoneme conversion Sun et al.(2019) and then we use our internal alignment tool aligned with speech to obtain the phoneme-level duration. We adopt a hop size of 200 for all frame-level features. The pitch sequence is extracted using PyWorld2. we adopt EnCodec (2022) as our audio codec. We use a modified version3and train it on MLS. We use the dense features extracted before the residual quantization layer as our latent vector z.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#412training-details","title":"4.1.2.Training Details","text":"<p>Our training consists of two stages, in the first stage we train LCM and the prosody regression part. We use 8 H800 80GB GPUs with a batch size of 20k frames of latent vectors per GPU for 650k steps. We use the AdamW optimizer with a learning rate of 3e-4, warm up the learning rate for the first 30k updates and then linear decay it. We deactivate adversarial training with\u03bbadv= 0 before 600K training iterations. For hyper-parameters, we setain Equation(12)to 0.03. In equation(10), \u03c3i=\ufffd\u03c31/\u03c1min+i\u22121N(k)\u22121\ufffd\u03c31/\u03c1max\u2212 \u03c31/\u03c1min\ufffd\ufffd\u03c1,wherei \u2208 [1, N(k)],\u03c1 = 7, \u03c3min= 0.002, \u03c3max= 80. For N(k) in Equation(11), we sets0= 10, s1= 1280, K = 600k. After 600k steps, we activate adversarial loss, and N(k) can be considered as fixed to 1280. We crop the waveform length fed into the discriminator into minimum waveform length in a minibatch. In addition, the weight of the feature extractor WavLM and the codec decoder are frozen. In the second stage, we train 150k steps for the prosody refinement module with consistency training in Equation(10). Different from the above setting, we empirically sets1= 160,K = 150k. During training, only the weight of the prosody refinement part is updated.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#413model-details","title":"4.1.3.Model Details","text":"<p>The model structures of the prompt encoder and phoneme encoder are follow NaturalSpeech2 (2023). The neural function part in LCM is almost the same as the NaturalSpeech2 (2023). We rescale the sinusoidal position embedding in the neural function part by a factor of 1000. As for the prosody generator, we adopt 30 non-casual WaveNet (2016) layers for the neural function part in the prosody refinement module and the same configurations for prosody regression parts in NaturalSpeech2 (2023). And we set $\\alpha= 0.2$ for the prosody refinement module empirically. For the discriminator\u2019s head, we stack 5 convolutional layers with weight normalization Salimans and Kingma (2016) for binary classification.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#42evaluation-metrics","title":"4.2.Evaluation Metrics","text":"<p>We use both objective and subjective evaluation metrics, including - Real-time-factor (RTF): RTF measures the time taken for the system to generate one second of speech. This metric is crucial for evaluating the efficiency of our system, particularly for applications requiring real-time processing. We measure the time of our system end-to-end on an NVIDIA V100 GPU following NaturalSpeech2 (2023). - Sim-O and Sim-R: These metrics assess the speaker similarity. Sim-R measures the objective similarity between the synthesized speech and the reconstruction reference speech through the audio codec, using features embedding extracted from the pre-trained speaker verification model VALL-E (2023); [ClaM-TTS (2024)](2024.04.../_tmp/2024.04.03_CLaM-TTS.md Sim-O is calculated with the original reference speech. Higher scores in Sim-O and Sim-R indicate a higher speaker similarity. - WER (Word Error Rate): To evaluate the accuracy and clarity of synthesized speech from the TTS system, we employ the Automatic Speech Recognition (ASR) model Wang et al. (2023a)5to transcribe generated audio. The discrepancies between these transcriptions and original texts are quantified using the Word Error Rate (WER), a crucial metric indicating intelligibility and robustness. - CMOS, SMOS, UTMOS: we rank the comparative mean option score (CMOS) and similarity mean option score (SMOS) using mturk. The prompt for CMOS refers to \u2019Please focus on the audio quality and naturalness and ignore other factors.\u2019. The prompt for SMOS refers to \u2019Please focus on the similarity of the speaker to the reference, and ignore the differences of content, grammar or audio quality.\u2019 Each audio has been listened to by at least 10 listeners. UTMOS Saeki et al.(2022) is a Speech MOS predictor6to measure the naturalness of speech. We use it in ablation studies which reduced the cost for evaluation. - Prosody JS Divergence: To evaluate the diversity and accuracy of the prosody prediction in our TTS system, we include the Prosody JS Divergence metric. This metric employs the Jensen-Shannon (JS) divergence Men\u00e9ndez et al.(1997) to quantify the divergence between the predicted and ground truth prosody feature distributions. Prosody features, including pitch, and duration, are quantized and their distributions in both synthesized and natural speech are compared. Lower JS divergence values indicate closer similarity between the predicted prosody features and those of the ground truth, suggesting a higher diversity of the synthesized speech.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#43experimental-results-on-zero-shot-tts","title":"4.3.Experimental Results on Zero-shot TTS","text":"<p>Following VALL-E (2023), We employ LibriSpeech Panayotov et al.(2015) test-clean for zero-shot TTS evaluation. We adopt the cross-sentence setting in VALL-E (2023) that we randomly select 3-second clips as prompts from the same speaker\u2019s speech. The results are summarized in table 1 and figure 5.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#431evaluation-baselines","title":"4.3.1.Evaluation Baselines","text":"<ul> <li>VALL-E (2023): VALL-E (2023) predicts codec tokens using both AR and NAR models. RTF is obtained from ClaM-TTS (2024); Voicebox (2023). We use our reproduced results for MOS, Sim, and WER. Additionally, we do a preference test with their official demo.</li> <li>Voicebox (2023): Voicebox uses flow-matching to predict maksed mel-spectrogram. RTF is from the original paper. We use our reproduced results for MOS, Sim, and WER. We also implement a preference test with their official demo.</li> <li>NaturalSpeech2 (2023): NaturalSpeech2 uses a latent diffusion model to predict latent features of codec. The RTF is from the original paper. the Sim, WER and samples for MOS are obtained through communication with the authors. We also do a preference test with their official demo.</li> <li>Mega-TTS (2023) 8: Mega-TTS uses both language model and GAN to predict mel-spectrogram. We obtain RTF from mobilespeech Ji et al.(2024) and WER from the original paper. We do a preference test with their official demo.</li> <li>ClaM-TTS (2024): ClaM-TTS uses the AR model to predict mel codec tokens. We obtain the objective evaluation results from the original paper and do a preference test with their official demo.</li> </ul>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#432generation-quality","title":"4.3.2.Generation Quality","text":"<p>FlashSpeech stands out significantly in terms of speaker quality, surpassing other baselines in both CMOS and audio quality preference tests. Notably, our method closely approaches ground truth recordings, underscoring its effectiveness. These results affirm the superior quality of FlashSpeech in speech synthesis. our method.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#433generation-similarity","title":"4.3.3.Generation Similarity","text":"<p>Our evaluation of speaker similarity utilizes Sim, SMOS, and speaker similarity preference tests, where our methods achieve 1st, 2nd, and 3rd place rankings, respectively. These findings validate our methods\u2019 ability to achieve comparable speaker similarity to other methods. Despite our training data (MLS) containing approximately 5k speakers, fewer than most other methods (e.g., Librilight with about 7k speakers or self-collected data), we believe that increasing the number of speakers in our methods can further enhance speaker similarity.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#434robustness","title":"4.3.4.Robustness","text":"<p>Our methods achieve a WER of 2.7, placing them in the first echelon. This is due to the non-autoregressive nature of our methods, which ensures robustness.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#435generation-speed","title":"4.3.5.Generation Speed","text":"<p>FlashSpeech achieves a remarkable approximately 20x faster inference speed compared to previous work. Considering its excellent audio quality, robustness, and comparable speaker similarity, our method stands out as an efficient and effective solution in the field of large-scale speech synthesis.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#44ablation-studies","title":"4.4.Ablation Studies","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#441ablation-studies-of-lcm","title":"4.4.1.Ablation Studies of LCM","text":"<p>We explored the impact of different pre-trained models in adversarial training on UTMOS and Sim-O. As shown in the table 2, the baseline, which employs consistency training alone, achieved a UTMOS of 3.62 and a Sim-O of 0.45. Incorporating adversarial training using wav2vec2-large9, hubert-large10, and wavlm-large11as discriminators significantly improved both UTMOS and Sim-O scores. Notably, the application of adversarial training with Wavlm-large achieved the highest scores (UTMOS: 4.00, Sim-O: 0.52), underscoring the efficacy of this pre-trained model in enhancing the quality and speaker similarity of synthesized speech. Additionally, without using the audio prompt\u2019s feature as a condition the discriminator shows a slight decrease in performance (UTMOS: 3.97, Sim-O: 0.51), highlighting the importance of conditional features in guiding the adversarial training process. As shown in table 3, the effect of sampling steps (NFE) on UTMOS and Sim-O revealed that increasing NFE from 1 to 2 marginally improves UTMOS (3.99 to 4.00) and Sim-O (0.51 to 0.52). However, further increasing to 4 sampling steps slightly reduced UTMOS to 3.91 due to the accumulation of score estimation errors Chen et al.(2022a); Lyu et al.(2024). Therefore, we use 2 steps as the default setting for LCM.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#442ablation-studies-of-prosody-generator","title":"4.4.2.Ablation Studies of Prosody Generator","text":"<p>In this part, we investigated the effects of a control factor, denoted as $\\alpha$, on the prosodic features of pitch and duration in speech synthesis, by setting another influencing factor to zero. Our study specifically conducted an ablation analysis to assess how $\\alpha$ influences these features, emphasizing its critical role in balancing stability and diversity within our framework\u2019s prosodic outputs.</p> <p>Tab.04 elucidates the effects of varying $\\alpha$ on the pitch component. With $\\alpha$ set to 0, indicating no inclusion of the residual output from prosody refinement, we observed a Pitch JSD of 0.072 and a WER of 2.8. A slight modification to $\\alpha= 0.2$ resulted in a reduced Pitch JSD of 0.067, maintaining the same WER. Notably, setting $\\alpha$ to 1, fully incorporating the prosody refinement\u2019s residual output, further decreased the Pitch JSD to 0.063, albeit at the cost of increased WER to 3.7, suggesting a trade-off between prosody diversity and speech intelligibility.</p> <p>Similar trends in table 5 are observed in the duration component analysis. With $\\alpha$  = 0, the Duration JSD was 0.0175 with a WER of 2.8. Adjusting $\\alpha$ to 0.2 slightly improved the Duration JSD to 0.0168, without affecting WER. However, fully embracing the refinement module\u2019s output by setting $\\alpha$  = 1 yielded the most significant improvement in Duration JSD to 0.0153, which, similar to pitch analysis, came with an increased WER of 3.9. The results underline the delicate balance required in tuning $\\alpha$  to optimize between diversity and stability of prosody without compromising speech intelligibility.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#45evaluation-results-for-voice-conversion","title":"4.5.Evaluation Results for Voice Conversion","text":"<p>In this section, we present the evaluation results of our voice conversion system, FlashSpeech, in comparison with state-of-the-art methods, including YourTTS (2021) 12 and DDDM-VC 13 Choi et al.(2024).  We conduct the experiments with their official checkpoints in our internal test set.</p> <p>Our system outperforms both YourTTS and DDDM-VC in terms of CMOS, SMOS and Sim-O, demonstrating its capability to produce converted voices with high quality and similarity to the target speaker.  These results confirm the effectiveness of our FlashSpeech approach in voice conversion tasks.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.04.23_FlashSpeech/#46conclusions-and-future-work","title":"4.6.Conclusions and Future Work","text":"<p>In this paper, we presented FlashSpeech, a novel speech generation system that significantly reduces computational costs while maintaining high-quality speech output. Utilizing a novel adversarial consistency training method and an LCM, FlashSpeech outperforms existing zero-shot TTS systems in efficiency, achieving speeds about 20 times faster without compromising on voice quality, similarity, and robustness. In the future, we aim to further refine the model to improve the inference speed and reduce computational demands. In addition, we will expand the data scale and enhance the system\u2019s ability to convey a broader range of emotions and more nuanced prosody. For future applications, FlashSpeech can be integrated for real-time interactions in applications such as virtual assistants and educational tools.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/","title":"ControlSpeech","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: ControlSpeech: Towards Simultaneous Zero-shot Speaker Cloning and Zero-shot Language Style Control With Decoupled Codec - \u4f5c\u8005:   - [Shengpeng Ji](../../Authors/Shengpeng_Ji.md)   - [Jialong Zuo](../../Authors/Jialong_Zuo.md)    - [Minghui Fang](../../Authors/Minghui_Fang.md)    - [Siqi Zheng](../../Authors/Siqi_Zheng.md)   - [Qian Chen](../../Authors/Qian_Chen.md)   - [Wen Wang](../../Authors/Wen_Wang.md)   - [Ziyue Jiang](../../Authors/Ziyue_Jiang.md)   - [Hai Huang](../../Authors/Hai_Huang.md)   - [Xize Cheng](../../Authors/Xize_Cheng_(\u6210\u66e6\u6cfd).md)   - [Rongjie Huang](../../Authors/Rongjie_Huang_(\u9ec4\u878d\u6770).md)    - [Zhou Zhao](../../Authors/Zhou_Zhao_(\u8d75\u6d32).md) - \u673a\u6784:   - [\u6d59\u6c5f\u5927\u5b66](../../Institutions/ZJU_\u6d59\u6c5f\u5927\u5b66.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.06.03 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.13 - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2406.01205)   - [DOI]()   - [Github](https://github.com/jishengpeng/ControlSpeech)   - [Demo]() - \u6807\u7b7e:   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md) - \u9875\u6570: 16 - \u5f15\u7528: 54 - \u88ab\u5f15: 0"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#abstract","title":"Abstract","text":"\u539f\u6587  &gt; In this paper, we present ***ControlSpeech***, a text-to-speech (TTS) system capable of fully cloning the speaker\u2019s voice and enabling arbitrary control and adjustment of speaking style, merely based on a few seconds of audio prompt and a simple textual style description prompt. &gt; Prior zero-shot TTS models and controllable TTS models either could only mimic the speaker\u2019s voice without further control and adjustment capabilities or were unrelated to speaker-specific voice generation. &gt; Therefore, ***ControlSpeech*** focuses on a more challenging new task\u2014a TTS system with controllable timbre, content, and style at the same time. &gt; ***ControlSpeech*** takes speech prompts, content prompts, and style prompts as inputs and utilizes bidirectional attention and mask-based parallel decoding to capture corresponding codec representations in a discrete decoupling codec space. &gt; Moreover, we discovered the issue of text style controllability in a many-to-many mapping fashion and proposed the ***Style Mixture Semantic Density (SMSD)*** model to resolve this problem. &gt; ***SMSD*** module which is based on Gaussian mixture density networks, is designed to enhance the fine-grained partitioning and sampling capabilities of style semantic information and generate speech with more diverse styles. &gt; In terms of experiments, we make available a controllable model toolkit called ***ControlToolkit*** with a new style controllable dataset, some replicated baseline models and propose new metrics to evaluate both the control capability and the quality of generated audio in ***ControlSpeech***. &gt; The relevant ablation studies validate the necessity of each component in ***ControlSpeech*** is necessary. &gt; We hope that ***ControlSpeech*** can establish the next foundation paradigm of controllable speech synthesis. &gt; The relevant code and demo are available at https://github.com/jishengpeng/ControlSpeech.   <p>\u672c\u9879\u5de5\u4f5c\u5c55\u793a\u4e86 ControlSpeech, \u4e00\u4e2a\u80fd\u591f\u4ec5\u57fa\u4e8e\u6570\u79d2\u7684\u97f3\u9891\u63d0\u793a\u548c\u7b80\u5355\u7684\u6587\u672c\u98ce\u683c\u63cf\u8ff0\u63d0\u793a\u4ece\u800c\u5b8c\u6574\u514b\u9686\u8bf4\u8bdd\u4eba\u58f0\u97f3\u4e14\u80fd\u4efb\u610f\u63a7\u5236\u548c\u8c03\u8282\u8bf4\u8bdd\u98ce\u683c\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf. \u4e4b\u524d\u7684\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u548c\u53ef\u63a7\u5236\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u8981\u4e48\u53ea\u80fd\u6a21\u4eff\u8bf4\u8bdd\u4eba\u58f0\u97f3\u800c\u6ca1\u6709\u8fdb\u4e00\u6b65\u63a7\u5236\u548c\u8c03\u8282\u7684\u80fd\u529b, \u8981\u4e48\u548c\u5177\u4f53\u8bf4\u8bdd\u4eba\u7684\u58f0\u97f3\u751f\u6210\u65e0\u5173. \u56e0\u6b64, ControlSpeech \u805a\u7126\u4e8e\u66f4\u5177\u6311\u6218\u6027\u7684\u65b0\u4efb\u52a1: \u97f3\u8272, \u5185\u5bb9, \u98ce\u683c\u80fd\u540c\u65f6\u53ef\u63a7\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf. ControlSpeech \u63a5\u53d7\u97f3\u9891\u63d0\u793a, \u5185\u5bb9\u63d0\u793a, \u98ce\u683c\u63d0\u793a\u4f5c\u4e3a\u8f93\u5165, \u5e76\u91c7\u7528\u53cc\u5411\u6ce8\u610f\u529b\u548c\u57fa\u4e8e\u63a9\u7801\u7684\u5e76\u884c\u89e3\u7801\u5668\u6765\u6355\u6349\u79bb\u6563\u89e3\u8026\u7f16\u89e3\u7801\u7a7a\u95f4\u5185\u7684\u76f8\u5e94\u7f16\u89e3\u7801\u8868\u793a. \u6b64\u5916, \u6211\u4eec\u53d1\u73b0\u5728\u591a\u5230\u591a\u6620\u5c04\u8303\u5f0f\u4e0b\u6587\u672c\u98ce\u683c\u53ef\u63a7\u6027\u95ee\u9898, \u5e76\u63d0\u51fa \u98ce\u683c\u6df7\u5408\u8bed\u97f3\u5bc6\u5ea6\u6a21\u578b (Style Mixture Semantic Density, SMSD) \u6a21\u5757\u6765\u89e3\u51b3\u6b64\u95ee\u9898. SMSD \u6a21\u5757\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc, \u88ab\u8bbe\u8ba1\u7528\u4e8e\u589e\u5f3a\u98ce\u683c\u8bed\u4e49\u4fe1\u606f\u7684\u7ec6\u7c92\u5ea6\u5212\u5206\u548c\u91c7\u6837\u80fd\u529b, \u5e76\u7528\u66f4\u591a\u6837\u6027\u7684\u98ce\u683c\u751f\u6210\u8bed\u97f3. \u5728\u5b9e\u9a8c\u65b9\u9762, \u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u540d\u4e3a ControlToolkit \u7684\u53ef\u63a7\u6a21\u578b\u5de5\u5177\u7bb1, \u4ee5\u53ca\u4e00\u4e2a\u65b0\u7684\u98ce\u683c\u53ef\u63a7\u6570\u636e\u96c6, \u4e00\u4e9b\u590d\u73b0\u7684\u57fa\u7ebf\u6a21\u578b, \u5e76\u63d0\u51fa\u65b0\u7684\u6307\u6807\u6765\u8bc4\u4f30 ControlSpeech \u7684\u63a7\u5236\u80fd\u529b\u548c\u751f\u6210\u97f3\u9891\u8d28\u91cf. \u76f8\u5173\u7684\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 ControlSpeech \u4e2d\u5404\u4e2a\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027. \u6211\u4eec\u5e0c\u671b ControlSpeech \u80fd\u591f\u6210\u4e3a\u53ef\u63a7\u8bed\u97f3\u5408\u6210\u7684\u4e0b\u4e00\u4ee3\u57fa\u7840\u8303\u5f0f. \u76f8\u5173\u4ee3\u7801\u548c\u793a\u4f8b\u53ef\u5728 https://github.com/jishengpeng/ControlSpeech \u83b7\u5f97.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#1introduction","title":"1.Introduction","text":"\u539f\u6587  &gt; Over the past decade, the field of speech synthesis has seen remarkable advancements ([FastSpeech2](../TTS2_Acoustic/2020.06.08_FastSpeech2.md), [VITS](../E2E/2021.06.11_VITS.md), [FastDiff](../Diffusion/2022.04.21_FastDiff.md), [FastSpeech](../TTS2_Acoustic/2019.05.22_FastSpeech.md)), achieving synthesized speech that rivals real human speech in terms of expressiveness and naturalness ([NaturalSpeech](../E2E/2022.05.09_NaturalSpeech.md)). &gt; Recently, with the development of large language models (GPT-3, GPT-4, [LLaMA](../../Models/LLM/2023.02.27_LLaMA.md)) and generative models (DDPM, [HiFi-GAN](../TTS3_Vocoder/2020.10.12_HiFi-GAN.md), [Glow-TTS](../TTS2_Acoustic/2020.05.22_Glow-TTS.md), [BVAE-TTS](../TTS2_Acoustic/2021.01.13_BVAE-TTS.md)) in other domains, the tasks of zero-shot ([VALL-E](2023.01.05_VALL-E.md), [NaturalSpeech 2](../Diffusion/2023.04.18_NaturalSpeech2.md) [VoiceBox](2023.06.23_VoiceBox.md), [Mega-TTS](2023.06.06_Mega-TTS.md), [SoundStorm](2023.05.16_SoundStorm.md) and style-controllable speech synthesis ([PromptTTS](../_tmp/2022.11.22_PromptTTS.md), [PromptStyle](../E2E/2023.05.31_PromptStyle.md), [InstructTTS](../_tmp/2023.01.31_InstructTTS.md), [SALL-E](../../Datasets/2023.08.28_TextrolSpeech.md)) have garnered significant attention in the speech domain due to their powerful zero-shot generation and controllability capabilities. &gt; Zero-shot TTS ([VALL-E](2023.01.05_VALL-E.md), [NaturalSpeech 2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md); [SPEAR-TTS](2023.02.07_SPEAR-TTS.md)) refers to the ability to perfectly clone an unseen speaker\u2019s voice using only a few seconds of a speech prompt by significantly scaling up both the corpus and model sizes. &gt; Style-controllable TTS ([PromptTTS](../_tmp/2022.11.22_PromptTTS.md), [InstructTTS](../_tmp/2023.01.31_InstructTTS.md)), on the other hand, allows for the control of a speaker\u2019s style (prosody, accent, emotion, etc.) through textual descriptions.   <p>\u5728\u8fc7\u53bb\u7684\u5341\u5e74\u4e2d, \u8bed\u97f3\u5408\u6210\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65, \u4f8b\u5982 FastSpeech2, VITS, FastDiff, FastSpeech, \u5b9e\u73b0\u4e86\u5728\u8868\u8fbe\u6027\u548c\u81ea\u7136\u5ea6\u4e0a\u80fd\u4e0e\u771f\u4eba\u8bed\u97f3\u76f8\u5ab2\u7f8e\u7684\u5408\u6210\u8bed\u97f3. \u8fd1\u671f, \u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b (GPT-3, GPT-4, LLaMA) \u548c\u5728\u5176\u4ed6\u9886\u57df\u7684\u751f\u6210\u6a21\u578b (DDPM, HiFi-GAN, Glow-TTS, BVAE-TTS) \u7684\u53d1\u5c55, \u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3 (VALL-E, NaturalSpeech 2 VoiceBox, Mega-TTS, SoundStorm) \u548c\u98ce\u683c\u53ef\u63a7\u8bed\u97f3\u5408\u6210 (PromptTTS, PromptStyle, InstructTTS, SALL-E) \u5728\u8bed\u97f3\u9886\u57df\u53d7\u5230\u4e86\u6781\u5927\u7684\u5173\u6ce8, \u8fd9\u5f97\u76ca\u4e8e\u5b83\u4eec\u5f3a\u5927\u7684\u96f6\u6837\u672c\u751f\u6210\u548c\u53ef\u63a7\u6027\u80fd\u529b.</p> <ul> <li>\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3 (VALL-E, NaturalSpeech 2; SPEAR-TTS) \u6307\u7684\u662f\u901a\u8fc7\u663e\u8457\u6269\u6492\u8bed\u6599\u5e93\u548c\u6a21\u578b\u89c4\u6a21, \u4ec5\u4f7f\u7528\u51e0\u79d2\u949f\u7684\u8bed\u97f3\u63d0\u793a\u5c31\u80fd\u5b8c\u7f8e\u514b\u9686\u4e00\u4e2a\u672a\u89c1\u8fc7\u7684\u8bf4\u8bdd\u4eba\u7684\u58f0\u97f3.</li> <li>\u98ce\u683c\u53ef\u63a7\u6587\u672c\u8f6c\u8bed\u97f3 (PromptTTS, InstructTTS) \u5141\u8bb8\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u6765\u63a7\u5236\u8bf4\u8bdd\u4eba\u7684\u98ce\u683c (\u8bed\u8c03, \u53e3\u97f3, \u60c5\u7eea\u7b49).</li> </ul> \u539f\u6587  &gt; However, these two types of models have their own limitations. &gt; As illustrated in the right panel of Fig.01, zero-shot TTS can clone the voice of any speaker, but the style is fixed and cannot be further controlled or adjusted. &gt; Conversely, style-controllable TTS can synthesize speech in any desired style, but it cannot specify the timbre of the synthesized voice. &gt; Although some efforts ([InstructTTS](../_tmp/2023.01.31_InstructTTS.md), [PromptStyle](../E2E/2023.05.31_PromptStyle.md)) have been made to use speaker IDs for control, these approaches are limited to testing on constrained in-domain datasets. &gt; Therefore, we propose a novel model ***ControlSpeech***, which simultaneously controls timbre, content, and style, demonstrating powerful zero-shot cloning and control capabilities at the same time.   <p></p> <p>\u4f46\u662f, \u8fd9\u4e24\u79cd\u7c7b\u578b\u7684\u6a21\u578b\u6709\u7740\u5b83\u4eec\u7684\u5c40\u9650\u6027.</p> <p></p> <p>\u5982\u56fe 01 \u7684\u53f3\u4fa7, \u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u80fd\u591f\u514b\u9686\u4efb\u610f\u8bf4\u8bdd\u4eba\u7684\u58f0\u97f3, \u4f46\u98ce\u683c\u662f\u56fa\u5b9a\u7684, \u4e14\u65e0\u6cd5\u63a7\u5236\u6216\u8c03\u6574. \u800c\u98ce\u683c\u53ef\u63a7\u6587\u672c\u8f6c\u8bed\u97f3\u5219\u76f8\u53cd, \u80fd\u591f\u5408\u6210\u4efb\u4f55\u60f3\u8981\u7684\u98ce\u683c\u7684\u8bed\u97f3, \u4f46\u65e0\u6cd5\u6307\u5b9a\u5408\u6210\u58f0\u97f3\u7684\u97f3\u7d20. \u5c3d\u7ba1\u6709\u4e00\u4e9b\u5de5\u4f5c (InstructTTS, PromptStyle) \u8bd5\u56fe\u4f7f\u7528\u8bf4\u8bdd\u4eba ID \u6765\u63a7\u5236, \u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5728\u53d7\u9650\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5. \u56e0\u6b64, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b ControlSpeech, \u5b83\u80fd\u540c\u65f6\u63a7\u5236\u97f3\u8272, \u5185\u5bb9, \u98ce\u683c, \u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u514b\u9686\u548c\u63a7\u5236\u80fd\u529b.</p> \u539f\u6587  &gt; For this novel task, simply adding a style prompt control module or a speech prompt control module to previous model frameworks is evidently insufficient. &gt; The information from the two types of prompts can become entangled and interfere with each other. &gt; For instance, the speech prompt might contain a style different from that described by the text. &gt; Inspired by this observation, we attempted to incorporate the concept of an information bottleneck in ***ControlSpeech*** to achieve independent disentanglement of timbre, content, and style. &gt; Additionally, to attain robust zero-shot speaker cloning capabilities, a large-scale, multi-speaker training dataset ([MLS](../../Datasets/2020.12.07_MLS.md), [Libri-Light](../../Datasets/2019.12.17_Libri-Light.md)) is essential. &gt; Leveraging recent breakthroughs in the codec domain, we used [FACodec](../Diffusion/2024.03.05_NaturalSpeech3.md) which is pre-trained on 60,000 hours ([Libri-Light](../../Datasets/2019.12.17_Libri-Light.md)) speech as the speech tokenizer for ***ControlSpeech***. &gt; During the speech synthesis process, we adopted [FastSpeech2](../TTS2_Acoustic/2020.06.08_FastSpeech2.md) as the base synthesis framework and employed an advanced non-autoregressive confidence-based codec generator [5, 2, [Phenaki](../_Basis_CV/2022.10.05_Phenaki.md)] as our decoder.   <p></p> <p>\u5bf9\u4e8e\u8fd9\u4e00\u65b0\u4efb\u52a1, \u4ec5\u4ec5\u5728\u4e4b\u524d\u7684\u6a21\u578b\u6846\u67b6\u4e0a\u7b80\u5355\u5730\u6dfb\u52a0\u98ce\u683c\u63d0\u793a\u63a7\u5236\u6a21\u5757\u6216\u8bed\u97f3\u63d0\u793a\u63a7\u5236\u6a21\u5757\u662f\u4e0d\u591f\u7684. \u8fd9\u4e24\u7c7b\u63d0\u793a\u5305\u542b\u7684\u4fe1\u606f\u53ef\u80fd\u4f1a\u4e92\u76f8\u7ea0\u7f20\u76f8\u4e92\u5e72\u6270. \u4f8b\u5982, \u8bed\u97f3\u63d0\u793a\u53ef\u80fd\u5305\u542b\u4e8e\u6587\u672c\u63cf\u8ff0\u4e0d\u540c\u7684\u98ce\u683c. \u53d7\u6b64\u89c2\u5bdf\u542f\u53d1, \u6211\u4eec\u5c1d\u8bd5\u5728 ControlSpeech \u4e2d\u5f15\u5165\u4fe1\u606f\u74f6\u9888\u7684\u6982\u5ff5, \u4ee5\u5b9e\u73b0\u97f3\u8272, \u5185\u5bb9\u548c\u98ce\u683c\u7684\u72ec\u7acb\u89e3\u8026. \u6b64\u5916, \u4e3a\u4e86\u83b7\u5f97\u5f3a\u5927\u7684\u96f6\u6837\u672c\u8bf4\u8bdd\u4eba\u514b\u9686\u80fd\u529b, \u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u591a\u8bf4\u8bdd\u4eba\u8bad\u7ec3\u6570\u636e\u96c6 (MLS, Libri-Light) \u662f\u5fc5\u8981\u7684. \u7ed3\u5408\u8fd1\u671f\u5728\u7f16\u89e3\u7801\u5668\u9886\u57df\u7684\u7a81\u7834, \u6211\u4eec\u4f7f\u7528\u5728 60,000 \u5c0f\u65f6\u7684 Libri-Light \u8bed\u97f3\u4e0a\u9884\u8bad\u7ec3\u7684 FACodec \u4f5c\u4e3a ControlSpeech \u7684\u8bed\u97f3\u5206\u8bcd\u5668. \u5728\u8bed\u97f3\u5408\u6210\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u91c7\u7528 FastSpeech2 \u4f5c\u4e3a\u57fa\u7840\u5408\u6210\u6846\u67b6, \u5e76\u91c7\u7528\u5148\u8fdb\u7684\u975e\u81ea\u56de\u5f52\u7f6e\u4fe1\u5ea6\u7f16\u89e3\u7801\u5668\u751f\u6210\u5668 (MaskGIT,2023.05.16_SoundStorm.md_SoundStorm.md), Phenaki) \u4f5c\u4e3a\u89e3\u7801\u5668.</p> \u539f\u6587  &gt; Moreover, during our experiments, we identified a many-to-many problem in style control: different style descriptions might correspond to the same audio, and a single style description might correspond to varying degrees of one style for the same speaker. &gt; Therefore, we designed a novel ***Style Mixture Semantic Density Sampling (SMSD)*** module to address the many-to-many issue in style control.  &gt; We incorporate the global semantic information of the style control and exploit sampling from a mixed distribution [53, 18] of style descriptions to achieve hierarchical control. &gt; Additionally, we also design a noise disturbance module in ***SMSD*** to further enhance the diversity of styles.   <p></p> <p>\u6b64\u5916, \u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u53d1\u73b0\u98ce\u683c\u63a7\u5236\u4e2d\u7684\u591a\u5bf9\u591a\u95ee\u9898: \u4e0d\u540c\u98ce\u683c\u63cf\u8ff0\u53ef\u80fd\u5bf9\u5e94\u540c\u4e00\u4e2a\u97f3\u9891, \u5355\u4e2a\u98ce\u683c\u63cf\u8ff0\u53ef\u80fd\u5bf9\u5e94\u540c\u4e00\u4e2a\u8bf4\u8bdd\u4eba\u5355\u4e2a\u98ce\u683c\u7684\u4e0d\u540c\u7a0b\u5ea6. \u56e0\u6b64, \u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684 \u98ce\u683c\u6df7\u5408\u8bed\u4e49\u5bc6\u5ea6\u91c7\u6837 (Style Mixture Semantic Density Sampling, SMSD) \u6a21\u5757\u6765\u5904\u7406\u8fd9\u4e00\u98ce\u683c\u63a7\u5236\u4e2d\u7684\u591a\u5bf9\u591a\u95ee\u9898. \u6211\u4eec\u5408\u5e76\u4e86\u98ce\u683c\u63a7\u5236\u7684\u5168\u5c40\u8bed\u8bed\u4e49\u4fe1\u606f\u5e76\u5229\u7528\u4ece\u98ce\u683c\u63cf\u8ff0\u7684\u6df7\u5408\u5206\u5e03\u4e2d\u91c7\u6837\u6765\u8fbe\u5230\u5c42\u6b21\u63a7\u5236\u7684\u6548\u679c. \u6b64\u5916, \u6211\u4eec\u5728 SMSD \u4e2d\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u566a\u58f0\u6270\u52a8\u6a21\u5757, \u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u98ce\u683c\u7684\u591a\u6837\u6027.</p> \u539f\u6587  &gt; To comprehensively evaluate ***ControlSpeech***\u2019s controllability, timbre similarity, audio quality, diversity, and generalization, we created a new dataset called ***VccmDataset*** and established new metrics based on [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md). &gt; Considering the lack of open-source text style-controllable TTS models, we have consolidated the re-implemented baselines, ***VccmDataset*** and some evaluation scripts into a toolkit named ***ControlToolkit*** to foster further advancements in the controllable TTS field.   <p></p> <p>\u4e3a\u4e86\u5168\u9762\u8bc4\u4f30 ControlSpeech \u7684\u53ef\u63a7\u6027, \u97f3\u8272\u76f8\u4f3c\u6027, \u97f3\u9891\u8d28\u91cf, \u591a\u6837\u6027, \u6cdb\u5316\u80fd\u529b, \u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6 VccmDataset, \u5e76\u57fa\u4e8e TextrolSpeech \u5efa\u7acb\u4e86\u65b0\u7684\u8bc4\u4ef7\u6307\u6807. \u8003\u8651\u5230\u7f3a\u4e4f\u5f00\u6e90\u7684\u6587\u672c\u98ce\u683c\u53ef\u63a7 TTS \u6a21\u578b, \u6211\u4eec\u5c06\u91cd\u65b0\u5b9e\u73b0\u7684\u57fa\u7ebf, VccmDataset \u4ee5\u53ca\u4e00\u4e9b\u8bc4\u4ef7\u811a\u672c\u6574\u5408\u5230\u540d\u4e3a ControlToolkit \u7684\u5de5\u5177\u7bb1\u4e2d, \u4ee5\u4fc3\u8fdb\u53ef\u63a7 TTS \u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55.</p> \u539f\u6587  &gt; In summary, our contributions are as follows: &gt; - We broadened the scope of controllable speech generation tasks by proposing the ***ControlSpeech***. &gt; To the best of our knowledge, ***ControlSpeech*** is the first TTS model capable of zero-shot control over both timbre and style simultaneously. &gt; - We designed a TTS technology pathway based on disentangled codec representations and validated the necessity of this disentanglement approach. &gt; Additionally, within ***ControlSpeech***, we introduced the novel ***Style Mixture Semantic Density (SMSD)*** module to address the many-to-many problem in style control. &gt; - Given the lack of baselines and datasets in the style-controllable TTS field, we compiled a new dataset called ***VccmDataset*** and established relevant new metrics. &gt; We also reproduced related baselines and open-sourced them along with ***VccmDataset*** in the ***ControlToolkit***. &gt; - We conducted comprehensive experiments, demonstrating that ***ControlSpeech*** exhibits comparable or SOTA performance in terms of controllability, timbre similarity, audio quality, robustness, and generalization.   <p></p> <p>\u7efc\u4e0a\u6240\u8ff0, \u6211\u4eec\u7684\u8d21\u732e\u5982\u4e0b: - \u6211\u4eec\u63d0\u51fa\u4e86 ControlSpeech, \u62d3\u5bbd\u4e86\u53ef\u63a7\u8bed\u97f3\u5408\u6210\u4efb\u52a1\u7684\u8303\u56f4;   \u636e\u6211\u4eec\u6240\u77e5, ControlSpeech \u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5bf9\u97f3\u8272\u548c\u98ce\u683c\u8fdb\u884c\u96f6\u6837\u672c\u63a7\u5236\u7684 TTS \u6a21\u578b. - \u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u5957\u57fa\u4e8e\u5206\u79bb\u7684\u7f16\u89e3\u7801\u5668\u8868\u793a\u7684 TTS \u6280\u672f\u8def\u7ebf, \u5e76\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u5206\u79bb\u65b9\u6cd5\u7684\u5fc5\u8981\u6027.   \u6b64\u5916, \u5728 ControlSpeech \u4e2d, \u6211\u4eec\u5f15\u5165\u4e86\u65b0\u7684 \u98ce\u683c\u6df7\u5408\u8bed\u4e49\u5bc6\u5ea6 (Style Mixture Semantic Density, SMSD) \u6a21\u5757\u6765\u89e3\u51b3\u98ce\u683c\u63a7\u5236\u4e2d\u7684\u591a\u5bf9\u591a\u95ee\u9898. - \u7531\u4e8e\u98ce\u683c\u53ef\u63a7 TTS \u9886\u57df\u7f3a\u4e4f\u57fa\u7ebf\u548c\u6570\u636e\u96c6, \u6211\u4eec\u7f16\u8bd1\u4e86\u4e00\u4efd\u540d\u4e3a VccmDataset \u7684\u65b0\u6570\u636e\u96c6, \u5e76\u5efa\u7acb\u4e86\u76f8\u5173\u7684\u65b0\u8bc4\u4ef7\u6307\u6807.   \u6211\u4eec\u8fd8\u5728 ControlToolkit \u4e2d\u590d\u73b0\u4e86\u76f8\u5173\u7684\u57fa\u7ebf\u5e76\u5f00\u6e90\u4e86 VccmDataset. - \u6211\u4eec\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c, \u8bc1\u660e\u4e86 ControlSpeech \u5728\u53ef\u63a7\u6027, \u97f3\u8272\u76f8\u4f3c\u6027, \u97f3\u9891\u8d28\u91cf, \u5065\u58ee\u6027, \u6cdb\u5316\u80fd\u529b\u65b9\u9762\u90fd\u5177\u6709\u4e0e SOTA \u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#2related-works","title":"2.Related Works","text":"\u539f\u6587  &gt; In this section, we will introduce zero-shot TTS, text prompt-based controllable TTS, and discrete codec tasks related to ***ControlSpeech***. &gt; Due to space constraints, the detailed discussion of related work is provided in appendix A. &gt; However, we encourage reviewers to closely examine the connections and distinctions between ***ControlSpeech*** and prior related work.   <p>\u672c\u8282\u6211\u4eec\u4ecb\u7ecd\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3, \u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u53ef\u63a7\u6587\u672c\u8f6c\u8bed\u97f3, \u4ee5\u53ca\u548c ControlSpeech \u76f8\u5173\u7684\u79bb\u6563\u7f16\u89e3\u7801\u4efb\u52a1. \u7531\u4e8e\u7bc7\u5e45\u9650\u5236, \u76f8\u5173\u5de5\u4f5c\u7684\u8be6\u7ec6\u8ba8\u8bba\u8bf7\u53c2\u89c1\u9644\u5f55 A. \u4f46\u662f, \u6211\u4eec\u9f13\u52b1\u8bc4\u5ba1\u8005\u4ed4\u7ec6\u5ba1\u67e5 ControlSpeech \u4e0e\u76f8\u5173\u5de5\u4f5c\u4e4b\u95f4\u7684\u8054\u7cfb\u548c\u533a\u522b. \u6ce8: \u7531\u9644\u5f55 A \u79fb\u52a8\u81f3\u6b64.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#a1discrete-codec-model","title":"A.1.Discrete Codec Model","text":"\u539f\u6587  &gt; In recent times, neural acoustic codecs ([SoundStream](../Speech_Neural_Codec/2021.07.07_SoundStream.md), [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md), [HiFi-Codec](../Speech_Neural_Codec/2023.05.04_HiFi-Codec.md), [Vocos](../TTS3_Vocoder/2023.03.01_Vocos.md), [SpeechTokenizer](../Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md), [FunCodec](../Speech_Neural_Codec/2023.09.14_FunCodec.md)) have demonstrated remarkable capabilities in reconstructing high-quality audio at extremely low bitrates. &gt; To elaborate, [SoundStream](../Speech_Neural_Codec/2021.07.07_SoundStream.md) utilizes a model architecture comprising a fully convolutional encoder/decoder network and a residual vector quantizer (RVQ) to effectively compress speech. &gt; [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md) employs a streaming encoder-decoder architecture with a quantized latent space, trained in an end-to-end fashion. &gt; [HiFi-Codec](../Speech_Neural_Codec/2023.05.04_HiFi-Codec.md) introduces a group-residual vector quantization (GRVQ) technique to reduce the number of quantizers. &gt; [Vocos](../TTS3_Vocoder/2023.03.01_Vocos.md) aims to bridge the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis. &gt; In order to narrow the gaps between text and acoustic codec tokens, [SpeechTokenizer](../Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md) and [FunCodec](../Speech_Neural_Codec/2023.09.14_FunCodec.md) introduces the concept of using semantic tokens in the first channel of discrete codecs. &gt; This transition helps mitigate the disparity between text and acoustic tokens. &gt; [Language-Codec](../Speech_Neural_Codec/2024.02.19_Language-Codec.md) employs the MCRVQ mechanism to evenly distribute information from the first three channels, thereby reducing the gap between text and acoustic tokens. &gt; [DAC](../Speech_Neural_Codec/2023.06.11_Descript-Audio-Codec.md) greatly improves the quality of codec reconstruction by introducing multi-scale discriminators loss. &gt; [FACodec](../Diffusion/2024.03.05_NaturalSpeech3.md) pioneered the concept of a disentangled codec, decomposing speech into timbre, content, prosody, and acoustic details within its discrete space, and demonstrated a certain degree of voice conversion capability. &gt; In an endeavor to enhance the disentangled codec, we explored modifying the parallel vector quantization structure to a residual vector quantization form and incorporating additional stylistic supervisory signals. &gt; However, our efforts did not yield substantial improvements compared to [FACodec](../Diffusion/2024.03.05_NaturalSpeech3.md). &gt; Therefore, we utilize the pre-trained [FACodec](../Diffusion/2024.03.05_NaturalSpeech3.md) to extract the corresponding codec representations in ***ControlSpeech***.   <p>\u8fd1\u6bb5\u65f6\u95f4, \u795e\u7ecf\u58f0\u5b66\u7f16\u89e3\u7801\u5668 (SoundStream, EnCodec, HiFi-Codec, Vocos, SpeechTokenizer, FunCodec) \u5df2\u7ecf\u5728\u4ee5\u76f8\u5f53\u4f4e\u6bd4\u7279\u7387\u91cd\u6784\u9ad8\u8d28\u91cf\u97f3\u9891\u65b9\u9762\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u80fd\u529b. \u4e0b\u9762\u505a\u8fdb\u4e00\u6b65\u7684\u8bf4\u660e: - SoundStream \u5229\u7528\u4e00\u4e2a\u7531\u5168\u5377\u79ef\u7f16\u7801\u5668/\u89e3\u7801\u5668\u7f51\u7edc\u548c\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u5668 (RVQ) \u7ec4\u6210\u7684\u6a21\u578b\u67b6\u6784, \u4ee5\u6709\u6548\u5730\u538b\u7f29\u8bed\u97f3. - EnCodec \u91c7\u7528\u5177\u6709\u91cf\u5316\u9690\u53d8\u91cf\u7684\u6d41\u5f0f\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784, \u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u8bad\u7ec3. - HiFi-Codec \u5f15\u5165\u4e86\u7ec4\u6b8b\u5dee\u5411\u91cf\u91cf\u5316 (GRVQ) \u6280\u672f, \u4ee5\u51cf\u5c11\u91cf\u5316\u5668\u7684\u6570\u91cf. - Vocos \u65e8\u5728\u5c06\u65f6\u57df\u548c\u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u795e\u7ecf\u8bed\u97f3\u5408\u6210\u5668\u4e4b\u95f4\u7684\u5dee\u8ddd\u7f29\u5c0f\u5230\u8db3\u4ee5\u4ea7\u751f\u9ad8\u8d28\u91cf\u97f3\u9891. - SpeechTokenizer \u548c FunCodec \u5728\u79bb\u6563\u7f16\u89e3\u7801\u7684\u7b2c\u4e00\u4e2a\u901a\u9053\u4e2d\u4f7f\u7528\u8bed\u4e49 Token \u4ee5\u51cf\u5c11\u6587\u672c\u548c\u97f3\u9891 Token \u7684\u5dee\u8ddd. - \u8fd9\u79cd\u8f6c\u53d8\u6709\u52a9\u4e8e\u7f13\u89e3\u6587\u672c\u548c\u97f3\u9891 Token \u4e4b\u95f4\u7684\u5dee\u8ddd. - Language-Codec \u91c7\u7528 MCRVQ \u673a\u5236, \u5c06\u4fe1\u606f\u5747\u5300\u5206\u914d\u5230\u524d\u4e09\u4e2a\u901a\u9053, \u4ece\u800c\u51cf\u5c11\u6587\u672c\u548c\u58f0\u5b66 Token \u4e4b\u95f4\u7684\u5dee\u8ddd. - DAC \u901a\u8fc7\u5f15\u5165\u591a\u5c3a\u5ea6\u5224\u522b\u5668\u635f\u5931\u6765\u5927\u5e45\u63d0\u9ad8\u7f16\u89e3\u7801\u5668\u91cd\u5efa\u7684\u8d28\u91cf. - FACodec \u5f00\u521b\u4e86\u89e3\u8026\u7f16\u89e3\u7801\u5668\u7684\u6982\u5ff5, \u5c06\u8bed\u97f3\u5206\u89e3\u4e3a\u97f3\u8272, \u5185\u5bb9, \u8bed\u8c03, \u548c\u58f0\u5b66\u7ec6\u8282, \u5e76\u5c55\u793a\u4e86\u4e00\u5b9a\u7a0b\u5ea6\u7684\u8bed\u97f3\u8f6c\u6362\u80fd\u529b.</p> <p>\u4e3a\u4e86\u589e\u5f3a\u89e3\u8026\u7f16\u89e3\u7801\u5668, \u6211\u4eec\u63a2\u7d22\u4e86\u4fee\u6539\u5e76\u884c\u5411\u91cf\u91cf\u5316\u7ed3\u6784\u4e3a\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u5f62\u5f0f, \u5e76\u5c06\u989d\u5916\u7684\u98ce\u683c\u76d1\u7763\u4fe1\u53f7\u7eb3\u5165\u5176\u4e2d. \u7136\u800c, \u4e0e FACodec \u76f8\u6bd4\u6211\u4eec\u7684\u52aa\u529b\u6ca1\u6709\u4ea7\u751f\u660e\u663e\u7684\u6539\u5584. \u56e0\u6b64, \u6211\u4eec\u4f7f\u7528\u9884\u8bad\u7ec3\u7684 FACodec \u63d0\u53d6 ControlSpeech \u4e2d\u7684\u76f8\u5e94\u7f16\u89e3\u7801\u5668\u8868\u793a.  </p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#a2zero-shot-tts","title":"A.2.Zero-shot TTS","text":"\u539f\u6587  &gt; Zero-shot speech synthesis refers to the ability to synthesize the voice of an unseen speaker based solely on a few seconds of audio prompt, also known as voice cloning. &gt; In recent months, with the advancement of generative large-scale models, a plethora of outstanding works have emerged. &gt; [VALL-E](2023.01.05_VALL-E.md) leverages discrete codec representations and combines autoregressive and non-autoregressive models in a cascaded manner, preserving the powerful contextual capabilities of language models. &gt; [NaturalSpeech 2](../Diffusion/2023.04.18_NaturalSpeech2.md) employs continuous vectors instead of discrete neural codec tokens and introduces in-context learning to a latent diffusion model. &gt; [SPEAR-TTS](2023.02.07_SPEAR-TTS.md) and [Make-A-Voice](../../Models/_../_tmp/2023.05.30_Make-A-Voice.mdntic tokens to reduce the gap between text and acoustic features. &gt; [VoiceBox](2023.06.23_VoiceBox.md) is a nonautoregressive flow-matching model trained to infill speech, given audio context and text. &gt; [Mega-TTS](2023.06.06_Mega-TTS.md), [Mega-TTS2](2023.07.14_Mega-TTS2.md), on the other hand, utilizes traditional mel-spectrograms, decoupling timbre and prosody and further modeling the prosody using an autoregressive approach. &gt; [VoiceBox](2023.06.23_VoiceBox.md) and [P-Flow](../../Models/_../_tmp/P-Flow.mdng models as generators, demonstrating robust generative performance. &gt; [SoundStorm](2023.05.16_SoundStorm.md) and [MobileSpeech](../../Models/_../_tmp/2024.02.14_MobileSpeech.mdn-autoregressive and mask-based iterative generation method, achieving an excellent balance between inference speed and generation quality. &gt; Most relevant to our work is [NaturalSpeech 3](../Diffusion/2024.03.05_NaturalSpeech3.md), which also leverages the latest disentangled codec but primarily aims to enhance zero-shot model performance. &gt; In contrast, we argue that a disentangled codec is inherently more suitable as the speech representation for controllable generative models. &gt; It is noteworthy that current zero-shot TTS models are unable to achieve arbitrary language style control. &gt; ***ControlSpeech*** is the first TTS model capable of simultaneously performing zero-shot timbre cloning and style control.   <p>\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u6307\u7684\u662f\u4ec5\u57fa\u4e8e\u6570\u79d2\u7684\u97f3\u9891\u63d0\u793a\u5408\u6210\u672a\u89c1\u8fc7\u7684\u8bf4\u8bdd\u4eba\u58f0\u97f3, \u4e5f\u79f0\u4e3a\u58f0\u97f3\u514b\u9686. \u8fd1\u51e0\u4e2a\u6708, \u968f\u7740\u751f\u6210\u5f0f\u5927\u89c4\u6a21\u6a21\u578b\u7684\u8fdb\u6b65, \u8d8a\u6765\u8d8a\u591a\u7684\u4f18\u79c0\u5de5\u4f5c\u51fa\u73b0. - VALL-E \u5229\u7528\u79bb\u6563\u7f16\u89e3\u7801\u5668\u8868\u793a, \u4ee5\u7ea7\u8054\u7684\u65b9\u5f0f\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u975e\u81ea\u56de\u5f52\u6a21\u578b, \u4fdd\u7559\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u4e0a\u4e0b\u6587\u80fd\u529b. - NaturalSpeech 2 \u4ee5\u8fde\u7eed\u5411\u91cf\u800c\u4e0d\u662f\u79bb\u6563\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u6807\u8bb0, \u5e76\u5f15\u5165\u9690\u53d8\u91cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5185\u5728\u5b66\u4e60. - SPEAR-TTS \u548c [Make-A-Voice](../../Models/../_tmp/2023.05.30_Make-A-Voice.md\u97f3\u9891\u7279\u5f81\u4e4b\u95f4\u7684\u5dee\u8ddd. - Mega-TTS, Mega-TTS2 \u91c7\u7528\u4f20\u7edf\u7684\u6885\u5c14\u9891\u8c31, \u5206\u79bb\u97f3\u8272\u548c\u8bed\u8c03, \u5e76\u4f7f\u7528\u81ea\u56de\u5f52\u65b9\u6cd5\u8fdb\u4e00\u6b65\u5efa\u6a21\u8bed\u8c03. - VoiceBox \u548c [P-Flow](../../Models/../tmp/P-Flow.md\u5c55\u793a\u51fa\u5065\u58ee\u7684\u751f\u6210\u6027\u80fd. - SoundStorm \u548c [MobileSpeech](../../Models/../_tmp/2024.02.14_MobileSpeech.md\u4ee3\u751f\u6210\u65b9\u6cd5, \u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u7684\u826f\u597d\u5e73\u8861. - \u4e0e\u6211\u4eec\u7684\u5de5\u4f5c\u6700\u76f8\u5173\u7684\u662f NaturalSpeech 3, \u5b83\u4e5f\u91c7\u7528\u6700\u65b0\u89e3\u8026\u7f16\u89e3\u7801\u5668, \u4f46\u4e3b\u8981\u76ee\u6807\u662f\u63d0\u9ad8\u96f6\u6837\u672c\u6a21\u578b\u6027\u80fd.</p> <p>\u4f46\u6211\u4eec\u8ba4\u4e3a\u89e3\u8026\u7f16\u89e3\u7801\u5668\u5728\u63a7\u5236\u751f\u6210\u6a21\u578b\u4e2d\u66f4\u4e3a\u5408\u9002. \u503c\u5f97\u6ce8\u610f\u7684\u662f, \u5f53\u524d\u7684\u96f6\u6837\u672c TTS \u6a21\u578b\u65e0\u6cd5\u5b9e\u73b0\u4efb\u610f\u8bed\u8a00\u98ce\u683c\u63a7\u5236. ControlSpeech \u662f\u7b2c\u4e00\u4e2a TTS \u6a21\u578b, \u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u96f6\u6837\u672c\u97f3\u8272\u514b\u9686\u548c\u98ce\u683c\u63a7\u5236.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#a3text-prompt-based-controllable-tts","title":"A.3.Text prompt based controllable TTS","text":"\u539f\u6587  &gt; Some recent studies propose to control speech style through natural language prompts, providing a more interpretable and user-friendly approach of control. &gt; [PromptTTS](../_tmp/2022.11.22_PromptTTS.md) employs manually annotated text prompts to describe four to five attributes of speech (gender, pitch, speaking speed, energy, and emotion) and trains model on two synthesized speaker datasets and LibriTTS. &gt; [InstructTTS](../_tmp/2023.01.31_InstructTTS.md) is a three-stage training approach to capture semantic information from natural language style prompts as conditioning to the TTS system. &gt; [Textrolspeech](../../Datasets/2023.08.28_TextrolSpeech.md) introduces an efficient architecture which treats text controllable TTS as a language model task. &gt; [PromptStyle](../E2E/2023.05.31_PromptStyle.md) proposes a two-stage TTS approach for cross-speaker style transfer with natural language descriptions based on [VITS](../E2E/2021.06.11_VITS.md). &gt; [PromptTTS 2](../_tmp/2023.09.05_PromptTTS2.md) proposes an automatic description creation pipeline leveraging LLM [4] and adopts a diffusion model to capture the one-to-many relationship. &gt; It is noteworthy that existing style-controllable TTS models are either speaker-independent or can only control timbre using speaker IDs, without the capability for timbre cloning. &gt; The introduction of ***ControlSpeech*** significantly expands the boundaries of the controllable TTS task.   <p>\u4e00\u4e9b\u8fd1\u671f\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u63a7\u5236\u8bed\u97f3\u98ce\u683c, \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u53cb\u597d\u7684\u63a7\u5236\u65b9\u5f0f. - PromptTTS \u4ee5\u624b\u5de5\u6ce8\u91ca\u7684\u6587\u672c\u63d0\u793a\u63cf\u8ff0\u8bed\u97f3\u7684\u56db\u5230\u4e94\u4e2a\u5c5e\u6027 (\u6027\u522b, \u97f3\u9ad8, \u8bf4\u8bdd\u901f\u5ea6, \u80fd\u91cf, \u4ee5\u53ca\u60c5\u7eea), \u5e76\u5728\u4e24\u4e2a\u5408\u6210\u8bf4\u8bdd\u4eba\u6570\u636e\u96c6\u548c LibriTTS \u4e0a\u8bad\u7ec3\u6a21\u578b. - InstructTTS \u662f\u4e00\u79cd\u4e09\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5, \u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u98ce\u683c\u63d0\u793a\u4e2d\u6355\u83b7\u8bed\u4e49\u4fe1\u606f, \u5e76\u5c06\u5176\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u5230 TTS \u7cfb\u7edf. - Textrolspeech \u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u67b6\u6784, \u5c06\u6587\u672c\u53ef\u63a7 TTS \u89c6\u4e3a\u8bed\u8a00\u6a21\u578b\u4efb\u52a1. - PromptStyle \u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e VITS \u7684\u8de8\u8bf4\u8bdd\u4eba\u98ce\u683c\u8f6c\u79fb\u7684\u4e24\u9636\u6bb5 TTS \u65b9\u6cd5, \u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0. - PromptTTS 2 \u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528 LLM [4] \u7684\u81ea\u52a8\u63cf\u8ff0\u521b\u5efa\u7ba1\u9053, \u5e76\u91c7\u7528\u6269\u6563\u6a21\u578b\u6765\u6355\u83b7\u4e00\u5bf9\u591a\u5173\u7cfb.</p> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\u73b0\u6709\u98ce\u683c\u53ef\u63a7\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u8981\u4e48\u548c\u8bf4\u8bdd\u4eba\u65e0\u5173, \u8981\u4e48\u53ea\u80fd\u901a\u8fc7\u8bf4\u8bdd\u4eba ID \u63a7\u5236\u97f3\u8272, \u6ca1\u6709\u58f0\u97f3\u514b\u9686\u80fd\u529b. ControlSpeech \u5f15\u5165\u4e86\u8bed\u97f3\u98ce\u683c\u53ef\u63a7\u7684 TTS \u6a21\u578b, \u6269\u5c55\u4e86\u63a7\u5236 TTS \u4efb\u52a1\u7684\u8fb9\u754c.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#3controlspeech","title":"3.ControlSpeech","text":"\u539f\u6587  &gt; In this section, we will first introduce the motivation and overall design of ***ControlSpeech***. &gt; Next, we will provide a detailed explanation of the disentanglement and generation process of the codec, followed by the ***Style Mixture Semantic Density (SMSD)*** module. &gt; Finally, we will discuss the training loss and inference process of ***ControlSpeech***.   <p>\u672c\u8282, \u6211\u4eec\u9996\u5148\u4ecb\u7ecd ControlSpeech \u7684\u52a8\u673a\u548c\u603b\u4f53\u8bbe\u8ba1. \u7136\u540e, \u6211\u4eec\u5c06\u8be6\u7ec6\u89e3\u91ca\u7f16\u89e3\u7801\u5668\u7684\u89e3\u8026\u548c\u751f\u6210\u8fc7\u7a0b, \u5e76\u4ecb\u7ecd Style Mixture Semantic Density (SMSD) \u6a21\u5757. \u6700\u540e, \u6211\u4eec\u5c06\u8ba8\u8bba ControlSpeech \u7684\u8bad\u7ec3\u635f\u5931\u548c\u63a8\u7406\u8fc7\u7a0b.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#31overall","title":"3.1.Overall","text":"\u539f\u6587  &gt; To achieve simultaneous zero-shot timbre cloning and style cloning, one viable approach is to leverage a large-scale pre-trained disentangled codec space. &gt; On one hand, extensive pre-training on a diverse speaker dataset ensures fundamental zero-shot capabilities. &gt; On the other hand, disentangled codec representations effectively reduce the entanglement between timbre and style. &gt; As illustrated in Fig.02, ***ControlSpeech*** is fundamentally an encoder-decoder model based on [FastSpeech2](../TTS2_Acoustic/2020.06.08_FastSpeech2.md) for codec generation. &gt; The dashed box represents frame-level features, while the solid box represents global features. &gt; The ***Style Mixture Semantic Density (SMSD)*** module samples style text to generate corresponding global style representations, which are then combined with text representations from the text encoder via a cross-attention module. &gt; These combined representations are fed into the duration prediction model and subsequently into the codec generator, which is a non-autoregressive [Conformer](../ASR/2020.05.16_Conformer.md) based on mask iteration and parallel generation. &gt; The timbre extractor is a Transformer encoder that converts the output of the speech encoder into a global vector, representing the timbre attributes. &gt; By inputting a style description $X_s$, a content text $X_c$, and a speech prompt $X_t$, ***ControlSpeech*** aims to sequentially generate the corresponding style codec $Y_s$, content codec $Y_c$, and timbre embedding $Y_t$. &gt; These representations are then concatenated and upsampled into speech through the [pre-trained codec decoder](../Diffusion/2024.03.05_NaturalSpeech3.md).   <p>\u4e3a\u4e86\u5b9e\u73b0\u540c\u6b65\u7684\u96f6\u6837\u672c\u97f3\u8272\u514b\u9686\u548c\u98ce\u683c\u514b\u9686, \u4e00\u4e2a\u53ef\u884c\u7684\u65b9\u6cd5\u662f\u5229\u7528\u4e00\u4e2a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89e3\u8026\u7f16\u89e3\u7801\u7a7a\u95f4. \u4e00\u65b9\u9762, \u5728\u591a\u6837\u8bf4\u8bdd\u4eba\u6570\u636e\u96c6\u4e0a\u5e7f\u6cdb\u5730\u9884\u8bad\u7ec3\u80fd\u591f\u786e\u4fdd\u57fa\u672c\u7684\u96f6\u6837\u672c\u80fd\u529b. \u53e6\u4e00\u65b9\u9762, \u89e3\u8026\u7f16\u89e3\u7801\u8868\u793a\u80fd\u6709\u6548\u5730\u51cf\u5c11\u97f3\u8272\u548c\u98ce\u683c\u7684\u7ea0\u7f20. \u5982\u56fe 02 \u6240\u793a, ControlSpeech \u662f\u57fa\u4e8e FastSpeech2 \u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u7528\u4e8e\u7f16\u89e3\u7801\u751f\u6210. \u865a\u7ebf\u6846\u8868\u793a\u5e27\u7ea7\u7279\u5f81, \u5b9e\u7ebf\u6846\u8868\u793a\u5168\u5c40\u7279\u5f81.</p> <p></p> <p>\u98ce\u683c\u6df7\u5408\u8bed\u4e49\u5bc6\u5ea6 (Style Mixture Semantic Density, SMSD) \u6a21\u5757\u91c7\u6837\u98ce\u683c\u6587\u672c\u4ee5\u751f\u6210\u5bf9\u5e94\u7684\u5168\u5c40\u98ce\u683c\u8868\u793a, \u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u6a21\u5757\u4e0e\u6587\u672c\u7f16\u7801\u5668\u7684\u8868\u793a\u76f8\u7ed3\u5408. \u8fd9\u4e9b\u7ed3\u5408\u7684\u8868\u793a\u88ab\u8f93\u5165\u5230\u65f6\u957f\u9884\u6d4b\u6a21\u578b\u7136\u540e\u4f20\u5165\u7f16\u89e3\u7801\u751f\u6210\u5668, \u8be5\u7f16\u89e3\u7801\u751f\u6210\u5668\u662f\u57fa\u4e8e\u63a9\u7801\u8fed\u4ee3\u548c\u5e76\u884c\u751f\u6210\u7684\u975e\u81ea\u56de\u5f52 Conformer. \u97f3\u8272\u63d0\u53d6\u5668\u5219\u662f\u4e00\u4e2a Transformer \u7f16\u7801\u5668, \u5b83\u5c06\u8bed\u97f3\u7f16\u7801\u5668\u7684\u8f93\u51fa\u8f6c\u6362\u4e3a\u5168\u5c40\u5411\u91cf, \u4ee3\u8868\u97f3\u8272\u5c5e\u6027. \u901a\u8fc7\u8f93\u5165\u98ce\u683c\u63cf\u8ff0 $X_s$, \u5185\u5bb9\u6587\u672c $X_c$, \u8bed\u97f3\u63d0\u793a $X_t$, ControlSpeech \u8bd5\u56fe\u4f9d\u6b21\u751f\u6210\u5bf9\u5e94\u7684\u98ce\u683c\u7f16\u89e3\u7801 $Y_s$, \u5185\u5bb9\u7f16\u89e3\u7801 $Y_c$, \u97f3\u8272\u5d4c\u5165 $Y_t$. \u8fd9\u4e9b\u8868\u793a\u4e4b\u540e\u62fc\u63a5\u540e\u901a\u8fc7\u9884\u8bad\u7ec3\u7f16\u89e3\u7801\u5668\u89e3\u7801\u5668 FACodec \u8fdb\u884c\u4e0a\u91c7\u6837, \u751f\u6210\u8bed\u97f3.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#32decoupling-and-generation-of-codec","title":"3.2.Decoupling and Generation of Codec","text":"\u539f\u6587  &gt; We utilize [FACodec](../Diffusion/2024.03.05_NaturalSpeech3.md) as our codec disentangler. &gt; During the training of ***ControlSpeech***, we freeze the corresponding codec encoder to obtain downsampled compressed audio frames $h$ from the speech $Y$. &gt; The frames $h$ are processed through the disentangling quantizer module and the timbre extractor module to derive the original content codec $Y_c$, prosody codec $Y_p$, acoustic codec $Y_a$, and timbre information $Y_t$. &gt; We exclude the content $Y_c$ and timbre information $Y_t$ of the representation collectively referred to as style $Y_s$. &gt; In practice, we concatenate the prosody codec $Y_p$ and the acoustic codec $Y_a$ along the channel dimension to obtain the corresponding style codec $Y_s$, as represented below:   <p>$$   Y_s = h-Y_c-Y_t = \\text{concat}(Y_p,Y_a)\\tag{01} $$</p> <p>\u6211\u4eec\u4f7f\u7528 FACodec \u4f5c\u4e3a\u6211\u4eec\u7684\u7f16\u89e3\u7801\u89e3\u8026\u5668. \u5728 ControlSpeech \u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u51bb\u7ed3\u76f8\u5e94\u7684\u7f16\u89e3\u7801\u5668\u7f16\u7801\u5668\u4ee5\u4ece\u8bed\u97f3 $Y$ \u83b7\u5f97\u4e0b\u91c7\u6837\u538b\u7f29\u7684\u97f3\u9891\u5e27 $h$. \u5e27 $h$ \u901a\u8fc7\u89e3\u8026\u91cf\u5316\u5668\u6a21\u5757\u548c\u97f3\u8272\u63d0\u53d6\u5668\u6a21\u5757, \u5bfc\u51fa\u539f\u59cb\u5185\u5bb9\u7f16\u89e3\u7801 $Y_c$, \u8bed\u8c03\u7f16\u89e3\u7801 $Y_p$, \u58f0\u5b66\u7f16\u89e3\u7801 $Y_a$, \u4ee5\u53ca\u97f3\u8272\u4fe1\u606f $Y_t$. \u6211\u4eec\u6392\u9664\u5185\u5bb9 $Y_c$ \u548c\u97f3\u8272\u4fe1\u606f $Y_t$ \u540e\u5e76\u5c06\u5176\u7edf\u79f0\u4e3a\u98ce\u683c $Y_s$. \u5b9e\u9645\u4e0a, \u6211\u4eec\u6cbf\u7740\u901a\u9053\u7ef4\u5ea6\u5c06\u8bed\u8c03\u7f16\u89e3\u7801 $Y_p$ \u548c\u58f0\u5b66\u7f16\u89e3\u7801 $Y_a$ \u8fde\u63a5\u8d77\u6765, \u5f97\u5230\u76f8\u5e94\u7684\u98ce\u683c\u7f16\u89e3\u7801 $Y_s$, \u5982\u516c\u5f0f 01 \u6240\u793a.</p> \u539f\u6587  &gt; The codec generation process comprises two stages. &gt; In the first stage, based on paired text-speech data $\\{X, Y_{codec}\\}$, where $X = \\{x_1, x_2, x_3, \\cdots, x_T \\}$ represents the fusion of global style and aligned text and $Y_{codec}$ denotes the representation of speech through vector quantization, formula as:  $$   Y_{codec} = Y_s+Y_c = C_{1:T,1:N}\\in \\mathbb{R}^{T\\times N}\\tag{02} $$  &gt; we consider $T$ as the downsampled utterance length, which is equal to the length of the text. &gt; $N$ represents the number of channels for every frame. &gt; The row vector of each acoustic code matrix $C_{t,:}$ represents the six codes for frame $t$, and the column vector of each acoustic code matrix $C_{:,i}$ represents the code sequence from the $j$-th codebook, where $i \\in \\{1, 2, \\cdots , N \\}$.   <p></p> <p>\u7f16\u89e3\u7801\u751f\u6210\u8fc7\u7a0b\u7531\u4e24\u4e2a\u9636\u6bb5\u7ec4\u6210. \u5728\u7b2c\u4e00\u4e2a\u9636\u6bb5, \u57fa\u4e8e\u6210\u5bf9\u7684\u6587\u672c-\u8bed\u97f3\u6570\u636e ${X, Y_{codec}}$, \u5176\u4e2d $X = {x_1, x_2, x_3, \\cdots, x_T }$ \u8868\u793a\u5168\u5c40\u98ce\u683c\u548c\u5bf9\u9f50\u6587\u672c\u7684\u878d\u5408, $Y_{codec}$ \u4ee3\u8868\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u8868\u793a\u7684\u8bed\u97f3\u8868\u793a.</p> <p>$$   Y_{codec} = Y_s+Y_c = C_{1:T,1:N}\\in \\mathbb{R}^{T\\times N}\\tag{02} $$</p> <p>\u6211\u4eec\u8ba4\u4e3a $T$ \u4e3a\u4e0b\u91c7\u6837\u7684\u53d1\u8a00\u957f\u5ea6, \u7b49\u4e8e\u6587\u672c\u7684\u957f\u5ea6. $N$ \u8868\u793a\u6bcf\u5e27\u7684\u901a\u9053\u6570. \u6bcf\u4e00\u884c\u5411\u91cf $C_{t,:}$ \u8868\u793a\u7b2c $t$ \u5e27\u7684\u516d\u4e2a\u7f16\u7801, \u800c\u6bcf\u4e00\u5217\u5411\u91cf $C_{:,i}$ \u8868\u793a\u7b2c $j$ \u4e2a\u7801\u672c\u7684\u7801\u5e8f\u5217, \u5176\u4e2d $i \\in {1, 2, \\cdots , N }$.</p> \u539f\u6587  &gt; Follow [VALL-E](2023.01.05_VALL-E.md), in the training process of ***ControlSpeech***, we randomly select a certain channel of $C_{1:T,1:N}$ for training. &gt; For the generation of the $i$ channel $P(C_{1:T,i} | X_{1:T}; \\theta)$, we employ a mask-based generative model as our parallel decoder. &gt; We sample the mask $M_i \\in \\{0, 1\\}^T$ according to a cosine schedule [5] for codec level $i$, specifically sampling the masking ratio $p = \\cos(u')$ where ($u'\\sim U[0,\\dfrac{\\pi}{2}]$) and the mask $M_i \\sim Bernoulli(p)$. &gt; Here, $M_i$ represents the portion to be masked in the $i$-th level, while $M'_i$ denotes the unmasked portion in the $i$-th level. &gt; As shown in Fig.02 (c), the prediction of this portion is refined based on the prompt $j$ ($j &lt; i$) channels and the concatenation of target text and the unmasked portion of the $i$ channel. &gt; Therefore, the prediction for this part can be specified as follows:  $$   P(C_{1:T,i} | X_{1:T}; \\theta) = P(M_i C_{1:T,i}|C_{1:T,   \u9075\u5faa [VALL-E](2023.01.05_VALL-E.md), \u5728 ***ControlSpeech*** \u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u968f\u673a\u9009\u62e9 $C_{1:T,1:N}$ \u4e2d\u7684\u67d0\u4e00\u901a\u9053\u7528\u4e8e\u8bad\u7ec3. \u5bf9\u4e8e\u7b2c $i$ \u4e2a\u901a\u9053\u7684\u751f\u6210 $P(C_{1:T,i} | X_{1:T}; \\theta)$, \u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u63a9\u7801\u7684\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u5e76\u884c\u89e3\u7801\u5668. \u6211\u4eec\u6839\u636e\u4f59\u5f26\u8c03\u5ea6 [5] \u91c7\u6837\u63a9\u7801 $M_i \\in \\{0, 1\\}^T$, \u5177\u4f53\u5730, \u6211\u4eec\u91c7\u6837\u63a9\u7801\u6bd4\u4f8b $p = \\cos(u')$ \u5176\u4e2d ($u'\\sim U[0,\\dfrac{\\pi}{2}]$) \u4ee5\u53ca\u63a9\u7801 $M_i \\sim Bernoulli(p)$. \u8fd9\u91cc, $M_i$ \u8868\u793a\u7b2c $i$ \u7ea7\u7684\u63a9\u7801\u90e8\u5206, \u800c $M'_i$ \u8868\u793a\u7b2c $i$ \u7ea7\u7684\u672a\u63a9\u7801\u90e8\u5206. \u5982\u56fe 02 C \u6240\u793a, \u8fd9\u90e8\u5206\u7684\u9884\u6d4b\u57fa\u4e8e\u63d0\u793a $j$ ($j &lt; i$) \u901a\u9053\u548c\u76ee\u6807\u6587\u672c\u548c\u672a\u63a9\u7801\u90e8\u5206\u7684\u62fc\u63a5. \u56e0\u6b64, \u5bf9\u4e8e\u8fd9\u4e00\u90e8\u5206\u7684\u9884\u6d4b, \u53ef\u4ee5\u5982\u4e0b\u5b9a\u4e49:   $$   P(C_{1:T,i} | X_{1:T}; \\theta) = P(M_i C_{1:T,i}|C_{1:T, \u539f\u6587  &gt; In the second stage, following [AdaSpeech](../TTS2_Acoustic/2021.03.01_AdaSpeech.md), we utilize a conditional normalization layer to fuse the previously obtained $Y_{codec} and $Y_t$, producing the input for the codec decoder. &gt; This input is then processed by the pre-trained decoder to generate the final speech output $Y$. &gt; Specifically, we first use two simple linear layers $W_{\\gamma}$ and $W_{\\beta}$, which take the speaker embedding $Y_s$ as input and output the scale vectors $\\gamma$ and bias vectors $\\beta$ respectively. &gt; These lightweight, learnable scale vectors $\\gamma$ and bias vectors $\\beta$ are then fused with $Y_{codec}$. &gt; This process can be described by the following formula:  $$   Y = \\text{CodecDecoder}(W_{\\gamma}Y_t\\dfrac{Y_{codec}-\\mu_c}{\\sigma_c^2} +W_{\\beta} Y_t)\\tag{04} $$  &gt; where $\\mu_c$ and $\\sigma_c^2$ are the mean and variance of the hidden representation of $Y_{codec}$.   <p></p> <p>\u7b2c\u4e8c\u4e2a\u9636\u6bb5, \u9075\u5faa AdaSpeech, \u6211\u4eec\u5229\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u5c42\u878d\u5408\u4e4b\u524d\u83b7\u5f97\u7684 $Y_{codec}$ \u548c $Y_t$, \u4ea7\u751f\u7f16\u89e3\u7801\u5668\u89e3\u7801\u5668\u7684\u8f93\u5165. \u8f93\u5165\u88ab\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u5904\u7406\u540e\u751f\u6210\u6700\u7ec8\u7684\u8bed\u97f3\u8f93\u51fa $Y$. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u9996\u5148\u4f7f\u7528\u4e24\u4e2a\u7b80\u5355\u7ebf\u6027\u5c42 $W_{\\gamma}$ \u548c $W_{\\beta}$, \u5b83\u4eec\u63a5\u6536\u8bf4\u8bdd\u8005\u5d4c\u5165 $Y_s$ \u4f5c\u4e3a\u8f93\u5165, \u8f93\u51fa\u5c3a\u5ea6\u5411\u91cf $\\gamma$ \u548c\u504f\u5dee\u5411\u91cf $\\beta$ \u5404\u81ea. \u8fd9\u4e9b\u8f7b\u91cf\u5316\u7684, \u53ef\u5b66\u4e60\u7684\u5c3a\u5ea6\u5411\u91cf $\\gamma$ \u548c\u504f\u5dee\u5411\u91cf $\\beta$ \u968f\u540e\u4e0e $Y_{codec}$ \u878d\u5408. \u8fd9\u4e00\u8fc7\u7a0b\u53ef\u4ee5\u7528\u4e0b\u9762\u7684\u516c\u5f0f\u8868\u793a:</p> <p>$$   Y = \\text{CodecDecoder}(W_{\\gamma}Y_t\\dfrac{Y_{codec}-\\mu_c}{\\sigma_c^2} +W_{\\beta} Y_t)\\tag{04} $$</p> <p>\u5176\u4e2d $\\mu_c$ \u548c $\\sigma_c^2$ \u662f $Y_{codec}$ \u7684\u9690\u85cf\u8868\u793a\u7684\u5747\u503c\u548c\u65b9\u5dee.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#33style-mixture-semantic-density-modules","title":"3.3.Style Mixture Semantic Density Modules","text":"\u539f\u6587  &gt; During our experiments, we identified a many-to-many problem in the text style control module. &gt; Specifically, different style texts can describe the same style of speech. &gt; Similar to previous approaches ([PromptTTS](../_tmp/2022.11.22_PromptTTS.md), [PromptStyle](../E2E/2023.05.31_PromptStyle.md)), we utilize a pre-trained BERT model to extract the semantic information of style descriptions, thereby enhancing the generalization of out-of-domain style descriptions. &gt; Moreover, it has been largely overlooked that a single style text can correspond to multiple speech instances from the same speaker. &gt; For example, when using a text describing a happy tone to control audio, various degrees of happiness can be attributed to that description. &gt; [PromptTTS 2](../_tmp/2023.09.05_PromptTTS2.md) recognized this issue and introduced a speech prompt to compensate for the details of the text description style, alleviating the one-to-many problem by incorporating additional information. &gt; However, in the task of simultaneously zero-shot cloning timbre and zero-shot controlling style in speech synthesis, this approach can cause interference between the speech prompt and the style prompt. &gt; Therefore, in ***ControlSpeech***, we designed a novel ***Style Mixture Semantic Density (SMSD)*** module to address the one-to-many problem of style representation. &gt; We hypothesize that the semantic representation of style $X'_s$ is a global mixture of Gaussian distributions, where different Gaussian distributions represent varying degrees of a particular style. &gt; During inference, we sample from the mixture of style semantic distributions to obtain an independent Gaussian distribution, with each sampled distribution reflecting different degrees of the same style. &gt; Additionally, to further enhance the diversity of style control, we incorporated a noise perturbation module within the MDN network of the ***SMSD*** in the ***ControlSpeech*** model.  &gt; **This module controls the isotropy of perturbations across different dimensions**.   <p>\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u53d1\u73b0\u6587\u672c\u98ce\u683c\u63a7\u5236\u6a21\u5757\u4e2d\u5b58\u5728\u591a\u5bf9\u591a\u95ee\u9898. \u5177\u4f53\u5730, \u4e0d\u540c\u98ce\u683c\u6587\u672c\u53ef\u4ee5\u63cf\u8ff0\u76f8\u540c\u98ce\u683c\u7684\u8bed\u97f3. \u7c7b\u4f3c\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5 (PromptTTS, PromptStyle), \u6211\u4eec\u4f7f\u7528\u9884\u8bad\u7ec3\u7684 BERT \u6a21\u578b\u7528\u4e8e\u63d0\u53d6\u98ce\u683c\u63cf\u8ff0\u7684\u8bed\u97f3\u4fe1\u606f, \u4ece\u800c\u589e\u5f3a\u9886\u57df\u5916\u98ce\u683c\u63cf\u8ff0\u7684\u6cdb\u5316. \u6b64\u5916, \u5355\u4e2a\u98ce\u683c\u6587\u672c\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2a\u6765\u81ea\u540c\u4e00\u8bf4\u8bdd\u4eba\u7684\u8bed\u97f3\u5b9e\u4f8b\u7684\u60c5\u51b5\u88ab\u6781\u5927\u5ffd\u89c6. \u4f8b\u5982, \u5f53\u4f7f\u7528\u6587\u672c\u63cf\u8ff0\u5f00\u5fc3\u7684\u8bed\u6c14\u6765\u63a7\u5236\u97f3\u9891\u65f6, \u4e0d\u540c\u7684\u5f00\u5fc3\u7a0b\u5ea6\u53ef\u4ee5\u5f52\u56e0\u4e8e\u8be5\u63cf\u8ff0. PromptTTS 2 \u53d1\u73b0\u4e86\u8fd9\u4e2a\u95ee\u9898, \u5e76\u5f15\u5165\u4e86\u8bed\u97f3\u63d0\u793a\u6765\u8865\u507f\u6587\u672c\u63cf\u8ff0\u98ce\u683c\u7684\u7ec6\u8282, \u901a\u8fc7\u5f15\u5165\u989d\u5916\u4fe1\u606f\u6765\u7f13\u89e3\u4e00\u5bf9\u591a\u95ee\u9898. \u7136\u800c, \u5728\u96f6\u6837\u672c\u514b\u9686\u97f3\u8272\u548c\u96f6\u6837\u672c\u63a7\u5236\u98ce\u683c\u7684\u8bed\u97f3\u5408\u6210\u4efb\u52a1\u4e2d, \u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8bed\u97f3\u63d0\u793a\u548c\u98ce\u683c\u63d0\u793a\u4e4b\u95f4\u7684\u5e72\u6270. \u56e0\u6b64, \u5728 ControlSpeech \u4e2d, \u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684 Style Mixture Semantic Density (SMSD) \u6a21\u5757\u6765\u89e3\u51b3\u98ce\u683c\u8868\u793a\u4e2d\u7684\u4e00\u5bf9\u591a\u95ee\u9898. \u6211\u4eec\u5047\u8bbe\u98ce\u683c\u8bed\u4e49\u8868\u793a $X'_s$ \u662f\u9ad8\u65af\u5206\u5e03\u7684\u5168\u5c40\u6df7\u5408, \u5176\u4e2d\u4e0d\u540c\u7684\u9ad8\u65af\u5206\u5e03\u4ee3\u8868\u4e0d\u540c\u7a0b\u5ea6\u7684\u7279\u5b9a\u98ce\u683c. \u5728\u63a8\u7406\u65f6, \u6211\u4eec\u4ece\u98ce\u683c\u8bed\u4e49\u5206\u5e03\u7684\u6df7\u5408\u91c7\u6837\u4ee5\u83b7\u5f97\u72ec\u7acb\u9ad8\u65af\u5206\u5e03, \u6bcf\u4e2a\u91c7\u6837\u7684\u5206\u5e03\u53cd\u6620\u51fa\u76f8\u540c\u98ce\u683c\u7684\u4e0d\u540c\u7a0b\u5ea6. \u6b64\u5916, \u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u98ce\u683c\u63a7\u5236\u7684\u591a\u6837\u6027, \u6211\u4eec\u5728 ControlSpeech \u6a21\u578b\u7684 SMSD \u7f51\u7edc\u4e2d\u5f15\u5165\u4e86\u566a\u58f0\u6270\u52a8\u6a21\u5757. \u8be5\u6a21\u5757\u63a7\u5236\u4e0d\u540c\u7ef4\u5ea6\u7684\u6270\u52a8\u7684\u540c\u8d28\u6027.</p> \u539f\u6587  &gt; Specifically, the raw style prompt sequence $X_s = [X_1, X_2, X_3, \\cdots, X_L]$ is prepended with a `[CLS]` token, converted into a word embedding, and fed into the BERT model, where $L$ refers to the length of style prompt. &gt; The hidden vector corresponding to the `[CLS]` token is regarded as the global style semantic representation $X_s$ to guide the generation and sampling of subsequent models.  &gt; Based on MDN network [53, 18, 10, 12], suppose we want to regress response target style representation $Y'_s\\in\\mathbb{R}^d$ by using covariates style semantic input representation $X'_s\\in\\mathbb{R}^{n}$.  &gt; We model the conditional distribution as a mixture of Gaussian distribution, The formula is as follows:  $$   P_{\\theta}(Y'_s|X'_s) = \\sum_{k=1}^K \\pi_k\\mathcal{N}(Y'_s|\\mu^{(k)}, \\sigma^{2(k)})\\tag{05} $$  &gt; where $K$ is a hyperparameter about the number of independent Gaussian distribution and other mixture distribution parameters $\\pi_k$, $\\mu^{(k)}$, $\\sigma^{2(k)}$ are output by a neural MDN network $f_{\\theta}$ dependent on input style semantic representation $X'_s$, formula as:  $$   \\pi\\in \\Delta^{K-1},\\quad \\mu^{(k)}\\in\\mathbb{R}^d,\\quad \\sigma^{2(k)}\\in S_{+}^{d}=f_{\\theta}(X'_s)\\tag{06} $$  &gt; It should be noted that the sum of the mixture weights is constrained to 1 during the training phase, which can be achieved by applying a softmax function on the corresponding neural network output $a_k$, formalized as:  $$   \\pi_{k} = \\dfrac{\\exp(a_k)}{\\sum_{k=1}^{K} \\exp(a_k)}\\tag{07} $$   <p></p> <p>\u5177\u4f53\u5730, \u539f\u59cb\u98ce\u683c\u63d0\u793a\u5e8f\u5217 $X_s = [X_1, X_2, X_3, \\cdots, X_L]$ \u7528 <code>[CLS]</code> \u6807\u8bc6\u7b26\u52a0\u5728\u524d\u9762, \u7136\u540e\u8f6c\u6362\u4e3a\u8bcd\u5d4c\u5165, \u5e76\u8f93\u5165 BERT \u6a21\u578b, \u5176\u4e2d $L$ \u8868\u793a\u98ce\u683c\u63d0\u793a\u7684\u957f\u5ea6. \u5bf9\u5e94 <code>[CLS]</code> \u6807\u8bc6\u7b26\u7684\u9690\u85cf\u5411\u91cf\u88ab\u89c6\u4e3a\u5168\u5c40\u98ce\u683c\u8bed\u4e49\u8868\u793a $X_s$ \u6765\u5f15\u5bfc\u540e\u7eed\u751f\u6210\u548c\u91c7\u6837\u6a21\u578b.</p> <p>\u57fa\u4e8e MDN \u7f51\u7edc, \u5047\u8bbe\u6211\u4eec\u60f3\u901a\u8fc7\u534f\u53d8\u91cf\u98ce\u683c\u8bed\u4e49\u8f93\u5165\u8868\u793a $X'_s\\in\\mathbb{R}^{n}$ \u6765\u56de\u5f52\u54cd\u5e94\u76ee\u6807\u98ce\u683c\u8868\u793a $Y'_s\\in\\mathbb{R}^d$. \u6211\u4eec\u5c06\u6761\u4ef6\u5206\u5e03\u5efa\u6a21\u4e3a\u9ad8\u65af\u6df7\u5408\u5206\u5e03, \u516c\u5f0f\u5982\u4e0b:</p> <p>$$   P_{\\theta}(Y's|X'_s) = \\sum{k=1}^K \\pi_k\\mathcal{N}(Y'_s|\\mu^{(k)}, \\sigma^{2(k)})\\tag{05} $$</p> <p>\u5176\u4e2d $K$ \u662f\u5173\u4e8e\u72ec\u7acb\u9ad8\u65af\u5206\u5e03\u6570\u91cf\u7684\u8d85\u53c2\u6570, \u5176\u4ed6\u6df7\u5408\u5206\u5e03\u53c2\u6570 $\\pi_k$, $\\mu^{(k)}$, $\\sigma^{2(k)}$ \u7531 MDN \u7f51\u7edc $f_\\theta$ \u8f93\u51fa, \u4f9d\u8d56\u4e8e\u8f93\u5165\u98ce\u683c\u8bed\u4e49\u8868\u793a $X'_s$, \u516c\u5f0f\u5982\u4e0b:</p> <p>$$   \\pi\\in \\Delta^{K-1},\\quad \\mu^{(k)}\\in\\mathbb{R}^d,\\quad \\sigma^{2(k)}\\in S_{+}^{d}=f_{\\theta}(X'_s)\\tag{06} $$</p> <p>\u5e94\u8be5\u6ce8\u610f\u5230\u8bad\u7ec3\u9636\u6bb5\u6df7\u5408\u6743\u91cd\u7684\u548c\u4e3a 1, \u8fd9\u53ef\u4ee5\u5bf9\u76f8\u5e94\u7f51\u7edc\u8f93\u51fa $a_k$ \u5e94\u7528 Softmax \u51fd\u6570, \u5373</p> <p>$$   \\pi_{k} = \\dfrac{\\exp(a_k)}{\\sum_{k=1}^{K} \\exp(a_k)}\\tag{07} $$</p> \u539f\u6587  &gt; To further enhance the diversity of style control, we designed a specialized noise perturbation module within the ***SMSD*** module to constrain the noise model. &gt; As illustrated by the circles of ***SMSD*** module in Fig.02 (b), this noise perturbation module regulates the isotropy of perturbations $\\varepsilon$ across different dimensions in variance $\\sigma^{2(k)}$. &gt; The four types of perturbations from left to right in Fig.02 (b) are as follows: &gt; &gt; - Fully factored, let $\\sigma^{2(k)}=f_{\\theta}(X'_s)+f_{\\theta}(\\varepsilon)=\\text{diag}(\\sigma^{2(k)})\\in \\mathbb{R}_{+}^{d}$ where the noise level for each dimension is predicted separately. &gt; - Isotropic, let $\\sigma^{2(k)}=f_{\\theta}(X'_s)+f_{\\theta}(\\varepsilon)=\\sigma^{2(k)}I\\in \\mathbb{R}_{+}$ which assumes the same noise level for each dimension over $d$. &gt; - Isotropic across clusters, let $\\sigma^{2(k)}=f_{\\theta}(X'_s)+f_{\\theta}(\\varepsilon)=\\sigma^{2}I\\in \\mathbb{R}_{+}$ which assumes the same noise level for each dimension over $d$ and cluster. &gt; - Fixed isotropic, same as above but do not learn $\\sigma^2$.   <p></p> <p>\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u98ce\u683c\u63a7\u5236\u7684\u591a\u6837\u6027, \u6211\u4eec\u5728 SMSD \u6a21\u5757\u5185\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u566a\u58f0\u6270\u52a8\u6a21\u5757, \u4ee5\u7ea6\u675f\u566a\u58f0\u6a21\u578b. \u5982\u56fe 02 (b) \u4e2d SMSD \u6a21\u5757\u7684\u5706\u5708\u6240\u793a, \u8fd9\u4e2a\u566a\u58f0\u6270\u52a8\u6a21\u5757\u8c03\u8282\u4e86\u6270\u52a8 $\\varepsilon$ \u5728\u4e0d\u540c\u7ef4\u5ea6\u4e0a\u7684\u5404\u5411\u540c\u6027, \u8fd9\u4e9b\u6270\u52a8\u4f4d\u4e8e\u65b9\u5dee $\\sigma^{2(k)}$ \u4e2d.  \u56fe 02 (b) \u4e2d\u4ece\u5de6\u81f3\u53f3\u7684\u56db\u79cd\u6270\u52a8\u7c7b\u578b\u5982\u4e0b: - \u5b8c\u5168\u5206\u89e3, \u4ee4 $\\sigma^{2(k)}=f_{\\theta}(X's)+f{\\theta}(\\varepsilon)=\\text{diag}(\\sigma^{2(k)})\\in \\mathbb{R}{+}^{d}$ \u5176\u4e2d, \u6bcf\u4e2a\u7ef4\u5ea6\u7684\u566a\u58f0\u6c34\u5e73\u662f\u5206\u522b\u9884\u6d4b\u7684. - \u5404\u5411\u540c\u6027, \u4ee4 $\\sigma^{2(k)}=f{\\theta}(X's)+f{\\theta}(\\varepsilon)=\\sigma^{2(k)}I\\in \\mathbb{R}{+}$ \u5176\u4e2d, \u6bcf\u4e2a\u7ef4\u5ea6\u7684\u566a\u58f0\u6c34\u5e73\u662f\u76f8\u540c\u7684. - \u5404\u5411\u540c\u6027\u7684\u7c07, \u4ee4 $\\sigma^{2(k)}=f{\\theta}(X's)+f{\\theta}(\\varepsilon)=\\sigma^{2}I\\in \\mathbb{R}_{+}$ \u5176\u4e2d, \u6bcf\u4e2a\u7ef4\u5ea6\u7684\u566a\u58f0\u6c34\u5e73\u662f\u76f8\u540c\u7684, \u4f46\u5728\u7c07\u4e2d. - \u56fa\u5b9a\u5404\u5411\u540c\u6027, \u4e0e\u4e0a\u8ff0\u76f8\u540c, \u4f46\u4e0d\u5b66\u4e60 $\\sigma^2$.</p> \u539f\u6587  &gt; Through the experimental analysis in the appendix G, isotropic across clusters is used as the mode of noise perturbation. &gt; Following the noise perturbation model, we obtain more robust mean, variance, and weight parameters for the mixture of Gaussian distributions. &gt; The criterion for training the ***SMSD*** module is the negative log-likelihood of the observation $Y_s$ given its input $X_s$. &gt; The loss function can be formulated as follows, the objective is clearly non-convex. &gt; Details about $Loss_{SMSD}$ are derived in the appendix B.   <p></p> <p>\u901a\u8fc7\u9644\u5f55 G \u7684\u5b9e\u9a8c\u5206\u6790, \u6211\u4eec\u91c7\u7528\u5404\u5411\u540c\u6027\u7684\u7c07\u4f5c\u4e3a\u566a\u58f0\u6270\u52a8\u6a21\u578b. \u57fa\u4e8e\u566a\u58f0\u6270\u52a8\u6a21\u578b, \u6211\u4eec\u83b7\u5f97\u4e86\u66f4\u52a0\u9c81\u68d2\u7684\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u7684\u5747\u503c, \u65b9\u5dee, \u6743\u91cd\u53c2\u6570. \u8bad\u7ec3 SMSD \u6a21\u5757\u7684\u6807\u51c6\u662f\u5bf9\u89c2\u6d4b\u503c $Y_s$ \u7ed9\u5b9a\u5176\u8f93\u5165 $X_s$ \u7684\u8d1f\u5bf9\u6570\u4f3c\u7136. \u635f\u5931\u51fd\u6570\u5b9a\u4e49\u5982\u4e0b, \u76ee\u6807\u51fd\u6570\u662f\u975e\u51f8\u7684. $Loss_{SMSD}$ \u7684\u7ec6\u8282\u5728\u9644\u5f55 B \u4e2d\u7ed9\u51fa.</p> <p>$$ \\begin{aligned}   \\mathcal{L}{SMSD   }&amp; =-\\log P{\\theta}(Y'{s}|X'{s})  \\   &amp;\\propto-\\sum_{k=1}^K(\\pi_k \\exp(-\\dfrac{1}{2}(Y'_s-\\mu^{(k)})^T (\\sigma^{2(k)})^{-1} (Y'_s-\\mu^{(k)})-\\frac{1}{2} \\text{logdet}\\sigma^{2(k)})) \\   &amp;=-\\text{logsumexp}_k(log\\pi_k-\\frac{1}{2}(Y_s^{'}-\\mu^{(k)})^T  (\\sigma^{2(k)})^{-1} (Y'_s-\\mu^{(k)})-\\frac{1}{2}\\text{logdet}\\sigma^{2(k)}) \\   &amp;=-\\text{logsumexp}_k(\\log\\pi_k-\\frac{1}{2}\\left|\\frac{Y'_s-\\mu^{(k)}}\\sigma\\right|^2) \\end{aligned}\\tag{08} $$</p> <p>\u6ce8: \u7531\u9644\u5f55 B \u79fb\u52a8\u81f3\u6b64.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#todo","title":"TODO","text":""},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#34training-and-inference","title":"3.4.Training and Inference","text":"\u539f\u6587  &gt; During the training process, the Duration Predictor is optimized using the mean square error loss, with the extracted duration serving as the training target. &gt; We employ the Montreal Forced Alignment (MFA) tool [39] to extract phoneme durations, and we denote the loss for the Duration Predictor as $Loss_{dur}$. &gt; The codec generator module is optimized using the cross-entropy loss function. &gt; We randomly select a channel for optimization and denote this loss as $Loss_{codec}$. &gt; In the ***SMSD*** module, the target style is the global style representation obtained by passing style codec $Y_{s}$ through the representation $Y_{s}$ style extractor. &gt; During training, we feed the ground truth style representation $Y_s$ and ground truth duration into the codec generator and duration predictor, respectively. &gt; Therefore, the overall loss $Loss$ for ***ControlSpeech*** can be expressed as follows:  $$   Loss = Loss_{codec} + Loss_{dur} + Loss_{SMSD}\\tag{09} $$  &gt; During the inference stage, we initiate the process by inputting the original stylistic descriptor $X_s$ into the BERT module to obtain the stylized semantic representation $X'_s$ into the ***SMSD*** subsequent module to obtain the corresponding $\\pi$, $\\mu$ and $\\sigma^2$. &gt; By directly sampling $X_s$, we can derive the predicted style distribution. &gt; Subsequently, we iteratively generate discrete acoustic tokens by incorporating the predicted style into the text state and employing the confidence based sampling scheme proposed by [5, 2]. &gt; Specifically, we perform multiple forward passes, and at each iteration $j$, we sample candidates for the masked positions. &gt; We then retain $P_j$ candidates based on their confidence scores, where $P_j$ follows a cosine schedule. &gt; Finally, by integrating the timbre prompt through the condition normalization layer and feeding it into the codec decoder, we generate the final speech output.   <p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u65f6\u957f\u9884\u6d4b\u5668\u901a\u8fc7\u5747\u65b9\u8bef\u5dee\u635f\u5931\u8fdb\u884c\u4f18\u5316, \u5176\u4e2d\u63d0\u53d6\u7684\u65f6\u957f\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807.  \u6211\u4eec\u91c7\u7528\u8499\u7279\u5229\u5c14\u5f3a\u5236\u5bf9\u9f50 (MFA) \u5de5\u5177 [39] \u6765\u63d0\u53d6\u97f3\u7d20\u65f6\u957f, \u5e76\u5c06\u65f6\u957f\u9884\u6d4b\u5668\u7684\u635f\u5931\u8868\u793a\u4e3a$Loss_{dur}$.  \u7f16\u89e3\u7801\u5668\u751f\u6210\u6a21\u5757\u5219\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4f18\u5316.  \u6211\u4eec\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u901a\u9053\u8fdb\u884c\u4f18\u5316, \u5e76\u5c06\u6b64\u635f\u5931\u8868\u793a\u4e3a $Loss_{codec}$.  \u5728 SMSD \u6a21\u5757\u4e2d, \u76ee\u6807\u98ce\u683c\u662f\u901a\u8fc7\u5c06\u98ce\u683c\u7f16\u89e3\u7801\u5668 $Y_{s}$ \u4f20\u9012\u7ed9\u8868\u793a $Y_{s}$ \u98ce\u683c\u63d0\u53d6\u5668\u83b7\u5f97\u7684\u5168\u7403\u98ce\u683c\u8868\u793a.  \u5728\u8bad\u7ec3\u671f\u95f4, \u6211\u4eec\u5c06\u771f\u5b9e\u98ce\u683c\u8868\u793a$Y_s$\u548c\u771f\u5b9e\u65f6\u957f\u5206\u522b\u8f93\u5165\u5230\u7f16\u89e3\u7801\u5668\u751f\u6210\u5668\u548c\u65f6\u957f\u9884\u6d4b\u5668\u4e2d.  \u56e0\u6b64, ControlSpeech \u7684\u6574\u4f53\u635f\u5931$Loss$\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> <p>$$   Loss = Loss_{codec} + Loss_{dur} + Loss_{SMSD}\\tag{09} $$</p> <p>\u5728\u63a8\u7406\u9636\u6bb5, \u6211\u4eec\u901a\u8fc7\u5c06\u539f\u59cb\u98ce\u683c\u63cf\u8ff0\u7b26$X_s$\u8f93\u5165\u5230BERT\u6a21\u5757\u4e2d, \u4ee5\u83b7\u5f97\u98ce\u683c\u5316\u7684\u8bed\u4e49\u8868\u793a $X'_s$, \u5e76\u5c06\u5176\u8f93\u5165\u5230SMSD\u540e\u7eed\u6a21\u5757\u4e2d, \u4ee5\u83b7\u5f97\u76f8\u5e94\u7684 $\\pi$, $\\mu$\u548c$\\sigma^2$.  \u901a\u8fc7\u76f4\u63a5\u91c7\u6837 $X_s$, \u6211\u4eec\u53ef\u4ee5\u63a8\u5bfc\u51fa\u9884\u6d4b\u7684\u98ce\u683c\u5206\u5e03.  \u968f\u540e, \u6211\u4eec\u901a\u8fc7\u5c06\u9884\u6d4b\u7684\u98ce\u683c\u878d\u5165\u6587\u672c\u72b6\u6001, \u5e76\u91c7\u7528[5, 2]\u63d0\u51fa\u7684\u57fa\u4e8e\u4fe1\u5fc3\u7684\u91c7\u6837\u65b9\u6848, \u8fed\u4ee3\u751f\u6210\u79bb\u6563\u7684\u58f0\u5b66\u6807\u8bb0.  \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u8fdb\u884c\u591a\u6b21\u524d\u5411\u4f20\u9012, \u5e76\u5728\u6bcf\u6b21\u8fed\u4ee3 $j$ \u65f6, \u5bf9\u63a9\u7801\u4f4d\u7f6e\u7684\u5019\u9009\u8005\u8fdb\u884c\u91c7\u6837.  \u7136\u540e, \u6211\u4eec\u6839\u636e\u5176\u7f6e\u4fe1\u5ea6\u5206\u6570\u4fdd\u7559 $P_j$ \u4e2a\u5019\u9009\u8005, \u5176\u4e2d $P_j$ \u9075\u5faa\u4f59\u5f26\u8c03\u5ea6.  \u6700\u540e, \u901a\u8fc7\u5c06\u97f3\u8272\u63d0\u793a\u901a\u8fc7\u6761\u4ef6\u5f52\u4e00\u5316\u5c42\u96c6\u6210, \u5e76\u5c06\u5176\u8f93\u5165\u5230\u7f16\u89e3\u7801\u5668\u89e3\u7801\u5668\u4e2d, \u6211\u4eec\u751f\u6210\u6700\u7ec8\u7684\u8bed\u97f3\u8f93\u51fa. </p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#4experiment","title":"4.Experiment","text":""},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#41controltoolkit","title":"4.1.ControlToolkit","text":"\u539f\u6587  &gt; In this section, we will introduce the ***VccmDataset***, related baselines, and evaluation metrics in ***ControlToolkit***. &gt; Detailed experimental settings for training and inference as well as model architecture specifics, are provided in appendix C and appendix D respectively.  &gt; ***VccmDataset*** To the best of our knowledge, there is no large-scale dataset that includes both text style prompts and speaker prompts. &gt; Building upon the existing [TextrolSpeech dataset](../../Datasets/2023.08.28_TextrolSpeech.md), we have developed the ***VccmDataset***. &gt; [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md) is currently the only open-source dataset in the field of text style control, containing a total of 330 hours of speech data along with 236203 style description texts. &gt; Based on [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md), we have optimized the pitch distribution, label boundaries, and the training and dataset splits. &gt; Specifically, we use the LibriTTS and emotional data from [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md) as the base databases, annotating each speech sample with five attribute labels: gender, volume, speed, pitch, and emotion. &gt; Considering the close proximity of data values between adjacent categories, we exclude the 5% of data at the boundaries of each interval for each attribute. &gt; This ensures greater distinctiveness for each label. &gt; Particularly, due to the significant difference in pitch distribution between male and female voices, we use gender-dependent thresholds to bin the pitch into three different levels. &gt; We randomly selected 1,500 audio samples as the ***ControlSpeech*** test set and matched the corresponding prompt voice based on speaker IDs. &gt; Additionally, to evaluate ***ControlSpeech***\u2019s performance on out-of-domain voices and styles, we further filtered an appropriate test set and enlisted language experts to compose style descriptions distinct from those in [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md). &gt; Further details on the creation of ***VccmDataset*** can be found in appendix E.  &gt; Baselines We have reproduced several state-of-the-art style-controllable models, including [PromptTTS](../_tmp/2022.11.22_PromptTTS.md), [PromptStyle](../E2E/2023.05.31_PromptStyle.md), [SALL-E](../../Datasets/2023.08.28_TextrolSpeech.md) and [InstructTTS](../_tmp/2023.01.31_InstructTTS.md), to serve as primary comparative models for evaluating the controllability of ***ControlSpeech***. &gt; For the comparison of voice cloning effectiveness, we have reproduced the [VALL-E](2023.01.05_VALL-E.md) model and the [MobileSpeech](../../Models/_../_tmp/2024.02.14_MobileSpeech.mdsenting the autoregressive paradigm and the parallel generation paradigm. &gt; The relevant baseline reproduction code has been integrated into the ***ControlToolkit***.  &gt; Evaluation metrics We have aligned our objective experiments with [[PromptTTS](../_tmp/2022.11.22_PromptTTS.md), [SALL-E](../../Datasets/2023.08.28_TextrolSpeech.md), [PromptTTS 2](../_tmp/2023.09.05_PromptTTS2.md)]. &gt; To evaluate the model\u2019s style controllability, we use accuracy to represent the evaluation metric, which measures the correspondence between the style factors in the output speech and those in the prompts. &gt; The accuracy of pitch, speaking speed, and volume is calculated using signal processing tools, while emotion classification accuracy is computed using a fine-tuned emotion classification model. &gt; We employ the official version of the [Emotion2Vec](../Speech_Representaion/2023.12.23_Emotion2Vec.md) model for the speech emotion recognition task, which is fine-tuned on the emotional dataset of ***VccmDataset***. &gt; To evaluate timbre similarity (Spk-sv) between the original prompt and synthesized speech, we utilize the base-plus-sv version of [WavLM](../Speech_Representaion/2021.10.26_WavLM.md). &gt; Additionally, we conduct Automatic Speech Recognition (ASR) using the Whisper3 [41] model on the generated audio and calculate the word error rate (Wer) compared to the original transcriptions. &gt; For subjective testing, we conduct mean opinion score (MOS) evaluations on the test set to measure audio naturalness via crowdsourcing. &gt; We randomly select 30 samples from the test set of each dataset for subjective evaluation, and each audio sample is listened to by at least 10 testers. &gt; We analyze the MOS in two aspects: MOS-Q (Quality, assessing clarity and naturalness of duration and pitch) and MOS-S (Speaker similarity). &gt; Furthermore, for the evaluation of style-controllable many-to-many scenarios, we design specific metrics: MOS-TS (Timbre Similarity), MOS-SD (Style Diversity), and MOS-SA (Style Accuracy). &gt; We provide a detailed explanation of these metrics and their use cases in the many-to-many experiment section.   <p>\u5728\u672c\u8282\u4e2d, \u6211\u4eec\u5c06\u4ecb\u7ecd VccmDataset, \u76f8\u5173\u7684\u57fa\u7ebf\u6a21\u578b\u4ee5\u53ca ControlToolkit \u4e2d\u7684\u8bc4\u4f30\u6307\u6807.  \u8bad\u7ec3\u548c\u63a8\u7406\u7684\u8be6\u7ec6\u5b9e\u9a8c\u8bbe\u7f6e\u4ee5\u53ca\u6a21\u578b\u67b6\u6784\u7684\u5177\u4f53\u7ec6\u8282, \u5206\u522b\u5728\u9644\u5f55C\u548c\u9644\u5f55D\u4e2d\u63d0\u4f9b. </p> <p>VccmDataset \u636e\u6211\u4eec\u6240\u77e5, \u76ee\u524d\u6ca1\u6709\u5305\u542b\u6587\u672c\u98ce\u683c\u63d0\u793a\u548c\u8bf4\u8bdd\u8005\u63d0\u793a\u7684\u5927\u578b\u6570\u636e\u96c6.  \u6211\u4eec\u57fa\u4e8e\u73b0\u6709\u7684 TextrolSpeech\u6570\u636e\u96c6 \u5f00\u53d1\u4e86 VccmDataset.  TextrolSpeech \u662f\u76ee\u524d\u6587\u672c\u98ce\u683c\u63a7\u5236\u9886\u57df\u552f\u4e00\u7684\u5f00\u6e90\u6570\u636e\u96c6, \u5305\u542b\u603b\u5171330\u5c0f\u65f6\u7684\u8bed\u97f3\u6570\u636e\u4ee5\u53ca236203\u6761\u98ce\u683c\u63cf\u8ff0\u6587\u672c.  \u57fa\u4e8e TextrolSpeech, \u6211\u4eec\u4f18\u5316\u4e86\u97f3\u9ad8\u5206\u5e03, \u6807\u7b7e\u8fb9\u754c\u4ee5\u53ca\u8bad\u7ec3\u548c\u6570\u636e\u96c6\u5206\u5272.  \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u4f7f\u7528 LibriTTS \u548c\u6765\u81ea TextrolSpeech \u7684\u60c5\u611f\u6570\u636e\u4f5c\u4e3a\u57fa\u7840\u6570\u636e\u5e93, \u4e3a\u6bcf\u4e2a\u8bed\u97f3\u6837\u672c\u6807\u6ce8\u4e86\u4e94\u4e2a\u5c5e\u6027\u6807\u7b7e\uff1a\u6027\u522b, \u97f3\u91cf, \u901f\u5ea6, \u97f3\u9ad8\u548c\u60c5\u611f.  \u8003\u8651\u5230\u76f8\u90bb\u7c7b\u522b\u4e4b\u95f4\u6570\u636e\u503c\u7684\u7d27\u5bc6\u63a5\u8fd1, \u6211\u4eec\u6392\u9664\u4e86\u6bcf\u4e2a\u5c5e\u6027\u6bcf\u4e2a\u533a\u95f4\u8fb9\u754c\u4e0a\u76845%\u7684\u6570\u636e.  \u8fd9\u786e\u4fdd\u4e86\u6bcf\u4e2a\u6807\u7b7e\u7684\u66f4\u5927\u72ec\u7279\u6027.  \u7279\u522b\u662f, \u7531\u4e8e\u7537\u6027\u548c\u5973\u6027\u58f0\u97f3\u4e4b\u95f4\u7684\u97f3\u9ad8\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02, \u6211\u4eec\u4f7f\u7528\u6027\u522b\u4f9d\u8d56\u7684\u9608\u503c\u5c06\u97f3\u9ad8\u5206\u4e3a\u4e09\u4e2a\u4e0d\u540c\u7684\u7ea7\u522b.  \u6211\u4eec\u968f\u673a\u9009\u62e9\u4e861,500\u4e2a\u97f3\u9891\u6837\u672c\u4f5c\u4e3a ControlSpeech \u6d4b\u8bd5\u96c6, \u5e76\u6839\u636e\u8bf4\u8bdd\u8005ID\u5339\u914d\u76f8\u5e94\u7684\u63d0\u793a\u58f0\u97f3.  \u6b64\u5916, \u4e3a\u4e86\u8bc4\u4f30 ControlSpeech \u5728\u57df\u5916\u58f0\u97f3\u548c\u98ce\u683c\u4e0a\u7684\u8868\u73b0, \u6211\u4eec\u8fdb\u4e00\u6b65\u7b5b\u9009\u4e86\u4e00\u4e2a\u9002\u5f53\u7684\u6d4b\u8bd5\u96c6, \u5e76\u8bf7\u8bed\u8a00\u4e13\u5bb6\u7f16\u5199\u4e86\u4e0eTextrolSpeech \u4e2d\u4e0d\u540c\u7684\u98ce\u683c\u63cf\u8ff0.  \u5173\u4e8e VccmDataset \u521b\u5efa\u7684\u66f4\u591a\u7ec6\u8282\u53ef\u4ee5\u5728\u9644\u5f55 E \u4e2d\u627e\u5230. </p> <p>\u57fa\u7ebf\u6a21\u578b \u6211\u4eec\u91cd\u73b0\u4e86\u51e0\u4e2a\u6700\u5148\u8fdb\u7684\u98ce\u683c\u53ef\u63a7\u6a21\u578b, \u5305\u62ec PromptTTS, PromptStyle, SALL-E\u548cInstructTTS, \u4f5c\u4e3a\u8bc4\u4f30ControlSpeech\u53ef\u63a7\u6027\u7684\u4e3b\u8981\u6bd4\u8f83\u6a21\u578b.  \u4e3a\u4e86\u6bd4\u8f83\u58f0\u97f3\u514b\u9686\u7684\u6709\u6548\u6027, \u6211\u4eec\u91cd\u73b0\u4e86VALL-E\u6a21\u578b\u548c[MobileSpeech](../../Models/_../_tmp/2024.02.14_MobileSpeech.md\u5f0f\u548c\u5e73\u884c\u751f\u6210\u8303\u5f0f.  \u76f8\u5173\u7684\u57fa\u7ebf\u91cd\u73b0\u4ee3\u7801\u5df2\u96c6\u6210\u5230ControlToolkit\u4e2d. </p> <p>\u8bc4\u4f30\u6307\u6807 \u6211\u4eec\u7684\u5ba2\u89c2\u5b9e\u9a8c\u4e0e (PromptTTS, SALL-E, PromptTTS 2) \u4fdd\u6301\u4e00\u81f4.  \u4e3a\u4e86\u8bc4\u4f30\u6a21\u578b\u7684\u98ce\u683c\u53ef\u63a7\u6027, \u6211\u4eec\u4f7f\u7528\u51c6\u786e\u5ea6\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807, \u8be5\u6307\u6807\u8861\u91cf\u8f93\u51fa\u8bed\u97f3\u4e2d\u7684\u98ce\u683c\u56e0\u7d20\u4e0e\u63d0\u793a\u4e2d\u7684\u98ce\u683c\u56e0\u7d20\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb.  \u97f3\u9ad8, \u8bf4\u8bdd\u901f\u5ea6\u548c\u97f3\u91cf\u7684\u51c6\u786e\u5ea6\u4f7f\u7528\u4fe1\u53f7\u5904\u7406\u5de5\u5177\u8ba1\u7b97, \u800c\u60c5\u611f\u5206\u7c7b\u51c6\u786e\u5ea6\u5219\u4f7f\u7528\u5728VccmDataset\u60c5\u611f\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u60c5\u611f\u5206\u7c7b\u6a21\u578b\u8ba1\u7b97.  \u6211\u4eec\u4f7f\u7528Emotion2Vec\u6a21\u578b\u7684\u5b98\u65b9\u7248\u672c\u6765\u8fdb\u884c\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1, \u8be5\u6a21\u578b\u5728VccmDataset\u7684\u60c5\u611f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03.  \u4e3a\u4e86\u8bc4\u4f30\u539f\u59cb\u63d0\u793a\u4e0e\u5408\u6210\u8bed\u97f3\u4e4b\u95f4\u7684\u97f3\u8272\u76f8\u4f3c\u5ea6(Spk-sv), \u6211\u4eec\u4f7f\u7528\u4e86WavLM\u7684base-plus-sv\u7248\u672c.  \u6b64\u5916, \u6211\u4eec\u4f7f\u7528Whisper3 [41]\u6a21\u578b\u5bf9\u751f\u6210\u7684\u97f3\u9891\u8fdb\u884c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR), \u5e76\u8ba1\u7b97\u4e0e\u539f\u59cb\u8f6c\u5f55\u76f8\u6bd4\u7684\u5b57\u9519\u8bef\u7387(Wer).  \u5bf9\u4e8e\u4e3b\u89c2\u6d4b\u8bd5, \u6211\u4eec\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u5e73\u5747\u610f\u89c1\u5206\u6570(MOS)\u8bc4\u4f30, \u901a\u8fc7\u4f17\u5305\u6765\u8861\u91cf\u97f3\u9891\u7684\u81ea\u7136\u5ea6.  \u6211\u4eec\u4ece\u6bcf\u4e2a\u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u96c6\u4e2d\u968f\u673a\u9009\u62e930\u4e2a\u6837\u672c\u8fdb\u884c\u4e3b\u89c2\u8bc4\u4f30, \u6bcf\u4e2a\u97f3\u9891\u6837\u672c\u81f3\u5c11\u753110\u540d\u6d4b\u8bd5\u8005\u542c\u53d6.  \u6211\u4eec\u5206\u6790MOS\u5728\u4e24\u4e2a\u65b9\u9762\uff1aMOS-Q (\u8d28\u91cf, \u8bc4\u4f30\u65f6\u957f\u548c\u97f3\u9ad8\u7684\u6e05\u6670\u5ea6\u548c\u81ea\u7136\u5ea6) \u548cMOS-S (\u8bf4\u8bdd\u8005\u76f8\u4f3c\u6027) .  \u6b64\u5916, \u4e3a\u4e86\u8bc4\u4f30\u98ce\u683c\u53ef\u63a7\u7684\u591a\u5bf9\u591a\u573a\u666f, \u6211\u4eec\u8bbe\u8ba1\u4e86\u7279\u5b9a\u7684\u6307\u6807\uff1aMOS-TS (\u97f3\u8272\u76f8\u4f3c\u5ea6) , MOS-SD (\u98ce\u683c\u591a\u6837\u6027) \u548cMOS-SA (\u98ce\u683c\u51c6\u786e\u5ea6) .  \u6211\u4eec\u5c06\u5728\u591a\u5bf9\u591a\u5b9e\u9a8c\u90e8\u5206\u8be6\u7ec6\u89e3\u91ca\u8fd9\u4e9b\u6307\u6807\u53ca\u5176\u7528\u4f8b. </p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#42evaluation-on-style-controllability","title":"4.2.Evaluation on style controllability","text":"<p>We first compared the performance of ControlSpeech with various models on the style controllability task. The evaluation was conducted on the 1,500-sample controllable test set from the VccmDataset. To eliminate the influence of timbre variations on the controllability results of ControlSpeech, we used the ground truth (GT) timbre as the prompt. We compared the controllability of the models using pitch accuracy, speed accuracy, energy accuracy, and emotion accuracy. Additionally, we measured the audio quality generated by the models using word error rate (Wer), timbre similarity (Spk-sv), and MOS-Q (Mean Opinion Score for Quality). The experimental results are shown in Table 1, and we drew the following conclusions: 1) The GTcodec version exhibits high reconstruction quality. However, it shows limitations in Wer, emotion classification accuracy, and speech speed classification accuracy. We believe this may be due to accumulated errors introduced by the test model. Additionally, the emotion classification experiment did not include neutral emotion classification results, which better highlights the model\u2019s emotion control capabilities and presents a more challenging task for all models. 2) Comparing ControlSpeech with other baseline models on controllability metrics, we found that, except for pitch accuracy, ControlSpeech achieved state-of-the-art results in energy, speed, and emotion classification accuracy. Upon analyzing the synthesized audio of ControlSpeech, we noticed some instances of unnatural pitch. We attribute this to possible conflicts arising from simultaneously controlling different timbres and styles. 3) In terms of Spk-sv, Wer, and MOS-Q metrics, the audio generated by ControlSpeech demonstrates good timbre similarity and audio quality. However, it falls slightly behind PromptStyle and InstructTTS in Wer. We believe this is due to the limitations imposed by the FACodec approach, which constrains the upper bound of ControlSpeech\u2019s performance. 4) We further attempted to replace the GT timbre in the test set with the timbre from the prompt test set. We found that while the audio quality-related metrics remained unchanged, the controllability-related metrics, except for speech speed accuracy, generally decreased by 5%-10%.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#43evaluation-on-the-timbre-cloning-task","title":"4.3.Evaluation on the timbre cloning task","text":"<p>To evaluate the timbre cloning capability of ControlSpeech in an out-of-domain speaker scenario, we compared the performance of ControlSpeech with models such as VALL-E and MobileSpeech on the out-of-domain speaker test set from the VccmDataset. The out-of-domain speaker test set consists of 1,086 test utterances, ensuring that none of the speakers were present in the training set. VALL-E and MobileSpeech used Encodec for feature extraction during training. The experimental results are shown in Table 2. We observed that in terms of robustness metrics, the zero-shot TTS systems trained on small datasets performed even lower than ControlSpeech. We attribute this to the performance gains from the pre-trained speaker prompt component in ControlSpeech, which was trained on 60,000 hours of speaker data. Additionally, in the MOS metric, we found that although ControlSpeech can further control styles, it still maintains performance comparable to zero-shot TTS systems in terms of timbre cloning.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#44evaluation-on-the-out-of-domain-style-control-task","title":"4.4.Evaluation on the out-of-domain style control task","text":"<p>We further tested the controllability of style-controllable models under out-of-domain style descriptions. We compared the performance of ControlSpeech with controllable baseline models on the out-of-domain style test set from the VccmDataset. The out-of-domain style test set comprises 100 test utterances, with style prompts rewritten by experts. The experimental results are shown in Table 3. We found that the generalization performance of ControlSpeech is significantly better than that of the baseline models. The accuracy of speech speed and energy is markedly higher for ControlSpeech, especially in terms of energy accuracy, where there is no significant difference between the main test set and the out-of-domain style test set. ControlSpeech exhibits a slightly higher wer, which may be due to the limitations imposed by the decoupled codec approach. Similar to the results of Table 1, the pitch accuracy of ControlSpeech is slightly lower. We believe this is due to pitch inconsistencies arising from the simultaneous control of style and timbre cloning.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#45evaluation-on-style-controlled-many-to-many-problems","title":"4.5.Evaluation on style controlled many-to-many problems","text":"<p>To better evaluate the performance of style-controllable models on many-to-many tasks, we compared the results of ControlSpeech with controllable baseline models on the many-to-many test set from the VccmDataset. Specifically, the MOS-TS metric was used to assess whether the timbre remains stable across 60 different style descriptions for four speakers. Additionally, we selected three pairs of six different style descriptions under 50 different timbre prompts, with pitch, speed, and energy labels set to normal, fast, normal; normal, slow, normal; high, normal, normal; low, normal, normal; normal, normal, high; and normal, normal, low, respectively. The MOS-SA and MOS-SD metrics represent the accuracy and diversity of style control for each pair respectively. The experimental results are shown in Table 4. We found that ControlSpeech significantly outperforms PromptStyle and InstructTTS in both the MOS-SA and MOS-SD metrics. This indicates that the unique SMSD module design in ControlSpeech enables it to synthesize speech that is both accurate and diverse.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#46ablation-experiment","title":"4.6.Ablation experiment","text":"<p>In this section, we validated the necessity of the decoupling scheme and the SMSD module. Additionally, in appendix F and appendix G, we examined the impact of hyperparameters for mixed distributions and various noise models.</p> <p>Decouple codec For the decouple codec experiment, we maintained the main framework of ControlSpeech but used a non-decoupled Encodec to represent discrete audio in the TTS model. Furthermore, during training, we input the speech prompt into the codec generator, allowing the model to learn corresponding timbre, content, and style representations from the speech prompt, content prompt, and style prompt, respectively. We refer to this experimental model as ControlSpeech w/o decoupling and evaluated it using the prompt version of the main test set from the VccmDataset. As shown in Table 1, ControlSpeech w/o decoupling performed significantly worse in controllability compared to ControlSpeech. This suggests that the speech prompt and style prompt may interfere with each other, making it difficult to simultaneously clone timbre and control style with this approach.</p> <p>SMSD module Regarding the SMSD module, we evaluated its effectiveness in addressing the many-to-many style control problem. Specifically, we replaced the SMSD module with a style encoder and referred to this experimental model as ControlSpeech w/o SMSD. As shown in Table 4, ControlSpeech w/o SMSD performed significantly worse in the MOS-SA and MOS-SD metrics compared to ControlSpeech, thus strongly validating that the SMSD module enables more fine-grained control of the model\u2019s style and increases style diversity through style sampling.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.03_ControlSpeech/#5conclusion","title":"5.Conclusion","text":"\u539f\u6587  &gt; In this paper, we present ***ControlSpeech***, the first TTS system capable of simultaneously performing zero-shot timbre cloning and zero-shot style control. &gt; Leveraging large-scale pre-trained discrete codec representations, we disentangle style, content, and timbre, and generate the corresponding codec representations through a non-autoregressive, mask-based iterative codec generator. &gt; Additionally, we identified a many-to-many problem in style control and designed a unique ***Style Mixed Semantic Density (SMSD)*** module to mitigate this issue. &gt; We constructed a new dataset called ***VccmDataset***, and established relevant evaluation metrics and baselines for the new task, all of which are open-sourced in the ***ControlToolkit***. &gt; The limitations of ***ControlSpeech*** and directions for future work are discussed in the appendix H.   <p>\u5728\u672c\u6587\u4e2d, \u6211\u4eec\u4ecb\u7ecd\u4e86 ControlSpeech, \u8fd9\u662f\u9996\u4e2a\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u96f6\u6837\u672c\u97f3\u8272\u514b\u9686\u548c\u96f6\u6837\u672c\u98ce\u683c\u63a7\u5236\u7684\u6587\u672c\u5230\u8bed\u97f3 (TTS) \u7cfb\u7edf. \u6211\u4eec\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u79bb\u6563\u7f16\u89e3\u7801\u5668\u8868\u793a, \u5c06\u98ce\u683c\u3001\u5185\u5bb9\u548c\u97f3\u8272\u89e3\u8026, \u5e76\u901a\u8fc7\u975e\u81ea\u56de\u5f52\u3001\u57fa\u4e8e\u63a9\u7801\u7684\u8fed\u4ee3\u7f16\u89e3\u7801\u5668\u751f\u6210\u5668\u751f\u6210\u76f8\u5e94\u7684\u7f16\u89e3\u7801\u5668\u8868\u793a. \u6b64\u5916, \u6211\u4eec\u8bc6\u522b\u4e86\u98ce\u683c\u63a7\u5236\u4e2d\u7684\u591a\u5bf9\u591a\u95ee\u9898, \u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u72ec\u7279\u7684 \u98ce\u683c\u6df7\u5408\u8bed\u4e49\u5bc6\u5ea6 (SMSD) \u6a21\u5757\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898. \u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a VccmDataset \u7684\u65b0\u6570\u636e\u96c6, \u5e76\u4e3a\u65b0\u4efb\u52a1\u5efa\u7acb\u4e86\u76f8\u5173\u7684\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u7ebf, \u6240\u6709\u8fd9\u4e9b\u90fd\u5728 ControlToolkit \u4e2d\u5f00\u6e90. ControlSpeech \u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u5de5\u4f5c\u65b9\u5411\u5728\u9644\u5f55 H \u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba.</p> TODO  B ***SMSD*** Loss The loss function for the ***SMSD*** module represents the conditional probability of the input style \u2032 representation Xs . &gt; We further refine this into a maximum likelihood loss involving the style distribution parameters \u03c0k, \u00b5(k), \u03c32(k) derived through the MDN network and noise perturbation module. &gt; The detailed derivation of the loss function is as follows. C Training and inference settings ***ControlSpeech*** was trained on ***VccmDataset*** using 8 NVIDIA A100 40G GPUs with each batch accommodating 3500 frames of the discrete codec. &gt; We optimized the models using the AdamW optimizer with parameters \u03b21 = 0.9 and \u03b22 = 0.95. &gt; The learning rate was warmed up for the first 5k updates, reaching a peak of 5 \u00d7 10\u22124, and then linearly decayed. &gt; We utilized the open-source [FACodec](../Diffusion/2024.03.05_NaturalSpeech3.md)\u2019s voice conversion version as the codec encoder and decoder for ***ControlSpeech***. &gt; The style-controllable baseline models were trained on the same sets ***VccmDataset*** to eliminate potential biases. &gt; We utilize a pre-trained [BERT](../LLM/2018.10.11_BERT.md) model consisting of 12 hidden layers with 110M parameters. &gt; For the implementation of the basic MDN network model, we largely followed the approach described in [12]. D Model architecture in the ***ControlSpeech*** Follow [NaturalSpeech 3](../Diffusion/2024.03.05_NaturalSpeech3.md), the basic architecture of codec encoder and codec decoder follows [DAC](../Speech_Neural_Codec/2023.06.11_Descript-Audio-Codec.md) and employs the SnakeBeta activation function ([BigVGAN](../TTS3_Vocoder/2022.06.09_BigVGAN.md)). &gt; The timbre extractor consists of several [Conformer](../ASR/2020.05.16_Conformer.md) blocks. &gt; We use Nqc = 2, Nqp = 1, Nqd = 3 as the number of quantizers for each of the three FVQ Qc, Qp, Qd, the codebook size for all the quantizers is 1024. &gt; Text encoder and variance adaptor share the similar architecture which is comprised of several FFT blocks or attention layers as used by [FastSpeech2](../TTS2_Acoustic/2020.06.08_FastSpeech2.md). &gt; The codec generator is a decoder primarily based on [Conformer](../ASR/2020.05.16_Conformer.md) blocks, similar to [MobileSpeech](../_tmp/2024.02.14_MobileSpeech.md). &gt; However, we opted for fewer encoder layers and a smaller parameter count in the codec generator. E ***VccmDataset*** details We use the gender labels available in the online metadata. &gt; For the remaining style factors, such as pitch, speaking speed, and volume, we initially utilize the Montreal forced alignment (MFA) [39] tool to extract phoneme durations and silence segments. &gt; Subsequently, we calculate the average duration of each phoneme pronunciation within voiced segments for speaking speed. &gt; Regarding energy, we compute the L2-norm of the ampli tude of each short-time Fourier transform (STFT) frame. &gt; Then the Parselmouth 3 tool is employed to extract fundamental frequency (f0) and calculate the geometric mean across all voiced regions as pitch values. &gt; After obtaining the three style factors\u2019 values, we divide speech into 3 categories (high/normal/low) according to the proportion. &gt; To further validate ***ControlSpeech***\u2019s ability to simultaneously control style and clone speaker timbre, the ***VccmDataset*** includes four types of test sets: the main test set, the out-of-domain speaker test set, the out-ofdomain style test set, and the special case test set. &gt; Each test set corresponds to different experiments: style controllability experiments, out-of-domain speaker cloning experiments, out-of-domain style controllability experiments, and many-to-many style control experiments, respectively.  ## F.Ablation experiments about mixed distributions In this section, we investigate the impact of the number of mixtures in the ***SMSD*** module on model performance. &gt; We conducted ablation studies under the isotropic across clusters noise perturbation mode, examining the effects of using 3, 5, and 7 mixtures. &gt; As shown in Table 5, the differences in the MOS-SD metric were negligible. &gt; However, an increase in the number of mixtures led to a decline in the MOS-SA metric, indicating that an excessive number of mixtures may reduce the model\u2019s control accuracy. G Ablation experiments about various noise modes We analyzed the impact of different noise perturbation modes on the many-to-many style control problem, with the number of mixture distributions fixed at 5. &gt; As shown in Table 6, we found that the noise perturbation mode maintaining isotropy at the cluster centers achieved a balance between the MOS-SA and MOS-SD metrics. H Future work and limitations In this work, we introduced ***ControlSpeech***, the first TTS system capable of simultaneously cloning timbre and controlling style. &gt; While ***ControlSpeech*** has demonstrated notable controllability and cloning capabilities, there remains considerable scope for further research and improvement based on the current framework. &gt; - Decoupled Codec Optimization There is substantial room for performance enhancement within the existing decoupled codec model. &gt; Although we attempted further optimization based on [FACodec](../Diffusion/2024.03.05_NaturalSpeech3.md), no significant improvements were observed. &gt; However, the decoupling method of [FACodec](../Diffusion/2024.03.05_NaturalSpeech3.md) and the timbre conversion performance based on decoupled codecs still have potential for improvement, which might enhance the naturalness of synthesized speech pitch. &gt; Future work could explore more efficient forms of vector quantization and decoupled codec representations. &gt; Additionally, experimenting with a broader range of supervision signals and different methods of information fusion could provide further advancements. &gt; - Larger Training Datasets The field of style-controllable TTS urgently needs larger training datasets. &gt; Although [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md) and the ***VccmDataset*** have established a foundation, achieving more advanced speech controllability will require datasets comprising tens of thousands of hours of style descriptions. &gt; - Diversity in Style Descriptions Current style description texts predominantly include labels such as emotion, speed, intonation, energy, and gender. &gt; There is still a gap between these descriptions and the diversity of human-level style descriptions. &gt; - Exploration of Generative Models We experimented with decoupled codecs and non-autoregressive parallel generative models. &gt; Future research could explore a broader range of generative model architectures and additional audio representations."},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/","title":"VALL-E R","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment - \u4f5c\u8005:   - 01 [Bing Han](../../Authors/Bing_Han.md)   - 02 [Long Zhou](../../Authors/Long_Zhou_(\u5468\u9f99).md)   - 03 [Shujie Liu](../../Authors/Shujie_Liu_(\u5218\u6811\u6770).md)   - 04 [Sanyuan Chen](../../Authors/Sanyuan_Chen_(\u9648\u4e09\u5143).md)   - 05 [Lingwei Meng](../../Authors/Lingwei_Meng.md)   - 06 [Yanming Qian](../../Authors/Yanming_Qian.md)   - 07 [Yanqing Liu](../../Authors/Yanqing_Liu.md)   - 08 [Sheng Zhao](../../Authors/Sheng_Zhao_(\u8d75\u80dc).md)   - 09 [Jinyu Li](../../Authors/Jinyu_Li_(\u674e\u52b2\u5b87).md)   - 10 [Furu Wei](../../Authors/Furu_Wei_(\u97e6\u798f\u5982).md) - \u673a\u6784:   - [\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66](../../Institutions/SJTU_\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66.md)   - [\u5fae\u8f6f](../../Institutions/Microsoft.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.06.12 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.19 - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2406.07855)   - [DOI]()   - [Github]()   - [Demo](https://aka.ms/valler)   - [Scholar](https://scholar.google.com/scholar?cluster=) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md) - \u9875\u6570: 15 - \u5f15\u7528: ? - \u88ab\u5f15: ? - \u6570\u636e:"},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#abstract","title":"Abstract: \u6458\u8981","text":"<p>With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E [Wang et al., 2023a]. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllability over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#1introduction","title":"1.Introduction: \u5f15\u8a00","text":""},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#2related-works","title":"2.Related Works: \u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#3methodology","title":"3.Methodology: \u65b9\u6cd5","text":"<p>In this study, we propose a robust and efficient zero-shot TTS system named VALL-E R. We first introduce the codec-merging approach in Section 3.1 which can improve inference speed without retraining the codec, and then illustrate the monotonic alignment strategy in decoder-only neural codec LM in Section 3.2.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#31codec-merging-approach","title":"3.1.Codec-Merging Approach","text":"<p>Building upon the foundational work of EnCodec, we introduce the concept of a merged codec. This innovative approach enables the downsampling of discrete codes across various layers through alterations in the inference forward process. Remarkably, this is achieved without necessitating any retraining or fine-tuning of the model, presenting a significant advancement in the efficient manipulation of audio data representations.</p> <p>The visual description of the proposed codec can be seen in Figure.02. The overall architecture of the model remains consistent with Encodec, which comprises of three components:  (1) a convolution-based encoder that maps waveform data $x^{1\\times L}$ into a sequence of latent representations $z^{F\\times T}$, where $F$ is channel and $T$ is length of extracted codes;  (2) a decoder that reconstructs the data $\\hat{x}^{1\\times L}$ from a sequence of the quantized latent representations $\\hat{z}^{F\\times T}$;  (3) an 8-layer residual vector quantizer (RVQ) module, which can convert the continual latent vector $z^{F\\times T}$ into the discrete code representation $C^{8\\times T}$ iteratively. The main difference is that our merged codec inserts a codec-merging module before the vector quantizer module to downsample the representation $z^{F\\times T}$.</p> <p>Assuming the merged rate of layer $d$ is $m_d$, $r_{d}^{F\\times T}$ represents the residual input of layer $d$. Then the codec-merging module consists of two steps: the first one is downsampling the residual input $r_{d}^{F\\times T}$ to $r_{md}^{F\\times (T/m_{d})}$ by average pooling, and then upsampling $r_{md}$ to its original length through repeat operation. Next, the residual representation processed by the Merge module will be feed into the following VQ layer to quantized into discrete code $C^{1\\times T}{d}$ through nearest-neighbor lookup over the codebook embeddings. Through the merge module, we reduced the resolution of $C^{1\\times T}{d}$ by ensuring consistent code of consecutive $m_d$ frames.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#32neural-codec-lm-with-monotonic-alignment","title":"3.2.Neural Codec LM with Monotonic Alignment","text":"<p>Previously, monotonic strategies were only applicable to encoder-decoder structures. To address the robustness issue in the decoder-only transformer based TTS, we integrated phonemes prediction into the training of neural codec LM and design the monotonic alignment strategy during the inference process. The overview is illustrated in Figure.03 and details of training and inference are discussed in the subsequent sections.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#321training-with-phoneme-prediction","title":"3.2.1.Training with Phoneme Prediction","text":"<p>To achieve a good trade-off between speech quality and inference speed, our VALL-E R includes two models: autoregressive (AR) and non-autoregressive (NAR), which is following VALL-E. Specifically, given a training data pair ${\\mathbf{s}, \\mathbf{p}}$, where $s$ is speech sample, and $\\mathbf{p} = {p_1, p_2, \\cdots, p_L}$ is its corresponding phoneme transcription. Then, the codec-merging model introduced in Sec.3.1 is utilized to compress speech waveform $s$ into discrete acoustic tokens $\\mathbf{A}$ with 8 quantizers, formulated as: $\\text{MergeCodec}(x) = \\mathbf{A}^{8\u00d7T}= {\\mathbf{a}^1, \\cdots, \\mathbf{a}^8}$, where $T$ is length of discrete codes and $\\mathbf{a}^i= {a_1, \\cdots, a_T}$ represent the tokens in the $i$-th layer. Because the training of VALL-E R requires the aligned phonemes and acoustic tokens, aligner tool is adopted here to align $p$ with acoustic tokens $\\mathbf{A}$, denoted as $\\hat{\\mathbf{p}}{1:T}= {\\hat{p}_1, \\hat{p}_2, \\cdots, \\hat{p}_L}$ where $\\hat{p}_i$ contains $N_i$ repetitions of $p_i$ and $\\sum{i=1}^{L}N_i = T$.</p> <p>For AR stage, to enhance the connection between phoneme and acoustic sequence, we build a neural codec LM $\\theta_{AR}$ to model the discrete acoustic tokens $\\mathbf{a}^{1}{1:T}$ from the first quantizer of codec-merging model with phoneme prediction. As shown in Figure.03, it\u2019s conditioned on the phoneme sequence $\\mathbf{p}$ to generate both the acoustic token $\\mathbf{a}^{1}{1:T}$ and aligned phonemes $\\hat{p}_{1:T}$ simultaneously, formulated as maximizing the following probability:</p> <p>$$   p(\\mathbf{a}^{1}{1:T}, \\hat{\\mathbf{p}}{1:T}|\\mathbf{p};\\theta_{AR}) = \\prod_{t=1}^{T} p(a_t,p_t|a_{1:t-1},\\hat{\\mathbf{p}}{1:t-1}, \\mathbf{p}{1:L};\\theta_{AR}) $$</p> <p>In the second stage, we train a NAR LM $\\theta_{NAR}$ to generate the acoustic tokens from $2$nd to $8$-th layer quantizers iteratively. It\u2019s conditioned on phoneme sequences $\\mathbf{p}$, the previous few layers of generated acoustic tokens $\\mathbf{a}^{1:n}$ and phonemes alignment $l_{1:T}$ to predict next layer of acoustic token $\\mathbf{a}^{n+1}$, formulated as maximizing:</p> <p>$$   p(\\mathbf{a}^{2:8}{1:T}|\\hat{\\mathbf{p}}{1:T}, \\mathbf{p}{1:L};\\theta{NAR}) = \\prod_{n=2}^{8} p(\\mathbf{a}{1:T}^n|\\hat{\\mathbf{p}}{1:T}, \\mathbf{a}{1:T}^{1:n-1}, \\mathbf{p}{1:L};\\theta_{NAR}) $$</p> <p>We also share the parameters of the acoustic embedding layer and the output prediction layer, which means the weights of the $j$-th prediction layer are the same as the $(j + 1)$-th acoustic embedding layer.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#322inference-with-monotonic-alignment","title":"3.2.2.Inference with Monotonic Alignment","text":"<p>After training with teacher forcing, the neural codec LM we obtained is surprising in context learning ability. With just a 3 seconds acoustic prompt, we can replicate the timbre and prosody of unseen speakers without any fine-tuning or adaptation methods (VALL-E). Take Figure.03 as an example of autoregressive inference, we convert the text input into phonemes sequence $\\mathbf{p}^t= {p^{t}1, \\cdots, p^{t}_3}$ and concatenate it with transcription phoneme of acoustic prompt $\\mathbf{p}^p= {p^{p}_1, p^{p}_2}$ to form phoneme tokens for model. In the following, we will use acoustic tokens $\\mathbf{a} = {a^p_1, \\cdots, a^t_n}$ and corresponding aligned phoneme tokens $\\mathbf{p}^a= {p^p_1, \\cdots, p^t_n}$ as condition to predict the audio and phoneme of next step autoregressively. In order to improve the robustness of the model, we adopted the Monotonic Alignment (MA) strategy during the inference process, which means that the predicted phone needs to be aligned with the text input $\\mathbf{p}^t$ and can only maintain the current or jump to the next phoneme. Specifically, at each step $i$, the current input phoneme token is $p^t_j$. The output representation is $\\mathbf{e}_i$, and corresponding probability of phoneme $p^t_j$ and $p^t{j+1}$ is denoted as $e_{i,j}$ and $e_{i,j+1}=1 \u2212 e_{i,j}$ respectively. Then, the phoneme pointer would decide to keep $p^{t}{j}$ unmoved or jump to $p^{t}{j+1}$ by sampling:</p> <p>$$   z_{i,j} \\sim \\text{Bernoulli}(\\dfrac{1}{1+\\exp(e_{i,j})}) $$</p> <p>The sampled phoneme will be used as input for the $i + 1$ step and this AR process will repeat until all phonemes have been covered. To increase the diversity of synthesis process, sampling based decoding is used for acoustic tokens prediction. And for NAR model, we adopt greedy search method to generate the $(j + 1)$-th layer acoustic tokens based on the self-aligned phoneme sequence and previous few layers of generated acoustic tokens. Finally, decoder of neural codec is adopted here to convert generated discrete codes into waveform. In summary, our inference process has the following three characteristics (A Survey on Neural Speech Synthesis) by using MA strategy:</p> <ul> <li>Locality: Each phoneme token can correspond to one or several consecutive acoustic tokens, ensuring a flexible and accurate mapping. Conversely, each acoustic token is uniquely aligned to a single phoneme token. This one-to-one alignment strategy effectively prevents issues such as misreading, enhancing the clarity and stability of the model.</li> <li>Monotonicity: If phoneme $p_a$ follows $p_b$ in the phonemes sequence, the corresponding acoustic tokens for $p_a$ are also positioned sequentially after those for $p_b$. This sequential alignment is critical as it inherently prevents the repetition of words.</li> <li>Completeness: It is mandated that each phoneme token is represented by at least one corresponding acoustic token. This coverage requirement is essential for preventing the word skipping.</li> </ul>"},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#323inference-for-controlling-prosody","title":"3.2.3.Inference for Controlling Prosody","text":"<p>In the inference process, benefiting from the powerful in context learning ability of the LM, our proposed VALL-E R can automatically clone both timbre and prosody of the speaker in the prompt by predicting the acoustic and phoneme autoregressively. Because VALL-E R explicitly models phoneme, it has strong control over prosody: when we use preset phoneme sequences to replace the self-predicted phoneme sequences in the inference process, we can use the preset prosody to generate speech, thereby achieving the effect of controlling prosody and timbre separately. It can also be regarded as a voice conversion task, whose goal is to make the timbre of target speech sound like that of prompt speech without changing the linguistic information and prosody of source speech.</p>"},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#4experiments","title":"4.Experiments: \u5b9e\u9a8c","text":""},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#5results","title":"5.Results: \u7ed3\u679c","text":""},{"location":"TTS/Models/Speech_LLM/2024.06.12_VALL-E_R/#6conclusion","title":"6.Conclusion: \u7ed3\u8bba","text":"<p>In this paper, we propose VALL-E R, a robust and efficient zero-shot text-to-speech model based on neural codec language models. We first employ codec-merging method to downsample the discrete codes to improve inference speed without affecting the sound quality. More importantly, we incorporate phoneme prediction into the training process and utilize monotonic alignment strategy during the inference. Experimental results demonstrate that VALL-E R can effectively model the correlation between phonemes and audio, significantly improve the robustness of speech synthesis, and endow the ability to flexibly control prosody.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/","title":"2021.07.07 SoundStream","text":"<p>@import \"../../style.less\"</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#soundstream-an-end-to-end-neural-audio-codec","title":"SoundStream: An End-to-End Neural Audio Codec","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: SoundStream: An End-to-End Neural Audio Codec - \u4f5c\u8005:   - [Neil Zeghidour](../../Authors/Neil_Zeghidour.md)   - [Alejandro Luebs](../../Authors/Alejandro_Luebs.md)   - [Ahmed Omran](../../Authors/Ahmed_Omran.md)   - [Jan Skoglund](../../Authors/Jan_Skoglund.md)   - [Marco Tagliasacchi](../../Authors/Marco_Tagliasacchi.md) - \u673a\u6784:   - [Google Research](../../Institutions/Google.md)  - \u65f6\u95f4:   - 2021.07.07 ArXiv v1   - 2021.11.23 [IEEE TASLP](../../Publications/TASLP.md) - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2107.03312)   - [DOI](https://doi.org/10.1109/TASLP.2021.3129994)   - [Demo](https://google-research.github.io/seanet/soundstream/examples/) - \u6807\u7b7e:   - Codec (\u7f16\u89e3\u7801\u5668) - \u9875\u6570: 13 - \u5f15\u7528: 68 - \u88ab\u5f15: 356"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#0abstract","title":"0\u00b7Abstract\u00b7\u6458\u8981","text":"\u539f\u6587  &gt; We present ***SoundStream***, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. &gt; ***SoundStream*** relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. &gt; Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. &gt; By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. &gt; In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. &gt; In subjective evaluations using audio at 24 kHz sampling rate, ***SoundStream*** at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. &gt; Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#1introduction","title":"1\u00b7Introduction\u00b7\u5f15\u8a00","text":"\u539f\u6587  &gt; Audio codec can be partitioned into two broad categories: waveform codecs and parametric codecs. &gt; Waveform codecs aim at producing at the decoder side a faithful reconstruction of the input audio samples. &gt; Often these codecs rely on transform coding techniques: a (usually invertible) transform is used to map an input time-domain waveform to the time-frequency domain. &gt; Then, transform coefficients are quantized and entropy coded. &gt; At the decoder side the transform is inverted to reconstruct a time-domain waveform. &gt; Many codecs today combine transform coding with linear predictive coding in the time-domain, especially at medium bitrates and/or narrower signal bandwidths. &gt; The bit allocation at the encoder is driven by a perceptual model, which determines the quantization process. &gt; Generally, waveform codecs make little or no assumptions about the type of audio content and can thus operate on general audio. &gt; As a consequence of this, they produce very high-quality audio at medium-to-high bitrates, but they tend to introduce coding artifacts when operating at low bitrates. &gt; Parametric codecs [1] take a different approach, by making specific assumptions about the source audio to be encoded (in most cases, speech) and introducing strong priors in the form of a parametric model that describes the audio synthesis process. &gt; The encoder estimates the parameters of the model, which are then quantized. &gt; The decoder generates a time-domain waveform using a synthesis model driven by quantized parameters. &gt; Unlike waveform codecs, the goal is not to obtain a faithful reconstruction on a sample-by-sample basis, but rather to generate audio that is perceptually similar to the original.   \u539f\u6587  &gt; Traditional waveform and parametric codecs rely on signal processing pipelines and carefully engineered design choices, which exploit in-domain knowledge of psycho-acoustics and speech production to improve coding efficiency. &gt; More recently, machine learning models have been successfully applied to the field of audio compression, demonstrating the additional value brought by data-driven solutions. &gt; For example, it is possible to apply them as a post-processing step to improve the quality of existing codecs. &gt; This can be accomplished either via audio superresolution, i.e., extending the frequency bandwidth [2], via audio denoising, i.e., removing lossy coding artifacts [3], or via packet loss concealment [4], [5].   \u539f\u6587  &gt; Other solutions adopt models based on machine learning as an integral part of the audio codec architecture. &gt; In these areas, recent advances in text-to-speech (TTS) technology proved to be a key ingredient. &gt; For example, [WaveNet](../TTS3_Vocoder/2016.09.12_WaveNet.md), a strong generative model originally applied to generate speech from text, was adopted as a decoder in a neural codec [7],[8]. &gt; Other neural audio codecs adopt different model architectures, e.g., WaveRNN in LPCNet [9] and WaveGRU in Lyra [10], all targeting speech at low bitrates.   \u539f\u6587  &gt; In this paper we propose ***SoundStream***, a novel audio codec that can compress speech, music and general audio more efficiently than previous codecs, as illustrated in Fig.01. &gt; ***SoundStream*** leverages state-of-the-art solutions in the field of neural audio synthesis, and introduces a new learnable quantization module, to deliver audio at high perceptual quality, while operating at low-to-medium bitrates. &gt; Fig.02 illustrates the high-level model architecture of the codec. &gt; A fully convolutional encoder receives as input a time-domain waveform and produces a sequence of embeddings at a lower sampling rate, which are quantized by a residual vector quantizer. &gt; A fully convolutional decoder receives the quantized embeddings and reconstructs an approximation of the original waveform. &gt; The model is trained end-to-end using both reconstruction and adversarial losses. &gt; To this end, one (or more) discriminators are trained jointly, with the goal of distinguishing the decoded audio from the original audio and, as a by-product, to provide a space where a feature-based reconstruction loss can be computed. &gt; Both the encoder and the decoder only use causal convolutions, so the overall architectural latency of the model is determined solely by the temporal resampling ratio between the original time-domain waveform and the embeddings. &gt; In summary, we make the following key contributions:  &gt; - We propose ***SoundStream***, a neural audio codec in which all the constituent components (encoder, decoder and quantizer) are trained end-to-end with a mix of reconstruction and adversarial losses to achieve superior audio quality. &gt; - We introduce a new residual vector quantizer, and investigate the rate-distortion-complexity trade-off simplied by its design. &gt; In addition, we propose a novel \u201cquantizer dropout\u201d technique for training the residual vector quantizer, which enables a single model to handle different bitrates. &gt; - We demonstrate that learning the encoder brings a very significant coding efficiency improvement, with respect to a solution that adopts mel-spectrogram features. &gt; - We demonstrate by means of subjective quality metrics that ***SoundStream*** outperforms both Opus and EVS over a wide range of bitrates. &gt; - We design our model to support streamable inference, which can operate at low-latency. &gt; When deployed on a smartphone, it runs in real-time on a single CPU thread. &gt; - We propose a variant of the ***SoundStream*** codec that performs joint audio compression and enhancement, without introducing additional latency."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#2related-work","title":"2\u00b7Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#traditional-audio-codecs","title":"Traditional Audio Codecs","text":"\u539f\u6587  &gt; Opus [11], EVS [12], and USAC [13] are state-of-the-art audio codecs, which combine traditional coding tools, such as linear predictive techniques and the modified discrete cosine transform, to deliver high coding efficiency over different content types, bitrates and sampling rates, while ensuring low-latency for real-time audio communications. &gt; We compare ***SoundStream*** with both Opus and EVS in our subjective evaluation."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#audio-generative-models","title":"Audio Generative Models","text":"\u539f\u6587  &gt; Several generative models have been developed for converting text or coded features into audio waveforms. &gt; [WaveNet (2016)](../TTS3_Vocoder/2016.09.12_WaveNet.md) allows for global and local signal conditioning to synthesize both speech and music. &gt; [SampleRNN (2016)](../TTS3_Vocoder/2016.12.22_SampleRNN.md) uses recurrent networks in a similar fashion, but it relies on previous samples at different scales. &gt; These auto-regressive models deliver very high-quality audio, at the cost of an increased computational complexity, since samples are generated one by one. &gt; To overcome this issue, Parallel WaveNet [15] allows for parallel computation, yielding considerable speedup during inference. &gt; Other approaches involve lightweight and sparse models [16] and networks mimicking the fast Fourier transform as part of the model [9], [17]. &gt; More recently, generative adversarial models have emerged as a solution able to deliver high-quality audio with a lower computational complexity. &gt; [MelGAN (2019)](../TTS3_Vocoder/2019.10.08_MelGAN.md) is trained to produce audio waveforms when conditioned on mel-spectrograms, training a multi-scale waveform discriminator together with the generator. &gt; [HiFi-GAN (2020)](../TTS3_Vocoder/2020.10.12_HiFi-GAN.md) takes a similar approach but it applies discriminators to both multiple scales and multiple periods of the audio samples. &gt; The design of the decoder and the losses in ***SoundStream*** is based on this class of audio generative models."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#audio-enhancement","title":"Audio Enhancement","text":"\u539f\u6587  &gt; Deep neural networks have been applied to different audio enhancement tasks, ranging from denoising [20]\u2013[24] to dereverberation [25], [26], lossy coding denoising [3] and frequency bandwidth extension [2], [27]. &gt; In this paper we show that it is possible to jointly perform audio compression and speech enhancement with a single model, without introducing additional latency."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#vector-quantization","title":"Vector Quantization","text":"\u539f\u6587  &gt; Learning the optimal quantizer is a key element to achieve high coding efficiency. &gt; Optimal scalar quantization based on Lloyd\u2019s algorithm [28] can be extended to a high-dimensional space via the generalized Lloyd algorithm (GLA) [29], which is very similar to k-means clustering [30]. &gt; In vector quantization [31], a point in a high-dimensional space is mapped onto a discrete set of code vectors. &gt; Vector quantization has been commonly used as a building block of traditional audio codecs [32]. &gt; For example, Code-excited Linear Prediction (CELP [33]) encodes an excitation signal via a vector quantizer codebook. &gt; More recently, vector quantization has been applied in the context of neural network models to compress the latent representation of input features. &gt; For example, in variational autoencoders, vector quantization has been used to generate images [34], [35] and music [36], [37]. &gt; Vector quantization can become prohibitively expensive, as the size of the codebook grows exponentially when rate is increased. &gt; For this reason, structured vector quantizers [38], [39] (e.g., residual, product, lattice vector quantizers, etc.) have been proposed to obtain a trade-off between computational complexity and coding efficiency in traditional codecs. &gt; In ***SoundStream***, we extend the learnable vector quantizer of VQ-VAE [34] and introduce a residual (a.k.a. multi-stage) vector quantizer, which is learned end-to-end with the rest of the model. &gt; To the best of the authors knowledge, this is the first time that this form of vector quantization is used in the context of neural networks and trained end-to-end with the rest of the model."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#neural-audio-codecs","title":"Neural Audio Codecs","text":"\u539f\u6587  &gt; End-to-end neural audio codecs rely on data-driven methods to learn efficient audio representations, instead of relying on handcrafted signal processing components. &gt; Autoencoder networks with quantization of hidden features were applied to speech coding early on [40]. &gt; More recently, a more sophisticated deep convolutional network for speech compression was described in [41]. &gt; Efficient compression of audio using neural networks has been demonstrated in several works, mostly targeting speech coding at low bitrates. &gt; A VQ-VAE speech codec was proposed in [8], operating at 1.6 kbps. &gt; Lyra [10] is a generative model that encodes quantized mel-spectrogram features of speech, which are decoded with an auto-regressive WaveGRU model to achieve state-of-the-art results at 3 kbps. &gt; A very low-bitrate codec proposed in [42] decodes speech representations obtained via self-supervised learning. &gt; An end-to-end audio codec targeting general audio at high bitrates (i.e., above 64 kbps) was proposed in [43]. &gt; The model architecture adopts a residual coding pipeline, which consists of multiple autoencoding modules and a psycho-acoustic model is used to drive the loss function during training.   \u539f\u6587  &gt; Unlike [42] which specifically targets speech by combining speaker, phonetic and pitch embeddings, ***SoundStream*** does not make assumptions on the nature of the signal it encodes, and thus works for diverse audio content types. &gt; While [10] learns a decoder on fixed features, ***SoundStream*** is trained in an end-to-end fashion. &gt; Our experiments (see [Section 4](#Sec04)) show that learning the encoder increases the audio quality substantially. &gt; ***SoundStream*** achieves bitrate scalability, i.e., the ability of a single model to operate at different bitrates at no additional cost, thanks to its residual vector quantizer and to our original quantizer dropout training scheme (see [Section 3.3](#Sec03-03)). &gt; This is unlike the work in [41], [43], [44] which enforce a specific bitrate and require training a different model for each target bitrate. &gt; A single ***SoundStream*** model is able to compress speech, music and general audio, while operating at a 24 kHz sampling rate and low-to-medium bitrates (3 kbps to 18 kbps in our experiments), in real time on a smartphone CPU. &gt; This is the first time that a neural audio codec is shown to outperform state-of-the-art codecs like Opus and EVS over this broad range of bitrates."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#joint-compression-and-enhancement","title":"Joint Compression and Enhancement","text":"\u539f\u6587  &gt; Recent work has explored joint compression and enhancement. &gt; The work in [45] trains a speech enhancement system with a quantized bottleneck. &gt; Instead, ***SoundStream*** integrates a time-dependent conditioning layer, which allows for real-time controllable denoising. &gt; As we design ***SoundStream*** as a general-purpose audio codec, controlling when to denoise allows for encoding acoustic scenes and natural sounds that would be otherwise removed."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#3methodology","title":"3\u00b7Methodology\u00b7\u65b9\u6cd5\u8bba","text":"\u539f\u6587  &gt; We consider a single channel recording $x\\in\\mathbb{R}^T$ of duration $T$ and sampled at $f_s$. &gt; The ***SoundStream*** model consists of a sequence of three building blocks, as illustrated in Fig.02: &gt; - An encoder, which maps $x$ to a sequence of embeddings (see [Section 3.1](#Sec03-01)), &gt; - A residual vector quantizer, which replaces each embedding by the sum of vectors from a set of finite codebooks, thus compressing the representation with a target number of bits (see [Section 3.3](#Sec03-03)), &gt; - A decoder, which produces a lossy reconstruction $\\hat{x}\\in\\mathbb{R}^T$ from quantized embeddings (see [Section 3.2](#Sec03-02)). &gt; &gt; The model is trained end-to-end together with a discriminator (see [Section 3.4](#Sec03-04)), using the mix of adversarial and reconstruction losses described in [Section 3.5](#Sec03-05). &gt; Optionally, a conditioning signal can be added, which determines whether denoising is applied at the encoder or decoder side, as detailed in [Section 3.6](#Sec03-06)."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#31encoder-architecture","title":"3.1\u00b7Encoder Architecture","text":"\u539f\u6587  &gt; The encoder architecture is illustrated in Fig.03 and follows the same structure as the streaming SEANet encoder described in [2], but without skip connections. &gt; It consists of a 1D convolution layer (with $C_{enc}$ channels), followed by $B_{enc}$ convolution blocks. &gt; Each of the blocks consists of three residual units, containing  dilated convolutions with dilation rates of $1$, $3$, and $9$, respectively, followed by a down-sampling layer in the form of a strided convolution. &gt; The number of channels is doubled whenever down-sampling, starting from $C_{enc}$. &gt; A final 1D convolution layer with a kernel of length $3$ and a stride of $1$ is used to set the dimensionality of the embeddings to $D$ ($D = 256$ in our experiments). &gt; To guarantee real-time inference, all convolutions are causal. &gt; This means that padding is only applied to the past but not the future in both training and offline inference, whereas no padding is used in streaming inference. &gt; We use the ELU activation [46] and we do not apply any normalization. &gt; The number $B_{enc}$ of convolution blocks and the corresponding striding sequence determines the temporal resampling ratio between the input waveform and the embeddings. &gt; For example, when $B_{enc}=4$ and using $(2,4,5,8)$ as strides, one embedding is computed every $M = 2 \\times 4 \\times 5 \\times 8 = 320$ input samples. &gt; Thus, the encoder outputs $enc(x) \\in\\mathbb{R}^{S\\times D}$, with $S = T/M$."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#32decoder-architecture","title":"3.2\u00b7Decoder Architecture","text":"\u539f\u6587  &gt; The decoder architecture follows a similar design, as illustrated in Fig.03. &gt; A 1D convolution layer is followed by a sequence of $B_{dec}$ convolution blocks. &gt; The decoder block mirrors the encoder block, and consists of a transposed convolution for up-sampling followed by the same three residual units. &gt; We use the same strides as the encoder, but in reverse order, to reconstruct a waveform with the same resolution as the input waveform. &gt; The number of channels is halved whenever upsampling, so that the last decoder block outputs $C_{dec}$ channels. &gt; A final 1D convolution layer with one filter, a kernel of size $7$ and stride $1$ projects the embeddings back to the waveform domain to produce $\\hat{x}$. &gt; In Fig.03, the same number of channels in both the encoder and the decoder is controlled by the same parameter, i.e., $C_{enc}= C_{dec}=C$. &gt; We also investigate cases in which $C_{enc}\\neq C_{dec}$, which results in a computationally lighter encoder and a heavier decoder, or vice-versa (see Section 5.4)."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#33residual-vector-quantizer","title":"3.3\u00b7Residual Vector Quantizer","text":"\u539f\u6587  &gt; The goal of the quantizer is to compress the output of the encoder $enc(x)$ to a target bitrate $R$, expressed in bits/second (bps). &gt; In order to train ***SoundStream*** in an end-to-end fashion, the quantizer needs to be jointly trained with the encoder and the decoder by backpropagation. &gt; The **Vector Quantizer (VQ)** proposed in [34], [35] in the context of [VQ-VAEs]() meets this requirement. &gt; This vector quantizer learns a codebook of $N$ vectors to encode each $D$-dimensional frame of $enc(x)$. &gt; The encoded audio $enc(x) \\in\\mathbb{R}^{S\\times D}$ is then mapped to a sequence of one-hot vectors of shape $S \\times N$, which can be represented using $S \\log_{2} N$ bits.  &gt; #### Limitations of Vector Quantization &gt; As a concrete example, let us consider a codec targeting a bitrate $R = 6000$ bps. &gt; When using a striding factor $M = 320$, each second of audio at a sampling rate $f_{s}= 24000$ Hz is represented by $S = 75$ frames at the output of the encoder. &gt; This corresponds to $r = 6000/75 = 80$ bits allocated to each frame. &gt; Using a plain vector quantizer, this requires storing a codebook with $N = 2^{80}$ vectors, which is obviously unfeasible.  &gt; #### Residual Vector Quantizer &gt; To address this issue we adopt a Residual Vector Quantizer (a.k.a. multi-stage vector quantizer [39]), which cascades $N_q$ layers of VQ as follows. &gt; The unquantized input vector is passed through a first VQ and quantization residuals are computed. &gt; The residuals are then iteratively quantized by a sequence of additional $N_q\u22121$ vector quantizers, as described in [Algorithm 1](). &gt; The total rate budget is uniformly allocated to each VQ, i.e., $r_{i}= r/N_{q}= \\log_{2}N$. &gt; For example, when using $N_{q}=8$, each quantizer uses a codebook of size $N = 2^{r/N_{q}}=2^{80/8}=1024$. &gt; For a target rate budget $r$, the parameter $N_q$ controls the trade-off between computational complexity and coding efficiency, which we investigate in [Section 5.4]().  &gt; The codebook of each quantizer is trained with exponential moving average updates, following the method proposed in [VQ-VAE-2]() [35]. &gt; We also experimented with the original VQ-VAE layer [34], as well as one using a Gumbel softmax [47] but they performed significantly worse. &gt; To improve the usage of the codebooks we use two additional methods. &gt; - First, instead of using a random initialization for the codebook vectors, we run the k-means algorithm on the first training batch and use the learned centroids as initialization. &gt; This allows the codebook to be close to the distribution of its inputs at initialization. &gt; - Second, as proposed in [37], when a codebook vector has not been assigned any input frame for several batches, we replace it with an input frame randomly sampled within the current batch. &gt; More precisely, we track the exponential moving average of the assignments to each vector (with a decay factor of 0.99) and replace the vectors of which this statistic falls below 2.  &gt; #### Enabling Bitrate Scalability with Quantizer Dropout &gt; Residual vector quantization provides a convenient framework for controlling the bitrate. &gt; For a fixed size $N$ of each codebook, the number of VQ layers Nq determines the bitrate. &gt; Since the vector quantizers are trained jointly with the encoder/decoder, in principle a different ***SoundStream*** model should be trained for each target bitrate. &gt; Instead, having a single bitrate scalable model that can operate at several target bitrates is much more practical, since this reduces the memory footprint needed to store model parameters both at the encoder and decoder side.  &gt; To train such a model, we modify [Algorithm 1]() in the following way: for each input example, we sample $n_q$ uniformly at random in $[1; N_{q}]$ and only use quantizers $Q_{i}$ for $i = 1\\cdots n_{q}$. &gt; This can be seen as a form of structured dropout [48] applied to quantization layers. &gt; Consequently, the model is trained to encode and decode audio for all target bitrates corresponding to the range $n_{q}= 1\\cdots N_{q}$. &gt; During inference, the value of nq is selected based on the desired bitrate. &gt; Previous models for neural compression have relied on product quantization (e.g.[wav2vec 2.0](../Speech_Representaion/2020.06.20_Wav2Vec2.0.md)), or on concatenating the output of several VQ layers [7], [8]. &gt; With such approaches, changing the bitrate requires either changing the architecture of the encoder and/or the decoder, as the dimensionality changes, or retraining an appropriate codebook. &gt; A key advantage of our residual vector quantizer is that the dimensionality of the embeddings does not change with the bitrate. &gt; Indeed, the additive composition of the outputs of each VQ layer progressively refines the quantized embeddings, while keeping the same shape. &gt; Hence, no architectural changes are needed in neither the encoder nor the decoder to accommodate different bitrates. &gt; In [Section 5.3](), we show that this method allows one to train a single ***SoundStream*** model, which matches the performance of models trained specifically for a given bitrate."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#34discriminator-architecture","title":"3.4\u00b7Discriminator Architecture","text":"\u539f\u6587  &gt; To compute the adversarial losses described in [Section 3.5](), we define two different discriminators:  &gt; - a wave-based discriminator, which receives as input a single waveform;  &gt; - an STFT-based discriminator, which receives as input the complex-valued STFT of the input waveform, expressed in terms of real and imaginary parts. &gt;  &gt; Since both discriminators are fully convolutional, the number of logits in the output is proportional to the length of the input audio. &gt; Consistently with previous work [19], we observed during development that a wave-based discriminator was enough to reconstruct speech with high quality, however using both the wave-based and STFT-based discriminators reduces artifacts when compressing music.  &gt; For the wave-based discriminator, we use the same multi-resolution convolutional discriminator proposed in [18] and adopted in [50]. &gt; Three structurally identical models are applied to the input audio at different resolutions: original, $2$-times down-sampled, and $4$-times down-sampled. &gt; Each single-scale discriminator consists of an initial plain convolution followed by four grouped convolutions, each of which has a group size of $4$, a down-sampling factor of $4$, and a channel multiplier of $4$ up to a maximum of 1024 output channels. &gt; They are followed by two more plain convolution layers to produce the final output, i.e., the logits.  &gt; The STFT-based discriminator is illustrated in [Fig.04]() and operates on a single scale, computing the STFT with a window length of $W = 1024$ samples and a hop length of $H = 256$ samples. &gt; A 2D-convolution (with kernel size $7 \\times 7$ and $32$ channels) is followed by a sequence of residual blocks. &gt; Each block starts with a $3 \\times 3$ convolution, followed by a $3 \\times 4$ or a $4 \\times 4$ convolution, with strides equal to $(1,2)$ or $(2,2)$, where $(s_{t},s_{f})$ indicates the down-sampling factor along the time and frequency axes. &gt; We alternate between $(1,2)$ and $(2,2)$ strides, for a total of 6 residual blocks. &gt; The number of channels is progressively increased with the depth of the network. &gt; At the output of the last residual block, the activations have shape $T/(H \\cdot 2^{3}) \\times F/2^{6}$, where $T$ is the number of samples in the time domain and $F = W/2$ is the number of frequency bins. &gt; The last layer aggregates the logits across the(down-sampled)frequency bins with a fully connected layer (implemented as a $1 \\times F/2^{6}$ convolution), to obtain a 1-dimensional signal in the (down-sampled) time domain."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#35training-objective","title":"3.5\u00b7Training Objective","text":"\u539f\u6587  &gt; Let $\\mathcal{G}(x) = dec(Q(enc(x)))$ denote the ***SoundStream*** generator, which processes the input waveform x through the encoder, the quantizer and the decoder, and \u02c6x = G(x) be the decoded waveform. &gt; We train ***SoundStream*** with a mix of losses to achieve both signal reconstruction fidelity and perceptual quality,following the principles of the perception-distortion trade-off discussed in [51].  &gt; The adversarial loss is used to promote perceptual quality and it is defined as a hinge loss over the logits of the discriminator, averaged over multiple discriminators and over time. &gt; More formally, let $k \\in \\{0,\\cdots,K\\}$ index over the individual discriminators, where $k = 0$ denotes the STFT-based discriminator and $k \\in \\{1,\\cdots,K\\}$ the different resolutions of the waveform-based discriminator ($K = 3$ in our case). &gt; Let $T_k$ denote the number of logits at the output of the $k$-th discriminator along the time dimension. &gt; The discriminator is trained to classify original vs decoded audio, by minimizing  $$ \\begin{aligned}   \\mathcal{L}_{\\mathcal{D}}    &amp;= \\mathbb{E}_x\\left[\\dfrac{1}{K}\\sum_{k}\\dfrac{1}{T_{k}}\\sum_{t}\\max(0, 1-\\mathcal{D}_{k,t}(x))\\right]\\\\   &amp;+ \\mathbb{E}_x\\left[\\dfrac{1}{K}\\sum_{k}\\dfrac{1}{T_{k}}\\sum_{t}\\max(0, 1+\\mathcal{D}_{k,t}(\\mathcal{G}(x)))\\right]\\\\ \\end{aligned}\\tag{01} $$  &gt; while the adversarial loss for the generator is  $$ \\begin{aligned}   \\mathcal{L}_{\\mathcal{G}}^{adv}    &amp;= \\mathbb{E}_x\\left[\\dfrac{1}{K}\\sum_{k}\\dfrac{1}{T_{k}}\\sum_{t}\\max(0, 1-\\mathcal{D}_{k,t}(\\mathcal{G}(x)))\\right] \\end{aligned}\\tag{02} $$  &gt; To promote fidelity of the decoded signal $\\hat{x}$ with respect to the original $x$ we adopt two additional losses:  &gt; 1. a \u201cfeature\u201d loss $\\mathcal{L}_{\\mathcal{G}}^{feat}$, computed in the feature space defined by the discriminator(s) [18];  &gt; 2. a multi-scale spectral reconstruction loss $\\mathcal{L}_{\\mathcal{G}}^{rec}$ [52].  &gt; More specifically, the feature loss is computed by taking the average absolute difference between the discriminator\u2019s internal layer outputs for the generated audio and those for the corresponding target audio.  $$ \\begin{aligned}   \\mathcal{L}_{\\mathcal{G}}^{feat}    &amp;= \\mathbb{E}_x\\left[\\dfrac{1}{KL}\\sum_{k,l}\\dfrac{1}{T_{k,l}}\\sum_{t}|\\mathcal{D}_{k,t}^{(l)}(x)-\\mathcal{D}_{k,t}^{(l)}(\\mathcal{G}(x))|\\right] \\end{aligned}\\tag{03} $$  &gt; where $L$ is the number of internal layers, $\\mathcal{D}_{k,t}^{(l)}, l\\in\\{1,\\cdots,L\\}$ is the $t$-th output of layer $l$ of discriminator $k$, and $T_{k,l}$ denotes the length of the layer in the time dimension.  &gt; The multi-scale spectral reconstruction loss follows the specifications described in [53]:  $$ \\begin{aligned}   \\mathcal{L}_{\\mathcal{G}}^{\\mathrm{rec}}   &amp;=\\sum_{s\\in 2^{6},\\ldots,2^{11}}\\sum_{t}\\|\\mathcal{S}_{t}^{s}(x)-\\mathcal{S}_{t}^{s}(\\mathcal{G}(x))\\|_{1}\\\\   &amp;+\\alpha_{s}\\sum_{t}\\|\\log\\mathcal{S}_{t}^{s}(x)-\\log\\mathcal{S}_{t}^{s}(\\mathcal{G}(x))\\|_{2}, \\end{aligned}\\tag{04} $$  &gt; where $S_{t}^{s}(x)$ denotes the $t$-th frame of a 64-bin mel-spectrogram computed with window length equal to $s$ and hop length equal to $s/4$. &gt; We set $\\alpha_{s}=\\sqrt{s/2}$ as in [53].  &gt; The overall generator loss is a weighted sum of the different loss components:  $$   \\mathcal{L}_{\\mathcal{G}} = \\lambda_{adv} \\mathcal{L}_{\\mathcal{G}}^{adv} + \\lambda_{feat} \\mathcal{L}_{\\mathcal{G}}^{feat} + \\lambda_{rec} \\mathcal{L}_{\\mathcal{G}}^{rec}\\tag{05} $$  &gt; In all our experiments we set $\\lambda_{adv}=1$, $\\lambda_{feat}=100$ and $\\lambda_{rec}=1$"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#36joint-compression-and-enhancement","title":"3.6\u00b7Joint Compression and Enhancement","text":"\u539f\u6587  &gt; In traditional audio processing pipelines, compression and enhancement are typically performed by different modules. &gt; For example, it is possible to apply an audio enhancement algorithm at the transmitter side, before audio is compressed, or at the receiver side, after audio is decoded. &gt; In this setup, each processing step contributes to the end-to-end latency, e.g., due to buffering the input audio to the expected frame length determined by the specific algorithm adopted. &gt; Conversely, we design ***SoundStream*** in such a way that compression and enhancement can be carried out jointly by the same model, without increasing the overall latency.  &gt; The nature of the enhancement can be determined by the choice of the training data. &gt; As a concrete example, in this paper we show that it is possible to combine compression with background noise suppression. &gt; More specifically, we train a model in such a way that one can flexibly enable or disable denoising at inference time, by feeding a conditioning signal that represents the two modes (denoising enabled or disabled). &gt; To this end, we prepare the training data to consist of tuples of the form: `(inputs,targets,denoise)`. &gt; When `denoise = false`, `targets = inputs`;  &gt; when `denoise = true`, `targets` contain the clean speech component of the corresponding `inputs`. &gt; Hence, the network is trained to reconstruct noisy speech if the conditioning signal is disabled, and to produce a clean version of the noisy input if it is enabled. &gt; Note that when `inputs` consist of clean audio (speech or music), `targets = inputs` and `denoise` can be either true or false. &gt; This is done to prevent ***SoundStream*** from adversely affecting clean audio when denoising is enabled.  &gt; To process the conditioning signal, we use Feature-wise Linear Modulation (FiLM) layers [54] in between residual units, which take network features as inputs and transform them as   $$   \\tilde{a}_{n,c} = \\gamma_{n,c} a_{n,c} + \\beta_{n,c}\\tag{06} $$  &gt; where $a_{n,c}$ is the $n$-th activation in the $c$-th channel. &gt; The coefficients $\\gamma_{n,c}$ and $\\beta_{n,c}$ are computed by a linear layer that takes as input a (potentially time-varying) two-dimensional one-hot encoding that determines the denoising mode. &gt; This allows one to adjust the level of denoising over time.  &gt; In principle, FiLM layers can be used anywhere throughout the encoder and decoder architecture. &gt; However, in our preliminary experiments, we found that applying conditioning at the bottleneck either at the encoder or at the decoder side (as illustrated in [Fig.03]()) was effective and no further improvements were observed by applying FiLM layers at different depths. &gt; In [Section 5.5](#Sec05-05), we quantify the impact of enabling denoising at either the encoder or decoder side both in terms of audio quality and bitrate."},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#4experiments","title":"4\u00b7Experiments\u00b7\u5b9e\u9a8c","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#41datasets-training","title":"4.1\u00b7Datasets &amp; Training","text":"<p>We train a single SoundStream model on three types of audio content: clean speech, noisy speech and music, all at 24 kHz sampling rate. For clean speech, we use the LibriTTS dataset [55], with the following training splits: <code>train-clean-100</code>, <code>train-clean-360</code> and <code>train-other-500</code>. We discard samples that do not contain frequencies above 8 kHz, that is, those which were upsampled from 16 kHz to 24 kHz. This filter results in 30 k, 102 k and 186 k in each of the three splits. Note that some of these samples, especially in the <code>train-other-500</code> split, might contain a mild level of early reverberation. For noisy speech, we synthesize samples by mixing speech from LibriTTS with noise from Freesound [56]. This results in a dataset of 37 k noisy samples 2 to 20 s long, selecting noise segments which do not contain speech and have a CC0 license. We apply peak normalization to randomly selected crops of 3 seconds and adjust the mixing gain of the noise component sampling uniformly in the interval <code>[\u221230 dB,0 dB]</code>. For music, we use the MagnaTagATune dataset [57], sampling a random subset of 114 k 5-second samples. In addition, we collect a real-world dataset, which contains two speakers (one male and one female speaker) with spontaneous English speech collected indoors in different rooms, located either in an apartment or in an office environment. Each clip is collected with two microphones, one located close to the mouth of the speaker and one at a varying distance of 2 to 5 meters. We select half of the samples from the near-field microphone and half from the far-field microphone, to evaluate on various reverberation conditions. For a subset of the samples, a loudspeaker positioned in the same room is used to generate background noise.</p> <p>We train with Adam [58], using a learning rate of $10_{\u22124}$ and a batch size of $128$, for $10^6$ steps. As sequences have variable lengths, we prepare batches by cropping random segments of $360$ milliseconds. Each segment is normalized to a peak value of $0.95$ and multiplied by a random gain in the interval $[0.3, 1.0]$ to ensure that the model is robust to a wide range of amplitudes. We evaluate our models on disjoint test splits of the datasets above. Unless stated otherwise, objective and subjective metrics are computed on a set of 200 audio clips 2\u20134 seconds long, with 50 samples from each of the four datasets listed above (i.e., clean speech, noisy speech, music, noisy/reverberant speech).</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#42evaluation-metrics","title":"4.2\u00b7Evaluation Metrics","text":"<p>To evaluate SoundStream, we perform subjective evaluations by human raters. We have chosen a crowd-sourced methodology inspired by MUSHRA [59], with a hidden reference but no lowpass-filtered anchor. Each of the 200 samples of the evaluation dataset, which include clean, noisy and reverberant speech, as well as music, was rated 20 times. The raters were required to be native English speakers and be using headphones. The number of raters for the 3 kbps, 6 kbps and 12 kbps MUSHRA tests were 275, 273 and 273 respectively. Additionally, to avoid noisy data, a post-screening was put in place to exclude listeners who rated the reference below 90 more than 20% of the time or rated non-reference samples above 90 more than 50% of the time. After post-screening the number of raters was reduced to 131, 76 and 29 respectively. We checked a-posteriori the impact of changing the rejection threshold, but this did not change the outcome of the experiments.</p> <p>For development and hyperparameter selection, we rely on computational, objective metrics. Numerous metrics have been developed in the past for assessing the perceived similarity between a reference and a processed audio signal. The ITU-T standards PESQ [60] and its replacement POLQA [61] are commonly used metrics. However, both are inconvenient to use owing to licensing restrictions. We choose the freely available and recently open-sourced ViSQOL [62], [63] metric, which has previously shown comparable performance to POLQA. In particular we use \u201caudio\u201d ViSQOL, which operates on audio resampled at 48 kHz. In early experiments, we found this metric to be strongly correlated with subjective evaluations. We thus use it for model selection and ablation studies.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#43baselines","title":"4.3\u00b7Baselines","text":"<p>Opus [11] is a versatile speech and audio codec supporting signal bandwidths from 4 kHz to 24 kHz and bitrates from 6 kbps to 510 kbps. Since its standardization by the IETF in 2012 it has been widely deployed for speech communication over the internet. As the audio codec in applications such as Zoom and applications based on WebRTC [64], [65], such as Microsoft Teams and Google Meet, Opus has hundreds of millions of daily users. Opus is also one of the main audio codecs used in YouTube for streaming. Enhanced Voice Services (EVS) [12] is the latest codec standardized by the 3GPP and was primarily designed for Voice over LTE (VoLTE). Like Opus, it is a versatile codec operating at multiple signal bandwidths, 4 kHz to 20 kHz, and bitrates, 5.9 kbps to 128 kbps. It is replacing AMR-WB [66] and retains full backward operability. We use these two systems as baselines for comparison with the SoundStream codec. For the lowest bitrates, we also compare SoundStream to the recently proposed Lyra codec [10], an autoregressive generative codec operating at 3 kbps. We provide audio processed by SoundStream and baselines at different bitrates on a public webpage.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#5results","title":"5\u00b7Results\u00b7\u7ed3\u679c","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#51comparison-with-other-codecs","title":"5.1\u00b7Comparison with Other Codecs","text":"<p>Fig.05 reports the main result of the paper, where we compare SoundStream to Opus and EVS at different bitrates. Namely, we repeated a subjective evaluation based on a MUSHRA-inspired crowd sourced scheme, when SoundStream operates at three different bitrates: i) low (3 kbps); ii) medium (6 kbps); iii) high (12 kbps). Fig.05(a) shows that SoundStream at 3 kbps significantly outperforms both Opus at 6 kbps and EVS at 5.9 kbps (i.e., the lowest bitrates at which these codecs can operate), despite using half of the bitrate. To match the quality of SoundStream, EVS needs to use at least 9.6 kbps and Opus at least 12 kbps, i.e., 3.2\u00d7 to 4\u00d7 more bits than SoundStream. We also observe that SoundStream outperforms Lyra when they both operate at 3 kbps. We observe similar results when SoundStream operates at 6 kbps and 12 kbps. At medium bitrates, EVS and Opus require, respectively, 2.2\u00d7 to 2.6\u00d7 more bits to match the same quality. At high bitrates, 1.3\u00d7 to 1.6\u00d7 more bits.</p> <p>Fig.06 illustrates the results of the subjective evaluation by content type. The quality of SoundStream remains consistent when encoding clean speech and noisy speech. In addition, SoundStream can encode music when using as little as 3 kbps, with quality significantly better than Opus at 12 kbps and EVS at 5.9 kbps. This is the first time that a codec is shown to operate on diverse content types at such a low bitrate.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#52objective-quality-metrics","title":"5.2\u00b7Objective Quality Metrics","text":"<p>Fig.07a shows the rate-quality curve of SoundStream over a wide range of bitrates, from 3 kbps to 18 kbps. We observe that quality, as measured by means of ViSQOL, gracefully decreases as the bitrate is reduced and it remains above 3.7 even at the lowest bitrate. In our work, SoundStream operates at constant bitrate, i.e., the same number of bits is allocated to each encoded frame. At the same time, we measure the bitrate lower bound by computing the empirical entropy of the quantization symbols of the vector quantizers, assuming each vector quantizer to be a discrete memoryless source, i.e., no statistical redundancy is exploited across different layers of the residual vector quantizer, nor across time. Fig.07a indicates a potential rate saving between 7% and 20%.</p> <p>We also investigate the rate-quality trade-off achieved when encoding different content types, as illustrated in Fig.07b. Unsurprisingly, the highest quality is achieved when encoding clean speech. Music represents a more challenging case, due to its inherent diversity of content.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#53bitrate-scalability","title":"5.3\u00b7Bitrate Scalability","text":"<p>We investigate the bitrate scalability provided by training a single model that can serve different bitrates. To evaluate this aspect, for each bitrate R we consider three SoundStream configurations:  a) a non-scalable model trained and evaluated at bitrate R (bitrate specific);  b) a non-scalable model trained at 18 kbps and evaluated at bitrate R by using only the first nq quantizers during inference (18 kbps - no dropout);  c) a scalable model trained with quantizer dropout and evaluated at bitrate $R$ (bitrate scalable).</p> <p>Fig.07c shows the ViSQOL scores for these three scenarios. Remarkably, a model trained specifically at 18 kbps retains good performance when evaluated at lower bitrates, even though the model was not trained in these conditions. Unsurprisingly, the quality drop increases as the bitrate decreases, i.e., when there is a more significant difference between training and inference. This gap vanishes when using the quantizer dropout strategy described in Section 3.3. Surprisingly, the bitrate scalable model seems to marginally outperform bitrate specific models at 9 kbps and 12 kbps. This suggests that quantizer dropout, beyond providing bitrate scalability, may act as a regularizer.</p> <p>We confirm these results by including the bitrate scalable variant of SoundStream in the MUSHRA subjective evaluation (see Fig.05). When operating at 3 kbps, the bitrate scalable variant of SoundStream is only slightly worse than the bitrate specific variant. Conversely, both at 6 kbps and 12 kbps it matches the same quality as the bitrate specific variant.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#54ablation-studies","title":"5.4\u00b7Ablation Studies","text":"<p>We carried out several additional experiments to evaluate the impact of some of the design choices applied to SoundStream. Unless stated otherwise, all these experiments operate at 6 kbps.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#advantage-of-learning-the-encoder","title":"Advantage of Learning the Encoder","text":"<p>We explore the impact of replacing the learnable encoder of SoundStream with a fixed mel-filterbank, similarly to Lyra [10]. In this setting, we learn both the quantizer and the decoder and observe a significant drop in objective quality, with ViSQOL going from 3.96 to 3.33. Note that this is significantly worse than what can be achieved when learning the encoder and halving the bitrate (i.e., ViSQOL equal to 3.76 at 3 kbps). This demonstrates that the additional complexity of having a learnable encoder translates to a very significant improvement in the rate-quality trade-off.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#encoder-and-decoder-capacity","title":"Encoder and Decoder Capacity","text":"<p>The main drawback of using a learnable encoder is the computational cost of the neural architecture, which can be significantly higher than computing fixed, non-learnable features such as mel-filterbanks. For SoundStream to be competitive with traditional codecs, not only should it provide a better perceptual quality at an equivalent bitrate, but it must also run in real-time on resource-limited hardware. Table I shows how computational efficiency and audio quality are impacted by the number of channels in the encoder $C_{enc}$ and the decoder $C_{dec}$. We measured the real-time factor (RTF), defined as the ratio between the temporal length of the input audio and the time needed for encoding/decoding it with SoundStream. We profiled these models on a single CPU thread of a Pixel4 smartphone. We observe that the default model ($C_{enc}=C_{dec}= 32$) runs in real-time ($RTF &gt; 2.3times$). Decreasing the model capacity by setting $C_{enc}=C_{dec}= 16$ only marginally affects the reconstruction quality while increasing the real-time factor significantly ($RTF &gt; 7.1\\times$). We also investigated configurations with asymmetric model capacities. Using a smaller encoder, it is possible to achieve a significant speedup without sacrificing quality (ViSQOL drops from 3.96 to 3.94, while the encoder RTF increases to $18.6\\times$). Instead, decreasing the capacity of the decoder has a more significant impact on quality (ViSQOL drops from 3.96 to 3.84). This is aligned with recent findings in the field of neural image compression [67], which also adopt a lighter encoder and a heavier decoder.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#vector-quantizer-depth-and-codebook-size","title":"Vector Quantizer Depth and Codebook Size","text":"<p>The number of bits used to encode a single frame is equal to Nqlog2N, where Nq denotes the number of quantizers and N the codebook size. Hence, it is possible to achieve the same target bitrate for different combinations of Nqand N. Table II shows three configurations, all operating at 6 kbps. As expected, using fewer vector quantizers, each with a larger codebook, achieves the highest coding efficiency at the cost of higher computational complexity. Remarkably, using a sequence of 80 1-bit quantizers leads only to a modest quality degradation. This demonstrates that it is possible to successfully train very deep residual vector quantizers without facing optimization issues. On the other side, as discussed in Section 3.3, growing the codebook size can quickly lead to unmanageable memory requirements. Thus, the proposed residual vector quantizer offers a practical and effective solution for learning neural codecs operating at high bitrates, as it scales gracefully when using many quantizers, each with a smaller codebook.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#latency","title":"Latency","text":"<p>The architectural latency M of the model is defined by the product of the strides, as explained in Section 3.1. In our default configuration, $M = 2 \\times 4 \\times 5 \\times 8 = 320$ samples, which means that one frame corresponds to 13.3ms of audio at 24 kHz. The bit budget allocated to the residual vector quantizer needs to be adjusted based on the target architectural latency. For example, when operating at 6 kbps, the residual vector quantizer has a budget of 80 bits per frame. If we double the latency, one frame corresponds to 26.6 ms, so the per-frame budget needs to be increased to 160 bits. Table III compares three configurations, all operating at 6 kbps, where the budget is adjusted by changing the number of quantizers, while keeping the codebook size fixed. We observe that these three configurations are equivalent in terms of audio quality. At the same time, increasing the latency of the model significantly increases the real-time factor, as encoding/decoding of a single frame corresponds to a longer audio sample.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#55joint-compression-and-enhancement","title":"5.5\u00b7Joint Compression and Enhancement","text":"<p>We evaluate a variant of SoundStream that is able to jointly perform compression and background noise suppression, which was trained as described in Section 3.6. During training, we set the denoise flag 50% of the time in each batch. We consider two configurations, in which the conditioning signal is applied to the embeddings:  - one where the conditioning signal is added at the encoder side, just before quantization;  - another where it is added at the decoder side.</p> <p>For each configuration, we train models at different bitrates. For evaluation we use 1000 samples of noisy speech, generated as described in Section 4.1 and compute ViSQOL scores when denoising is enabled or disabled, using clean speech references as targets. Figures 8 shows a substantial improvement of quality when denoising is enabled, with no significant difference between denoising either at the encoder or at the decoder. We observe that the proposed model, which is able to flexibly enable or disable denoising at inference time, does not incur a cost in performance, when compared with a model in which denoising is always enabled. This can be seen comparing Fig.08c with Fig.08a and Fig.08b.</p> <p>We also investigate whether denoising affects the potential bitrate savings that would be achievable by entropy coding. To evaluate this aspect, we first measured the empirical probability distributions $p_{i}^{(q)},i = 1 \\cdots N,q = 1 \\cdots N_{q}$ on $3200$ samples of training data. Then, we measured the empirical distribution r(q)ion the 1000 test samples and computed the cross-entropy $H(r,p) = \u2212 \\sum_{i,q}r_{i}^{(q)}\\log_{2}p_{i}^{(q)}$, as an estimate of the bitrate lower bound needed to encode the test samples. Fig.08 shows that both the encoder-side denoising and fixed denoising offer substantial bitrate savings when compared with decoder-side denoising. Hence, applying denoising before quantization leads to a representation that can be encoded with fewer bits.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#56joint-vs-disjoint-compression-and-enhancement","title":"5.6\u00b7Joint vs Disjoint Compression and Enhancement","text":"<p>We compare the proposed model, which is able to perform joint compression and enhancement, with a configuration in which compression is performed by SoundStream (with denoising disabled) and enhancement by a dedicated denoising model. For the latter, we adopt SEANet, which features a very similar model architecture, with the notable exception of skip connections between encoder and decoder layers and the absence of quantization. We consider two variants:  - one in which compression is followed by denoising (i.e., denoising is applied at the decoder side);  - another one in which denoising is followed by compression (i.e., denoising is applied at the encoder side).</p> <p>We evaluate the different models using the VCTK dataset, which was neither used for training SoundStream nor SEANet. The input samples are 2 s clips of noisy speech cropped to reduce periods of silence and resampled at 24 kHz. For each of the four input signal-to-noise ratios (0 dB, 5 dB, 10 dB and 15 dB) and clean audio, we run inference on 1000 samples and compute ViSQOL scores. As shown in Table IV, one single model trained for joint compression and enhancement achieves a level of quality that is almost on par with using two disjoint models. Also, the former requires only half of the computational cost and incurs no additional architectural latency, which would be introduced when stacking disjoint models. We also observe that the performance gap decreases as the input SNR increases.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2021.07.07_SoundStream/#6conclusion","title":"6\u00b7Conclusion\u00b7\u7ed3\u8bba","text":"\u539f\u6587  &gt; We propose ***SoundStream***, a novel neural audio codec that outperforms state-of-the-art audio codecs over a wide range of bitrates and content types. &gt; ***SoundStream*** consists of an encoder, a residual vector quantizer and a decoder, which are trained end-to-end using a mix of adversarial and reconstruction losses to achieve superior audio quality. &gt; The model supports streamable inference and can run in real-time on a single smartphone CPU. &gt; When trained with quantizer dropout, a single ***SoundStream*** model achieves bitrate scalability with a minimal loss in performance when compared with bitrate-specific models. &gt; In addition, we show that it is possible to combine compression and enhancement in a single model without introducing additional latency. &gt; In future work, we plan to extend the range of bitrates over which ***SoundStream*** operates, covering higher bitrates, aiming to attain perceptually lossless audio and encoding multi-channel audio.   <p>\u672c\u6587\u63d0\u51fa\u4e86 SoundStream, \u4e00\u79cd\u65b0\u5f0f\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668, \u5176\u6027\u80fd\u5728\u4e0d\u540c\u6bd4\u7279\u7387\u548c\u5185\u5bb9\u7c7b\u578b\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u7684\u97f3\u9891\u7f16\u89e3\u7801\u5668. SoundStream \u7531\u4e00\u4e2a\u7f16\u7801\u5668, \u4e00\u4e2a\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u5668\u548c\u4e00\u4e2a\u89e3\u7801\u5668\u7ec4\u6210, \u91c7\u7528\u5bf9\u6297\u635f\u5931\u548c\u91cd\u6784\u635f\u5931\u7684\u6df7\u5408\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\u4ee5\u8fbe\u5230\u4f18\u8d28\u7684\u97f3\u9891\u8d28\u91cf. \u8be5\u6a21\u578b\u652f\u6301\u6d41\u5f0f\u63a8\u7406, \u4e14\u53ef\u4ee5\u5728\u5355\u4e2a\u667a\u80fd\u624b\u673a CPU \u4e0a\u5b9e\u65f6\u8fd0\u884c. \u5f53\u91c7\u7528\u91cf\u5316\u5668\u5931\u6d3b\u65f6, \u4e0e\u5176\u4ed6\u7279\u5b9a\u6bd4\u7279\u7387\u7684\u6a21\u578b\u76f8\u6bd4, \u5355\u4e2a SoundStream \u6a21\u578b\u5728\u83b7\u5f97\u6027\u80fd\u4e0a\u635f\u5931\u6700\u5c0f\u65f6\u83b7\u5f97\u6bd4\u7279\u7387\u53ef\u6269\u5c55\u6027. \u6b64\u5916, \u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u5728\u5355\u4e2a\u6a21\u578b\u4e0a\u7ed3\u5408\u538b\u7f29\u548c\u589e\u5f3a\u662f\u53ef\u884c\u7684, \u4e14\u4e0d\u4f1a\u5f15\u5165\u989d\u5916\u7684\u5ef6\u8fdf. \u5728\u672a\u6765\u7684\u5de5\u4f5c\u4e2d, \u6211\u4eec\u8ba1\u5212\u6269\u5c55 SoundStream \u80fd\u5904\u7406\u7684\u6bd4\u7279\u7387\u8303\u56f4, \u5305\u62ec\u9ad8\u6bd4\u7279\u7387, \u76ee\u6807\u662f\u5b9e\u73b0\u65e0\u635f\u97f3\u9891\u7684\u611f\u77e5\u8d28\u91cf, \u5e76\u7f16\u7801\u591a\u901a\u9053\u97f3\u9891.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/","title":"EnCodec","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: High Fidelity Neural Audio Compression - \u4f5c\u8005:   - [Alexandre D\u00e9fossez](../../Authors/Alexandre_D\u00e9fossez.md)   - [Jade Copet](../../Authors/Jade_Copet.md)   - [Gabriel Synnaeve](../../Authors/Gabriel_Synnaeve.md)   - [Yossi Adi](../../Authors/Yossi_Adi.md) - \u673a\u6784:   - [Meta AI](../../Institutions/Meta.AI.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2022.10.24 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.14 - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2210.13438)   - [DOI]()   - [Github](https://github.com/facebookresearch/encodec)   - [Demo]() - \u6807\u7b7e:   - [\u7f16\u89e3\u7801\u5668](../../Tags/Codec.md)   - [\u5f00\u6e90](../../Tags/OpenSource.md) - \u9875\u6570: 19 - \u5f15\u7528: ? - \u88ab\u5f15: 293"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#abstract","title":"Abstract","text":"<p>We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at https://github.com/facebookresearch/encodec.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#1introduction","title":"1.Introduction","text":"<p>Recent studies suggest that streaming audio and video have accounted for the majority of the internet traffic in 2021 (82% according to (Cisco, 2021)). With the internet traffic expected to grow, audio compression is an increasingly important problem. In lossy signal compression we aim at minimizing the bitrate of a sample while also minimizing the amount of distortion according to a given metric, ideally correlated with human perception. Audio codecs typically employ a carefully engineered pipeline combining an encoder and a decoder to remove redundancies in the audio content and yield a compact bitstream. Traditionally, this is achieved by decomposing the input with a signal processing transform and trading off the quality of the components that are less likely to influence perception. Leveraging neural networks as trained transforms via an encoder-decoder mechanism has been explored by Morishima et al.(1990); Rippel et al.(2019); SoundStream (2021). Our research work is in the continuity of this line of work, with a focus on audio signals.</p> <p>The problems arising in lossy neural compression models are twofold: first, the model has to represent a wide range of signals, such as not to overfit the training set or produce artifact laden audio outside its comfort zone. We solve this by having a large and diverse training set (described in Section 4.1), as well as discriminator networks (see Section 3.4) that serve as perceptual losses, which we study extensively in Section 4.5.1, Tab.02. The other problem is that of compressing efficiently, both in compute time and in size. For the former, we limit ourselves to models that run in real-time on a single CPU core. For the latter, we use residual vector quantization of the neural encoder floating-point output, for which various approaches have been proposed (Van Den Oord et al., 2017; SoundStream (2021)).</p> <p>Accompanying those technical contributions, we posit that designing end-to-end neural compression models is a set of intertwined choices, among which at least the encoder-decoder architecture, the quantization method, and the perceptual loss play key parts. Objective evaluations exist and we report scores on them in our ablations (Section 4.5.1). But the evaluation of lossy audio codecs necessarily relies on human perception, so we ran extensive human evaluation for multiple points in this design space, both for speech and music. Those evaluations (MUSHRA) consist in having humans listen to, compare, and rate excerpts of speech or music compressed with competitive codecs and variants of our method, and the uncompressed ground truth. This allows to compare variants of the whole pipeline in isolation, as well as their combined effect, in Section 4.5.1 (Fig.03 and Tab.01). Finally, our best model, EnCodec, reaches state-of-the-art scores for speech and for music at 1.5, 3, 6, 12 kbps at 24 kHz, and at 6, 12, and 24 kbps for 48 kHz with stereo channels.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#2related-works","title":"2.Related Works\u00b7\u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#speech-and-audio-synthesis","title":"Speech and Audio Synthesis\u00b7\u8bed\u97f3\u4e0e\u97f3\u9891\u5408\u6210","text":"<p>Recent advancements in neural audio generation enabled computers to efficiently generate natural sounding audio. The first convincing results were achieved by autoregressive models such as WaveNet (2016), at the cost of slow inference. While many other approaches were explored (Parallel WaveGAN (2019); Kalchbrenner et al., 2018; Goel et al., 2022), the most relevant ones here are those based on Generative Adversarial Networks (GAN) (MelGAN (2019); Parallel WaveGAN (2019); HiFi-GAN (2020); HiFi++ (2022)) were able to match the quality of autoregressive by combining various adversarial networks operate at different multi-scale and multi-period resolutions. Our work uses and extends similar adversarial losses to limit artifacts during audio generation.</p> <p>\u8fd1\u671f\u5728\u795e\u7ecf\u97f3\u9891\u751f\u6210\u7684\u8fdb\u5c55\u4f7f\u5f97\u8ba1\u7b97\u673a\u80fd\u591f\u9ad8\u6548\u5730\u751f\u6210\u542c\u8d77\u6765\u5f88\u81ea\u7136\u7684\u97f3\u9891. \u81ea\u56de\u5f52\u6a21\u578b (\u5982 WaveNet (2016)) \u83b7\u5f97\u4e86\u9996\u4e2a\u4ee4\u4eba\u4fe1\u670d\u7684\u6210\u679c, \u5176\u4ee3\u4ef7\u662f\u63a8\u7406\u6548\u7387\u4f4e. \u5c3d\u7ba1\u8fd9\u671f\u95f4\u51fa\u73b0\u4e86\u8bb8\u591a\u5176\u4ed6\u65b9\u6cd5, \u4f46\u6700\u76f8\u5173\u7684\u662f\u90a3\u4e9b\u57fa\u4e8e GAN (2014) \u7684\u5de5\u4f5c, \u5b83\u4eec\u901a\u8fc7\u5728\u4e0d\u540c\u7684\u591a\u5c3a\u5ea6\u548c\u591a\u5468\u671f\u5206\u8fa8\u7387\u4e0a\u7ed3\u5408\u5404\u79cd\u5bf9\u6297\u7f51\u7edc\u4ece\u800c\u80fd\u591f\u4e0e\u81ea\u56de\u5f52\u751f\u6210\u7684\u8d28\u91cf\u76f8\u5ab2\u7f8e. \u672c\u9879\u5de5\u4f5c\u4f7f\u7528\u4e14\u6269\u5c55\u4e86\u7c7b\u4f3c\u7684\u5bf9\u6297\u635f\u5931\u4ee5\u9650\u5236\u97f3\u9891\u751f\u6210\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u5404\u79cd\u5e72\u6270\u548c\u5931\u771f.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#audio-codec","title":"Audio Codec\u00b7\u97f3\u9891\u7f16\u89e3\u7801\u5668","text":"<p>Low bitrate parametric speech and audio codecs have long been studied (Atal &amp; Hanauer, 1971; Juang &amp; Gray, 1982), but their quality has been severely limited. Despite some advances (Griffin &amp; Lim, 1985; McCree et al., 1996), modeling the excitation signal has remained a challenging task. The current state-of-the-art traditional audio codecs are Opus (Valin et al., 2012) and Enhanced Voice Service (EVS) (Dietz et al., 2015). These methods produce high coding efficiency for general audio while supporting various bitrates, sampling rates, and real-time compression.</p> <p>\u4f4e\u7801\u7387\u7684\u53c2\u6570\u5316\u8bed\u97f3\u548c\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5df2\u7ecf\u7ecf\u8fc7\u4e86\u5f88\u957f\u671f\u7684\u7814\u7a76, \u4f46\u5b83\u4eec\u7684\u8d28\u91cf\u4e25\u91cd\u53d7\u9650. \u5c3d\u7ba1\u6709\u4e00\u4e9b\u8fdb\u5c55, \u4f46\u5efa\u6a21\u523a\u6fc0\u4fe1\u53f7\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1. \u5f53\u524d\u6700\u4f73\u7684\u4f20\u7edf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u662f Opus (2012) \u548c EVS (2015). \u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u4e3a\u4e00\u822c\u97f3\u9891\u63d0\u4f9b\u9ad8\u6548\u7387\u7684\u7f16\u7801, \u540c\u65f6\u652f\u6301\u5404\u79cd\u7801\u7387, \u91c7\u6837\u7387, \u4ee5\u53ca\u5b9e\u65f6\u538b\u7f29.</p> <p>Neural based audio codecs have been recently proposed and demonstrated promising results (Kleijn et al., 2018; Valin &amp; Skoglund, 2019b; Lim et al., 2020; Kleijn et al., 2021; SoundStream (2021); Omran et al., 2022; Lin et al., 2022; Jayashankar et al., 2022; Li et al.; Jiang et al., 2022), where most methods are based on quantizing the latent space before feeding it to the decoder. In Valin &amp; Skoglund (2019b), an LPCNet (Valin &amp; Skoglund, 2019a) vocoder was conditioned on hand-crafted features and a uniform quantizer. G\u00e2rbacea et al. (2019) conditioned a WaveNet based model on discrete units obtained from a VQ-VAE (Van Den Oord et al., 2017; Razavi et al., 2019) model, while Skoglund &amp; Valin (2019) tried feeding the Opus codec (Valin et al., 2012) to a WaveNet to further improve its perceptual quality. Jayashankar et al. (2022); Jiang et al. (2022) propose an auto-encoder with a vector quantization layer applied over the latent representation and minimizing the reconstruction loss, while Li et al. suggested using Gumbel-Softmax (GS) (Jang et al., 2017) for representation quantization. The most relevant related work to ours is the SoundStream (2021) model, in which the authors propose a fully convolutional encoder decoder architecture with a Residual Vector Quantization (RVQ) (Gray, 1984; Vasuki &amp; Vanathi, 2006) layers. The model was optimized using both reconstruction loss and adversarial perceptual losses.</p> <p>\u8fd1\u671f\u63d0\u51fa\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u7f16\u89e3\u7801\u5668\u5df2\u7ecf\u5c55\u793a\u4e86\u4e0d\u9519\u7684\u7ed3\u679c, \u5176\u4e2d\u5927\u591a\u6570\u65b9\u6cd5\u90fd\u57fa\u4e8e\u91cf\u5316\u9690\u7a7a\u95f4, \u7136\u540e\u5c06\u5176\u8f93\u5165\u5230\u89e3\u7801\u5668\u4e2d. - LPCNet (2019) \u58f0\u7801\u5668\u4f7f\u7528\u624b\u5de5\u7279\u5f81\u548c\u5747\u5300\u91cf\u5316\u5668\u8fdb\u884c\u6761\u4ef6\u5316; - G\u00e2rbacea \u7b49\u4eba\u4f7f\u7528 VQ-VAE (2017) \u83b7\u5f97\u7684\u79bb\u6563\u5355\u5143\u5bf9\u57fa\u4e8e WaveNet \u7684\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u5316; - Skoglund \u7b49\u4eba\u5c1d\u8bd5\u5c06 Opus \u7f16\u89e3\u7801\u5668\u8f93\u5165\u5230 WaveNet \u4e2d\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u611f\u77e5\u8d28\u91cf; - Jayashankar \u7b49\u4eba\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u9690\u8868\u793a\u4e0a\u4f7f\u7528\u77e2\u91cf\u91cf\u5316\u5c42\u7684\u81ea\u7f16\u7801\u5668, \u5e76\u6700\u5c0f\u5316\u91cd\u6784\u635f\u5931; - Li \u7b49\u4eba\u63d0\u51fa\u7528 Gumbel-Softmax \u4ee5\u8868\u793a\u91cf\u5316.</p> <p>\u4e0e\u672c\u6587\u6700\u76f8\u5173\u7684\u5de5\u4f5c\u662f SoundStream (2021), \u5b83\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e26\u6709\u6b8b\u5dee\u77e2\u91cf\u91cf\u5316 (Residual Vector Quantization, RVQ) \u5c42\u7684\u5168\u5377\u79ef\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784. \u8be5\u6a21\u578b\u901a\u8fc7\u91cd\u6784\u635f\u5931\u548c\u5bf9\u6297\u611f\u77e5\u635f\u5931\u8fdb\u884c\u4f18\u5316.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#audio-discretization","title":"Audio Discretization\u00b7\u97f3\u9891\u79bb\u6563\u5316","text":"<p>Representing audio and speech using discrete values was proposed to various tasks recently. Dieleman et al. (2018); Dhariwal et al. (2020) proposed a hierarchical VQ-VAE based model for learning discrete representation of raw audio, next combined with an autoregressive model, demonstrating the ability to generate high quality music. Similarly, Lakhotia et al. (2021); Kharitonov et al. (2021) demonstrated that self-supervised learning methods for speech (e.g., HuBERT (2021)), can be quantized and used for conditional and unconditional speech generation. Similar methods were applied to speech resynthesis (Polyak et al., 2021), speech emotion conversion (Kreuk et al., 2021), spoken dialog system (Nguyen et al., 2022), and speech-to-speech translation (Lee et al., 2021a;b; Popuri et al., 2022).</p> <p>\u8fd1\u671f\u4f7f\u7528\u79bb\u6563\u503c\u8868\u793a\u97f3\u9891\u548c\u8bed\u97f3\u88ab\u63d0\u51fa\u7528\u4e8e\u5404\u79cd\u4efb\u52a1. - Dieleman \u7b49\u4eba\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5c42\u6b21\u5316 VQ-VAE \u7684\u6a21\u578b\u7528\u4e8e\u5b66\u4e60\u539f\u59cb\u97f3\u9891\u7684\u79bb\u6563\u8868\u793a, \u7136\u540e\u548c\u81ea\u56de\u5f52\u6a21\u578b\u7ed3\u5408, \u5c55\u793a\u4e86\u751f\u6210\u9ad8\u8d28\u91cf\u97f3\u4e50\u7684\u80fd\u529b; - \u7c7b\u4f3c\u5730, Lakhotia \u7b49\u4eba\u5c55\u793a\u4e86\u8bed\u97f3\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5 (\u5982 HuBERT (2021)) \u80fd\u591f\u91cf\u5316\u5e76\u7528\u4e8e\u6761\u4ef6\u548c\u975e\u6761\u4ef6\u8bed\u97f3\u751f\u6210; - \u7c7b\u4f3c\u7684\u65b9\u6cd5\u88ab\u7528\u4e8e\u8bed\u97f3\u5408\u6210, \u8bed\u97f3\u60c5\u611f\u8f6c\u6362, \u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf, \u4ee5\u53ca\u8bed\u97f3\u5230\u8bed\u97f3\u7684\u7ffb\u8bd1.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#3model","title":"3.Model\u00b7\u6a21\u578b","text":"<p>An audio signal of duration $d$ can be represented by a sequence $\\mathbf{x}\\in[\u22121,1]^{C_a\\times T}$ with $C_a$ the number of audio channels, $T=d \\cdot f_{sr}$ the number of audio samples at a given sample rate $f_{sr}$. The EnCodec model is composed of three main components:  1. First, an encoder network $E$ is input an audio extract and outputs a latent representation $\\mathbf{z}$; 2. Next, a quantization layer $Q$ produces a compressed representation $\\mathbf{z}_q$, using vector quantization;  3. Lastly, a decoder network $G$ reconstructs the time-domain signal, $\\hat{\\mathbf{x}}$, from the compressed latent representation $\\mathbf{z}_q$.</p> <p>The whole system is trained end-to-end to minimize a reconstruction loss applied over both time and frequency domain, together with a perceptual loss in the form of discriminators operating at different resolutions.</p> <p>\u65f6\u957f\u4e3a $d$ \u7684\u97f3\u9891\u4fe1\u53f7\u53ef\u4ee5\u7531\u4e00\u4e2a\u5e8f\u5217 $\\mathbf{x}\\in [-1,1]^{C_a\\times T}$ \u8868\u793a, \u5176\u4e2d $C_a$ \u662f\u97f3\u9891\u901a\u9053\u6570, $T=d \\cdot f_{sr}$ \u662f\u7ed9\u5b9a\u91c7\u6837\u7387 $f_{sr}$ \u4e0b\u97f3\u9891\u6837\u672c\u6570. EnCodec \u6a21\u578b\u7531\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\u7ec4\u6210: 1. \u9996\u5148, \u7f16\u7801\u5668\u7f51\u7edc $E$ \u8f93\u5165\u97f3\u9891\u4fe1\u53f7\u5e76\u8f93\u51fa\u9690\u53d8\u91cf\u8868\u793a $\\mathbf{z}$; 2. \u7136\u540e, \u91cf\u5316\u5c42 $Q$ \u4f7f\u7528\u77e2\u91cf\u91cf\u5316\u4ea7\u751f\u538b\u7f29\u8868\u793a $\\mathbf{z}_q$; 3. \u6700\u540e, \u89e3\u7801\u5668\u7f51\u7edc $G$ \u4ece\u538b\u7f29\u7684\u9690\u53d8\u91cf\u8868\u793a $\\mathbf{z}_q$ \u91cd\u6784\u65f6\u57df\u4fe1\u53f7 $\\hat{\\mathbf{x}}$.</p> <p>\u6574\u4e2a\u7cfb\u7edf\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u8bad\u7ec3, \u6700\u5c0f\u5316\u540c\u65f6\u5e94\u7528\u5728\u65f6\u57df\u548c\u9891\u57df\u4e0a\u7684\u91cd\u6784\u635f\u5931, \u4ee5\u53ca\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u64cd\u4f5c\u7684\u9274\u522b\u5668\u5f62\u5f0f\u7684\u611f\u77e5\u635f\u5931.</p> <p>A visual description of the proposed method can be seen in Fig.01.</p> <p>\u56fe 01 \u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u53ef\u89c6\u5316\u63cf\u8ff0.</p> <p></p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#31encoder-decoder-architecture","title":"3.1.Encoder &amp; Decoder Architecture\u00b7\u7f16\u7801\u5668\u4e0e\u89e3\u7801\u5668\u67b6\u6784","text":"<p>The EnCodec model is a simple streaming, convolutional-based encoder-decoder architecture with sequential modeling component applied over the latent representation, both on the encoder and on the decoder side. Such modeling framework was shown to provide great results in various audio-related tasks, e.g., source separation and enhancement (D\u00e9fossez et al., 2019; Defossez et al., 2020), neural vocoders (MelGAN (2019); HiFi-GAN (2020)), audio codec (SoundStream (2021)), and artificial bandwidth extension (SEANet (2020); Li et al., 2021). We use the same architecture for 24 kHz and 48 kHz audio.</p> <p>EnCodec \u6a21\u578b\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u6d41\u5f0f, \u57fa\u4e8e\u5377\u79ef\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784, \u5176\u4e2d\u5728\u9690\u8868\u793a\u4e0a\u5e94\u7528\u5e8f\u5217\u5efa\u6a21\u7ec4\u4ef6, \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e24\u4fa7\u90fd\u6709. \u8fd9\u79cd\u5efa\u6a21\u6846\u67b6\u5728\u97f3\u9891\u76f8\u5173\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u7ed3\u679c, \u5982\u97f3\u6e90\u5206\u79bb\u548c\u589e\u5f3a, \u795e\u7ecf\u58f0\u7801\u5668, \u97f3\u9891\u7f16\u89e3\u7801\u5668, \u4eba\u5de5\u5e26\u5bbd\u6269\u5c55. \u6211\u4eec\u5bf9 24 kHz \u548c 48 kHz \u97f3\u9891\u4f7f\u7528\u76f8\u540c\u7684\u67b6\u6784.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#encoder-decoder-","title":"Encoder-Decoder\u00b7\u7f16\u7801\u5668-\u89e3\u7801\u5668","text":"<p>The encoder model $E$ consists in a 1D convolution with $C$ channels and a kernel size of 7 followed by $B$ convolution blocks. Each convolution block is composed of a single residual unit followed by a down-sampling layer consisting in a strided convolution, with a kernel size $K$ of twice the stride $S$. The residual unit contains two convolutions with kernel size 3 and a skip-connection. The number of channels is doubled whenever down-sampling occurred. The convolution blocks are followed by a two-layer LSTM for sequence modeling and a final 1D convolution layer with a kernel size of 7 and $D$ output channels. Following SoundStream (2021); Li et al. (2021), we use $C= 32$, $B=4$ and $(2, 4, 5, 8)$ as strides. We use ELU as a non-linear activation function (Clevert et al., 2015) either layer normalization (Ba et al., 2016) or weight normalization (Salimans &amp; Kingma, 2016). We use two variants of the model, depending on whether we target the low-latency streamable setup, or a high fidelity non-streamable usage. With this setup, the encoder outputs 75 latent steps per second of audio at 24 kHz, and 150 at 48 kHz. The decoder mirrors the encoder, using transposed convolutions instead of strided convolutions, and with the strides in reverse order as in the encoder, outputting the final mono or stereo audio.</p> <p>\u7f16\u7801\u5668\u6a21\u578b $E$ \u7531\u5177\u6709 $C$ \u4e2a\u901a\u9053\u6570\u7684\u4e00\u7ef4\u5377\u79ef, \u540e\u8ddf $B$ \u4e2a\u5377\u79ef\u6838\u5927\u5c0f\u4e3a 7 \u7684\u5377\u79ef\u5757. \u6bcf\u4e2a\u5377\u79ef\u5757\u7531\u4e00\u4e2a\u6b8b\u5dee\u5355\u5143\u548c\u4e00\u4e2a\u5e26\u6b65\u957f\u7684\u5377\u79ef\u7ec4\u6210\u7684\u4e0b\u91c7\u6837\u5c42\u6784\u6210, \u5176\u4e2d\u5377\u79ef\u6838\u5927\u5c0f $K$ \u4e3a\u6b65\u957f $S$ \u7684\u4e24\u500d. \u6b8b\u5dee\u5355\u5143\u7531\u4e24\u4e2a\u5377\u79ef\u6838\u5927\u5c0f\u4e3a 3 \u7684\u5377\u79ef\u548c\u4e00\u4e2a\u8df3\u8dc3\u8fde\u63a5\u7ec4\u6210. \u901a\u9053\u6570\u6bcf\u5f53\u4e0b\u91c7\u6837\u53d1\u751f\u65f6\u8fdb\u884c\u7ffb\u500d. \u5377\u79ef\u5757\u540e\u9762\u662f\u4e00\u4e2a\u4e24\u5c42 LSTM \u7528\u4e8e\u5e8f\u5217\u5efa\u6a21, \u6700\u540e\u4e00\u4e2a 1D \u5377\u79ef\u5c42\u5177\u6709\u5377\u79ef\u6838\u5927\u5c0f\u4e3a 7 \u548c $D$ \u4e2a\u8f93\u51fa\u901a\u9053.</p> <p>\u9075\u5faa SoundStream (2021) \u7684\u8bbe\u7f6e, \u6211\u4eec\u4f7f\u7528 $C=32$, $B=4$, $(2, 4, 5, 8)$ \u4f5c\u4e3a\u6b65\u957f. \u6211\u4eec\u4f7f\u7528 ELU \u4f5c\u4e3a\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570, \u5c42\u5f52\u4e00\u5316\u6216\u6743\u91cd\u5f52\u4e00\u5316. \u6211\u4eec\u4f7f\u7528\u4e24\u4e2a\u53d8\u4f53\u6a21\u578b, \u53d6\u51b3\u4e8e\u6211\u4eec\u7684\u76ee\u6807\u662f\u4f4e\u5ef6\u8fdf\u6d41\u5f0f, \u6216\u9ad8\u7cbe\u5ea6\u975e\u6d41\u5f0f. \u5728\u8fd9\u4e00\u8bbe\u7f6e\u4e0b, \u7f16\u7801\u5668 24 kHz \u65f6\u8f93\u51fa 75 \u9690\u6b65\u957f\u6bcf\u79d2\u7684\u97f3\u9891, 48 kHz \u65f6\u4e3a 150 \u4e2a.</p> <p>\u89e3\u7801\u5668\u4e0e\u7f16\u7801\u5668\u955c\u50cf, \u4f7f\u7528\u8f6c\u7f6e\u5377\u79ef\u4ee3\u66ff\u6b65\u957f\u5377\u79ef, \u6b65\u957f\u987a\u5e8f\u4e0e\u7f16\u7801\u5668\u76f8\u53cd, \u8f93\u51fa\u6700\u7ec8\u5355\u58f0\u9053\u6216\u7acb\u4f53\u58f0\u97f3\u9891.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#non-streamable","title":"Non-streamable\u00b7\u975e\u6d41\u5f0f","text":"<p>In the non-streamable setup, we use for each convolution a total padding of $K\u2212S$, split equally before the first time step and after the last one (with one more before if $K\u2212S$ is odd). We further split the input into chunks of 1 seconds, with an overlap of 10 ms to avoid clicks, and normalize each chunk before feeding it to the model, applying the inverse operation on the output of the decoder, adding a negligible bandwidth overhead to transmit the scale. We use layer normalization (Ba et al., 2016), computing the statistics including also the time dimension in order to keep the relative scale information.</p> <p>\u5728\u975e\u6d41\u5f0f\u8bbe\u7f6e\u4e0b, \u6211\u4eec\u4e3a\u6bcf\u4e2a\u5377\u79ef\u4f7f\u7528 $K\u2212S$ \u7684\u603b\u586b\u5145, \u5728\u7b2c\u4e00\u4e2a\u65f6\u95f4\u6b65\u4e4b\u524d\u548c\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65\u4e4b\u540e\u5747\u5300\u5206\u5272 (\u5982\u679c $K\u2212S$ \u4e3a\u5947\u6570, \u5219\u518d\u591a\u5206\u5272\u4e00\u6b21). \u6211\u4eec\u8fdb\u4e00\u6b65\u5c06\u8f93\u5165\u5212\u5206\u4e3a\u4e00\u79d2\u7684\u5757, \u5e76\u4f7f\u7528 10 \u6beb\u79d2\u7684\u91cd\u53e0\u907f\u514d\u70b9\u51fb, \u5e76\u5728\u8f93\u5165\u5230\u6a21\u578b\u4e4b\u524d\u5bf9\u6bcf\u4e2a\u5757\u8fdb\u884c\u5f52\u4e00\u5316, \u7136\u540e\u5728\u89e3\u7801\u5668\u8f93\u51fa\u4e0a\u5e94\u7528\u9006\u64cd\u4f5c, \u5e76\u5728\u8f93\u51fa\u4e0a\u6dfb\u52a0\u5fae\u5c0f\u7684\u5e26\u5bbd\u5f00\u9500\u4ee5\u4f20\u8f93\u5c3a\u5ea6. \u6211\u4eec\u4f7f\u7528\u5c42\u5f52\u4e00\u5316, \u8ba1\u7b97\u5305\u62ec\u65f6\u95f4\u7ef4\u5ea6\u7684\u7edf\u8ba1\u4fe1\u606f, \u4ee5\u4fdd\u6301\u76f8\u5bf9\u5c3a\u5ea6\u4fe1\u606f.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#streamable","title":"Streamable\u00b7\u6d41\u5f0f","text":"<p>For the streamable setup, all padding is put before the first time step. For a transposed convolution with stride $s$, we output the $s$ first time steps, and keep the remaining $s$ steps in memory for completion when the next frame is available, or discarding it at the end of a stream. Thanks to this padding scheme, the model can output 320 samples (13 ms) as soon as the first 320 samples (13 ms) are received. We replace the layer normalization with statistics computed over the time dimension with weight normalization (Salimans &amp; Kingma, 2016), as the former is ill-suited for a streaming setup. We notice a small gain over the objective metrics by keeping a form of normalization, as demonstrated in Tab.A.3.</p> <p>\u5bf9\u4e8e\u6d41\u5f0f\u8bbe\u7f6e, \u6240\u6709\u586b\u5145\u5728\u7b2c\u4e00\u4e2a\u65f6\u95f4\u6b65\u524d\u8fdb\u884c. \u5bf9\u4e8e\u5177\u6709\u6b65\u957f\u4e3a $s$ \u7684\u8f6c\u7f6e\u5377\u79ef, \u6211\u4eec\u8f93\u51fa\u524d $s$ \u4e2a\u65f6\u95f4\u6b65, \u5e76\u5c06\u5269\u4f59 $s$ \u4e2a\u65f6\u95f4\u6b65\u4fdd\u5b58\u5728\u5185\u5b58\u4e2d, \u4ee5\u4fbf\u5728\u4e0b\u4e00\u5e27\u53ef\u7528\u65f6\u5b8c\u6210, \u6216\u8005\u5728\u6d41\u7ed3\u675f\u65f6\u4e22\u5f03. \u7531\u4e8e\u8fd9\u79cd\u586b\u5145\u65b9\u6848, \u6a21\u578b\u53ef\u4ee5\u5728\u6536\u5230\u524d 320 \u4e2a\u6837\u672c (13 \u6beb\u79d2) \u540e\u7acb\u5373\u8f93\u51fa 320 \u4e2a\u6837\u672c (13 \u6beb\u79d2). \u6211\u4eec\u7528\u6743\u91cd\u5f52\u4e00\u5316\u66ff\u6362\u5c42\u5f52\u4e00\u5316, \u56e0\u4e3a\u524d\u8005\u4e0d\u9002\u5408\u6d41\u5f0f\u8bbe\u7f6e. \u6211\u4eec\u6ce8\u610f\u5230\u5728\u4fdd\u6301\u67d0\u79cd\u5f62\u5f0f\u7684\u5f52\u4e00\u5316\u7684\u60c5\u51b5\u4e0b, \u76ee\u6807\u6307\u6807\u7684\u6027\u80fd\u6709\u6240\u63d0\u5347, \u5982\u8868 A.3 \u6240\u793a.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#32residual-vector-quantization","title":"3.2.Residual Vector Quantization\u00b7\u6b8b\u5dee\u77e2\u91cf\u91cf\u5316","text":"<p>We use Residual Vector Quantization (RVQ) to quantize the output of the encoder as introduced by SoundStream (2021). Vector quantization consists in projecting an input vector onto the closest entry in a codebook of a given size. RVQ refines this process by computing the residual after quantization, and further quantizing it using a second codebook, and so forth.</p> <p>\u6211\u4eec\u4f7f\u7528\u6b8b\u5dee\u77e2\u91cf\u91cf\u5316 (Residual Vector Quantization, RVQ) \u6765\u91cf\u5316\u7f16\u7801\u5668\u8f93\u51fa. \u77e2\u91cf\u91cf\u5316\u662f\u5c06\u8f93\u5165\u5411\u91cf\u6295\u5f71\u5230\u7ed9\u5b9a\u5927\u5c0f\u7684\u7801\u672c\u4e2d\u6700\u8fd1\u7684\u5143\u7d20. RVQ \u901a\u8fc7\u5728\u91cf\u5316\u540e\u8ba1\u7b97\u6b8b\u5dee\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b, \u5e76\u4f7f\u7528\u7b2c\u4e8c\u4e2a\u7801\u672c\u8fdb\u4e00\u6b65\u91cf\u5316, \u4f9d\u6b64\u7c7b\u63a8.</p> <p>We follow the same training procedure as described by Dhariwal et al. (2020) and SoundStream (2021). The codebook entry selected for each input is updated using an exponential moving average with a decay of 0.99, and entries that are not used are replaced with a candidate sampled from the current batch. We use a straight-through-estimator (Bengio et al., 2013) to compute the gradient of the encoder, e.g. as if the quantization step was the identity function during the backward phase. Finally, a commitment loss, consisting of the MSE between the input of the quantizer and its output, with gradient only computed with respect to its input, is added to the overall training loss.</p> <p>\u6211\u4eec\u9075\u5faa Dhariwal \u7b49\u4eba\u7684\u76f8\u540c\u8bad\u7ec3\u8fc7\u7a0b. \u5bf9\u4e8e\u6bcf\u4e2a\u8f93\u5165, \u9009\u62e9\u7684\u7801\u672c\u5143\u7d20\u662f\u4f7f\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\u66f4\u65b0\u7684, \u8870\u51cf\u7387\u4e3a 0.99, \u4e14\u672a\u4f7f\u7528\u7684\u5143\u7d20\u88ab\u66ff\u6362\u4e3a\u5f53\u524d\u6279\u6b21\u4e2d\u5019\u9009\u91c7\u6837\u7684\u5143\u7d20. \u6211\u4eec\u4f7f\u7528\u76f4\u901a\u4f30\u8ba1\u5668\u7528\u4e8e\u8ba1\u7b97\u7f16\u7801\u5668\u7684\u68af\u5ea6, \u4f8b\u5982, \u5047\u8bbe\u91cf\u5316\u6b65\u9aa4\u5728\u53cd\u5411\u9636\u6bb5\u662f\u6052\u7b49\u51fd\u6570. \u6700\u540e, \u4e00\u4e2a\u63d0\u4ea4\u635f\u5931, \u7531\u91cf\u5316\u5668\u7684\u8f93\u5165\u4e0e\u5176\u8f93\u51fa\u4e4b\u95f4\u7684 MSE \u7ec4\u6210, \u53ea\u8ba1\u7b97\u5176\u8f93\u5165\u7684\u68af\u5ea6, \u88ab\u6dfb\u52a0\u5230\u6574\u4f53\u8bad\u7ec3\u635f\u5931\u4e2d.</p> <p>By selecting a variable number of residual steps at train time, a single model can be used to support multiple bandwidth target (SoundStream (2021)). For all of our models, we use at most 32 codebooks (16 for the 48 kHz models) with 1024 entries each, e.g. 10 bits per codebook. When doing variable bandwidth training, we select randomly a number of codebooks as a multiple of 4, i.e. corresponding to a bandwidth 1.5, 3, 6, 12 or 24 kbps at 24 kHz. Given a continuous latent represention with shape [B, D, T] that comes out of the encoder, this procedure turns it into a discrete set of indexes [B, Nq, T] with Nq the number of codebooks selected. This discrete representation can changed again to a vector by summing the corresponding codebook entries, which is done just before going into the decoder.</p> <p>\u901a\u8fc7\u5728\u8bad\u7ec3\u65f6\u9009\u62e9\u53ef\u53d8\u6570\u91cf\u7684\u6b8b\u5dee\u6b65\u9aa4, \u5355\u4e2a\u6a21\u578b\u53ef\u4ee5\u652f\u6301\u591a\u4e2a\u5e26\u5bbd\u76ee\u6807. \u5bf9\u4e8e\u6211\u4eec\u7684\u6240\u6709\u6a21\u578b, \u6211\u4eec\u4f7f\u7528\u81f3\u591a 32 \u4e2a\u7801\u672c (48 kHz \u6a21\u578b\u4e3a 16 \u4e2a), \u6bcf\u4e2a\u7801\u672c\u5177\u6709 1024 \u4e2a\u5143\u7d20, \u4f8b\u5982, \u6bcf\u4e2a\u7801\u672c 10 \u6bd4\u7279. \u5728\u8fdb\u884c\u53ef\u53d8\u5e26\u5bbd\u8bad\u7ec3\u65f6, \u6211\u4eec\u968f\u673a\u9009\u62e9 4 \u7684\u500d\u6570\u4e2a\u7801\u672c, \u5373\u5bf9\u5e94\u4e8e 24 kHz \u65f6\u5e26\u5bbd\u4e3a 1.5, 3, 6, 12 \u6216 24 kbps. \u7ed9\u5b9a\u7f16\u7801\u5668\u8f93\u51fa\u7684\u8fde\u7eed\u6f5c\u5728\u8868\u793a [B, D, T], \u8fd9\u4e00\u8fc7\u7a0b\u5c06\u5176\u8f6c\u6362\u4e3a\u79bb\u6563\u7d22\u5f15\u96c6 [B, Nq, T], \u5176\u4e2d Nq \u662f\u6240\u9009\u7801\u672c\u7684\u6570\u91cf. \u8fd9\u4e2a\u79bb\u6563\u8868\u793a\u53ef\u4ee5\u518d\u8f6c\u6362\u4e3a\u5411\u91cf, \u8fd9\u5c31\u662f\u5c06\u76f8\u5e94\u7801\u672c\u5143\u7d20\u6c42\u548c\u5f97\u5230\u7684\u7ed3\u679c.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#33language-modeling-and-entropy-coding","title":"3.3.Language Modeling and Entropy Coding\u00b7\u8bed\u8a00\u6a21\u578b\u548c\u71b5\u7f16\u7801","text":"<p>We additionally train a small Transformer based language model (Vaswani et al., 2017) with the objective of keeping faster than real time end-to-end compression/decompression on a single CPU core. The model consists of 5 layers, 8 heads, 200 channels, a dimension of 800 for the feed-forward blocks, and no dropout. At train time, we select a bandwidth and the corresponding number of codebooks <code>Nq=32</code>. For a time step t, the discrete representation obtained at time t \u22121 is transformed into a continuous representation using learnt embedding tables, one for each codebook, and which are summed. For t=0, a special token is used instead. The output of the Transformer (<code>out</code>, <code>states</code>, <code>offset</code>) is fed into Nq linear layers with as many output channels as the cardinality of each codebook (e.g. 1024), giving us the logits of the estimated distribution over each codebook for time t. We thus neglect potential mutual information between the codebooks at a single time step. This allows to speedup inference (as opposed to having one time step per codebook, or a multi-stage prediction) with a limited impact over the final cross entropy. Each attention layer has a causal receptive field of 3.5 seconds, and we offset by a random amount the initial position of the sinusoidal position embedding to emulate being in a longer sequence. We train the model on sequences of 5 seconds.</p> <p>\u6211\u4eec\u8fd8\u8bad\u7ec3\u4e86\u4e00\u4e2a\u57fa\u4e8e Transformer \u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b, \u76ee\u7684\u662f\u5728\u5355\u4e2a CPU \u6838\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u7aef\u5230\u7aef\u538b\u7f29/\u89e3\u538b\u7f29\u901f\u5ea6\u66f4\u5feb. \u8be5\u6a21\u578b\u7531\u4e94\u5c42 <code>num_layers=5</code>, \u516b\u4e2a\u6ce8\u610f\u529b\u5934 <code>n_heads=8</code>, \u4e24\u767e\u4e2a\u901a\u9053 <code>dim=200</code>, \u4e00\u4e2a\u7ef4\u5ea6\u4e3a\u516b\u767e\u7684\u524d\u9988\u5757 <code>hidden_dim=dim*hidden_scale=200*4=800</code>, \u4e14\u65e0\u968f\u673a\u5931\u6d3b. \u5728\u8bad\u7ec3\u65f6, \u6211\u4eec\u9009\u62e9\u4e00\u4e2a\u5e26\u5bbd\u548c\u5bf9\u5e94\u7684\u7801\u672c\u6570\u91cf Nq. \u5bf9\u4e8e t \u65f6\u95f4\u6b65, \u5728 t-1 \u65f6\u95f4\u6b65\u83b7\u5f97\u7684\u79bb\u6563\u8868\u793a, \u4f7f\u7528\u6bcf\u4e2a\u7801\u672c\u7684\u5b66\u4e60\u5d4c\u5165\u8868, \u5e76\u6c42\u548c\u5f97\u5230\u8fde\u7eed\u8868\u793a. \u5f53 t=0 \u65f6, \u91c7\u7528\u7279\u6b8a\u6807\u8bb0. Transformer \u7684\u8f93\u51fa\u88ab\u8f93\u5165\u5230 Nq \u4e2a\u7ebf\u6027\u5c42, \u8f93\u51fa\u901a\u9053\u6570\u4e0e\u6bcf\u4e2a\u7801\u672c\u7684\u57fa\u6570\u76f8\u540c (\u4f8b\u5982, 1024), \u7ed9\u51fa\u4e86 t \u65f6\u95f4\u6b65\u6bcf\u4e2a\u7801\u672c\u7684\u4f30\u8ba1\u5206\u5e03\u7684\u5bf9\u6570\u6982\u7387. \u56e0\u6b64, \u6211\u4eec\u5ffd\u7565\u4e86\u5728\u5355\u4e2a\u65f6\u95f4\u6b65\u4e2d\u7801\u672c\u4e4b\u95f4\u7684\u6f5c\u5728\u4fe1\u606f. \u8fd9\u4f7f\u5f97\u63a8\u7406 (\u4e0e\u5177\u6709\u591a\u4e2a\u7801\u672c\u6216\u591a\u9636\u6bb5\u9884\u6d4b\u7684\u9884\u6d4b\u76f8\u6bd4) \u5f97\u4ee5\u52a0\u901f, \u4e14\u5bf9\u6700\u7ec8\u4ea4\u53c9\u71b5\u7684\u5f71\u54cd\u6709\u9650. \u6bcf\u4e2a\u6ce8\u610f\u529b\u5c42\u90fd\u6709\u4e00\u4e2a 3.5 \u79d2\u7684\u56e0\u679c\u611f\u53d7\u91ce, \u4e14\u6211\u4eec\u968f\u673a\u504f\u79fb\u4e86\u521d\u59cb\u4f4d\u7f6e\u7684\u6b63\u5f26\u4f4d\u7f6e\u5d4c\u5165, \u4ee5\u6a21\u62df\u5904\u4e8e\u66f4\u957f\u7684\u5e8f\u5217\u4e2d. \u6211\u4eec\u5728 5 \u79d2\u5e8f\u5217\u4e0a\u8bad\u7ec3\u6a21\u578b.</p> \u4ee3\u7801 <pre><code>class StreamingTransformerEncoderLayer(nn.TransformerEncoderLayer):\n    def forward(self, x: torch.Tensor, x_past: torch.Tensor, past_context: int):  # type: ignore\n        if self.norm_first:\n            sa_input = self.norm1(x)\n            x = x + self._sa_block(sa_input, x_past, past_context)\n\n\n            x = x + self._ff_block(self.norm2(x))\n        else:\n            sa_input = x\n            x = self.norm1(x + self._sa_block(sa_input, x_past, past_context))\n            x = self.norm2(x + self._ff_block(x))\n\n        return x, sa_input\n\n    # self-attention block\n    def _sa_block(self, x: torch.Tensor, x_past: torch.Tensor, past_context: int):  # type: ignore\n        _, T, _ = x.shape\n        _, H, _ = x_past.shape\n\n        queries = x\n        keys = torch.cat([x_past, x], dim=1)\n        values = keys\n\n        queries_pos = torch.arange(H, T + H, device=x.device).view(-1, 1)\n        keys_pos = torch.arange(T + H, device=x.device).view(1, -1)\n        delta = queries_pos - keys_pos\n        valid_access = (delta &gt;= 0) &amp; (delta &lt;= past_context)\n        x = self.self_attn(queries, keys, values,\n   attn_mask=~valid_access,\n   need_weights=False)[0]\n        return self.dropout1(x)\n\nclass StreamingTransformerEncoder(nn.Module):\n    def __init__(self, dim, hidden_scale: float = 4., num_heads: int = 8, num_layers: int = 5,\n                 max_period: float = 10000, past_context: int = 1000, gelu: bool = True,\n                 norm_in: bool = True, dropout: float = 0., **kwargs):\n        super().__init__()\n        assert dim % num_heads == 0\n        hidden_dim = int(dim * hidden_scale)\n\n        self.max_period = max_period\n        self.past_context = past_context\n        activation: tp.Any = F.gelu if gelu else F.relu\n\n        self.norm_in: nn.Module\n        if norm_in:\n            self.norm_in = nn.LayerNorm(dim)\n        else:\n            self.norm_in = nn.Identity()\n\n        self.layers = nn.ModuleList()\n        for idx in range(num_layers):\n            self.layers.append(\n                StreamingTransformerEncoderLayer(\n                    dim, num_heads, hidden_dim,\n                    activation=activation, batch_first=True, dropout=dropout, **kwargs))\n\n    def forward(self, x: torch.Tensor,\n                states: tp.Optional[tp.List[torch.Tensor]] = None,\n                offset: tp.Union[int, torch.Tensor] = 0):\n        B, T, C = x.shape\n        if states is None:\n            states = [torch.zeros_like(x[:, :1]) for _ in range(1 + len(self.layers))]\n\n        positions = torch.arange(T, device=x.device).view(1, -1, 1) + offset\n        pos_emb = create_sin_embedding(positions, C, max_period=self.max_period)\n\n        new_state: tp.List[torch.Tensor] = []\n        x = self.norm_in(x)\n        x = x + pos_emb\n\n        for layer_state, layer in zip(states, self.layers):\n            x, new_layer_state = layer(x, layer_state, self.past_context)\n            new_layer_state = torch.cat([layer_state, new_layer_state], dim=1)\n            new_state.append(new_layer_state[:, -self.past_context:, :])\n        return x, new_state, offset + T\n\n\nclass LMModel(nn.Module):\n    def __init__(\n      self, \n      n_q : int = 32,\n      card: int = 1024,\n      dim : int = 200,\n      **kwargs):\n        super().__init__()\n        self.card = card\n        self.n_q = n_q\n        self.dim = dim\n        self.transformer = m.StreamingTransformerEncoder(dim=dim, **kwargs)\n        self.emb = nn.ModuleList([nn.Embedding(card + 1, dim) for _ in range(n_q)])\n        self.linears = nn.ModuleList([nn.Linear(dim, card) for _ in range(n_q)])\n\n    def forward(\n      self, \n      indices: torch.Tensor,\n      states: tp.Optional[tp.List[torch.Tensor]] = None, \n      offset: int = 0):\n        B, K, T = indices.shape\n        input_ = sum([self.emb[k](indices[:, k]) for k in range(K)])\n        out, states, offset = self.transformer(input_, states, offset)\n        logits = torch.stack([self.linears[k](out) for k in range(K)], dim=1).permute(0, 3, 1, 2)\n        return torch.softmax(logits, dim=1), states, offset\n</code></pre> <p></p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#entropy-encoding","title":"Entropy Encoding\u00b7\u71b5\u7f16\u7801","text":"<p>We use a range based arithmetic coder (Pasco, 1976; Rissanen &amp; Langdon, 1981) in order to leverage the estimated probabilities given by the language model. As noted by Ball\u00e9 et al. (2018), evaluation of the same model might lead to different results on different architectures, or with different evaluation procedures due to floating point approximations. This can lead to decoding errors as the encoder and decoder will not use the exact same code. We observe in particular that the difference between batch evaluation (e.g. all time steps at once), and the real-life streaming evaluation that occurs in the decoder can lead to difference larger than 10\u22128. We first round the estimated probabilities with a precision of 10\u22126, although evaluations in more contexts would be needed for practical deployment. We use a total range width of 224, and assign a minimum range width of 2. We discuss the impact on the processing time in Section 4.6.</p> <p>\u6211\u4eec\u4f7f\u7528\u57fa\u4e8e\u8303\u56f4\u7684\u7b97\u672f\u7f16\u7801\u5668\u6765\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7ed9\u51fa\u7684\u4f30\u8ba1\u6982\u7387. \u6b63\u5982 Ball\u00e9 \u7b49\u4eba\u6240\u6307\u51fa, \u76f8\u540c\u6a21\u578b\u7684\u8bc4\u4f30\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u540c\u67b6\u6784\u6216\u4e0d\u540c\u8bc4\u4f30\u8fc7\u7a0b\u7684\u7ed3\u679c, \u56e0\u4e3a\u6d6e\u70b9\u8fd1\u4f3c\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7ed3\u679c. \u8fd9\u53ef\u80fd\u5bfc\u81f4\u89e3\u7801\u9519\u8bef, \u56e0\u4e3a\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5c06\u4e0d\u4f1a\u4f7f\u7528\u76f8\u540c\u7684\u4ee3\u7801. \u7279\u522b\u662f, \u6211\u4eec\u89c2\u5bdf\u5230\u6279\u5904\u7406\u8bc4\u4f30 (\u4f8b\u5982, \u4e00\u6b21\u5904\u7406\u6240\u6709\u65f6\u95f4\u6b65) \u4e0e\u5b9e\u9645\u6d41\u5f0f\u89e3\u7801\u8fc7\u7a0b\u4e2d\u53d1\u751f\u7684\u8bc4\u4f30\u4e4b\u95f4\u7684\u5dee\u5f02\u53ef\u4ee5\u5bfc\u81f4\u5dee\u5f02\u5927\u4e8e 10\u22128. \u6211\u4eec\u9996\u5148\u5c06\u4f30\u8ba1\u6982\u7387\u56db\u820d\u4e94\u5165\u5230 10\u22126 \u7684\u7cbe\u5ea6, \u867d\u7136\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u53ef\u80fd\u9700\u8981\u66f4\u591a\u7684\u8bc4\u4f30. \u6211\u4eec\u4f7f\u7528\u603b\u8303\u56f4\u5bbd\u5ea6\u4e3a 224, \u5e76\u8d4b\u4e88\u6700\u5c0f\u8303\u56f4\u5bbd\u5ea6\u4e3a 2. \u6211\u4eec\u5728 4.6 \u8282\u4e2d\u8ba8\u8bba\u5904\u7406\u65f6\u95f4\u7684\u5f71\u54cd.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#34training-objectives","title":"3.4.Training Objectives\u00b7\u8bad\u7ec3\u76ee\u6807","text":"<p>We detail the training objective that combines a reconstruction loss term, a perceptual loss term (via discriminators), and the RVQ commitment loss.</p> <p>\u6211\u4eec\u5c06\u8be6\u7ec6\u4ecb\u7ecd\u8bad\u7ec3\u76ee\u6807: \u91cd\u6784\u635f\u5931\u9879, \u611f\u77e5\u635f\u5931\u9879 (\u901a\u8fc7\u5224\u522b\u5668), \u548c RVQ \u63d0\u4ea4\u635f\u5931\u9879.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#reconstruction-loss","title":"Reconstruction Loss\u00b7\u91cd\u6784\u635f\u5931","text":"<p>The reconstruction loss term is comprised of a time and a frequency domain loss term. We minimize the L1 distance between the target and compressed audio over the time domain, i.e. $l_t(\\mathbf{x}, \\hat{\\mathbf{x}})=|\\mathbf{x}-\\hat{\\mathbf{x}}|_1$. For the frequency domain, we use a linear combination between the L1 and L2 losses over the mel-spectrogram using several time scales (Parallel WaveGAN (2019); Gritsenko et al., 2020). Formally, \u2018f(x, \u02c6x) =1 |\u03b1| \u00b7 |s|X \u03b1i\u2208\u03b1 X i\u2208e kSi(x) \u2212 Si(\u02c6x)k1+ \u03b1ikSi(x) \u2212 Si(\u02c6x)k2,(1) whereSiis a 64-bins mel-spectrogram using a normalized STFT with window size of 2iand hop length of 2i/4,e=5, .., 11is the set of scales, and\u03b1represents the set of scalar coefficients balancing between the L1 and L2 terms. Unlike Gritsenko et al. (2020), we take \u03b1i= 1.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#discriminative-loss","title":"Discriminative Loss\u00b7\u5224\u522b\u635f\u5931","text":"<p>To further improve the quality of the generated samples, we introduce a perceptual loss term based on a multi-scale STFT-based (MS-STFT) discriminator, illustrated in Fig.02. Multi scale discriminators are popular for capturing different structures in audio signals (MelGAN (2019); HiFi-GAN (2020); GAN Vocoder (2021)). The MS-STFT discriminator consists in identically structured networks operating on multi-scaled complex-valued STFT with the real and imaginary parts concatenated. Each sub-network is composed of a 2D convolutional layer (using kernel size 3 x 8 with 32 channels), followed by 2D convolutions with increasing dilation rates in the time dimension of 1, 2 and 4, and a stride of 2 over the frequency axis. A final 2D convolution with kernel size 3 x 3 and stride (1, 1) provide the final prediction. We use 5 different scales with STFT window lengths of [2048, 1024, 512, 256, 128]. For 48 kHz audio, we double the size of each STFT window and train the discriminator every two batches, and for stereophonic audio, we process separately the left and right channels. We use LeakyReLU as a no weight normalization (Salimans &amp; Kingma, 2016) to our discriminator network. The MS-STFT discriminator model architecture is visually depicted in Fig.02. The adversarial loss for the generator is constructed as follows,\u2018g(\u02c6x) =1 KPkmax(0,1\u2212Dk(\u02c6x))), whereKis the number of discriminators. Similarly to previous work on neural vocoders (MelGAN (2019); HiFi-GAN (2020); GAN Vocoder (2021)), we additionally include a relative feature matching loss for the generator. Formally, \u2018feat(x, \u02c6x) =1 KL K X k=1 L X l=1 kDlk(x) \u2212 Dlk(\u02c6x)k1 mean\ufffdkDlk(x)k1\ufffd ,(2) where themeanis computed over all dimensions, (Dk) are the discriminators, andLis the number of layers in discriminators. The discriminators are trained to minimize the following hinge-loss adversarial loss function: Ld(x, \u02c6x) =1 KPKk=1max(0,1\u2212 Dk(x)) +max(0,1 +Dk(\u02c6x)), whereKis the number of discriminators. Given that the discriminator tend to overpower easily the decoder, we update its weight with a probability of 2/3 at 24 kHz, and 0.5 at 48 kHz.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#multi-bandwidth-training","title":"Multi-Bandwidth Training\u00b7\u591a\u5e26\u5bbd\u8bad\u7ec3","text":"<p>At 24 kHz, we train the model to support the bandwidths 1.5, 3, 6, 12, and 24 kbps by selecting the appropriate number of codebooks to keep in the RVQ step, as explained in Section 3.2. At 48 kHz, we train to support 3, 6, 12 and 24 kbps. We also noticed that using a dedicated discriminator per-bandwidth is beneficial to the audio quality. Thus, we select a given bandwidth for the entire batch, and evaluate and update only the corresponding discriminator.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#vq-commitment-lossvq","title":"VQ Commitment Loss\u00b7VQ \u63d0\u4ea4\u635f\u5931","text":"<p>As mentioned in Section 3.2, we add a commitment losslwbetween the output of the encoder, and its quantized value, with no gradient being computed for the quantized value. For each residual stepc \u2208 {1, ..C}(withCdepeding on the bandwidth target for the current batch), notingzcthe current residual and qc(zc) the nearest entry in the corresponding codebook, we define lwas lw= C X c=1 kzc\u2212 qc(zc)k22.(3) Overall, the generator is trained to optimize the following loss, summed over the batch, LG= \u03bbt\u00b7 \u2018t(x, \u02c6x) + \u03bbf\u00b7 \u2018f(x, \u02c6x) + \u03bbg\u00b7 \u2018g(\u02c6x) + \u03bbfeat\u00b7 \u2018feat(x, \u02c6x) + \u03bbw\u00b7 \u2018w(w),(4) where \u03bbt, \u03bbf, \u03bbg, \u03bbfeat, and \u03bbwthe scalar coefficients to balance between the terms.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#balancer","title":"Balancer","text":"<p>We introduce a loss balancer in order to stabilize training, in particular the varying scale of the gradients coming from the discriminators. We also find that the balancer makes it easier to reason about the different loss weights, independently of their scale. Let us take a number of losses (\u2018i)ithat depends only on the output of the model\u02c6x. We definegi=\u2202\u2018i \u2202\u02c6x, andhkgik2i\u03b2the exponential moving average ofgiover the last training batches. Given a set of weights (\u03bbi) and a reference norm R, we define \u02dcgi= R\u03bbi Pj\u03bbj\u00b7 gi hkgik2i\u03b2.(5) We then backpropagate into the networkPi\u02dcgi, instead of the originalPi\u03bbigi. This changes the optimization problem but allows to make the\u03bbiinterpretable irrespectively of the natural scale of each loss. IfPi\u03bbi= 1, then each weight can be interpreted as the fraction of the model gradient that come from the corresponding loss. We takeR= 1 and\u03b2= 0.999. All the generator losses from Eq. (4) fit into the balancer, except for the commitment loss, as it is not defined with respect to the output of the model.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#4experiments-and-results","title":"4.Experiments and Results","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#41dataset","title":"4.1.Dataset","text":"<p>We train EnCodec on 24 kHz monophonic across diverse domains, namely: speech, noisy speech, music and general audio while we train the fullband stereo EnCodec on only 48 kHz music. For speech, we use the clean speech segments from DNS Challenge 4 (Dubey et al., 2022) and the Common Voice dataset (Ardila et al., 2019). For general audio, we use on AudioSet (Gemmeke et al., 2017) together with FSD50K (Fonseca et al., 2021). For music, we rely on the Jamendo dataset (Bogdanov et al., 2019) for training and evaluation and we further evaluate our models on music using a proprietary music dataset. Data splits are detailed in Appendix A.1.</p> <p>For training and validation, we define a mixing strategy which consists in either sampling a single source from a dataset or performing on the fly mixing of two or three sources. Specifically, we have four strategies: (s1) we sample a single source from Jamendo with probability 0.32; (s2) we sample a single source from the other datasets with the same probability; (s3) we mix two sources from all datasets with a probability of 0.24; (s4) we mix three sources from all datasets except music with a probability of 0.12.</p> <p>The audio is normalized by file and we apply a random gain between -10 and 6 dB. We reject any sample that has been clipped. Finally we add reverberation using room impulse responses provided by the DNS challenge with probability 0.2, and RT60 in the range [0.3, 1.3] except for the single-source music samples. For testing, we use four categories: clean speech from DNS alone, clean speech mixed with FSDK50K sample, Jamendo sample alone, proprietary music sample alone.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#42baselines","title":"4.2.Baselines","text":"<p>Opus (Valin et al., 2012) is a versatile speech and audio codec standardized by the IETF in 2012. It scales from 6 kbps narrowband monophonic audio to 510 kbps fullband stereophonic audio. EVS (Dietz et al., 2015) is a codec standardized in 2014 by 3GPP and developed for Voice over LTE (VoLTE). It supports a range of bitrates from 5.9 kbps to 128 kbps, and audio bandwidths from 4 kHz to 20 kHz. It is the successor of AMR-WB (Bhagat et al., 2012).We use both codecs to serve as traditional digital signal processing baselines. We also utilize MP3 compression at 64 kbps as an additional baseline for the stereophonic signal compression case. MP3 uses lossy data compression by approximating the accuracy of certain components of sound that are considered to be beyond hearing capabilities of most humans. Finally, we compare EnCodec to the SoundStream model from the official implementation available in Lyra 21at 3.2 kbps and 6 kbps on audio upsampled to 32 kHz. We also reproduced a version of SoundStream (2021) with minor improvements. Namely, we use the relative feature loss introduce in Section 3.4, and layer normalization (applied separately for each time step) in the discriminators, except for the first and last layer, which improved the audio quality during our preliminary studies. Results a reported in Tab.A.2 in the Appendix A.3.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#43evaluation-methods","title":"4.3.Evaluation Methods","text":"<p>We consider both subjective and objective evaluation metrics. For the subjective tests we follow the MUSHRA protocol (Series, 2014), using both a hidden reference and a low anchor. Annotators were recruited using a crowd-sourcing platform, in which they were asked to rate the perceptual quality of the provided samples in a range between 1 to 100. We randomly select 50 samples of 5 seconds from each category of the the test set and force at least 10 annotations per samples. To filter noisy annotations and outliers we remove annotators who rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording above 80 more than 50% of the time. For objective metrics, we use ViSQOL (Hines et al., 2012; Chinen et al., 2020)2, together with the Scale-Invariant Signal-to-Noise Ration (SI-SNR) (Luo &amp; Mesgarani, 2019; Nachmani et al., 2020; Chazan et al., 2021).</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#44training","title":"4.4.Training","text":"<p>We train all models for 300 epochs, with one epoch being 2,000 updates with the Adam optimizer with a batch size of 64 examples of 1 second each, a learning rate of 3\u00b710\u22124,\u03b21= 0.5, and\u03b22= 0.9. All the models are traind using 8 A100 GPUs. We use the balancer introduced in Section 3.4 with weights\u03bbt= 0.1,\u03bbf= 1, \u03bbg= 3, \u03bbfeat= 3 for the 24 kHz models. For the 48 kHz model, we use instead \u03bbg= 4, \u03bbfeat= 4.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#45results","title":"4.5.Results","text":"<p>We start with the results for EnCodec with a bandwidth in{1.5,3,6,12}kbps and compare them to the baselines. Results for the streamable setup are reported in Fig.03 and a breakdown per category in Tab.01. We additionally explored other quantizers such as Gumbel-Softmax and DiffQ (see details in Appendix A.2), however, we found in preliminary results that they provide similar or worse results, hence we do not report them. When considering the same bandwidth, EnCodec is superior to all evaluated baselines considering the MUSHRA score. Notice, EnCodec at 3kbps reaches better performance on average than Lyra-v2 using 6kbps and Opus at 12kbps. When considering the additional language model over the codes, we can reduce the bandwidth by\u223c25\u221240%. For instance, we can reduce the bandwidth of the 3 kpbs model to 1.9 kbps. We observe that for higher bandwidth, the compression ratio is lower, which could be explained by the small size of the Transformer model used, making hard to model all codebooks together.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#451ablation-study","title":"4.5.1.Ablation study","text":"<p>Next, we perform an ablation study to better evaluate the effect of the discriminator setup, streaming, multi target bandwidth, and balancer. We provide more detailed ablation studies in the Appendix, Section A.3.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#the-effect-of-discriminators-setup","title":"The effect of discriminators setup","text":"<p>Various discriminators were proposed in prior work to improve the perceptual quality of the generated audio. The Multi-Scale Discriminator (MSD) model proposed by MelGAN (2019) and adopted in (HiFi-GAN (2020); HiFi++ (2022); SoundStream (2021)), operates on the raw waveform at different resolutions. We adopt the same MSD configuration as described in SoundStream (2021). Kong et al. (2020) additionally propose the Multi-Period Discriminator (MPD) model, which reshapes the waveform to a 2D input with multiple periods. Next, the STFT Discriminator (Mono-STFTD) model was introduced in SoundStream (2021), where a single network operates over the complex-valued STFT. We evaluate our MS-STFTD discriminator against three other discriminator configurations: (i) MSD+Mono STFTD (as in SoundStream (2021)); (ii) MPD only; (iii) MS-STFTD only; (vi) MS-STFTD+MPD. Results are reported in Tab.02. Results suggest that using only a multi-scale STFT-based discriminator such as MS STFTD, is enough to generate high quality audio. Additionally, it simplifies the model training and reduces training time. Including the MPD discriminator, adds a small gain when considering the MUSHRA score.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#the-effect-of-the-streamable-modeling","title":"The effect of the streamable modeling","text":"<p>We also investigate streamable vs. non-streamable setups and report results in Tab.03. Unsurprisingly, we notice a small degradation switching from non-streamable to streamable but the performance remains strong while this setting enables streaming inference.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#the-effect-of-the-balancer","title":"The effect of the balancer","text":"<p>Lastly, we present results evaluating the impact of the balancer. We train the  EnCodec model considering various values\u03bbt,\u03bbf,\u03bbg, and\u03bbfeatwith and without the balancer. Results are reported in Tab.A.4 in the Appendix. As expected, results suggest the balancer significantly stabilizes the training process. See Appendix A.3 for more details.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#452stereo-evaluation","title":"4.5.2.Stereo Evaluation","text":"<p>All previously reported results considered only the monophonic setup.Although it makes sense when considering speech data, however for music data, stereo compression is highly important. We adjust our current setup to stereo by only modifying our discriminator setup as described in Section 3.4. Results for EnCodec working at 6 kbps, EnCodec with Residual Vector Quantization (RVQ) at 6 kbps, and Opus at 6 kbps, and MP3 at 64 kbps are reported in Tab.04. EnCodec is significantly outperforms Opus at 6kbps and is comparable to MP3 at 64kbps, while EnCodec at 12kpbs achieve comparable performance to EnCodec at 24kbps. Using a language model and entropy coding gives a variable gain between 20% to 30%.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#46latency-and-computation-time","title":"4.6.Latency and computation time","text":"<p>We report the initial latency and real time factor on Tab.05. The real-time factor is here defined as the ratio between the duration of the audio and the processing time, so that it is greater than one when the method is faster than real time. We profiled all models on a single thread of a MacBook Pro 2019 CPU at 6 kbps.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#initial-latency","title":"Initial latency","text":"<p>The 24 kHz streaming  EnCodec model has an initial latency (i.e., without the computation time) of 13.3 ms. The 48 kHz non-streaming version has an initial latency of 1 second, due to the normalizations used. Note that using entropy coding increases the initial latency, because the stream cannot be \u201cflushed\u201dwith each frame, in order to keep the overhead small. Thus decoding the frame at time t, requires for the frame t + 1 to be partially received, increasing the latency by 13ms.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#real-time-factor","title":"Real time factor","text":"<p>While our model is worse than Lyra v2 in term of processing speed, it processes the audio 10 times faster than real time, making it a good candidate for real life applications. The gain from the entropy coding comes at a cost, although the processing is still faster than real time and could be used for applications where latency is not essential (e.g. streaming). At 48 kHz, the increased number of step size lead to a slower than real time processing, although a more efficient implementation, or using accelerated hardware would improve the RTF. It could also be used for archiving where real time processing is not required.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#5conclusion","title":"5.Conclusion","text":"<p>We presented EnCodec: a state-of-the-art real-time neural audio compression model, producing high-fidelity audio samples across a range of sample rates and bandwidth. We showed subjective and objective results from 24kHz monophonic at 1.5 kbps (Fig.03) to 48kHz stereophonic (Tab.04). We improved sample quality by developing a simple but potent spectrogram-only adversarial loss which efficiently reduces artifacts and produce high-quality samples. Besides, we stabilized training and improved the interpretability of the weights for losses through a novel gradient balancer. Finally, we also demonstrated that a small Transformer model can be used to further reduce the bandwidth by up to 40% without further degradation of quality, in particular for applications where low latency is not essential (e.g. music streaming). </p>"},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#appendix","title":"Appendix","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#a1experimental-details","title":"A.1.Experimental Details","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#datasets-details","title":"Datasets Details","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#soundstream-model","title":"SoundStream Model","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#a2alternative-quantizers","title":"A.2.Alternative Quantizers","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#a21diffq-quantizer","title":"A.2.1.DiffQ Quantizer","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#pseudo-quantization-noise","title":"Pseudo Quantization Noise","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#test-time-quantization","title":"Test Time Quantization","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#sparsity","title":"Sparsity","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#a22gumbel-softmax-quantizer","title":"A.2.2.Gumbel-Softmax Quantizer","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#a3additional-results","title":"A.3.Additional Results","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#comparing-to-soundstream","title":"Comparing to SoundStream","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#the-effect-of-the-model-architecture","title":"The Effect of the Model Architecture","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#the-effect-of-the-balancer_1","title":"The Effect of the Balancer","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2022.10.24_EnCodec/#a4societal-impact","title":"A.4.Societal Impact","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2024.03.05_FACodec/","title":"FACodec","text":"<p>NaturalSpeech3</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/","title":"Single-Codec","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation - \u4f5c\u8005:  - 01 [Hanzhao Li](../../Authors/Hanzhao_Li_(\u674e\u51fd\u662d).md)  - 02 [Liumeng Xue](../../Authors/Liumeng_Xue_(\u859b\u6d4f\u8499).md)  - 03 [Haohan Guo](../../Authors/Haohan_Guo_(\u90ed\u6d69\u7ff0).md)  - 04 [Xinfa Zhu](../../Authors/Xinfa_Zhu_(\u6731\u65b0\u53d1).md)  - 05 [Yuanjun Lv](../../Authors/Yuanjun_Lv_(\u5415\u5143\u9a8f).md)  - 06 [Lei Xie](../../Authors/Lei_Xie_(\u8c22\u78ca).md)  - 07 [Yunlin Chen](../../Authors/Yunlin_Chen_(\u9648\u4e91\u7433).md)   - 08 [Hao Yin](../../Authors/Hao_Yin_(\u6bb7\u660a).md)  - 09 [Zhifei Li](../../Authors/Zhifei_Li_(\u674e\u5fd7\u98de).md) - \u673a\u6784:  - [NPU](../../Institutions/NPU_\u897f\u5317\u5de5\u4e1a\u5927\u5b66.md)  - [\u9999\u6e2f\u4e2d\u6587\u5927\u5b66 \u6df1\u5733](../../Institutions/CUHK_\u9999\u6e2f\u4e2d\u6587\u5927\u5b66.md)  - [\u9999\u6e2f\u4e2d\u6587\u5927\u5b66](../../Institutions/CUHK_\u9999\u6e2f\u4e2d\u6587\u5927\u5b66.md)  - [\u51fa\u95e8\u95ee\u95ee Inc](../../Institutions/Mobvoi_\u58a8\u767e\u610f.md) - \u65f6\u95f4:  - \u9884\u5370\u65f6\u95f4: 2024.06.11 ArXiv v1  - \u66f4\u65b0\u7b14\u8bb0: 2024.06.20 - \u53d1\u8868:  - [InterSpeech 2024](../../Publications/InterSpeech.md) - \u94fe\u63a5:  - [ArXiv](https://www.arxiv.org/abs/2406.07422)  - [DOI]()  - [Github]()  - [Demo](https://kkksuper.github.io/Single-Codec)  - [Scholar](https://scholar.google.com/scholar?cluster=6055535157486740073) - \u6807\u7b7e:  - [\u7f16\u89e3\u7801\u5668](../../Tags/Codec.md) - \u9875\u6570: 5 - \u5f15\u7528: 33 - \u88ab\u5f15: 0 - \u6570\u636e:  - ?  - \u5bf9\u6bd4:  - [VQ-VAE](../../Models/_Basis/2017.11.02_VQ-VAE.md)  - [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md)  - [TiCodec](../../Models/Speech_Neural_Codec/TiCodec.md) - \u590d\u73b0:  - ?"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#abstract","title":"Abstract: \u6458\u8981","text":"<p>The multi-codebook speech codec enables the application of large language models (LLM) in TTS but bottlenecks efficiency and robustness due to multi-sequence prediction. To avoid this obstacle, we propose Single-Codec, a single-codebook single-sequence codec, which employs a disentangled VQ-VAE to decouple speech into a time-invariant embedding and a phonetically-rich discrete sequence. Furthermore, the encoder is enhanced with (1) contextual modeling with a BLSTM module to exploit the temporal information, (2) a hybrid sampling module to alleviate distortion from upsampling and downsampling, and (3) a resampling module to encourage discrete units to carry more phonetic information. Compared with multi-codebook codecs, e.g., EnCodec and TiCodec, SingleCodec demonstrates higher reconstruction quality with a lower bandwidth of only 304bps. The effectiveness of Single-Code is further validated by LLM-TTS experiments, showing improved naturalness and intelligibility.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#1introduction","title":"1.Introduction: \u5f15\u8a00","text":"<p>Large language models (LLMs) have attracted wide attention in the speech domain, particularly in text-to-speech synthesis (TTS) (VALL-E; SPEAR-TTS). In such LLM-based TTS systems, to operate speech synthesis as a simple next-token prediction problem, the first thing is to seek an appropriate speech codec for speech tokenization and waveform reconstruction. Multi-codebook codecs (EnCodec), as the SOTA approaches, are widely adopted in LLM-based TTS to achieve superior reconstruction quality. However, they also require the LLM to predict multiple discrete sequences, affecting efficiency and stability seriously, although various designs of codec (SpeechTokenizer; Language-Codec; TiCodec) and LLM (SoundStorm; UniAudio; MAGNeT) are proposed to adapt this multi-sequence discrete representation better. Hence, seeking an effective approach to obtain the single-sequence discrete speech representation is critical to bypass this limitation.</p> <p>However, unlike the text, it is impossible to completely represent the speech audio with abundant information in semantics and acoustics with only one discrete token sequence. Although Tortoise-TTS achieves the LLM with the single-sequence discrete speech representation, A diffusion model still needs to be trained to generate Mel Spectrograms from latent embeddings in the LLM of predicted speech units. These embeddings contain more information related to the input text and the target speaker to compensate for the compression loss. However, this operation introduces more training and inference costs. Recently, TiCodec proposes introducing an additional global encoder to disentangle time-invariant information out of speech units, reducing the amount of frame-level information that needs encoding. It inspires us to re-think speech codec from the perspective of feature disentanglement.</p> <p>In this study, we propose a single-codebook neural audio codec, Single-Codec, for high-performance speech generation. Single-Codec performs compression and reconstruction on Mel Spectrogram instead of the raw waveform, enabling efficient compression of speech information while preserving important details, as stated in Tortoise-TTS. To further enhance the codec performance and applicability to speech synthesis, Single-Codec incorporates several key components:  - A global reference encoder to decouple time-invariant features. Specifically, we utilize continuous global representations rather than discrete representations and longer reference segments to capture more acoustic details, enabling embedding sufficient phonetic information into single-codebook discrete units. - A BLSTM module for contextual modeling to help discover the correlation between adjacent frames, enhancing speech content clustering efficiency. - A hybrid sampling module that uses both convolution and pooling to achieve downsampling, and transposed convolution and replication to achieve upsampling, alleviating upsampling and downsampling distortion. - A resampling module to encourage the encoder to extract more phonetics-relevant information with lower short-time variance from the acoustic sequence.</p> <p>To the best of our knowledge, Single-Codec is the first single-codebook codec dedicatedly designed for LLM-based speech generation. We comprehensively compare SingleCodec with SOTA multi-codebook codecs, including EnCodec and TiCodec, by conducting both objective and subjective tests in analysis-synthesis and TTS. The results show that Single-Codec with the lower bandwidth has better speech reconstruction quality, and significantly improves intelligibility, naturalness, and speaker similarity of synthesized speech in zero-shot LLM-TTS. Audio samples are available at https://kkksuper.github.io/Single-Codec.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#2related-works","title":"2.Related Works: \u76f8\u5173\u5de5\u4f5c","text":"<p>None</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#3methodology","title":"3.Methodology: \u65b9\u6cd5","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#31architecture-of-single-codec","title":"3.1.Architecture of Single-Codec: \u5355\u7801\u672c\u7f16\u89e3\u7801\u5668\u67b6\u6784","text":"<p>The architecture of Single-Codec is shown in Figure.01. It is built on Vector Quantized-Variational AutoEncoder (VQ-VAE) with Mel Spectrogram input and reconstruction, similar to Tortoise-TTS. We adopt a Conformer-based encoder to encode a Mel Spectrogram segment seg2 into a latent content representation c, which is then passed to the Vector Quantizer (VQ) for vector quantization. The convolution-based decoder reconstructs the Mel Spectrogram \u02dcseg2 from the quantized content representation c. Additionally, we apply a discriminator to improve generation quality (VQGAN). Finally, we use a neural vocoder BigVGAN to reconstruct waveform from codec output, i.e., Mel Spectrogram.</p> <p>To achieve a high-quality single-codebook codec, we improve the codec architecture with four modules. Specifically, we add a reference encoder to decouple time-invariant information in speech from a Mel Spectrogram segment seg1, yielding a global representation g. A hybrid sampling module is adopted to alleviate sampling loss. Moreover, we introduce a BLSTM module and resampling module in both codec encoder and decoder to enhance contextual information and phonetics-relevant information, respectively.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#32reference-encoder","title":"3.2.Reference Encoder: \u53c2\u8003\u7f16\u7801\u5668","text":"<p>Speech contains multiple aspects of information, such as time-variant content, time-invariant timbre, and acoustic environment. Multiple codebook in codec makes it easy to encode these various information. However, for a single-codebook codec, it is challenging to compress all information into a limited number of discrete units. To solve this problem, we decouple global information (such as timbre and acoustic environment) that is almost invariable in all frames of the utterance and discretize speech content into code.</p> <p>We introduce a reference encoder to derive global representation g that is mainly related to timbre and acoustic environment. The input of the reference encoder is a segment seg1 randomly selected from the input utterance. We set the length of the segment seg1 for reference input to 600 frames while the input segment seg2 for codec encoder to 200 frames, where the short segment seg2 can reduce the amount of calculation and memory overhead, while the longer reference segment seg1 can help to obtain more robust global features. The output g of the reference encoder is fed to the codec encoder and decoder after passing through different linear layers, where it subtracts with output of the encoder blocks and adds to the input of the decoder blocks.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#33blstm-module-blstm","title":"3.3.BLSTM Module: BLSTM \u6a21\u5757","text":"<p>Codecs are generally trained on large-scale speech data to ensure good generalization. The diversity of speech content creates challenges for single-codebook codecs with appropriate sizes. Unlike EnCodec, which introduces the sequence modelling with LSTM and finds that it can improve the Scale-Invariant Signal-to-Noise Ration (SI-SNR), we add BLSTM modules before and after the quantizer to enhance contextual information. We found this can improve the efficiency of speech content modelling and make it easier to form stable clustering centers.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#34hybrid-sampling-module","title":"3.4.Hybrid Sampling Module: \u6df7\u5408\u91c7\u6837\u6a21\u5757","text":"<p>Neural codecs usually employ a sampling module to reduce the sequence length of the discrete representation. Currently, the up-sampling and down-sampling operations in codecs are usually implemented by convolution, transposed convolution, or pooling and repeat. The sampling process inevitably produces sampling loss, resulting in reduced encoding and decoding capabilities. Inspired by MR-HuBERT, we introduce an improved hybrid sampling module that uses both convolution and pooling to achieve downsampling and transposed convolution and replication to achieve upsampling. The combination of different sampling methods can alleviate sampling distortion.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#35resampling-module","title":"3.5.Resampling Module: \u91cd\u91c7\u6837\u6a21\u5757","text":"<p>The main goal of a single-codebook speech codec is to extract short-term invariant speech units from acoustic representations. The diversity of acoustic representations brings challenges to the learning of codebook vectors. To solve this problem, we propose a novel resampling module, which first downsamples the input feature for local modelling and then residual connect after upsampling. This bottlenecking operation along the time axis encourages the encoder to extract more phonetics-relevant information with lower short-time variance from the acoustic sequence.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#4experiments","title":"4.Experiments: \u5b9e\u9a8c","text":""},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#41dataset","title":"4.1.Dataset: \u6570\u636e\u96c6","text":"<p>We train speech codecs and a zero-shot TTS system, VALL-E, using five open-source datasets, including LibriTTS, Hi-Fi TTS, VCTK, AISHELL-1, and AISHELL3.  A total of 1165.3 hours of English and Chinese speech is used.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#42comparison-models","title":"4.2.Comparison Models: \u5bf9\u6bd4\u6a21\u578b","text":"<p>We adopt EnCodec with one codebook (EnCodec-1VQ) and TiCodec with one codebook (TiCodec-1VQ) and two codebooks (TiCodec-2VQ) as the baselines to compare with our proposed Single-Codec. For VALL-E, we use EnCodec with one, four, and eight codebooks, representing EnCodec-1VQ, EnCodec-4VQ, and EnCodec-8VQ, TiCodec with one codebook (TiCodec-1VQ) as the baselines to evaluate the performance of codecs on speech synthesis.</p> <p>To verify the effectiveness of our designed modules in Single-Codec, we conduct ablation studies on the following models. </p> <ul> <li>VQ-VAE: A basic VQ-VAE codec with a discriminator for perceptual loss, the structure and configuration of the VQ-VAE codec is similar to that in Tortoise-TTS.</li> <li>Ref-short: VQ-VAE with a reference encoder that consumes a short segment with 200 frames as input.</li> <li>Ref-long: VQ-VAE with a reference encoder that consumes a long segment with 600 frames as input.</li> <li>Ref-BLSTM: Ref-long with the BLSTM module to verify the effectiveness of the BLSTM module.</li> <li>Ref-HybSam: Ref-long with the hybrid sampling module to verify the effectiveness of the hybrid sampling module.</li> <li>Ref-BLSTM-HybSam: Ref-long with the BLSTM and hybrid sampling modules to verify the effectiveness of the combination of BLSTM and hybrid sampling modules.</li> <li>Ref-BLSTM-HybSam-Conf: Ref-BLSTM-HybSam with the Conformer-based encoder, excluding the resampling module.</li> </ul>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#43model-parameters-and-training-details","title":"4.3.Model Parameters and Training Details: \u6a21\u578b\u53c2\u6570\u548c\u8bad\u7ec3\u7ec6\u8282","text":"<p>The audio sample rate is 24khz, and the hop length and window length of the Mel Spectrogram are 256 and 1024, respectively. The downsample rate is 4, resulting in a total downsampling of 1024 times (about 23 discrete tokens per second). The codebook size is 8192. The model size of the codec is 256. The sizes of the intermediate hidden states in convolution blocks are 256, 512, and 1024, while the hidden size of the Conformer block is 1024. The reference encoder consists of 6 layers of 2D convolution with a kernel size of 3 and a GRU layer. The residual block consists of two residual units. Each residual unit includes 2 one-dimensional convolutions with kernel sizes of 3 and 1 respectively. Discriminator consists of 4 layers of 2D convolution with a kernel size of 5 and 2 layers of 2D convolution with a kernel size of 3. The BLSTM module contains two LSTM layers with a hidden size of 128.</p> <p>During training, we conduct 300k iterations using a batch size of 1024 on a single V100 GPU for Single-Codec. The baseline model EnCodec utilizes with the code reimplemented in HiFi-Codec, and is trained for 25 epochs. TiCodec is trained for 300k steps with a batch size of 40 on two V100 GPUs. For TTS, we employ VALL-E, reimplemented in Amphion, with dynamic batch sizing and a maximum token limit of 4000 per batch. The single-codebook codec only utilizes the AR stage, while the multiple-codebook codec trains both the AR and NAR stages simultaneously. Eight A800 GPUs and 70 epochs are employed for training VALL-E.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#44ablation-studies","title":"4.4.Ablation Studies: \u6d88\u878d\u7814\u7a76","text":"<p>We calculate STOI, PESQ, Mel cepstral distortion (MCD), UTMOS and speaker cosine similarity (SPK) to objectively evaluate the quality of speech reconstruction. The test set is composed of 100 randomly selected sentences from unseen speakers. The objective result is shown in Table 1.</p> <p>Compared with VQ-VAE, either Ref-short or Ref-long obtains better performance on all metrics. It indicates that it is effective to decouple global information from speech for the single-codebook codec. Moreover, Ref-long outperforms Ref-short in both reconstruction and speaker similarity, suggesting that longer reference segments help capture more accurate time-invariant information and enhance content modelling. Ref-BLSTM, Ref-HybSam, and Ref-BLSTM-HybSam get higher reconstruction quality, showing the effectiveness of the BLSTM and hybrid sampling modules. Moreover, RefBLSTM-HybSam-Con yields on-pair performance with Ref-BLSTM-HybSam but gets further improvement after adding the resampling module, i.e., our proposed Single-Code, achieving the best results.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#45commitment-loss-analysis","title":"4.5.Commitment Loss Analysis: \u627f\u8bfa\u635f\u5931\u5206\u6790","text":"<p>We further analyze the commitment loss in the training to explore the impact of different designed modules on the single-codebook codec. Commitment loss is the difference between representations before and after quantization. The degree of convergence of the commitment loss can reflect the relationship between the encoder output and the cluster center in the codebook. As shown in Figure.02, the commitment loss of VQ-VAE tends to diverge after model training, indicating that the entanglement of time-invariant global information and time-variant content information hinders forming a limited variety of content-related speech units. After considering time-invariant decoupled modelling, the loss of Ref-short increases slowly, indicating the effectiveness of global information disentanglement for speech unit learning. Ref-long further verifies this result, illustrating the effectiveness of a longer reference segment. The loss curve of Ref-HybSam is flat, indicating that the hybrid sampling module effectively improves codec performance. Moreover, the losses of the models with context modelling via the BLSTM module are all converged. It demonstrates that the models have learned stable phonetic units before quantization, indicating the effectiveness of context modelling in codecs.</p> <p>Furthermore, considering the results presented in Table 1, we observe that the commitment loss is not strictly inversely related to reconstruction quality. However, the convergence status of the commitment loss (divergence, flat, convergence) is indeed associated with reconstruction quality. Specifically, the converged codec surpasses the codec which is not converged. This result further highlights the significance of achieving a stable clustering center in the single codebook codec, which directly impacts the overall reconstruction quality.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#46speech-reconstruction-evaluation","title":"4.6.Speech reconstruction Evaluation: \u8bed\u97f3\u91cd\u5efa\u8bc4\u4f30","text":"<p>We compare the performance in speech reconstruction of the proposed Single-Codec with other codecs. The results, as presented in Table 1, demonstrate that despite lower bandwidth, the proposed Single-Codec surpasses other codecs with 1 codebook and is on par with the TiCodec with 2 codebooks in terms of reconstruction quality and speaker similarity. VQ-VAE performs better than EnCodec with 1 codebook, demonstrating the high quantization efficiency of codecs operated on Mel Spectrogram. Compared to TiCodec, which also quantized the decoupled time-invariant information, Single-Codec achieves higher speaker similarity and reconstruction quality, indicating the effectiveness of continuous time-invariant representations and longer reference length.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#47zero-shot-tts-evaluation","title":"4.7.Zero-Shot TTS Evaluation: \u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u8bc4\u4f30","text":"<p>To evaluate the performance of codecs applied in speech synthesis tasks, we train VALL-E using discrete tokens extracted from EnCodec in the number of 1,4,8 codebooks, TiCodec in 1 codebook, and Single-Codec. We conduct naturalness Mean Opinion Score (N-MOS) and speaker similarity MOS (S-MOS) for subjective evaluation of the synthesized speech. The test set consists of 30 sentences, including Chinese and English speech. The 20 listeners who are Chinese Mandarin native speakers and familiar to English are invited to participate in each MOS test. Meanwhile, we calculate the word error rate (WER) using an ASR model (Whisper) to measure speech intelligibility. We also use WeSpeaker to extract speaker embedding to calculate the speaker embedding cosine similarity.</p> <p>Table 2 shows the subjective and objective results. SingleCodec outperforms other models in terms of naturalness and speaker similarity. In single-codebook scenes, TiCodec-1VQ and Single-Codec are significantly better than other codec models in speaker similarity, naturalness, and stability. This is because decoupling global information makes the frame-level codebook pay more attention to content modelling and enables more global information transmission. Meanwhile, SingleCodec performs better than TiCodec, indicating the effectiveness of continuous global representation and additional content modelling. In addition, Single-Codec exceeds multiple-codebook codecs regarding speaker similarity and naturalness while WER is slightly higher than EnCodec-8VQ. This is mainly because the higher bandwidth brings higher-resolution speech unit perception.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/2024.06.11_Single-Codec/#5conclusions","title":"5.Conclusions: \u7ed3\u8bba","text":"<p>In this paper, we propose Single-Codec, the first single-codebook codec dedicatedly designed for LLM-based speech generation. Single-Codec employs a disentangled VQ-VAE on Mel Spectrograms to decouple speech into time-invariant global embedding and one phonetically-rich discrete sequence quantized by one codebook. Furthermore, the encoder is enhanced with a BLSTM module for contextual modelling, a hybrid sampling module to alleviate distortion from upsampling and downsampling, and a resampling module to encourage discrete units to carry more phonetic-relevant information with lower short-time variance. In experiments, compared with multi-codebook codecs, e.g. EnCodec and TiCodec, Single-Codec demonstrates higher speech reconstruction quality with lower bandwidth of only 304bps, and enables a higher-quality LLM-TTS with better naturalness and intelligibility. In the future, we will focus on developing a more efficient single-codebook codec for speech reconstruction and speech synthesis.</p>"},{"location":"TTS/Models/Speech_Neural_Codec/_ToC/","title":"\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u6982\u8ff0","text":""},{"location":"TTS/Models/Speech_Neural_Codec/_ToC/#20210707-soundstream-google-research","title":"2021.07.07 SoundStream - Google Research","text":"<p>SoundStream \u67b6\u6784\u56fe: - \u5377\u79ef\u7f16\u7801\u5668\u4e3a\u8f93\u5165\u97f3\u9891\u6837\u672c\u4ea7\u751f\u9690\u8868\u793a - \u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u5668\u4f7f\u7528\u53ef\u53d8\u7684 $n_q$ \u5c06\u9690\u8868\u793a\u91cf\u5316. - \u8bad\u7ec3\u65f6, \u6a21\u578b\u53c2\u6570\u4f7f\u7528\u91cd\u6784\u635f\u5931+\u5bf9\u6297\u635f\u5931\u8fdb\u884c\u4f18\u5316. - \u53ef\u9009\u7684\u6761\u4ef6\u8f93\u5165\u53ef\u7528\u4e8e\u8868\u660e\u4f55\u65f6\u80cc\u666f\u566a\u58f0\u9700\u8981\u88ab\u53bb\u9664. - \u90e8\u7f72\u6a21\u578b\u65f6, \u53d1\u9001\u7aef\u7684\u7f16\u7801\u5668\u548c\u91cf\u5316\u5668\u5c06\u538b\u7f29\u6bd4\u7279\u6d41\u53d1\u9001\u5230\u63a5\u6536\u7aef, \u7136\u540e\u89e3\u7801\u97f3\u9891\u4fe1\u53f7.</p> <p></p>"},{"location":"TTS/Models/Speech_Neural_Codec/_ToC/#20221024-encodec-meta-ai-fair","title":"2022.10.24 EnCodec - Meta AI FAIR","text":""},{"location":"TTS/Models/Speech_Neural_Codec/_ToC/#20230504-hifi-codec-tecent-ai-lab","title":"2023.05.04 HiFi-Codec - Tecent AI Lab","text":""},{"location":"TTS/Models/Speech_Neural_Codec/_ToC/#20230831-speechtokenizer-","title":"2023.08.31 SpeechTokenizer - \u590d\u65e6\u5927\u5b66","text":""},{"location":"TTS/Models/Speech_Neural_Codec/_ToC/#20230914-funcodec-","title":"2023.09.14 FunCodec - \u963f\u91cc\u5df4\u5df4\u8fbe\u6469\u9662\u8bed\u97f3\u5b9e\u9a8c\u5ba4","text":""},{"location":"TTS/Models/Speech_Neural_Codec/_ToC/#20240219-language-codec-","title":"2024.02.19 Language-Codec - \u6d59\u6c5f\u5927\u5b66+\u963f\u91cc\u5df4\u5df4\u8fbe\u6469\u8bed\u97f3\u5b9e\u9a8c\u5ba4","text":""},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/","title":"Tacotron","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: Tacotron: Towards End-to-End Speech Synthesis - \u4f5c\u8005:  - [Yuxuan Wang](../../Authors/Yuxuan_Wang_(\u738b\u96e8\u8f69).md)   - [RJ Skerry-Ryan](../../Authors/RJ_Skerry-Ryan.md)   - [Daisy Stanton](../../Authors/Daisy_Stanton.md)   - [Yonghui Wu](../../Authors/Yonghui_Wu.md)   - [Ron J. Weiss](../../Authors/Ron_J._Weiss.md)   - [Navdeep Jaitly](../../Authors/Navdeep_Jaitly.md)   - [Zongheng Yang](../../Authors/Zongheng_Yang.md)   - [Ying Xiao](../../Authors/Ying_Xiao.md)   - [Zhifeng Chen](../../Authors/Zhifeng_Chen.md)   - [Samy Bengio](../../Authors/Samy_Bengio.md)   - [Quoc Le](../../Authors/Quoc_Le.md)   - [Yannis Agiomyrgiannakis](../../Authors/Yannis_Agiomyrgiannakis.md)   - [Rob Clark](../../Authors/Rob_Clark.md)   - [Rif A. Saurous](../../Authors/Rif_A._Saurous.md)  - \u673a\u6784:  - [Google](../../Institutions/Google.md)  - \u65f6\u95f4:  - \u9884\u5370\u65f6\u95f4: 2017.03.29 ArXiv v1  - \u9884\u5370\u65f6\u95f4: 2017.04.06 ArXiv v2  - \u53d1\u8868\u65f6\u95f4: 2017.08.20  - \u66f4\u65b0\u7b14\u8bb0: 2024.06.16 - \u53d1\u8868:  - [InterSpeech 2017](../../Publications/InterSpeech.md)  - \u94fe\u63a5:  - [ArXiv](https://arxiv.org/abs/1703.10135)     - [Demo](https://google.github.io/tacotron)   - [Scholar](https://scholar.google.com/scholar?cluster=11291739830353377265)  - \u6807\u7b7e:  - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [Seq2Seq\u6a21\u578b](../../Tags/Model_Seq2Seq.md)   - [Tacotron\u6a21\u578b](../../Tags/Model_Tacotron.md)  - \u9875\u6570: 5 - \u5f15\u7528: 26 - \u88ab\u5f15: 2037"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#abstract","title":"Abstract: \u6458\u8981","text":"\u539f\u6587  &gt; A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. &gt; Building these components often requires extensive domain expertise and may contain brittle design choices. &gt; In this paper, we present ***Tacotron***, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. &gt; Given `` pairs, the model can be trained completely from scratch with random initialization. &gt; We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. &gt; ***Tacotron*** achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. &gt; In addition, since ***Tacotron*** generates speech at the frame level, it\u2019s substantially faster than sample-level autoregressive methods.    <p>\u4e00\u4e2a\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\u7684\u7cfb\u7edf\u901a\u5e38\u5305\u542b\u591a\u4e2a\u9636\u6bb5, \u4f8b\u5982\u6587\u672c\u5206\u6790\u524d\u7aef, \u58f0\u5b66\u6a21\u578b\u548c\u97f3\u9891\u5408\u6210\u6a21\u5757.</p> <p>\u6784\u5efa\u8fd9\u4e9b\u7ec4\u4ef6\u901a\u5e38\u9700\u8981\u5e7f\u6cdb\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6, \u5e76\u4e14\u53ef\u80fd\u5305\u542b\u8106\u5f31\u7684\u8bbe\u8ba1\u9009\u62e9.</p> <p>\u5728\u672c\u6587\u4e2d, \u6211\u4eec\u4ecb\u7ecd\u4e86 Tacotron, \u8fd9\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u751f\u6210\u5f0f\u6587\u672c\u5230\u8bed\u97f3\u6a21\u578b, \u80fd\u591f\u76f4\u63a5\u4ece\u5b57\u7b26\u5408\u6210\u8bed\u97f3.</p> <p>\u7ed9\u5b9a <code>&lt;\u6587\u672c, \u97f3\u9891&gt;</code> \u5bf9, \u8be5\u6a21\u578b\u53ef\u4ee5\u4ece\u968f\u673a\u521d\u59cb\u5316\u5f00\u59cb\u5b8c\u5168\u4ece\u5934\u8bad\u7ec3.</p> <p>\u6211\u4eec\u63d0\u51fa\u4e86\u51e0\u79cd\u5173\u952e\u6280\u672f, \u4f7f\u5f97\u5e8f\u5217\u5230\u5e8f\u5217\u6846\u67b6\u80fd\u591f\u5728\u8fd9\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272.</p> <p>Tacotron \u5728\u7f8e\u56fd\u82f1\u8bed\u4e0a\u83b7\u5f97\u4e86 3.82 \u7684\u4e3b\u89c2 5 \u5206\u5236\u5e73\u5747\u610f\u89c1\u5f97\u5206, \u5728\u81ea\u7136\u5ea6\u65b9\u9762\u8d85\u8fc7\u4e86\u751f\u4ea7\u7ea7\u7684\u53c2\u6570\u5316\u7cfb\u7edf.</p> <p>\u6b64\u5916, \u7531\u4e8e Tacotron \u5728\u5e27\u7ea7\u522b\u751f\u6210\u8bed\u97f3, \u5b83\u6bd4\u6837\u672c\u7ea7\u522b\u7684\u81ea\u56de\u5f52\u65b9\u6cd5\u8981\u5feb\u5f97\u591a.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#1introduction","title":"1.Introduction","text":"\u539f\u6587  &gt; Modern text-to-speech (TTS) pipelines are complex ([\"Text-to-Speech Synthesis (2009) \"](../../Books/2009_Text-to-Speech_Synthesis.md)) . &gt; For example, it is common for statistical parametric TTS to have a text frontend extracting various linguistic features, a duration model, an acoustic feature prediction model and a complex signal-processing-based vocoder ([\"Statistical Parametric Speech Synthesis (2009) \"](), [\"Vocaine the Vocoder and Applications in Speech Synthesis (2015) \"]()) . &gt; These components are based on extensive domain expertise and are laborious to design. &gt; They are also trained independently, so errors from each component may compound. &gt; The complexity of modern TTS designs thus leads to substantial engineering efforts when building a new system.    <p>\u73b0\u4ee3\u6587\u672c\u5230\u8bed\u97f3 (TTS) \u7684\u6d41\u7a0b\u662f\u590d\u6742\u7684 (\u201cText-to-Speech Synthesis (2009) \u201d) . \u4f8b\u5982, \u7edf\u8ba1\u53c2\u6570\u5316TTS\u901a\u5e38\u5305\u62ec\u4e00\u4e2a\u63d0\u53d6\u5404\u79cd\u8bed\u8a00\u7279\u5f81\u7684\u6587\u672c\u524d\u7aef, \u4e00\u4e2a\u65f6\u957f\u6a21\u578b, \u4e00\u4e2a\u58f0\u5b66\u7279\u5f81\u9884\u6d4b\u6a21\u578b\u548c\u4e00\u4e2a\u57fa\u4e8e\u590d\u6742\u4fe1\u53f7\u5904\u7406\u7684\u58f0\u7801\u5668 (\u201cStatistical Parametric Speech Synthesis (2009) \u201d, \u201cVocaine the Vocoder and Applications in Speech Synthesis (2015) \u201d) . \u8fd9\u4e9b\u7ec4\u4ef6\u57fa\u4e8e\u5e7f\u6cdb\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6, \u8bbe\u8ba1\u8d77\u6765\u8d39\u65f6\u8d39\u529b. \u5b83\u4eec\u4e5f\u662f\u72ec\u7acb\u8bad\u7ec3\u7684, \u56e0\u6b64\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u9519\u8bef\u53ef\u80fd\u4f1a\u7d2f\u79ef. \u56e0\u6b64, \u73b0\u4ee3TTS\u8bbe\u8ba1\u7684\u590d\u6742\u6027\u5728\u6784\u5efa\u65b0\u7cfb\u7edf\u65f6\u5bfc\u81f4\u4e86\u5927\u91cf\u7684\u5de5\u7a0b\u52aa\u529b.</p> \u539f\u6587  &gt; There are thus many advantages of an integrated end-to-end TTS system that can be trained on `` pairs with minimal human annotation. &gt; First, such a system alleviates the need for laborious feature engineering, which may involve heuristics and brittle design choices. &gt; Second, it more easily allows for rich conditioning on various attributes, such as speaker or language, or high-level features like sentiment. &gt; This is because conditioning can occur at the very beginning of the model rather than only on certain components. &gt; Similarly, adaptation to new data might also be easier. &gt; Finally, a single model is likely to be more robust than a multi-stage model where each component\u2019s errors can compound. &gt; These advantages imply that an end-to-end model could allow us to train on huge amounts of rich, expressive yet often noisy data found in the real world.    <p></p> <p>\u56e0\u6b64, \u4e00\u4e2a\u80fd\u591f\u57fa\u4e8e\u6700\u5c0f\u4eba\u7c7b\u6807\u6ce8\u7684<code>&lt;\u6587\u672c, \u97f3\u9891&gt;</code>\u5bf9\u8fdb\u884c\u8bad\u7ec3\u7684\u96c6\u6210\u7aef\u5230\u7aefTTS\u7cfb\u7edf\u5177\u6709\u8bb8\u591a\u4f18\u52bf. \u9996\u5148, \u8fd9\u6837\u7684\u7cfb\u7edf\u51cf\u8f7b\u4e86\u5bf9\u8d39\u529b\u7684\u7279\u5f81\u5de5\u7a0b\u7684\u9700\u6c42, \u8fd9\u53ef\u80fd\u6d89\u53ca\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u8106\u5f31\u7684\u8bbe\u8ba1\u9009\u62e9. \u5176\u6b21, \u5b83\u66f4\u5bb9\u6613\u5141\u8bb8\u5bf9\u5404\u79cd\u5c5e\u6027 (\u5982\u8bf4\u8bdd\u8005\u6216\u8bed\u8a00) \u6216\u9ad8\u7ea7\u7279\u5f81 (\u5982\u60c5\u611f) \u8fdb\u884c\u4e30\u5bcc\u7684\u6761\u4ef6\u5316. \u8fd9\u662f\u56e0\u4e3a\u6761\u4ef6\u5316\u53ef\u4ee5\u5728\u6a21\u578b\u7684\u6700\u5f00\u59cb\u53d1\u751f, \u800c\u4e0d\u4ec5\u4ec5\u662f\u5728\u67d0\u4e9b\u7ec4\u4ef6\u4e0a. \u540c\u6837, \u9002\u5e94\u65b0\u6570\u636e\u4e5f\u53ef\u80fd\u66f4\u5bb9\u6613. \u6700\u540e, \u5355\u4e2a\u6a21\u578b\u53ef\u80fd\u6bd4\u591a\u9636\u6bb5\u6a21\u578b\u66f4\u5065\u58ee, \u5176\u4e2d\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u9519\u8bef\u53ef\u80fd\u4f1a\u7d2f\u79ef. \u8fd9\u4e9b\u4f18\u52bf\u610f\u5473\u7740\u7aef\u5230\u7aef\u6a21\u578b\u53ef\u4ee5\u8ba9\u6211\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u627e\u5230\u7684\u5927\u91cf\u4e30\u5bcc, \u8868\u73b0\u529b\u5f3a\u4f46\u5f80\u5f80\u5608\u6742\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3.</p> \u539f\u6587  &gt; TTS is a large-scale inverse problem: a highly compressed source (text) is \u201cdecompressed\u201d into audio. &gt; Since the same text can correspond to different pronunciations or speaking styles, this is a particularly difficult learning task for an end-to-end model: it must cope with large variations at the signal level for a given input. &gt; Moreover, unlike end-to-end speech recognition [LAS](../../Models/ASR/LAS.md) or machine translation [GNMT](../../Models/NMT/2016.09.26_GNMT.md), TTS outputs are continuous, and output sequences are usually much longer than those of the input. &gt; These attributes cause prediction errors to accumulate quickly. &gt; In this paper, we propose ***Tacotron***, an end-to-end generative TTS model based on the [sequence-to-sequence (seq2seq) ](../../Models/_Basis/Seq2Seq.md) with [attention paradigm](../../Models/NMT/2014.09.01_NMT_by_Jointly_Learning_to_Align_&amp;_Translate.md) . &gt; Our model takes characters as input and outputs raw spectrogram, using several techniques to improve the capability of a vanilla seq2seq model. &gt; Given `` pairs, ***Tacotron*** can be trained completely from scratch with random initialization. &gt; It does not require phoneme-level alignment, so it can easily scale to using large amounts of acoustic data with transcripts. &gt; With a simple waveform synthesis technique, ***Tacotron*** produces a 3.82 mean opinion score (MOS) on an US English eval set, outperforming a production parametric system in terms of naturalness.   <p></p> <p>TTS\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u9006\u95ee\u9898\uff1a\u4e00\u4e2a\u9ad8\u5ea6\u538b\u7f29\u7684\u6e90 (\u6587\u672c) \u88ab\u201c\u89e3\u538b\u7f29\u201d\u6210\u97f3\u9891. \u7531\u4e8e\u76f8\u540c\u7684\u6587\u672c\u53ef\u4ee5\u5bf9\u5e94\u4e0d\u540c\u7684\u53d1\u97f3\u6216\u8bf4\u8bdd\u98ce\u683c, \u8fd9\u5bf9\u7aef\u5230\u7aef\u6a21\u578b\u6765\u8bf4\u662f\u4e00\u4e2a\u7279\u522b\u56f0\u96be\u7684\u5b66\u4e60\u4efb\u52a1\uff1a\u5b83\u5fc5\u987b\u4e3a\u7ed9\u5b9a\u7684\u8f93\u5165\u5904\u7406\u4fe1\u53f7\u7ea7\u522b\u7684\u5927\u91cf\u53d8\u5316. \u6b64\u5916, \u4e0e\u7aef\u5230\u7aef\u8bed\u97f3\u8bc6\u522b LAS \u6216\u673a\u5668\u7ffb\u8bd1 GNMT \u4e0d\u540c, TTS\u8f93\u51fa\u662f\u8fde\u7eed\u7684, \u8f93\u51fa\u5e8f\u5217\u901a\u5e38\u6bd4\u8f93\u5165\u5e8f\u5217\u957f\u5f97\u591a. \u8fd9\u4e9b\u5c5e\u6027\u5bfc\u81f4\u9884\u6d4b\u9519\u8bef\u8fc5\u901f\u7d2f\u79ef. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86Tacotron, \u4e00\u4e2a\u57fa\u4e8e \u5e8f\u5217\u5230\u5e8f\u5217 (seq2seq)  \u548c \u6ce8\u610f\u529b\u8303\u5f0f \u7684\u7aef\u5230\u7aef\u751f\u6210TTS\u6a21\u578b. \u6211\u4eec\u7684\u6a21\u578b\u4ee5\u5b57\u7b26\u4f5c\u4e3a\u8f93\u5165, \u8f93\u51fa\u539f\u59cb\u9891\u8c31\u56fe, \u4f7f\u7528\u51e0\u79cd\u6280\u672f\u6765\u63d0\u9ad8\u539f\u59cb seq2seq \u6a21\u578b\u7684\u80fd\u529b. \u7ed9\u5b9a<code>&lt;\u6587\u672c, \u97f3\u9891&gt;</code>\u5bf9, Tacotron\u53ef\u4ee5\u5b8c\u5168\u4ece\u5934\u5f00\u59cb\u968f\u673a\u521d\u59cb\u5316\u8bad\u7ec3. \u5b83\u4e0d\u9700\u8981\u97f3\u7d20\u7ea7\u522b\u7684\u5bf9\u9f50, \u56e0\u6b64\u53ef\u4ee5\u8f7b\u677e\u6269\u5c55\u5230\u4f7f\u7528\u5927\u91cf\u5e26\u6709\u8f6c\u5f55\u7684\u58f0\u5b66\u6570\u636e. \u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u6ce2\u5f62\u5408\u6210\u6280\u672f, Tacotron\u5728\u7f8e\u56fd\u82f1\u8bed\u8bc4\u4f30\u96c6\u4e0a\u4ea7\u751f\u4e863.82\u7684\u5e73\u5747\u610f\u89c1\u5f97\u5206 (MOS), \u5728\u81ea\u7136\u5ea6\u65b9\u9762\u8d85\u8fc7\u4e86\u751f\u4ea7\u7ea7\u7684\u53c2\u6570\u5316\u7cfb\u7edf.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#2related-works","title":"2.Related Works: \u76f8\u5173\u5de5\u4f5c","text":"\u539f\u6587  &gt; [WaveNet](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md) is a powerful generative model of audio. &gt; It works well for TTS, but is slow due to its sample-level autoregressive nature. &gt; It also requires conditioning on linguistic features from an existing TTS frontend, and thus is not end-to-end: it only replaces the vocoder and acoustic model. &gt; Another recently-developed neural model is [Deep Voice](../../Models/TTS0_System/2017.02.25_DeepVoice.md), which replaces every component in a typical TTS pipeline by a corresponding neural network. &gt; However, each component is independently trained, and it\u2019s nontrivial to change the system to train in an end-to-end fashion.    <p>WaveNet \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u97f3\u9891\u751f\u6210\u6a21\u578b. \u5b83\u5728TTS\u65b9\u9762\u8868\u73b0\u826f\u597d, \u4f46\u7531\u4e8e\u5176\u6837\u672c\u7ea7\u522b\u7684\u81ea\u56de\u5f52\u7279\u6027\u800c\u901f\u5ea6\u8f83\u6162. \u5b83\u8fd8\u9700\u8981\u4ece\u73b0\u6709\u7684TTS\u524d\u7aef\u83b7\u53d6\u8bed\u8a00\u7279\u5f81\u7684\u6761\u4ef6\u5316, \u56e0\u6b64\u4e0d\u662f\u7aef\u5230\u7aef\u7684\uff1a\u5b83\u53ea\u66ff\u6362\u4e86\u58f0\u7801\u5668\u548c\u58f0\u5b66\u6a21\u578b. \u53e6\u4e00\u4e2a\u6700\u8fd1\u5f00\u53d1\u7684\u795e\u7ecf\u6a21\u578b\u662f Deep Voice, \u5b83\u7528\u76f8\u5e94\u7684\u795e\u7ecf\u7f51\u7edc\u66ff\u6362\u4e86\u5178\u578bTTS\u6d41\u7a0b\u4e2d\u7684\u6bcf\u4e2a\u7ec4\u4ef6. \u7136\u800c, \u6bcf\u4e2a\u7ec4\u4ef6\u662f\u72ec\u7acb\u8bad\u7ec3\u7684, \u5e76\u4e14\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u8bad\u7ec3\u7cfb\u7edf\u5e76\u4e0d\u7b80\u5355.</p> \u539f\u6587  &gt; To our knowledge, [\"First Step Towards End-to-End Parametric TTS Synthesis: Generating Spectral Parameters with Neural Attention (2016) \"]() is the earliest work touching end-to-end TTS using seq2seq with attention. &gt; However, it requires a pre-trained hidden Markov model (HMM) aligner to help the seq2seq model learn the alignment. &gt; It\u2019s hard to tell how much alignment is learned by the seq2seq per se. &gt; Second, a few tricks are used to get the model trained, which the authors note hurts prosody. &gt; Third, it predicts vocoder parameters hence needs a vocoder. &gt; Furthermore, the model is trained on phoneme inputs and the experimental results seem to be somewhat limited.   <p></p> <p>\u636e\u6211\u4eec\u6240\u77e5, \"First Step Towards End-to-End Parametric TTS Synthesis: Generating Spectral Parameters with Neural Attention (2016) \" \u662f\u6700\u65e9\u4f7f\u7528\u5e26\u6709\u6ce8\u610f\u529b\u7684 seq2seq \u63a5\u89e6\u7aef\u5230\u7aefTTS\u7684\u5de5\u4f5c. \u7136\u800c, \u5b83\u9700\u8981\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b (HMM) \u5bf9\u9f50\u5668\u6765\u5e2e\u52a9 seq2seq \u6a21\u578b\u5b66\u4e60\u5bf9\u9f50. \u5f88\u96be\u8bf4 seq2seq \u672c\u8eab\u5b66\u5230\u4e86\u591a\u5c11\u5bf9\u9f50. \u5176\u6b21, \u4f7f\u7528\u4e86\u4e00\u4e9b\u6280\u5de7\u6765\u8bad\u7ec3\u6a21\u578b, \u4f5c\u8005\u6307\u51fa\u8fd9\u635f\u5bb3\u4e86\u97f5\u5f8b. \u7b2c\u4e09, \u5b83\u9884\u6d4b\u58f0\u7801\u5668\u53c2\u6570, \u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u58f0\u7801\u5668. \u6b64\u5916, \u8be5\u6a21\u578b\u662f\u57fa\u4e8e\u97f3\u7d20\u8f93\u5165\u8fdb\u884c\u8bad\u7ec3\u7684, \u5b9e\u9a8c\u7ed3\u679c\u4f3c\u4e4e\u6709\u4e9b\u6709\u9650.</p> \u539f\u6587  &gt; [Char2Wav](../../Models/E2E/2017.02.18_Char2Wav.md) is an independently-developed end-to-end model that can be trained on characters. &gt; However, [Char2Wav](../../Models/E2E/2017.02.18_Char2Wav.md) still predicts vocoder parameters before using a [SampleRNN](../../Models/TTS3_Vocoder/2016.12.22_SampleRNN.md) neural vocoder, whereas ***Tacotron*** directly predicts raw spectrogram. &gt; Also, their seq2seq and [SampleRNN](../../Models/TTS3_Vocoder/2016.12.22_SampleRNN.md) models need to be separately pre-trained, but our model can be trained from scratch. &gt; Finally, we made several key modifications to the vanilla seq2seq paradigm. &gt; As shown later, a vanilla seq2seq model does not work well for character-level inputs.    <p></p> <p>Char2Wav \u662f\u4e00\u4e2a\u72ec\u7acb\u5f00\u53d1\u7684\u7aef\u5230\u7aef\u6a21\u578b, \u53ef\u4ee5\u57fa\u4e8e\u5b57\u7b26\u8fdb\u884c\u8bad\u7ec3. \u7136\u800c, Char2Wav \u4ecd\u7136\u5728 SampleRNN \u795e\u7ecf\u58f0\u7801\u5668\u4e4b\u524d\u9884\u6d4b\u58f0\u7801\u5668\u53c2\u6570, \u800cTacotron\u76f4\u63a5\u9884\u6d4b\u539f\u59cb\u9891\u8c31\u56fe. \u6b64\u5916, \u4ed6\u4eec\u7684 seq2seq \u548c SampleRNN \u6a21\u578b\u9700\u8981\u5206\u522b\u9884\u8bad\u7ec3, \u4f46\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3. \u6700\u540e, \u6211\u4eec\u5bf9\u539f\u59cb seq2seq \u8303\u5f0f\u8fdb\u884c\u4e86\u51e0\u9879\u5173\u952e\u4fee\u6539. \u5982\u540e\u6587\u6240\u793a, \u539f\u59cb seq2seq \u6a21\u578b\u5728\u5b57\u7b26\u7ea7\u522b\u8f93\u5165\u4e0a\u8868\u73b0\u4e0d\u4f73.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#3methodology","title":"3.Methodology: \u65b9\u6cd5","text":"\u539f\u6587  &gt; The backbone of ***Tacotron*** is a seq2seq model with attention ([\"Neural Machine Translation by Jointly Learning to Align and Translate (2014) \"](../../Models/NMT/2014.09.01_NMT_by_Jointly_Learning_to_Align_&amp;_Translate.md), [\"Grammar as A Foreign Language (2015) \"]()) . &gt; Figure.01 depicts the model, which includes an encoder, an attention-based decoder, and a post-processing net. &gt; At a high-level, our model takes characters as input and produces spectrogram frames, which are then converted to waveforms. &gt; We describe these components below.    <p>Tacotron\u7684\u4e3b\u5e72\u662f\u4e00\u4e2a\u5e26\u6709\u6ce8\u610f\u529b\u7684 seq2seq \u6a21\u578b (\"Neural Machine Translation by Jointly Learning to Align and Translate (2014) \", \"Grammar as A Foreign Language (2015) \") . \u56fe01\u5c55\u793a\u4e86\u8be5\u6a21\u578b, \u5b83\u5305\u62ec\u4e00\u4e2a\u7f16\u7801\u5668, \u4e00\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u89e3\u7801\u5668\u548c\u4e00\u4e2a\u540e\u5904\u7406\u7f51\u7edc. \u4ece\u9ad8\u5c42\u6b21\u6765\u770b, \u6211\u4eec\u7684\u6a21\u578b\u4ee5\u5b57\u7b26\u4f5c\u4e3a\u8f93\u5165, \u5e76\u4ea7\u751f\u9891\u8c31\u56fe\u5e27, \u7136\u540e\u5c06\u5176\u8f6c\u6362\u4e3a\u6ce2\u5f62. \u6211\u4eec\u5c06\u5728\u4e0b\u9762\u63cf\u8ff0\u8fd9\u4e9b\u7ec4\u4ef6.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#31cbhg-module-cbhg","title":"3.1.CBHG module: CBHG \u6a21\u5757","text":"\u539f\u6587  &gt; We first describe a building block dubbed ***CBHG***, illustrated in Figure.02. &gt; ***CBHG*** consists of a bank of 1-D convolutional filters, followed by [Highway networks](../../Models/_Basis/HighwayNet.md) and a bidirectional [Gated Recurrent Unit (GRU) ](../../Models/_Basis/GRU.md) recurrent neural net (RNN) . &gt; ***CBHG*** is a powerful module for extracting representations from sequences. &gt; The input sequence is first convolved with $K$ sets of 1-D convolutional filters, where the $k$-th set contains $C_k$ filters of width $k$ (i.e. $k = 1, 2, \\cdots, K$) . &gt; These filters explicitly model local and contextual information (akin to modeling unigrams, bi-grams, up to $K$-grams) . &gt; The convolution outputs are stacked together and further max pooled along time to increase local invariances. &gt; Note that we use a stride of $1$ to preserve the original time resolution. &gt; We further pass the processed sequence to a few fixed-width 1-D convolutions, whose outputs are added with the original input sequence via residual connections [ResNet](../../Models/_Basis/ResNet.md) . &gt; [Batch normalization](../../Models/_Basis/BatchNorm.md) is used for all convolutional layers. &gt; The convolution outputs are fed into a multi-layer highway network to extract high-level features. &gt; Finally, we stack a bidirectional GRU RNN on top to extract sequential features from both forward and backward context. &gt; ***CBHG*** is inspired from work in machine translation [\"Fully Character-Level Neural Machine Translation without Explicit Segmentation (2016) \"](../../Models/NMT/2016.10.10_Fully_Character-Level_NMT_without_Explicit_Segmentation.md), where the main differences from [\"Fully Character-Level Neural Machine Translation without Explicit Segmentation (2016) \"](../../Models/NMT/2016.10.10_Fully_Character-Level_NMT_without_Explicit_Segmentation.md) include using non-causal convolutions, batch normalization, residual connections, and stride=1 max pooling. &gt; We found that these modifications improved generalization.    <p>\u6211\u4eec\u9996\u5148\u63cf\u8ff0\u4e86\u4e00\u4e2a\u540d\u4e3aCBHG\u7684\u6784\u5efa\u5757, \u5982\u56fe02\u6240\u793a. CBHG\u7531\u4e00\u7ec41D\u5377\u79ef\u6ee4\u6ce2\u5668, \u968f\u540e\u662f Highway\u7f51\u7edc \u548c\u4e00\u4e2a\u53cc\u5411 \u95e8\u63a7\u5faa\u73af\u5355\u5143 (GRU)  \u5faa\u73af\u795e\u7ecf\u7f51\u7edc (RNN) \u7ec4\u6210. CBHG\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u6a21\u5757, \u7528\u4e8e\u4ece\u5e8f\u5217\u4e2d\u63d0\u53d6\u8868\u793a. \u8f93\u5165\u5e8f\u5217\u9996\u5148\u4e0e$K$\u7ec41D\u5377\u79ef\u6ee4\u6ce2\u5668\u8fdb\u884c\u5377\u79ef, \u5176\u4e2d\u7b2c$k$\u7ec4\u5305\u542b$C_k$\u4e2a\u5bbd\u5ea6\u4e3a$k$\u7684\u6ee4\u6ce2\u5668 (\u5373$k = 1, 2, \\cdots, K$) . \u8fd9\u4e9b\u6ee4\u6ce2\u5668\u660e\u786e\u5730\u6a21\u62df\u4e86\u5c40\u90e8\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f (\u7c7b\u4f3c\u4e8e\u6a21\u62df\u5355\u5b57, \u53cc\u5b57, \u76f4\u5230$K$\u5b57) . \u5377\u79ef\u8f93\u51fa\u6cbf\u65f6\u95f4\u5806\u53e0\u5728\u4e00\u8d77, \u5e76\u8fdb\u4e00\u6b65\u8fdb\u884c\u6700\u5927\u6c60\u5316\u4ee5\u589e\u52a0\u5c40\u90e8\u4e0d\u53d8\u6027. \u8bf7\u6ce8\u610f, \u6211\u4eec\u4f7f\u7528\u6b65\u957f\u4e3a$1$\u4ee5\u4fdd\u6301\u539f\u59cb\u65f6\u95f4\u5206\u8fa8\u7387. \u6211\u4eec\u8fdb\u4e00\u6b65\u5c06\u5904\u7406\u540e\u7684\u5e8f\u5217\u901a\u8fc7\u51e0\u4e2a\u56fa\u5b9a\u5bbd\u5ea6\u76841D\u5377\u79ef, \u5176\u8f93\u51fa\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5 ResNet \u4e0e\u539f\u59cb\u8f93\u5165\u5e8f\u5217\u76f8\u52a0. \u6240\u6709\u5377\u79ef\u5c42\u90fd\u4f7f\u7528\u4e86 \u6279\u91cf\u5f52\u4e00\u5316 . \u5377\u79ef\u8f93\u51fa\u88ab\u9001\u5165\u4e00\u4e2a\u591a\u5c42Highway\u7f51\u7edc\u4ee5\u63d0\u53d6\u9ad8\u7ea7\u7279\u5f81. \u6700\u540e, \u6211\u4eec\u5728\u9876\u90e8\u5806\u53e0\u4e00\u4e2a\u53cc\u5411GRU RNN, \u4ee5\u4ece\u6b63\u5411\u548c\u53cd\u5411\u4e0a\u4e0b\u6587\u4e2d\u63d0\u53d6\u5e8f\u5217\u7279\u5f81. CBHG\u7684\u7075\u611f\u6765\u81ea\u4e8e\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u5de5\u4f5c \"Fully Character-Level Neural Machine Translation without Explicit Segmentation (2016) \", \u4e0e \"Fully Character-Level Neural Machine Translation without Explicit Segmentation (2016) \" \u7684\u4e3b\u8981\u533a\u522b\u5305\u62ec\u4f7f\u7528\u975e\u56e0\u679c\u5377\u79ef, \u6279\u91cf\u5f52\u4e00\u5316, \u6b8b\u5dee\u8fde\u63a5\u548c\u6b65\u957f=1\u7684\u6700\u5927\u6c60\u5316. \u6211\u4eec\u53d1\u73b0\u8fd9\u4e9b\u4fee\u6539\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#32encoder","title":"3.2.Encoder: \u7f16\u7801\u5668","text":"\u539f\u6587  &gt; The goal of the encoder is to extract robust sequential representations of text. &gt; The input to the encoder is a character sequence, where each character is represented as a one-hot vector and embedded into a continuous vector. &gt; We then apply a set of non-linear transformations, collectively called a \u201cpre-net\u201d, to each embedding. &gt; We use a bottleneck layer with dropout as the pre-net in this work, which helps convergence and improves generalization. &gt; A ***CBHG*** module transforms the pre-net outputs into the final encoder representation used by the attention module. &gt; We found that this ***CBHG***-based encoder not only reduces overfitting, but also makes fewer mispronunciations than a standard multi-layer RNN encoder (see our linked page of audio samples) .    <p>\u7f16\u7801\u5668\u7684\u76ee\u6807\u662f\u63d0\u53d6\u6587\u672c\u7684\u9c81\u68d2\u5e8f\u5217\u8868\u793a. \u7f16\u7801\u5668\u7684\u8f93\u5165\u662f\u4e00\u4e2a\u5b57\u7b26\u5e8f\u5217, \u5176\u4e2d\u6bcf\u4e2a\u5b57\u7b26\u88ab\u8868\u793a\u4e3a\u4e00\u4e2a\u72ec\u70ed\u5411\u91cf, \u5e76\u5d4c\u5165\u5230\u4e00\u4e2a\u8fde\u7eed\u5411\u91cf\u4e2d. \u7136\u540e\u6211\u4eec\u5bf9\u6bcf\u4e2a\u5d4c\u5165\u5e94\u7528\u4e00\u7ec4\u975e\u7ebf\u6027\u53d8\u6362, \u7edf\u79f0\u4e3a\u201c\u9884\u7f51\u7edc\u201d (pre-net) . \u5728\u672c\u5de5\u4f5c\u4e2d, \u6211\u4eec\u4f7f\u7528\u4e86\u4e00\u4e2a\u5e26\u6709dropout\u7684\u74f6\u9888\u5c42\u4f5c\u4e3a\u9884\u7f51\u7edc, \u8fd9\u6709\u52a9\u4e8e\u6536\u655b\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b. \u4e00\u4e2aCBHG\u6a21\u5757\u5c06\u9884\u7f51\u7edc\u8f93\u51fa\u8f6c\u6362\u4e3a\u6700\u7ec8\u7684\u7f16\u7801\u5668\u8868\u793a, \u4f9b\u6ce8\u610f\u529b\u6a21\u5757\u4f7f\u7528. \u6211\u4eec\u53d1\u73b0, \u8fd9\u79cd\u57fa\u4e8eCBHG\u7684\u7f16\u7801\u5668\u4e0d\u4ec5\u51cf\u5c11\u4e86\u8fc7\u62df\u5408, \u800c\u4e14\u6bd4\u6807\u51c6\u7684\u5206\u5c42RNN\u7f16\u7801\u5668\u4ea7\u751f\u66f4\u5c11\u7684\u9519\u8bef\u53d1\u97f3 (\u53c2\u89c1\u6211\u4eec\u94fe\u63a5\u7684\u97f3\u9891\u6837\u672c\u9875\u9762) .</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#33decoder","title":"3.3.Decoder: \u89e3\u7801\u5668","text":"\u539f\u6587  &gt; We use a content-based tanh attention decoder (see e.g. [\"Grammar as A Foreign Language (2015) \"]()), where a stateful recurrent layer produces the attention query at each decoder time step. &gt; We concatenate the context vector and the attention RNN cell output to form the input to the decoder RNNs. &gt; We use a stack of GRUs with vertical residual connections ([GNMT](../../Models/NMT/2016.09.26_GNMT.md)) for the decoder. &gt; We found the residual connections speed up convergence. &gt; The decoder target is an important design choice. &gt; While we could directly predict raw spectrogram, it\u2019s a highly redundant representation for the purpose of learning alignment between speech signal and text (which is really the motivation of using seq2seq for this task) . &gt; Because of this redundancy, we use a different target for seq2seq decoding and waveform synthesis. &gt; The seq2seq target can be highly compressed as long as it provides sufficient intelligibility and prosody information for an inversion process, which could be fixed or trained. &gt; We use 80-band mel-scale spectrogram as the target, though fewer bands or more concise targets such as cepstrum could be used. &gt; We use a post-processing network (discussed below) to convert from the seq2seq target to waveform.   <p>\u6211\u4eec\u4f7f\u7528\u57fa\u4e8e\u5185\u5bb9\u7684 tanh \u6ce8\u610f\u529b\u89e3\u7801\u5668 (\u4f8b\u5982, \u53c2\u89c1 \"Grammar as A Foreign Language (2015) \"), \u5176\u4e2d\u72b6\u6001\u5316\u7684\u5faa\u73af\u5c42\u5728\u6bcf\u4e2a\u89e3\u7801\u5668\u65f6\u95f4\u6b65\u751f\u6210\u6ce8\u610f\u529b\u67e5\u8be2. \u6211\u4eec\u5c06\u4e0a\u4e0b\u6587\u5411\u91cf\u548c\u6ce8\u610f\u529bRNN\u5355\u5143\u8f93\u51fa\u8fde\u63a5\u8d77\u6765, \u5f62\u6210\u89e3\u7801\u5668RNN\u7684\u8f93\u5165. \u6211\u4eec\u4f7f\u7528\u5e26\u6709\u5782\u76f4\u6b8b\u5dee\u8fde\u63a5 (GNMT) \u7684GRU\u5806\u6808\u4f5c\u4e3a\u89e3\u7801\u5668. \u6211\u4eec\u53d1\u73b0\u6b8b\u5dee\u8fde\u63a5\u52a0\u5feb\u4e86\u6536\u655b\u901f\u5ea6. \u89e3\u7801\u5668\u76ee\u6807\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8bbe\u8ba1\u9009\u62e9. \u867d\u7136\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u9884\u6d4b\u539f\u59cb\u9891\u8c31\u56fe, \u4f46\u5bf9\u4e8e\u5b66\u4e60\u8bed\u97f3\u4fe1\u53f7\u548c\u6587\u672c\u4e4b\u95f4\u7684\u5bf9\u9f50 (\u8fd9\u5b9e\u9645\u4e0a\u662f\u4f7f\u7528 seq2seq \u8fdb\u884c\u6b64\u4efb\u52a1\u7684\u52a8\u673a) \u6765\u8bf4, \u5b83\u662f\u4e00\u4e2a\u9ad8\u5ea6\u5197\u4f59\u7684\u8868\u793a. \u7531\u4e8e\u8fd9\u79cd\u5197\u4f59, \u6211\u4eec\u4e3a seq2seq \u89e3\u7801\u548c\u6ce2\u5f62\u5408\u6210\u4f7f\u7528\u4e0d\u540c\u7684\u76ee\u6807. \u53ea\u8981 seq2seq \u76ee\u6807\u4e3a\u53cd\u8f6c\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u8db3\u591f\u7684\u53ef\u7406\u89e3\u6027\u548c\u97f5\u5f8b\u4fe1\u606f, \u5b83\u5c31\u53ef\u4ee5\u9ad8\u5ea6\u538b\u7f29, \u8fd9\u4e2a\u53cd\u8f6c\u8fc7\u7a0b\u53ef\u4ee5\u662f\u56fa\u5b9a\u7684\u6216\u8bad\u7ec3\u7684. \u6211\u4eec\u4f7f\u752880\u9891\u5e26\u7684\u6885\u5c14\u5c3a\u5ea6\u9891\u8c31\u56fe\u4f5c\u4e3a\u76ee\u6807, \u5c3d\u7ba1\u53ef\u4ee5\u4f7f\u7528\u66f4\u5c11\u7684\u9891\u5e26\u6216\u66f4\u7b80\u6d01\u7684\u76ee\u6807, \u5982\u5012\u8c31. \u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u540e\u5904\u7406\u7f51\u7edc (\u4e0b\u9762\u8ba8\u8bba) \u5c06 seq2seq \u76ee\u6807\u8f6c\u6362\u4e3a\u6ce2\u5f62.</p> \u539f\u6587  &gt; We use a simple fully-connected output layer to predict the decoder targets. &gt; An important trick we discovered was predicting multiple, non-overlapping output frames at each decoder step. &gt; Predicting r frames at once divides the total number of decoder steps by r, which reduces model size, training time, and inference time. &gt; More importantly, we found this trick to substantially increase convergence speed, as measured by a much faster (and more stable) alignment learned from attention. &gt; This is likely because neighboring speech frames are correlated and each character usually corresponds to multiple frames. &gt; Emitting one frame at a time forces the model to attend to the same input token for multiple timesteps; emitting multiple frames allows the attention to move forward early in training. &gt; A similar trick is also used in [\"Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices (2016) \"]() but mainly to speed up inference.   <p></p> <p>\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u5168\u8fde\u63a5\u8f93\u51fa\u5c42\u6765\u9884\u6d4b\u89e3\u7801\u5668\u76ee\u6807. \u6211\u4eec\u53d1\u73b0\u7684\u4e00\u4e2a\u91cd\u8981\u6280\u5de7\u662f\u5728\u6bcf\u4e2a\u89e3\u7801\u5668\u6b65\u9aa4\u9884\u6d4b\u591a\u4e2a\u975e\u91cd\u53e0\u7684\u8f93\u51fa\u5e27. \u4e00\u6b21\u9884\u6d4br\u5e27\u5c06\u89e3\u7801\u5668\u6b65\u9aa4\u603b\u6570\u9664\u4ee5r, \u8fd9\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f, \u8bad\u7ec3\u65f6\u95f4\u548c\u63a8\u7406\u65f6\u95f4. \u66f4\u91cd\u8981\u7684\u662f, \u6211\u4eec\u53d1\u73b0\u8fd9\u4e2a\u6280\u5de7\u5927\u5927\u52a0\u5feb\u4e86\u6536\u655b\u901f\u5ea6, \u6b63\u5982\u901a\u8fc7\u6ce8\u610f\u529b\u5b66\u4e60\u5230\u7684\u66f4\u5feb (\u4e14\u66f4\u7a33\u5b9a) \u7684\u5bf9\u9f50\u6240\u8861\u91cf\u7684\u90a3\u6837. \u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u76f8\u90bb\u7684\u8bed\u97f3\u5e27\u662f\u76f8\u5173\u7684, \u5e76\u4e14\u6bcf\u4e2a\u5b57\u7b26\u901a\u5e38\u5bf9\u5e94\u591a\u4e2a\u5e27. \u4e00\u6b21\u53d1\u51fa\u4e00\u4e2a\u5e27\u8feb\u4f7f\u6a21\u578b\u5728\u591a\u4e2a\u65f6\u95f4\u6b65\u4e0a\u5173\u6ce8\u76f8\u540c\u7684\u8f93\u5165\u4ee4\u724c; \u4e00\u6b21\u53d1\u51fa\u591a\u4e2a\u5e27\u5141\u8bb8\u6ce8\u610f\u529b\u5728\u8bad\u7ec3\u65e9\u671f\u5411\u524d\u79fb\u52a8. \u7c7b\u4f3c\u6280\u5de7\u4e5f\u5728 \"Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices (2016) \" \u4e2d\u4f7f\u7528, \u4f46\u4e3b\u8981\u662f\u4e3a\u4e86\u52a0\u5feb\u63a8\u7406\u901f\u5ea6.</p> \u539f\u6587  &gt; The first decoder step is conditioned on an all-zero frame, which represents a `` frame. &gt; In inference, at decoder step $t$, the last frame of the $r$ predictions is fed as input to the decoder at step $t + 1$. &gt; Note that feeding the last prediction is an ad-hoc choice here \u2013 we could use all $r$ predictions. &gt; During training, we always feed every $r$-th ground truth frame to the decoder. &gt; The input frame is passed to a pre-net as is done in the encoder. &gt; Since we do not use techniques such as scheduled sampling [\"Scheduled Sampling for Sequence Prediction with RNNs (2015) \"]() (we found it to hurt audio quality), the dropout in the pre-net is critical for the model to generalize, as it provides a noise source to resolve the multiple modalities in the output distribution.    <p></p> <p>\u7b2c\u4e00\u4e2a\u89e3\u7801\u5668\u6b65\u9aa4\u662f\u57fa\u4e8e\u4e00\u4e2a\u5168\u96f6\u5e27\u7684\u6761\u4ef6, \u5b83\u4ee3\u8868\u4e00\u4e2a<code>&lt;GO&gt;</code>\u5e27. \u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d, \u5728\u89e3\u7801\u5668\u6b65\u9aa4 $t$, \u6700\u540e\u9884\u6d4b\u7684 $r$ \u5e27\u88ab\u4f5c\u4e3a\u8f93\u5165\u63d0\u4f9b\u7ed9\u6b65\u9aa4 $t + 1$ \u7684\u89e3\u7801\u5668. \u8bf7\u6ce8\u610f, \u5728\u8fd9\u91cc\u4f7f\u7528\u6700\u540e\u4e00\u4e2a\u9884\u6d4b\u662f\u4e00\u4e2a\u5373\u5174\u7684\u9009\u62e9\u2014\u2014\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6240\u6709 $r$ \u4e2a\u9884\u6d4b. \u5728\u8bad\u7ec3\u671f\u95f4, \u6211\u4eec\u603b\u662f\u5c06\u6bcf $r$ \u4e2a\u771f\u5b9e\u5e27\u63d0\u4f9b\u7ed9\u89e3\u7801\u5668. \u8f93\u5165\u5e27\u50cf\u5728\u7f16\u7801\u5668\u4e2d\u4e00\u6837\u901a\u8fc7\u9884\u7f51\u7edc\u4f20\u9012. \u7531\u4e8e\u6211\u4eec\u6ca1\u6709\u4f7f\u7528\u8bf8\u5982 \"Scheduled Sampling for Sequence Prediction with RNNs (2015) \" \u8fd9\u6837\u7684\u6280\u672f (\u6211\u4eec\u53d1\u73b0\u5b83\u4f1a\u635f\u5bb3\u97f3\u9891\u8d28\u91cf), \u9884\u7f51\u7edc\u4e2d\u7684dropout\u5bf9\u4e8e\u6a21\u578b\u6cdb\u5316\u81f3\u5173\u91cd\u8981, \u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u566a\u58f0\u6e90\u6765\u89e3\u51b3\u8f93\u51fa\u5206\u5e03\u4e2d\u7684\u591a\u6a21\u6001\u95ee\u9898.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#34post-processing-net-and-waveform-synthesis","title":"3.4.Post-processing Net and Waveform Synthesis: \u540e\u5904\u7406\u7f51\u7edc\u548c\u6ce2\u5f62\u5408\u6210","text":"\u539f\u6587  &gt; As mentioned above, the post-processing net\u2019s task is to convert the seq2seq target to a target that can be synthesized into waveforms. &gt; Since we use [Griffin-Lim](../../Models/_Basis/Griffin-Lim.md) as the synthesizer, the post-processing net learns to predict spectral magnitude sampled on a linear-frequency scale. &gt; Another motivation of the post-processing net is that it can see the full decoded sequence. &gt; In contrast to seq2seq, which always runs from left to right, it has both forward and backward information to correct the prediction error for each individual frame. &gt; In this work, we use a ***CBHG*** module for the post-processing net, though a simpler architecture likely works as well. &gt; The concept of a postprocessing network is highly general. &gt; It could be used to predict alternative targets such as vocoder parameters, or as a WaveNetlike neural vocoder ([WaveNet](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md), [SampleRNN](../../Models/TTS3_Vocoder/2016.12.22_SampleRNN.md), [Deep Voice](../../Models/TTS0_System/2017.02.25_DeepVoice.md)) that synthesizes waveform samples directly.   <p>\u5982\u4e0a\u6240\u8ff0, \u540e\u5904\u7406\u7f51\u7edc\u7684\u4efb\u52a1\u662f\u5c06 seq2seq \u76ee\u6807\u8f6c\u6362\u4e3a\u53ef\u4ee5\u5408\u6210\u6ce2\u5f62\u7684\u76ee\u6807. \u7531\u4e8e\u6211\u4eec\u4f7f\u7528 Griffin-Lim \u4f5c\u4e3a\u5408\u6210\u5668, \u540e\u5904\u7406\u7f51\u7edc\u5b66\u4e60\u9884\u6d4b\u7ebf\u6027\u9891\u7387\u5c3a\u5ea6\u4e0a\u7684\u9891\u8c31\u5e45\u5ea6\u91c7\u6837. \u540e\u5904\u7406\u7f51\u7edc\u7684\u53e6\u4e00\u4e2a\u52a8\u673a\u662f\u5b83\u53ef\u4ee5\u67e5\u770b\u5b8c\u6574\u7684\u89e3\u7801\u5e8f\u5217. \u4e0e\u59cb\u7ec8\u4ece\u5de6\u5230\u53f3\u8fd0\u884c\u7684 seq2seq \u4e0d\u540c, \u5b83\u65e2\u6709\u6b63\u5411\u4fe1\u606f\u4e5f\u6709\u53cd\u5411\u4fe1\u606f\u6765\u7ea0\u6b63\u6bcf\u4e2a\u5355\u72ec\u5e27\u7684\u9884\u6d4b\u9519\u8bef. \u5728\u672c\u5de5\u4f5c\u4e2d, \u6211\u4eec\u4f7f\u7528\u4e00\u4e2a CBHG \u6a21\u5757\u4f5c\u4e3a\u540e\u5904\u7406\u7f51\u7edc, \u5c3d\u7ba1\u4e00\u4e2a\u66f4\u7b80\u5355\u7684\u67b6\u6784\u53ef\u80fd\u540c\u6837\u6709\u6548. \u540e\u5904\u7406\u7f51\u7edc\u7684\u6982\u5ff5\u975e\u5e38\u901a\u7528. \u5b83\u53ef\u4ee5\u7528\u6765\u9884\u6d4b\u8bf8\u5982\u58f0\u7801\u5668\u53c2\u6570\u4e4b\u7c7b\u7684\u66ff\u4ee3\u76ee\u6807, \u6216\u8005\u4f5c\u4e3a\u7c7b\u4f3cWaveNet\u7684\u795e\u7ecf\u58f0\u7801\u5668 (WaveNet, SampleRNN, Deep Voice), \u76f4\u63a5\u5408\u6210\u6ce2\u5f62\u6837\u672c.</p> \u539f\u6587  &gt; We use the [Griffin-Lim](../../Models/_Basis/Griffin-Lim.md) algorithm to synthesize waveform from the predicted spectrogram. &gt; We found that raising the predicted magnitudes by a power of $1.2$ before feeding to [Griffin-Lim](../../Models/_Basis/Griffin-Lim.md) reduces artifacts, likely due to its harmonic enhancement effect. &gt; We observed that [Griffin-Lim](../../Models/_Basis/Griffin-Lim.md) converges after $50$ iterations (in fact, about $30$ iterations seems to be enough), which is reasonably fast. &gt; We implemented [Griffin-Lim](../../Models/_Basis/Griffin-Lim.md) in [TensorFlow](../../Models/_Basis/TensorFlow.md) hence it\u2019s also part of the model. &gt; While GriffinLim is differentiable (it does not have trainable weights), we do not impose any loss on it in this work. &gt; We emphasize that our choice of [Griffin-Lim](../../Models/_Basis/Griffin-Lim.md) is for simplicity; while it already yields strong results, developing a fast and high-quality trainable spectrogram to waveform inverter is ongoing work.    <p></p> <p>\u6211\u4eec\u4f7f\u7528 Griffin-Lim \u7b97\u6cd5\u4ece\u9884\u6d4b\u7684\u9891\u8c31\u56fe\u5408\u6210\u6ce2\u5f62. \u6211\u4eec\u53d1\u73b0, \u5728\u5c06\u9884\u6d4b\u7684\u5e45\u5ea6\u8f93\u5165\u5230 Griffin-Lim \u4e4b\u524d, \u5c06\u5176\u63d0\u9ad8\u5230$1.2$\u7684\u5e42\u6b21\u53ef\u4ee5\u51cf\u5c11\u4f2a\u5f71, \u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5176\u8c10\u6ce2\u589e\u5f3a\u6548\u679c. \u6211\u4eec\u89c2\u5bdf\u5230 Griffin-Lim \u5728$50$\u6b21\u8fed\u4ee3\u540e\u6536\u655b (\u5b9e\u9645\u4e0a, \u5927\u7ea6$30$\u6b21\u8fed\u4ee3\u4f3c\u4e4e\u5c31\u8db3\u591f\u4e86), \u8fd9\u662f\u76f8\u5f53\u5feb\u7684. \u6211\u4eec\u5728 TensorFlow \u4e2d\u5b9e\u73b0\u4e86 Griffin-Lim, \u56e0\u6b64\u5b83\u4e5f\u662f\u6a21\u578b\u7684\u4e00\u90e8\u5206. \u5c3d\u7ba1Griffin-Lim\u662f\u53ef\u5fae\u5206\u7684 (\u5b83\u6ca1\u6709\u53ef\u8bad\u7ec3\u7684\u6743\u91cd), \u4f46\u6211\u4eec\u5728\u672c\u5de5\u4f5c\u4e2d\u4e0d\u5bf9\u5b83\u65bd\u52a0\u4efb\u4f55\u635f\u5931. \u6211\u4eec\u5f3a\u8c03, \u6211\u4eec\u9009\u62e9 Griffin-Lim \u662f\u4e3a\u4e86\u7b80\u5355\u6027; \u867d\u7136\u5b83\u5df2\u7ecf\u4ea7\u751f\u4e86\u5f3a\u5927\u7684\u7ed3\u679c, \u4f46\u5f00\u53d1\u4e00\u79cd\u5feb\u901f\u4e14\u9ad8\u8d28\u91cf\u7684\u53ef\u8bad\u7ec3\u9891\u8c31\u56fe\u5230\u6ce2\u5f62\u53cd\u8f6c\u5668\u662f\u6b63\u5728\u8fdb\u884c\u7684\u5de5\u4f5c.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#35model-details","title":"3.5.Model Details: \u6a21\u578b\u7ec6\u8282","text":"\u539f\u6587  &gt; Table.01 lists the hyper-parameters and network architectures. &gt; We use log magnitude spectrogram with Hann windowing, 50 ms frame length, 12.5 ms frame shift, and 2048-point Fourier transform. &gt; We also found pre-emphasis (0.97) to be helpful. &gt; We use 24 kHz sampling rate for all experiments.   <p>\u886801\u5217\u51fa\u4e86\u8d85\u53c2\u6570\u548c\u7f51\u7edc\u67b6\u6784. \u6211\u4eec\u4f7f\u7528\u5e26\u6709\u6c49\u5b81\u7a97\u7684log\u5e45\u5ea6\u9891\u8c31\u56fe, 50\u6beb\u79d2\u5e27\u957f, 12.5\u6beb\u79d2\u5e27\u79fb, \u4ee5\u53ca2048\u70b9\u5085\u91cc\u53f6\u53d8\u6362. \u6211\u4eec\u8fd8\u53d1\u73b0\u9884\u52a0\u91cd (0.97) \u662f\u6709\u5e2e\u52a9\u7684. \u6211\u4eec\u4e3a\u6240\u6709\u5b9e\u9a8c\u4f7f\u752824 kHz\u7684\u91c7\u6837\u7387.</p> \u539f\u6587  &gt; We use $r = 2$ (output layer reduction factor) for the MOS results in this paper, though larger r values (e.g. $r = 5$) also work well. &gt; We use the [Adam](../../Models/_Basis/Adam.md) optimizer with learning rate decay, which starts from $0.001$ and is reduced to $0.0005$, $0.0003$, and $0.0001$ after $500K$, $1M$ and $2M$ global steps, respectively. &gt; We use a simple $l_1$ loss for both seq2seq decoder (mel-scale spectrogram) and post-processing net (linear-scale spectrogram) . &gt; The two losses have equal weights.   <p></p> <p>\u6211\u4eec\u5728\u8fd9\u7bc7\u8bba\u6587\u4e2d\u4f7f\u7528$r = 2$ (\u8f93\u51fa\u5c42\u51cf\u5c11\u56e0\u5b50) \u8fdb\u884cMOS\u7ed3\u679c, \u5c3d\u7ba1\u66f4\u5927\u7684$r$\u503c (\u4f8b\u5982$r = 5$) \u4e5f\u8868\u73b0\u826f\u597d. \u6211\u4eec\u4f7f\u7528 Adam \u4f18\u5316\u5668, \u5e26\u6709\u5b66\u4e60\u7387\u8870\u51cf, \u5b83\u4ece$0.001$\u5f00\u59cb, \u5e76\u5728$500K$, $1M$\u548c$2M$\u5168\u5c40\u6b65\u9aa4\u540e\u5206\u522b\u964d\u4f4e\u5230$0.0005$, $0.0003$\u548c$0.0001$. \u6211\u4eec\u4e3a seq2seq \u89e3\u7801\u5668 (\u6885\u5c14\u5c3a\u5ea6\u9891\u8c31\u56fe) \u548c\u540e\u5904\u7406\u7f51\u7edc (\u7ebf\u6027\u5c3a\u5ea6\u9891\u8c31\u56fe) \u4f7f\u7528\u7b80\u5355\u7684$l_1$\u635f\u5931. \u8fd9\u4e24\u4e2a\u635f\u5931\u5177\u6709\u76f8\u7b49\u7684\u6743\u91cd.</p> \u539f\u6587  &gt; We train using a batch size of $32$, where all sequences are padded to a max length. &gt; It\u2019s a common practice to train sequence models with a loss mask, which masks loss on zero-padded frames. &gt; However, we found that models trained this way don\u2019t know when to stop emitting outputs, causing repeated sounds towards the end. &gt; One simple trick to get around this problem is to also reconstruct the zero-padded frames.    <p></p> <p>\u6211\u4eec\u4f7f\u7528\u6279\u91cf\u5927\u5c0f\u4e3a$32$\u8fdb\u884c\u8bad\u7ec3, \u5176\u4e2d\u6240\u6709\u5e8f\u5217\u90fd\u88ab\u586b\u5145\u5230\u6700\u5927\u957f\u5ea6. \u8bad\u7ec3\u5e8f\u5217\u6a21\u578b\u65f6\u4f7f\u7528\u635f\u5931\u63a9\u7801\u662f\u4e00\u79cd\u5e38\u89c1\u505a\u6cd5, \u5b83\u63a9\u76d6\u4e86\u96f6\u586b\u5145\u5e27\u4e0a\u7684\u635f\u5931. \u7136\u800c, \u6211\u4eec\u53d1\u73b0\u8fd9\u6837\u8bad\u7ec3\u7684\u6a21\u578b\u4e0d\u77e5\u9053\u4f55\u65f6\u505c\u6b62\u53d1\u51fa\u8f93\u51fa, \u5bfc\u81f4\u5728\u7ed3\u5c3e\u5904\u51fa\u73b0\u91cd\u590d\u7684\u58f0\u97f3. \u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u4e2a\u7b80\u5355\u6280\u5de7\u662f\u4e5f\u91cd\u5efa\u96f6\u586b\u5145\u5e27.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#4experiments","title":"4.Experiments: \u5b9e\u9a8c","text":"\u539f\u6587  &gt; We train ***Tacotron*** on an internal North American English dataset, which contains about 24.6 hours of speech data spoken by a professional female speaker. &gt; The phrases are text normalized, e.g. \u201c16\u201d is converted to \u201csixteen\u201d.    <p>\u6211\u4eec\u5728\u4e00\u4e2a\u5185\u90e8\u5317\u7f8e\u82f1\u8bed\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3 Tacotron, \u8be5\u6570\u636e\u96c6\u5305\u542b\u5927\u7ea624.6\u5c0f\u65f6\u7684\u7531\u4e13\u4e1a\u5973\u6027\u53d1\u8a00\u4eba\u6240\u8bf4\u7684\u8bed\u97f3\u6570\u636e. \u77ed\u8bed\u5df2\u7ecf\u6587\u672c\u89c4\u8303\u5316, \u4f8b\u5982\u201c16\u201d\u88ab\u8f6c\u6362\u4e3a\u201csixteen\u201d.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#41ablation-analysis","title":"4.1.Ablation Analysis: \u6d88\u878d\u5206\u6790","text":"\u539f\u6587  &gt; We conduct a few ablation studies to understand the key components in our model. &gt; As is common for generative models, it\u2019s hard to compare models based on objective metrics, which often do not correlate well with perception [\"A Note on the Evaluation of Generative Models (2015) \"]() . &gt; We mainly rely on visual comparisons instead. &gt; We strongly encourage readers to listen to the provided samples. &gt; First, we compare with a vanilla seq2seq model. &gt; Both the encoder and decoder use 2 layers of residual RNNs, where each layer has 256 GRU cells (we tried LSTM and got similar results) . &gt; No pre-net or post-processing net is used, and the decoder directly predicts linear-scale log magnitude spectrogram. &gt; We found that scheduled sampling (sampling rate 0.5) is required for this model to learn alignments and generalize. &gt; We show the learned attention alignment in Figure.03. &gt; Figure.03(a) reveals that the vanilla seq2seq learns a poor alignment. &gt; One problem is that attention tends to get stuck for many frames before moving forward, which causes bad speech intelligibility in the synthesized signal. &gt; The naturalness and overall duration are destroyed as a result. &gt; In contrast, our model learns a clean and smooth alignment, as shown in Figure.03(c) . &gt; Second, we compare with a model with the ***CBHG*** encoder replaced by a 2-layer residual GRU encoder. &gt; The rest of the model, including the encoder pre-net, remain exactly the same. &gt; Comparing Figure.03(b) and Figure.03(c), we can see that the alignment from the GRU encoder is noisier. &gt; Listening to synthesized signals, we found that noisy alignment often leads to mispronunciations. &gt; The ***CBHG*** encoder reduces overfitting and generalizes well to long and complex phrases. &gt; Figure.04(a) and Figure.04(b) demonstrate the benefit of using the post-processing net. &gt; We trained a model without the postprocessing net while keeping all the other components untouched (except that the decoder RNN predicts linear-scale spectrogram) . &gt; With more contextual information, the prediction from the post-processing net contains better resolved harmonics (e.g. higher harmonics between bins $100$ and $400$) and high frequency formant structure, which reduces synthesis artifacts.    <p>\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u4e9b\u6d88\u878d\u7814\u7a76, \u4ee5\u4e86\u89e3\u6211\u4eec\u6a21\u578b\u4e2d\u7684\u5173\u952e\u7ec4\u4ef6. \u5bf9\u4e8e\u751f\u6210\u6a21\u578b\u6765\u8bf4, \u5f88\u96be\u57fa\u4e8e\u5ba2\u89c2\u6307\u6807\u6bd4\u8f83\u6a21\u578b, \u8fd9\u4e9b\u6307\u6807\u901a\u5e38\u4e0e\u611f\u77e5 \"A Note on the Evaluation of Generative Models (2015) \" \u4e0d\u592a\u76f8\u5173. \u6211\u4eec\u4e3b\u8981\u4f9d\u8d56\u4e8e\u89c6\u89c9\u6bd4\u8f83. \u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u8bfb\u8005\u542c\u53d6\u63d0\u4f9b\u7684\u6837\u672c. \u9996\u5148, \u6211\u4eec\u4e0e\u4e00\u4e2a\u539f\u59cb\u7684 seq2seq \u6a21\u578b\u8fdb\u884c\u6bd4\u8f83. \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u90fd\u4f7f\u7528 2 \u5c42\u6b8b\u5dee RNN, \u6bcf\u5c42\u6709 256 \u4e2a GRU \u5355\u5143 (\u6211\u4eec\u5c1d\u8bd5\u4e86 LSTM \u5e76\u5f97\u5230\u4e86\u7c7b\u4f3c\u7684\u7ed3\u679c) . \u6ca1\u6709\u4f7f\u7528\u9884\u7f51\u7edc\u6216\u540e\u5904\u7406\u7f51\u7edc, \u89e3\u7801\u5668\u76f4\u63a5\u9884\u6d4b\u7ebf\u6027\u5c3a\u5ea6\u7684\u5bf9\u6570\u5e45\u5ea6\u9891\u8c31\u56fe. \u6211\u4eec\u53d1\u73b0, \u4e3a\u4e86\u5b66\u4e60\u5bf9\u9f50\u5e76\u6cdb\u5316, \u8fd9\u4e2a\u6a21\u578b\u9700\u8981\u4f7f\u7528\u8ba1\u5212\u91c7\u6837 (\u91c7\u6837\u7387\u4e3a 0.5) . \u6211\u4eec\u5728\u56fe 03 \u4e2d\u5c55\u793a\u4e86\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u5bf9\u9f50. \u56fe 03(a) \u63ed\u793a\u4e86\u539f\u59cb seq2seq \u5b66\u4e60\u4e86\u4e00\u4e2a\u5dee\u7684\u5bf9\u9f50. \u4e00\u4e2a\u95ee\u9898\u662f\u5728\u5411\u524d\u79fb\u52a8\u4e4b\u524d, \u6ce8\u610f\u529b\u503e\u5411\u4e8e\u5728\u8bb8\u591a\u5e27\u4e0a\u505c\u6ede, \u8fd9\u5bfc\u81f4\u5408\u6210\u4fe1\u53f7\u4e2d\u7684\u8bed\u97f3\u53ef\u7406\u89e3\u6027\u5dee. \u7ed3\u679c\u662f\u81ea\u7136\u5ea6\u548c\u6574\u4f53\u6301\u7eed\u65f6\u95f4\u88ab\u7834\u574f. \u76f8\u6bd4\u4e4b\u4e0b, \u6211\u4eec\u7684\u6a21\u578b\u5b66\u4e60\u4e86\u4e00\u4e2a\u5e72\u51c0\u548c\u5149\u6ed1\u7684\u5bf9\u9f50, \u5982\u56fe 03(c) \u6240\u793a. \u5176\u6b21, \u6211\u4eec\u5c06\u5e26\u6709 CBHG \u7f16\u7801\u5668\u7684\u6a21\u578b\u4e0e\u4e00\u4e2a\u66ff\u6362\u4e3a2\u5c42\u6b8b\u5deeGRU\u7f16\u7801\u5668\u7684\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83. \u6a21\u578b\u7684\u5176\u4f59\u90e8\u5206, \u5305\u62ec\u7f16\u7801\u5668\u9884\u7f51\u7edc, \u4fdd\u6301\u5b8c\u5168\u76f8\u540c. \u6bd4\u8f83\u56fe 03(b) \u548c\u56fe 03(c), \u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6765\u81eaGRU\u7f16\u7801\u5668\u7684\u5bf9\u9f50\u66f4\u5608\u6742. \u542c\u53d6\u5408\u6210\u4fe1\u53f7, \u6211\u4eec\u53d1\u73b0\u5608\u6742\u7684\u5bf9\u9f50\u7ecf\u5e38\u5bfc\u81f4\u53d1\u97f3\u9519\u8bef. CBHG \u7f16\u7801\u5668\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\u5e76\u5f88\u597d\u5730\u6cdb\u5316\u5230\u957f\u800c\u590d\u6742\u7684\u77ed\u8bed. \u56fe 04(a) \u548c\u56fe 04(b) \u5c55\u793a\u4e86\u4f7f\u7528\u540e\u5904\u7406\u7f51\u7edc\u7684\u597d\u5904. \u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6ca1\u6709\u540e\u5904\u7406\u7f51\u7edc\u7684\u6a21\u578b, \u540c\u65f6\u4fdd\u6301\u6240\u6709\u5176\u4ed6\u7ec4\u4ef6\u4e0d\u53d8 (\u9664\u4e86\u89e3\u7801\u5668RNN\u9884\u6d4b\u7ebf\u6027\u5c3a\u5ea6\u9891\u8c31\u56fe) . \u6709\u4e86\u66f4\u591a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f, \u6765\u81ea\u540e\u5904\u7406\u7f51\u7edc\u7684\u9884\u6d4b\u5305\u542b\u4e86\u66f4\u597d\u5730\u89e3\u6790\u7684\u8c10\u6ce2 (\u4f8b\u5982, \u5728\u7bb1 $100$ \u548c $400$ \u4e4b\u95f4\u7684\u66f4\u9ad8\u8c10\u6ce2) \u548c\u9ad8\u9891\u5171\u632f\u5cf0\u7ed3\u6784, \u8fd9\u51cf\u5c11\u4e86\u5408\u6210\u4f2a\u5f71.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#42mean-opinion-score-tests","title":"4.2.Mean Opinion Score Tests: \u5e73\u5747\u610f\u89c1\u5f97\u5206\u6d4b\u8bd5","text":"\u539f\u6587  &gt; We conduct mean opinion score tests, where the subjects were asked to rate the naturalness of the stimuli in a $5$-point Likert scale score. &gt; The MOS tests were crowd-sourced from native speakers. 100 unseen phrases were used for the tests and each phrase received $8$ ratings. &gt; When computing MOS, we only include ratings where headphones were used. &gt; We compare our model with a parametric (based on LSTM [\"Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices (2016) \"]()) and a concatenative system [\"Recent Advances in Google Real-Time HMM-Driven Unit Selection Synthesizer (2016) \"](), both of which are in production. &gt; As shown in Table.02, ***Tacotron*** achieves an MOS of $3.82$, which outperforms the parametric system. &gt; Given the strong baselines and the artifacts introduced by the [Griffin-Lim](../../Models/_Basis/Griffin-Lim.md) synthesis, this represents a very promising result.    <p>\u6211\u4eec\u8fdb\u884c\u4e86\u5e73\u5747\u610f\u89c1\u5f97\u5206 (MOS) \u6d4b\u8bd5, \u5176\u4e2d\u53d7\u8bd5\u8005\u88ab\u8981\u6c42\u5728 5 \u70b9 Likert \u91cf\u8868\u4e0a\u8bc4\u4ef7\u523a\u6fc0\u7684\u81ea\u7136\u5ea6. MOS\u6d4b\u8bd5\u662f\u901a\u8fc7\u4f17\u5305\u4ece\u6bcd\u8bed\u8005\u90a3\u91cc\u8fdb\u884c\u7684. \u6211\u4eec\u4f7f\u7528\u4e86 100 \u4e2a\u672a\u89c1\u8fc7\u7684\u77ed\u8bed\u8fdb\u884c\u6d4b\u8bd5, \u6bcf\u4e2a\u77ed\u8bed\u6536\u5230\u4e86 8 \u4e2a\u8bc4\u5206. \u5728\u8ba1\u7b97 MOS \u65f6, \u6211\u4eec\u53ea\u5305\u62ec\u4f7f\u7528\u8033\u673a\u7684\u8bc4\u5206. \u6211\u4eec\u5c06\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u57fa\u4e8e LSTM \u7684\u53c2\u6570\u5316\u7cfb\u7edf (\"Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices (2016) \") \u548c\u4e00\u4e2a\u62fc\u63a5\u7cfb\u7edf (\"Recent Advances in Google Real-Time HMM-Driven Unit Selection Synthesizer (2016) \") \u8fdb\u884c\u4e86\u6bd4\u8f83, \u8fd9\u4e24\u4e2a\u7cfb\u7edf\u90fd\u5728\u751f\u4ea7\u4e2d. \u5982\u886802\u6240\u793a, Tacotron \u5b9e\u73b0\u4e863.82\u7684MOS, \u8fd9\u8d85\u8fc7\u4e86\u53c2\u6570\u5316\u7cfb\u7edf. \u8003\u8651\u5230\u5f3a\u5927\u7684\u57fa\u7ebf\u548c\u7531 Griffin-Lim \u5408\u6210\u5f15\u5165\u7684\u4f2a\u5f71, \u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u6709\u5e0c\u671b\u7684\u7ed3\u679c.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2017.03.29_Tacotron/#5conclusions","title":"5.Conclusions: \u7ed3\u8bba","text":"\u539f\u6587  &gt; We have proposed ***Tacotron***, an integrated end-to-end generative TTS model that takes a character sequence as input and outputs the corresponding spectrogram. &gt; With a very simple waveform synthesis module, it achieves a $3.82$ MOS score on US English, outperforming a production parametric system in terms of naturalness. &gt; ***Tacotron*** is frame-based, so the inference is substantially faster than sample-level autoregressive methods. &gt; Unlike previous work, ***Tacotron*** does not need hand-engineered linguistic features or complex components such as an HMM aligner. &gt; It can be trained from scratch with random initialization. &gt; We perform simple text normalization, though recent advancements in learned text normalization [\"RNN Approaches to Text Normalization: A Challenge (2016) \"]() may render this unnecessary in the future.   <p>\u6211\u4eec\u63d0\u51fa\u4e86 Tacotron, \u4e00\u4e2a\u96c6\u6210\u7684\u7aef\u5230\u7aef\u751f\u6210\u5f0fTTS\u6a21\u578b, \u5b83\u4ee5\u5b57\u7b26\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\u5e76\u8f93\u51fa\u76f8\u5e94\u7684\u9891\u8c31\u56fe. \u901a\u8fc7\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u6ce2\u5f62\u5408\u6210\u6a21\u5757, \u5b83\u5728\u7f8e\u5f0f\u82f1\u8bed\u4e0a\u5b9e\u73b0\u4e863.82\u7684MOS\u5f97\u5206, \u5728\u81ea\u7136\u5ea6\u65b9\u9762\u8d85\u8fc7\u4e86\u751f\u4ea7\u4e2d\u7684\u53c2\u6570\u5316\u7cfb\u7edf. Tacotron \u662f\u57fa\u4e8e\u5e27\u7684, \u56e0\u6b64\u63a8\u7406\u901f\u5ea6\u6bd4\u6837\u672c\u7ea7\u81ea\u56de\u5f52\u65b9\u6cd5\u5feb\u5f97\u591a. \u4e0e\u4ee5\u5f80\u7684\u5de5\u4f5c\u4e0d\u540c, Tacotron \u4e0d\u9700\u8981\u624b\u5de5\u8bbe\u8ba1\u7684\u8bed\u8a00\u7279\u5f81\u6216\u590d\u6742\u7684\u7ec4\u4ef6, \u5982HMM\u5bf9\u9f50\u5668. \u5b83\u53ef\u4ee5\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3, \u968f\u673a\u521d\u59cb\u5316. \u6211\u4eec\u8fdb\u884c\u4e86\u7b80\u5355\u7684\u6587\u672c\u89c4\u8303\u5316, \u5c3d\u7ba1\u6700\u8fd1\u5728\u5b66\u4e60\u7684\u6587\u672c\u89c4\u8303\u5316\u65b9\u9762\u7684\u8fdb\u5c55 (\"RNN Approaches to Text Normalization: A Challenge (2016) \") \u53ef\u80fd\u4f1a\u5728\u672a\u6765\u4f7f\u8fd9\u53d8\u5f97\u4e0d\u5fc5\u8981.</p> \u539f\u6587  &gt; We have yet to investigate many aspects of our model; many early design decisions have gone unchanged. &gt; Our output layer, attention module, loss function, and [Griffin-Lim](../../Models/_Basis/Griffin-Lim.md) -based waveform synthesizer are all ripe for improvement. &gt; For example, it\u2019s well known that [Griffin-Lim](../../Models/_Basis/Griffin-Lim.md) outputs may have audible artifacts. &gt; We are currently working on fast and high-quality neural-network-based spectrogram inversion.    <p></p> <p>\u6211\u4eec\u5c1a\u672a\u8c03\u67e5\u6211\u4eec\u6a21\u578b\u7684\u8bb8\u591a\u65b9\u9762; \u8bb8\u591a\u65e9\u671f\u8bbe\u8ba1\u51b3\u7b56\u4e00\u76f4\u672a\u53d8. \u6211\u4eec\u7684\u8f93\u51fa\u5c42, \u6ce8\u610f\u529b\u6a21\u5757, \u635f\u5931\u51fd\u6570\u548c\u57fa\u4e8e Griffin-Lim \u7684\u6ce2\u5f62\u5408\u6210\u5668\u90fd\u6709\u5f85\u6539\u8fdb. \u4f8b\u5982, \u4f17\u6240\u5468\u77e5, Griffin-Lim \u8f93\u51fa\u53ef\u80fd\u4f1a\u6709\u53ef\u542c\u89c1\u7684\u4f2a\u5f71. \u6211\u4eec\u76ee\u524d\u6b63\u5728\u7814\u7a76\u5feb\u901f\u4e14\u9ad8\u8d28\u91cf\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u9891\u8c31\u56fe\u53cd\u8f6c.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/","title":"Glow-TTS","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search - \u4f5c\u8005:   - [Jaehyeon Kim](../../Authors/Jaehyeon_Kim.md)   - [Sungwon Kim](../../Authors/Sungwon_Kim.md)   - [Jungil Kong](../../Authors/Jungil_Kong.md)   - [Sungroh Yoon](../../Authors/Sungroh_Yoon.md) - \u673a\u6784:   - [Kakao Enterprise](../../Institutions/KakaoEnterprise.md)   - [Seoul National University](../../Institutions/SeoulNationalUniversity.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2020.05.22 ArXiv v1   - \u9884\u5370\u65f6\u95f4: 2020.10.23 ArXiv v2   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.11 - \u53d1\u8868:   - [NeurIPS](../../Publications/NeurIPS.md) - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2005.11129)   - [DOI](https://proceedings.neurips.cc/paper/2020/hash/5c3b99e8f92532e5ad1556e53ceea00c-Abstract.html)   - [Github](https://github.com/jaywalnut310/glow-tts)   - [Demo](https://jaywalnut310.github.io/glow-tts-demo/index.html) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md) - \u9875\u6570: 11 - \u5f15\u7528: 34 - \u88ab\u5f15: 429"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#abstract","title":"Abstract","text":"<p>Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#1introduction","title":"1.Introduction","text":"<p>Text-to-speech (TTS) is a task in which speech is generated from text, and deep-learning-based TTS models have succeeded in producing natural speech. Among neural TTS models, autoregressive models, such as Tacotron 2 and Transformer TTS, have shown state-of-the-art performance. Despite the high synthesis quality of autoregressive TTS models, there are a few difficulties in deploying them directly in real-time services. As the inference time of the models grows linearly with the output length, undesirable delay caused by generating long utterances can be propagated to the multiple pipelines of TTS systems without designing sophisticated frameworks [14]. In addition, most of the autoregressive models show a lack of robustness in some cases (FastSpeech, ParaNet). For example, when an input text includes repeated words, autoregressive TTS models sometimes produce serious attention errors.</p> <p>To overcome such limitations of the autoregressive TTS models, parallel TTS models, such as FastSpeech, have been proposed. These models can synthesize mel-spectrograms significantly faster than the autoregressive models. In addition to the fast sampling, FastSpeech reduces the failure cases of synthesis, such as mispronouncing, skipping, or repeating words, by constraining its alignment to be monotonic. However, to train the parallel TTS models, well-aligned attention maps between text and speech are necessary. Recently proposed parallel models extract attention maps from their external aligners, pre-trained autoregressive TTS models (ParaNet, FastSpeech). Therefore, the performance of the models critically depends on that of the external aligners.</p> <p>In this work, we eliminate the necessity of any external aligner and simplify the training procedure of parallel TTS models. Here, we propose Glow-TTS, a flow-based generative model for parallel TTS that can internally learn its own alignment.</p> <p>By combining the properties of flows and dynamic programming, Glow-TTS efficiently searches for the most probable monotonic alignment between text and the latent representation of speech. The proposed model is directly trained to maximize the log-likelihood of speech with the alignment. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing flows enables fast, diverse, and controllable speech synthesis.</p> <p>Glow-TTS can generate mel-spectrograms 15.7 times faster than the autoregressive TTS model, Tacotron 2, while obtaining comparable performance. As for robustness, the proposed model outperforms Tacotron 2 significantly when input utterances are long. By altering the latent representation of speech, we can synthesize speech with various intonation patterns and regulate the pitch of speech. We further show that our model can be extended to a multi-speaker setting with only a few modifications. Our source code and synthesized audio samples are publicly available.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#2related-works","title":"2.Related Works","text":""},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#alignment-estimation-between-text-and-speech","title":"Alignment Estimation between Text and Speech.","text":"<p>Traditionally, hidden Markov models (HMMs) In speech have been used to estimate unknown alignments between text and speech [19, 25]. recognition, CTC has been proposed as a method of alleviating the downsides of HMMs, such as the assumption of conditional independence over observations, through a discriminative neural network model [6]. Both methods above can efficiently estimate alignments through forward-backward algorithms with dynamic programming. In this work, we introduce a similar dynamic programming method to search for the most probable alignment between text and the latent representation of speech, where our modeling differs from CTC in that it is generative, and from HMMs in that it can sample sequences in parallel without the assumption of conditional independence over observations.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#text-to-speech-models","title":"Text-to-Speech Models.","text":"<p>TTS models are a family of generative models that synthesize speech from text. TTS models, such as Tacotron 2, DeepVoice3 and Transformer TTS, generate a mel-spectrogram from text, which is comparable to that of the human voice. Enhancing the expressiveness of TTS models has also been studied. Auxiliary embedding methods have been proposed to generate diverse speech by controlling factors such as intonation and rhythm [Tacotron, 32], and some works have aimed at synthesizing speech in the voices of various speakers [9, DeepVoice2]. Recently, several works have proposed methods to generate mel-spectrogram frames in parallel. FastSpeech, and ParaNet significantly speed up mel-spectrogram generation over autoregressive TTS models, while preserving the quality of synthesized speech. However, both parallel TTS models need to extract alignments from pre-trained autoregressive TTS models to alleviate the length mismatch problem between text and speech. Our Glow-TTS is a standalone parallel TTS model that internally learns to align text and speech by leveraging the properties of flows and dynamic programming.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#flow-based-generative-models","title":"Flow-based Generative Models.","text":"<p>Flow-based generative models have received a lot of attention due to their advantages [7, 4, Blow]. They can estimate the exact likelihood of the data by applying invertible transformations. Generative flows are simply trained to maximize the likelihood. In addition to efficient density estimation, the transformations proposed in [2, 3, 12] guarantee fast and efficient sampling. WaveGlow and FloWaveNet introduced these transformations for speech synthesis to overcome the slow sampling speed of an autoregressive vocoder, WaveNet. Their proposed models both synthesized raw audio significantly faster than WaveNet. By applying these transformations, Glow-TTS can synthesize a mel-spectrogram given text in parallel.</p> <p>In parallel with our work, AlignTTS, Flowtron, and Flow-TTS have been proposed. AlignTTS and Flow-TTS are parallel TTS models without the need of external aligners, and Flowtron is a flow-based model which shows the ability of style transfer and controllability of speech variation. However, AlignTTS is not a flow-based model but a feed-forward network, and Flowtron and Flow TTS use soft attention modules. By employing both hard monotonic alignments and generative flows, our model combines the best of both worlds in terms of robustness, diversity and controllability.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#3methodology","title":"3.Methodology\u00b7\u65b9\u6cd5","text":"<p>Inspired by the fact that a human reads out text in order, without skipping any words, we design Glow-TTS to generate a mel-spectrogram conditioned on a monotonic and non-skipping alignment between text and speech representations. In Section 3.1, we formulate the training and inference procedures of the proposed model, which are also illustrated in Figure 1. We present our alignment search algorithm in Section 3.2, which removes the necessity of external aligners from training, and the architecture of all components of Glow-TTS (i.e., the text encoder, duration predictor, and flow-based decoder) is covered in Section 3.3.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#31training-and-inference-procedures","title":"3.1.Training and Inference Procedures","text":"<p>Glow-TTS models the conditional distribution of mel-spectrograms $P_{X} (x|c)$ by transforming a conditional prior distribution $P_{Z}(z|c)$ through the flow-based decoder $f_{dec}: z \\to x$, where $x$ and $c$ denote the input mel spectrogram and text sequence, respectively. By using the change of variables, we can calculate the exact log-likelihood of the data as follows:</p> <p>$$   \\log P_{X}(x|c) = \\log P_Z(z|c) + \\log \\left|\\text{det}\\dfrac{\\partial f_{dec}^{-1}(x)}{\\partial x}\\right|.\\tag{01} $$</p> <p>We parameterize the data and prior distributions with network parameters $\\theta$ and an alignment function $A$. The prior distribution $P_Z$ is the isotropic multivariate Gaussian distribution and all the statistics of the prior distribution, $\\mu$ and $\\sigma$, are obtained by the text encoder $f_{enc}$. The text encoder maps the text condition $c = c_{1:T_{text}}$ into the statistics, $\\mu = \\mu_{1:T_{text}}$ and $\\sigma = \\sigma_{1:T_{text}}$, where $T_{text}$ denotes the length of the text input. In our formulation, the alignment function $A$ stands for the mapping from the index of the latent representation of speech to that of statistics from $f_{enc}: A(j) = i$ if $z_j \\sim \\mathcal{N} (z_j; \\mu_i, \\sigma_i)$. We assume the alignment function $A$ to be monotonic and subjective to ensure Glow-TTS not to skip or repeat the text input. Then, the prior distribution can be expressed as follows:</p> <p>$$   \\log P_{Z}(z|c;\\theta,A) = \\sum_{j=1}^{T_{mel}} \\log \\mathcal{N}(z_j; \\mu_{A(j)}, \\sigma_{A(j)}).\\tag{02} $$</p> <p>where $T_{mel}$ denotes the length of the input mel-spectrogram.</p> <p>Our goal is to find the parameters $\\theta$ and the alignment $A$ that maximize the log-likelihood of the data, as in Equation 3. However, it is computationally intractable to find the global solution. To tackle the intractability, we reduce the search space of the parameters and alignment by decomposing the objective into two subsequent problems: (i) searching for the most probable monotonic alignment $A^{}$ with respect to the current parameters \u03b8, as in Equation 4, and (ii) updating the parameters \u03b8 to maximize the log-likelihood $\\log p_{X} (x|c; \\theta, A^{})$. In practice, we handle these two problems using an iterative approach. At each training step, we first find $A^{*}$, and then update \u03b8 using the gradient descent. The iterative procedure is actually one example of widely used Viterbi training [19], which maximizes log likelihood of the most likely hidden alignment. The modified objective does not guarantee the global solution of Equation 3, but it still provides a good lower bound of the global solution.</p> <p>$$   \\max_{\\theta,A} Loss(\\theta, A) = \\max_{\\theta,A} \\log P_{X}(x|c;\\theta,A).\\tag{03} $$</p> <p>$$   A^{*} =\\arg\\max_{A} \\log P_{X}(x|c;\\theta,A)=\\arg\\max_{A}\\sum_{j=1}^{T_{mel}} \\log\\mathcal{N}(z_j; \\mu_{A(j)}, \\sigma_{A(j)}).\\tag{04} $$</p> <p>To solve the alignment search problem (i), we introduce an alignment search algorithm, monotonic alignment search (MAS), which we describe in Section 3.2. To estimate the most probable monotonic alignment $A^{}$ at inference, we also train the duration predictor $f_{dur}$ to match the duration label calculated from the alignment $A^{}$, as in Equation 5. Following the architecture of FastSpeech, we append the duration predictor on top of the text encoder and train it with the mean squared error loss (MSE) in the logarithmic domain. We also apply the stop gradient operator <code>sg[\u00b7]</code>, which removes the gradient of input in the backward pass [30], to the input of the duration predictor to avoid affecting the maximum likelihood objective. The loss for the duration predictor is described in Equation 6.</p> <p>$$   d_i = \\sum_{j=1}^{T_{mel}} 1_{A^{*}(j)=i},\\quad i=1,\\cdots,T_{text}\\tag{05} $$</p> <p>$$   Loss_{dur} = MSE(f_{dur}(sg[f_{enc}(c)]), d),\\tag{06} $$</p> <p>During inference, as shown in Figure 1b, the statistics of the prior distribution and alignment are predicted by the text encoder and duration predictor. Then, a latent variable is sampled from the prior distribution, and a mel-spectrogram is synthesized in parallel by transforming the latent variable through the flow-based decoder.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#32monotonic-alignment-search","title":"3.2.Monotonic Alignment Search","text":"<p>As mentioned in Section 3.1, MAS searches for the most probable monotonic alignment between the latent variable and the statistics of the prior distribution, which are came from the input speech and text, respectively. Figure 2a shows one example of possible monotonic alignments.</p> <p>We present our alignment search algorithm in Algorithm 1. We first derive a recursive solution over partial alignments and then find the entire alignment.</p> <p>Let Qi,j be the maximum log-likelihood where the statistics of the prior distribution and the latent variable are partially given up to the i-th and j-th elements, respectively. Then, Qi,j can be recursively formulated with Qi\u22121,j\u22121 and Qi,j\u22121, as in Equation 7, because if the last elements of partial sequences, zj and {\\mui, \\sigmai}, are aligned, the previous latent variable zj\u22121 should have been aligned to either {\\mui\u22121, \\sigmai\u22121} or {\\mui, \\sigmai} to satisfy monotonicity and surjection.</p> <p>$$   Q_{i,j} =\\max_{A}\\sum_{k=1}^{j} \\log\\mathcal{N}(z_k; \\mu_{A(k)}, \\sigma_{A(k)}) =\\max (Q_{i-1,j-1}, Q_{i,j-1}) + \\log \\mathcal{N}(z_j; \\mu_{i}, \\sigma_{i}).\\tag{07} $$</p> <p>This process is illustrated in Figure 2b. We iteratively calculate all the values of Q up to QTtext,Tmel . Similarly, the most probable alignment A^{} can be obtained by determining which Q value is greater in the recurrence relation, Equation 7. Thus, A^{} can be found efficiently with dynamic programming by caching all Q values; all the values of A^{} are backtracked from the end of the alignment, A^{}(Tmel) = Ttext, as in Figure 2c. The time complexity of the algorithm is O(Ttext \u00d7 Tmel). Even though the algorithm is difficult to parallelize, it runs efficiently on CPU without the need for GPU executions. In our experiments, it spends less than 20 ms on each iteration, which amounts to less than 2% of the total training time. Furthermore, we do not need MAS during inference, as the duration predictor is used to estimate the alignment.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#33model-architecture","title":"3.3.Model Architecture","text":"<p>Each component of Glow-TTS is briefly explained in this section, and the overall model architecture and model configurations are shown in Appendix A.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#decoder","title":"Decoder.","text":"<p>The core part of Glow-TTS is the flow-based decoder. During training, we need to efficiently transform a mel-spectrogram into the latent representation for maximum likelihood estimation and our internal alignment search. During inference, it is necessary to transform the prior distribution into the mel-spectrogram distribution efficiently for parallel decoding. Therefore, our decoder is composed of a family of flows that can perform forward and inverse transformation in parallel. Specifically, our decoder is a stack of multiple blocks, each of which consists of an activation normalization layer, invertible 1x1 convolution layer, and affine coupling layer. We follow the affine coupling layer architecture of WaveGlow, except we do not use the local conditioning (WaveNet).</p> <p>For computational efficiency, we split 80-channel mel-spectrogram frames into two halves along the time dimension and group them into one 160-channel feature map before the flow operations. We also modify 1x1 convolution to reduce the time-consuming calculation of the Jacobian determinant.</p> <p>Before every 1x1 convolution, we split the feature map into 40 groups along the channel dimension and perform 1x1 convolution on them separately. To allow channel mixing in each group, the same number of channels are extracted from one half of the feature map separated by coupling layers and the other half, respectively. A detailed description can be found in Appendix A.1.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#encoder-and-duration-predictor","title":"Encoder and Duration Predictor.","text":"<p>We follow the encoder structure of Transformer TTS with two slight modifications. We remove the positional encoding and add relative position representations [22] into the self-attention modules instead. We also add a residual connection to the encoder pre-net. To estimate the statistics of the prior distribution, we append a linear projection layer at the end of the encoder. The duration predictor is composed of two convolutional layers with ReLU activation, layer normalization, and dropout followed by a projection layer. The architecture and configuration of the duration predictor are the same as those of FastSpeech.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#4experiments","title":"4.Experiments","text":"<p>To evaluate the proposed methods, we conduct experiments on two different datasets. For the single speaker setting, a single female speaker dataset, LJSpeech, is used, which consists of 13,100 short audio clips with a total duration of approximately 24 hours. We randomly split the dataset into the training set (12,500 samples), validation set (100 samples), and test set (500 samples). For the multi-speaker setting, the train-clean-100 subset of the LibriTTS corpus is used, which consists of audio recordings of 247 speakers with a total duration of about 54 hours. We first trim the beginning and ending silence of all the audio clips and filter out the data with text lengths over 190. We then split it into the training (29,181 samples), validation (88 samples), and test sets (442 samples). Additionally, out-of-distribution text data are collected for the robustness test. Similar to [1], we extract 227 utterances from the first two chapters of the book Harry Potter and the Philosopher\u2019s Stone. The maximum length of the collected data exceeds 800.</p> <p>We compare Glow-TTS with the best publicly available autoregressive TTS model, Tacotron 2. For all the experiments, phonemes are chosen as input text tokens. We follow the configuration for the mel-spectrogram of WaveGlow, and all the generated mel-spectrograms from both models are transformed to raw waveforms through the pre-trained vocoder, WaveGlow.</p> <p>During training, we simply set the standard deviation \\sigma of the learnable prior to be a constant 1. Glow-TTS was trained for 240K iterations using the Adam optimizer with the Noam learning rate schedule (Transformer). This required only 3 days with mixed precision training on two NVIDIA V100 GPUs.</p> <p>To train muli-speaker Glow-TTS, we add the speaker embedding and increase the hidden dimension. The speaker embedding is applied in all affine coupling layers of the decoder as a global conditioning [29]. The rest of the settings are the same as for the single speaker setting. For comparison, We also trained Tacotron 2 as a baseline, which concatenates the speaker embedding with the encoder output at each time step. We use the same model configuration as the single speaker one. All multi-speaker models were trained for 960K iterations on four NVIDIA V100 GPUs.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#5results","title":"5.Results","text":""},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#51audio-quality","title":"5.1.Audio Quality","text":"<p>We measure the mean opinion score (MOS) via Amazon Mechanical Turk to compare the quality of all audio clips, including ground truth (GT), and the synthesized samples; 50 sentences are randomly chosen from the test set for the evaluation. The results are shown in Table 1. The quality of speech converted from the GT mel-spectrograms by the vocoder (4.19\u00b10.07) is the upper limit of the TTS models. We vary the standard deviation (i.e., temperature T ) of the prior distribution at inference; Glow-TTS shows the best performance at the temperature of 0.333.</p> <p>For any temperature, it shows comparable performance to Tacotron 2. We also analyze side-by-side evaluation between Glow-TTS and Tacotron 2. The result is shown in Appendix B.2.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#52sampling-speed-and-robustness","title":"5.2.Sampling Speed and Robustness","text":""},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#sampling-speed","title":"Sampling Speed.","text":"<p>We use the test set to measure the sampling speed of the models. Figure 3a demonstrates that the inference time of our model is almost constant at 40ms, regardless of the length, whereas that of Tacotron 2 linearly increases with the length due to the sequential sampling. On average, Glow-TTS shows a 15.7 times faster synthesis speed than Tacotron 2.</p> <p>We also measure the total inference time for synthesizing 1-minute speech in an end-to-end setting with Glow-TTS and WaveGlow. The total inference time to synthesize the 1-minute speech is only 1.5 seconds4 and the inference time of Glow-TTS and WaveGlow accounts for 4% and 96% of the total inference time, respectively; the inference time of Glow-TTS takes only 55ms to synthesize the mel-spectrogram, which is negligible compared to that of the vocoder.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#robustness","title":"Robustness.","text":"<p>We measure the character error rate (CER) of the synthesized samples from long utterances in the book Harry Potter and the Philosopher\u2019s Stone via the Google Cloud Speech-ToText API.5 Figure 3b shows that the CER of Tacotron 2 starts to grow when the length of input characters exceeds about 260. On the other hand, even though our model has not seen such long texts during training, it shows robustness to long texts. We also analyze attention errors on specific sentences. The results are shown in Appendix B.1.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#53diversity-and-controllability","title":"5.3.Diversity and Controllability","text":"<p>Because Glow-TTS is a flow-based generative model, it can synthesize diverse samples; each latent representation z sampled from an input text is converted to a different mel-spectrogram $f_{dec}(z)$. Specifically, the latent representation $z \\sim \\mathcal{N}(\\mu, T)$ can be expressed as follows:</p> <p>$$   z = \\mu + \\varepsilon * T,\\tag{08} $$</p> <p>where $\\varepsilon$ denotes a sample from the standard normal distribution and $\\mu$ and $T$ denote the mean and standard deviation (i.e., temperature) of the prior distribution, respectively.</p> <p>To decompose the effect of $\\varepsilon$ and $T$, we draw pitch tracks of synthesized samples in Figure 4 by varying $\\varepsilon$ and $T$ one at a time. Figure 4a demonstrates that diverse stress or intonation patterns of speech arise from $\\varepsilon$, whereas Figure 4b demonstrates that we can control the pitch of speech while maintaining similar intonation by only varying $T$. Additionally, we can control speaking rates of speech by multiplying a positive scalar value across the predicted duration of the duration predictor. The result is visualized in Figure 5; the values multiplied by the predicted duration are 1.25, 1.0, 0.75, and 0.5.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#54multi-speaker-tts","title":"5.4.Multi-Speaker TTS","text":""},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#audio-quality","title":"Audio Quality.","text":"<p>We measure the MOS as done in Section 5.1; we select 50 speakers, and randomly sample one utterance per a speaker from the test set for evaluation. The results are presented in Table 2. The quality of speech converted from the GT mel-spectrograms (4.22\u00b10.07) is the upper limit of the TTS models. Our model with the best configuration achieves 3.45 MOS, which results in comparable performance to Tacotron 2.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#speaker-dependent-duration","title":"Speaker-Dependent Duration.","text":"<p>Figure 6a shows the pitch tracks of generated speech from the same sentence with different speaker identities. As the only difference in input is speaker identities, the result demonstrates that our model differently predicts the duration of each input token with respect to the speaker identities.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#voice-conversion","title":"Voice Conversion.","text":"<p>As we do not provide any speaker identity into the encoder, the prior distribution is forced to be independent from speaker identities. In other words, Glow-TTS learns to disentangle the latent representation z and the speaker identities. To investigate the degree of disentanglement, we transform a GT mel-spectrogram into the latent representation with the correct speaker identity and then invert it with different speaker identities. The detailed method can be found in Appendix B.3 The results are presented in Figure 6b. It shows that converted samples have different pitch levels while maintaining a similar trend.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#6conclusion","title":"6.Conclusion","text":"<p>In this paper, we proposed a new type of parallel TTS model, Glow-TTS. Glow-TTS is a flow-based generative model that is directly trained with maximum likelihood estimation. As the proposed model finds the most probable monotonic alignment between text and the latent representation of speech on its own, the entire training procedure is simplified without the necessity of external aligners. In addition to the simple training procedure, we showed that Glow-TTS synthesizes mel-spectrograms 15.7 times faster than the autoregressive baseline, Tacotron 2, while showing comparable performance. We also demonstrated additional advantages of Glow-TTS, such as the ability to control the speaking rate or pitch of synthesized speech, robustness, and extensibility to a multi-speaker setting. Thanks to these advantages, we believe the proposed model can be applied in various TTS tasks such as prosody transfer or style modeling.</p>"},{"location":"TTS/Models/TTS2_Acoustic/2020.05.22_Glow-TTS/#broader-impact","title":"Broader Impact","text":"<p>In this paper, researchers introduce Glow-TTS, a diverse, robust and fast text-to-speech (TTS) synthesis model. Neural TTS models including Glow-TTS, could be applied in many applications which require naturally synthesized speech. Some of the applications are AI voice assistant services, audiobook services, advertisements, automotive navigation systems and automated answering services. Therefore, by utilizing the models for synthesizing natural sounding speech, the providers of such applications could improve user satisfaction. In addition, the fast synthesis speed of the proposed model could be beneficial for some service providers who provide real time speech synthesis services. However, because of the ability to synthesize natural speech, the TTS models could also be abused through cyber crimes such as fake news or phishing. It means that TTS models could be used to impersonate voices of celebrities for manipulating behaviors of people, or to imitate voices of someone\u2019s friends or family for fraudulent purposes. With the development of speech synthesis technology, the growth of studies to detect real human voice from synthesized voices seems to be needed. Neural TTS models could sometimes synthesize undesirable speech with slurry or wrong pronunciations. Therefore, it should be used carefully in some domain where even a single pronunciation mistake is critical such as news broadcast. Additional concern is about the training data. Many corpus for speech synthesis contain speech data uttered by a handful of speakers. Without the detailed consideration and restriction about the range of uses the TTS models have, the voices of the speakers could be overused than they might expect.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/","title":"2019.10.08 MelGAN","text":"<p>@import \"../../style.less\"</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#melgan-generative-adversarial-networks-for-conditional-waveform-synthesis","title":"MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis - \u4f5c\u8005:   - Kundan Kumar   - Rithesh Kumar   - Thibault de Boissiere   - Lucas Gestin   - Wei Zhen Teoh   - Jose Sotelo   - Alexandre de Brebisson   - Yoshua Bengio   - Aaron Courville - \u673a\u6784:   - Lyrebird AI   - University of Montreal - \u65f6\u95f4:   - 2019.10.08 ArXiv v1   - 2019.10.28 ArXiv v2   - 2019.12.09 ArXiv v3   - NeurIPS2019 - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/1910.06711)   - [NeurIPS](https://proceedings.neurips.cc/paper/2019/hash/6804c9bca0a615bdb9374d00a9fcba59-Abstract.html)   - [Github](https://github.com/descriptinc/melgan-neurips)   - [Demo](https://melgan-neurips.github.io) - \u6807\u7b7e:   - Vocoder (\u58f0\u7801\u5668)   - GAN (\u751f\u6210\u5bf9\u6297\u7f51\u7edc)   - Adversarial Learning (\u5bf9\u6297\u5b66\u4e60)   - OpenSource (\u5f00\u6e90) - \u9875\u6570: 12 - \u5f15\u7528:  - \u88ab\u5f15: 954"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#0abstract","title":"0\u00b7Abstract\u00b7\u6458\u8981","text":"<p>Previous works (\"Adversarial Audio Synthesis\" (2018); GANSynth (2019)) have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#1introduction","title":"1\u00b7Introduction\u00b7\u5f15\u8a00","text":"<p>Modelling raw audio is a particularly challenging problem because of the high temporal resolution of the data (usually at least 16,000 samples per second) and the presence of structure at different timescales with short and long-term dependencies. Thus, instead of modelling the raw temporal audio directly, most approaches simplify the problem by modelling a lower-resolution representation that can be efficiently computed from the raw temporal signal. Such a representation is typically chosen to be easier to model than raw audio while preserving enough information to allow faithful inversion back to audio. In the case of speech, aligned linguistic features (WaveNet (2016)) and mel-spectrograms (Tacotron2 (2017); DeepVoice2 (2017)) are two commonly used intermediate representations. Audio modelling is thus typically decomposed into two stages. The first models the intermediate representation given text as input. The second transforms the intermediate representation back to audio. In this work, we focus on the latter stage, and choose mel-spectrogram as the intermediate representation. (Our methods can likely be used with other representations but this is beyond the scope of this paper.) Current approaches to mel-spectrogram inversion can be categorized into three distinct families: pure signal processing techniques, autoregressive and non-autoregressive neural networks. We describe these three main lines of research in the following paragraphs.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#pure-signal-processing-approaches","title":"Pure Signal Processing Approaches","text":"<p>Different signal processing approaches have been explored to find some convenient low-resolution audio representations that can both be easily modelled and efficiently converted back to temporal audio. For example, the Griffin-Lim (Griffin &amp; Lim, 1984) algorithm allows one to efficiently decode an STFT sequence back to the temporal signal at the cost of introducing strong, robotic artifacts as noted in Tacotron (2017). More sophisticated representations and signal processing techniques have been investigated. For instance, the WORLD vocoder (2016) introduces an intermediate representation tailored to speech modelling based on mel-spectrogram-like features. The WORLD vocoder is paired with a dedicated signal processing algorithm to map the intermediate representation back to the original audio. It has been successfully used to carry out text-to-speech synthesis, for example in Char2Wav, where WORLD vocoder features are modelled with an attention-based recurrent neural network (Sotelo et al., 2017; Tacotron2 (2017); DeepVoice3 (2017)). The main issue with these pure signal processing methods is that the mapping from intermediate features to audio usually introduces noticeable artifacts.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#autoregressive-neural-networks-based-models","title":"Autoregressive neural-networks-based models","text":"<p>WaveNet (2016) is a fully convolutional autoregressive sequence model that produces highly realistic speech samples conditioned on linguistic features that are temporally aligned with the raw audio. It is also capable of generating high quality unconditional speech and music samples. SampleRNN (Mehri et al., 2016) is an alternative architecture to perform unconditional waveform generation which explicitly models raw audio at different temporal resolutions using multi-scale recurrent neural networks. WaveRNN (Kalchbrenner et al., 2018) is a faster auto-regressive model based on a simple, single-layer recurrent neural network. WaveRNN introduces various techniques, such as sparsification and subscale generations to further increase synthesis speed. These methods have produced state-of-the-art results in text-to-speech synthesis (Sotelo et al., 2017; Tacotron2 (2017); DeepVoice3 (2017)) and other audio generation tasks (Engel et al., 2017). Unfortunately, inference with these models is inherently slow and inefficient because audio samples must be generated sequentially. Thus auto-regressive models are usually not suited for real-time applications.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#non-autoregressive-models","title":"Non autoregressive models","text":"<p>Recently, significant efforts have been dedicated to the development of non-autoregressive models to invert low-resolution audio representations. These models are orders of magnitudes faster than their auto-regressive counterparts because they are highly parallelizable and can fully exploit modern deep learning hardware (such as GPUs and TPUs). Two distinct methods have emerged to train such models.  1. Parallel WaveNet (2017) and Clarinet (2018) distill a trained auto-regressive decoder into a flow-based convolutional student model. The student is trained using a probability distillation objective based on the Kulback-Leibler divergence: KL[Pstudent||Pteacher], along with an additional perceptual loss terms.  2. WaveGlow (2018) is a flow-based generative model based on Glow (2018). WaveGlow is a very high capacity generative flow consisting of 12 coupling and 12 invertible 1x1 convolutions, with each coupling layer consisting of a stack of 8 layers of dilated convolutions. The authors note that it requires a week of training on 8 GPUs to get good quality results for a single speaker model. While inference is fast on the GPU, the large size of the model makes it impractical for applications with a constrained memory budget.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#gans-for-audio","title":"GANs for audio","text":"<p>One family of methods that has so far been little explored for audio modelling are Generative Adversarial Networks (GANs) (2014). GANs have made steady progress in unconditional image generation (Gulrajani et al., 2017; Karras et al., 2017, 2018), image-to-image translation (Isola et al., 2017; Zhu et al., 2017; Wang et al., 2018b) and video-to-video synthesis (Chan et al., 2018; Wang et al., 2018a). Despite their huge success in computer vision, we have not seen as much progress in using GANs for audio modelling. GANSynth (2019) use GANs to generate musical timbre by modelling STFT magnitudes and phase angles instead of modelling raw waveform directly. Neekhara et al. (2019) propose using GANs to learn mappings from mel-spectrogram to simple magnitude spectrogram, which is to be combined with phase estimations to recover raw audio waveform. Yamamoto et al. (2019) use GANs to distill an autoregressive model that generates raw speech audio, however their results show that adversarial loss alone is not sufficient for high quality waveform generation; it requires a KL-divergence based distillation objective as a critical component. To this date, making them work well in this domain has been challenging (\"Adversarial Audio Synthesis\" (2018)).</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#main-contributions","title":"Main Contributions","text":"<ul> <li>We introduce MelGAN, a non-autoregressive feed-forward convolutional architecture to perform audio waveform generation in a GAN setup. To the best of our knowledge, this is the first work that successfully trains GANs for raw audio generation without additional distillation or perceptual loss functions while still yielding a high quality text-to-speech synthesis model.</li> <li>We show that autoregressive models can be readily replaced with a fast and parallel MelGAN decoder for raw waveform generation through experiments in universal music translation, text-to-speech generation and unconditional music synthesis albeit with slight quality degradation.</li> <li>We also show that MelGAN is substantially faster than other mel-spectrogram inversion alternatives. In particular, it is 10 times faster than the fastest available model to date (WaveGlow (2018)) without considerable degradation in audio quality.</li> </ul>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#2methodology","title":"2\u00b7Methodology\u00b7\u65b9\u6cd5\u8bba","text":"<p>In this section, we describe our generator and discriminator architectures for mel-spectrogram inversion. We describe the core components of the model and discuss modifications to perform unconditional audio synthesis. We compare the proposed model with competing approaches in terms of number of parameters and inference speed on both CPU and GPU. Fig.01 shows the overall architecture.</p> <p></p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#21generator","title":"2.1\u00b7Generator","text":""},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#architecture","title":"Architecture","text":"<p>Our generator is a fully convolutional feed-forward network with mel-spectrogram s as input and raw waveform x as output. Since the mel-spectrogram (used for all experiments) is at a 256\u00d7 lower temporal resolution, we use a stack of transposed convolutional layers to upsample the input sequence. Each transposed convolutional layer is followed by a stack of residual blocks with dilated convolutions. Unlike traditional GANs, our generator does not use a global noise vector as input. We noticed in our experiments that there is little perceptual difference in the generated waveforms when additional noise is fed to the generator. This is a counter-intuitive result because the inversion of s \u2192 x involves a one-to-many mapping since s is a lossy-compression of x. However, this finding is consistent with Mathieu et al. (2015) and Isola et al. (2017), which show that noise input is not important if the conditioning information is very strong.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#induced-receptive-field","title":"Induced receptive field","text":"<p>In convolutional neural network based generators for images, there is an inductive bias that pixels which are spatially close-by are correlated because of high overlap among their induced receptive fields. We design our generator architecture to put an inductive bias that there is long range correlation among the audio timesteps. We added residual blocks with dilations after each upsampling layer, so that temporally far output activations of each subsequent layer has significant overlapping inputs. Receptive field of a stack of dilated convolution layers increases exponentially with the number of layers. Similar to WaveNet (2016), incorporating these in our generator allows us to efficiently increase the induced receptive fields of each output time-step. This effectively implies larger overlap in the induced receptive field of far apart time-steps, leading to better long range correlation.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#checkerboard-artifacts","title":"Checkerboard artifacts","text":"<p>As noticed in Odena et al. (2016), deconvolutional generators are susceptible to generating \"checkerboard\" patterns if the kernel-size and stride of the transposed convolutional layers are not carefully chosen. \"Adversarial Audio Synthesis\" (2018) examines this for raw waveform generation and finds that such repeated patterns lead to audible high frequency hissing noise. We solve this problem by carefully choosing the kernel-size and stride for our deconvolutional layers as a simpler alternative to PhaseShuffle layer introduced in \"Adversarial Audio Synthesis\" (2018). Following Odena et al. (2016), we use kernel-size as a multiple of stride. Another source of such repeated patterns, can be the dilated convolution stack if dilation and kernel size are not chosen correctly. We make sure that the dilation grows as a power of the kernel-size such that the receptive field of the stack looks like a fully balanced (seeing input uniformly) and symmetric tree with kernel-size as the branching factor.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#normalization-technique","title":"Normalization Technique","text":"<p>We noticed that the choice of normalization technique for the generator was extremely crucial for sample quality. Popular conditional GAN architectures for image generation (Isola et al., 2017; Wang et al., 2018b) use instance normalization (Ulyanov et al., 2016) in all the layers of the generator. However, in the case of audio generation we found that instance normalization washes away important important pitch information, making the audio sound metallic. We also obtained poor results when applying spectral normalization (Miyato et al., 2018) on the generator as suggested in Zhang et al. (2018); Park et al. (2019). We believe that the strong Lipshitz constraint on the discriminator impacts the feature matching objective (explained in Section 3.2) used to train the generator. Weight normalization (Salimans &amp; Kingma, 2016) worked best out of all the available normalization techniques since it does not limit the capacity of the discriminator or normalize the activations. It simply reparameterizes the weight matrices by decoupling the scale of the weight vector from the direction, to have better training dynamics. We therefore use weight normalization in all layers of the generator.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#22discriminator","title":"2.2\u00b7Discriminator","text":""},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#multi-scale-architecture","title":"Multi-Scale Architecture","text":"<p>Following Wang et al. (2018b), we adopt a multi-scale architecture with 3 discriminators (D1, D2, D3) that have identical network structure but operate on different audio scales. D1 operates on the scale of raw audio, whereas D2, D3 operate on raw audio downsampled by a factor of 2 and 4 respectively. The downsampling is performed using strided average pooling with kernel size 4. Multiple discriminators at different scales are motivated from the fact that audio has structure at different levels. This structure has an inductive bias that each discriminator learns features for different frequency range of the audio. For example, the discriminator operating on downsampled audio, does not have access to high frequency component, hence, it is biased to learn discriminative features based on low frequency components only.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#window-based-objective","title":"Window-Based Objective","text":"<p>Each individual discriminator is a Markovian window-based discriminator (analogues to image patches, Isola et al. (2017)) consisting of a sequence of strided convolutional layers with large kernel size. We utilize grouped convolutions to allow the use of larger kernel sizes while keeping number of parameters small. While a standard GAN discriminator learns to classify between distributions of entire audio sequences, window-based discriminator learns to classify between distribution of small audio chunks. Since the discriminator loss is computed over the overlapping windows where each window is very large (equal to the receptive field of the discriminator), the MelGAN model learns to maintain coherence across patches. We chose window-based discriminators since they have been shown to capture essential high frequency structure, require fewer parameters, run faster and can be applied to variable length audio sequences. Similar to the generator, we use weight normalization in all layers of the discriminator.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#23training-objective","title":"2.3\u00b7Training Objective","text":"<p>To train the GAN, we use the hinge loss version of the GAN objective (Lim &amp; Ye, 2017; Miyato et al., 2018). We also experimented with the least-squares (LSGAN) formulation (Mao et al., 2017) and noticed slight improvements with the hinge version.</p> <p>where x represents the raw waveform, s represents the conditioning information (eg. mel-spectrogram) and z represents the gaussian noise vector.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#feature-matching","title":"Feature Matching","text":"<p>In addition to the discriminator\u2019s signal, we use a feature matching objective (Larsen et al., 2015) to train the generator. This objective minimizes the L1 distance between the discriminator feature maps of real and synthetic audio. Intuitively, this can be seen as a learned similarity metric, where a discriminator learns a feature space that discriminates the fake data from real data. It is worth noting that we do not use any loss in the raw audio space. This is counter to other conditional GANs (Isola et al., 2017) where L1 loss is used to match conditionally generated images and their corresponding ground-truths, to enforce global coherence. In fact, in our case adding L1 loss in audio space introduces audible noise that hurts audio quality.</p> <p>For simplicity of notation, D(i) k represents the ith layer feature map output of the kth discriminator block, Ni denotes the number of units in each layer. Feature matching is similar to the perceptual loss (Dosovitskiy &amp; Brox, 2016; Gatys et al., 2016; Johnson et al., 2016). In our work, we use feature matching at each intermediate layer of all discriminator blocks.</p> <p>We use the following final objective to train the generator, with \u03bb = 10 as in (Wang et al., 2018b):</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#24number-of-parameters-and-inference-speed","title":"2.4\u00b7Number of parameters and inference speed","text":"<p>The inductive biases incorporated in our architecture make the overall model significantly smaller than competing models in terms of number of parameters. Being non-autoregressive and fully convolutional, our model is very fast at inference time, capable of running at a frequency of 2500kHz on GTX1080 Ti GPU in full precision (more than 10\u00d7 faster than the fastest competing model), and 50kHz on CPU (more than 25\u00d7 faster than the fastest competing model). We believe that our model is also well-suited for hardware specific inference optimization (such as half precision on Tesla V100 (Jia et al., 2018; Dosovitskiy &amp; Brox, 2016) and quantization (as done in Arik et al. (2017)) which will further boost inference speed. Table 1 shows the detailed comparison.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#3results","title":"3\u00b7Results","text":"<p>To encourage reproduciblity, we attach the code4 accompanying the paper.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#31ground-truth-mel-spectrogram-inversion","title":"3.1\u00b7Ground truth mel-spectrogram inversion","text":""},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#ablation-study","title":"Ablation Study","text":"<p>First, in order to understand the importance of various components of our proposed model, we perform qualitative and quantitative analysis of the reconstructed audio for the melspectrogram inversion task. We remove certain key architectural decisions and evaluate the audio quality using the test set. Table 2 shows the mean opinion score of audio quality as assessed via human listening tests. Each model is trained for 400k iterations on the LJ Speech dataset (Ito, 2017). Our analysis leads to the following conclusions: Absence of dilated convolutional stacks in the generator or removing weight normalization lead to high frequency artifacts. Using a single discriminator (instead of multi-scale discriminator) produces metallic audio, especially while the speaker is breathing. Moreover, on our internal 6 clean speakers dataset, we notice that this version of the model skips certain voiced portions, completely missing some words. Using spectral normalization or removing the window-based discriminator loss makes it harder to learn sharp high frequency patterns, causing samples to sound significantly noisy. Adding an extra L1 penalty between real and generated raw waveform makes samples sound metallic with additional high frequency artifacts.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#benchmarking-competing-models","title":"Benchmarking Competing Models","text":"<p>Next, in order to compare the performance of MelGAN for inverting ground truth mel-spectrograms to raw audio against existing methods such as WaveNet vocoder, WaveGlow, Griffin-Lim and Ground Truth audio, we run an independent MOS test where the MelGAN model is trained until convergence (around 2.5M iterations). Similar to the ablation study, these comparisons are made on models trained on the LJ Speech Datset. The results of this comparison are shown in Table 3.</p> <p>This experiment result indicates that MelGAN is comparable in quality to state-of-the-art high capacity WaveNet-based models such as WaveNet and WaveGlow. We believe that the performance gap can be quickly bridged in the future by further exploring this direction of using GANs for audio synthesis.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#generalization-to-unseen-speakers","title":"Generalization to unseen speakers","text":"<p>Interestingly, we noticed that when we train MelGAN on a dataset containing multiple speakers (internal 6 speaker dataset consisting of 3 male and 3 female speakers with roughly 10 hours per speaker), the resulting model is able to generalize to completely new (unseen) speakers outside the train set. This experiment verifies that MelGAN is able to learn a speakerinvariant mapping of mel spectrograms to raw waveforms.</p> <p>In an attempt to provide an easily comparable metric to systematically evaluate this generalization (for current and future work), we run an MOS hearing test for ground-truth mel-spectrogram inversion on the public available VCTK dataset (Veaux et al., 2017). The results of this test are shown in Table 4.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#32end-to-end-speech-synthesis","title":"3.2\u00b7End-to-end speech synthesis","text":"<p>We perform quantitative and qualitative comparisons between our proposed MelGAN vs competing models on mel-spectrogram inversion for end-to-end speech synthesis. We plug the MelGAN model in an end-to-end speech synthesis pipeline (Figure 2) and evaluate the text-to-speech sample quality with competing models.</p> <p>Specifically, we compare the sample quality when using MelGAN for spectrogram inversion vs WaveGlow using Text2mel - an improved version of the open-source char2wav model (Sotelo et al., 2017). Text2mel generates mel-spectrograms instead of vocoder frames, uses phonemes as the input representation and can be coupled with WaveGlow or MelGAN to invert the generated mel-spectrograms. We use this model since its sampler, trains faster and does not perform any mel-frequency clipping like Tacotron2. Additionally, we also include the state-of-the-art Tacotron2 model coupled with WaveGlow for baseline comparison. We use the open source implementations of Tacotron2 and WaveGlow provided by NVIDIA in the Pytorch Hub repository to generate the samples. When using WaveGlow, we use the Denoiser with strength 0.01 provided in the official repository to remove high frequency artifacts. The results of the MOS tests are shown in the table 5.</p> <p>For all experiments, MelGAN was trained with batch size 16 on a single NVIDIA RTX2080Ti GPU. We use Adam as the optimizer with learning rate of 1e-4 with \u03b21 = 0.5 and \u03b22 = 0.9 for the generator and the discriminators. Samples for qualitative analysis can be found on the accompanied web-page 5. You can try the speech correction application here 6 created based on the end-to-end speech synthesis pipeline described above.</p> <p>The results indicate that MelGAN is comparable to some of the best performing models to date as a vocoder component of TTS pipeline. To the best of our ability we also created a TTS model with Text2mel + WaveNet vocoder to add to our comparison. We use the pretrained WaveNet vocoder model provided by Yamamoto (2019) and train the Text2mel model with the required data preprocessing performed. However the model only obtained an MOS score of 3.40 \u00b1 0.04. We suspect that</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#33non-autoregressive-decoder-for-music-translation","title":"3.3\u00b7Non autoregressive decoder for music translation","text":"<p>To show that MelGAN is robust and can be plugged into any setup that currently operates using an autoregressive model to perform waveform synthesis, we replace the WaveNet-type autoregressive decoder in the Universal Music Translation Network (Mor et al., 2019) with a MelGAN generator.</p> <p>In this experiment, we use the pre-trained universal music encoder provided by the authors to transform 16kHz raw audio into a latent code sequence of 64 channels, with a downsampling factor of 800 in the time dimension. This implies a 12.5\u00d7 information compression rate in this domain independent latent representation. Using only the data from a target musical domain, our MelGAN decoder is trained to reconstruct raw waveform from latent code sequence in the GAN setup we described earlier. We adjust the model hyperparameters to obtain upsampling factors of 10, 10, 2, 2, 2 to reach the input resolution. For each selected domain on MusicNet (Thickstun et al., 2018), a decoder is trained for 4 days on an RTX2080 Ti GPU on the available data.</p> <p>The music translation network augmented with MelGAN decoder is able to perform music translation from any musical domain to the target domain it is trained on with decent quality. We compare qualitative samples from our model against the original model here 5. The augmented model requires only around 160 milliseconds to translate 1 second of input music audio on an RTX2080 Ti GPU, about 2500 times faster than the original model on the same hardware.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#34non-autoregressive-decoder-for-vq-vae","title":"3.4\u00b7Non-autoregressive decoder for VQ-VAE","text":"<p>Further establishing the generality of our approach, we substitute the decoder in Vector-Quantized VAEs (van den Oord et al., 2017) with our proposed adversarially learned decoder. VQ-VAE is a variational autoencoder which produces a downsampled discrete latent encoding of the input. VQ-VAE uses a high-capacity autoregressive WaveNet decoder to learn the data conditional p(x|zq).</p> <p>Figure 3 shows an adapted version of VQ-VAE for the task of music generation. In our variant, we use two encoders. The local encoder encodes the audio sequence into a 64\u00d7 downsampled time series ze. Each vector in this sequence is then mapped to 1 of 512 quantized vectors using a codebook. This follows the same structure as proposed in (van den Oord et al., 2017). The second encoder outputs a global continuous-valued latent vector y.</p> <p>We show qualitative samples of unconditional piano music generation following (Dieleman et al., 2018), where we learn a single tier VQ-VAE on a raw audio scale, and use a vanilla autoregressive model (4-layer LSTM with 1024 units) to learn the prior over the sequence of discrete latents. We sample zq unconditionally using the trained recurrent prior model, and y from a unit gaussian distribution. Qualitatively, conditioned on the same sequence of discrete latents, sampling from the global latent\u2019s prior distribution results in low level waveform variations such as phase shifts, but perceptually the outputs sound very similar. We find that the global latent is essential to improve reconstruction quality since it better captures stochasticity in the data conditional p(x|zq, y), as the discrete latent information learned via the local encoder (zq) is highly compressed. We use latent vector of size 256 and use the same hyper-parameters for training as mel-spectrogram inversion experiment. We used upsampling layers with 4x, 4x, 2x and 2x ratios to achieve 64x upsampling.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2019.10.08_MelGAN/#4conclusion-and-future-work","title":"4\u00b7Conclusion and future work","text":"<p>We have introduced a GAN architecture tailored for conditional audio synthesis and we demonstrated qualitative and quantitative results establishing the effectiveness and generality of the proposed methods. Our model has the following assets: it is very lightweight, can be trained quickly on a single desktop GPU, and it is very fast at inference time. We hope that our generator can be a plug-and-play replacement to compute-heavy alternatives in any higher-level audio related tasks.</p> <p>While the proposed model is well-suited to the task of training and generating sequences of varying length, it is limited by the requirement of time-aligned conditioning information. Indeed it has been designed to operate in the case where the output sequence length is a factor of the input sequence length, which is not always the case in practice. Likewise, feature matching with paired ground truth data is limiting because it is infeasible in some scenarios. For unconditional synthesis, the proposed model needs to defer learning a sequence of conditioning variables to other, better-suited methods such as VQ-VAE. Learning high quality unconditional GAN for audio is a very interesting direction for future work, which we believe will benefit from incorporating the specific architectural choices introduced in this work.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/","title":"HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis - \u4f5c\u8005:   - [Jungil Kong](../../Authors/Jungil_Kong.md)   - [Jaehyeon Kim](../../Authors/Jaehyeon_Kim.md)   - [Jaekyoung Bae](../../Authors/Jaekyoung_Bae.md) - \u673a\u6784:   - [Kakao Enterprise](../../Institutions/Kakao_Enterprise.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2020.10.12 ArXiv v1   - \u9884\u5370\u65f6\u95f4: 2020.10.23 ArXiv v2   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.15 - \u53d1\u8868:   - [NeurIPS 2020](../../Publications/NeurIPS.md) - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2010.05646)   - [Demo](https://jik876.github.io/hifi-gan-demo/)   - [DOI](https://proceedings.neurips.cc/paper_files/paper/2020/hash/c5d736809766d46260d816d8dbc9eb44-Abstract.html)   - [Github](https://github.com/jik876/hifi-gan) - \u6807\u7b7e:   - [\u58f0\u7801\u5668](../../Tags/Vocoder.md)   - [\u751f\u6210\u5bf9\u6297\u7f51\u7edc](../../Tags/Model_GAN.md)   - [\u5bf9\u6297\u5b66\u4e60](../../Tags/Learning_Adversarial.md) - \u9875\u6570: 14 - \u5f15\u7528: ? - \u88ab\u5f15: 1444"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"\u539f\u6587  &gt; Several recent work on speech synthesis have employed [Generative Adversarial Networks (GANs)](../_Basis/2014.06.10_GAN.md) to produce raw waveforms.  &gt; Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models.  &gt; In this work, we propose ***HiFi-GAN***, which achieves both efficient and high-fidelity speech synthesis.  &gt; As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality.  &gt; A subjective human evaluation ([Mean Opinion Score, MOS](../../Evaluations/MOS.md)) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU.  &gt; We further show the generality of ***HiFi-GAN*** to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis.  &gt; Finally, a small footprint version of ***HiFi-GAN*** generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.   <p>\u8bed\u97f3\u5408\u6210\u9886\u57df\u7684\u51e0\u9879\u8fd1\u671f\u5de5\u4f5c\u90fd\u91c7\u7528\u4e86\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u6765\u751f\u6210\u539f\u59cb\u6ce2\u5f62. \u5c3d\u7ba1\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u63d0\u5347\u91c7\u6837\u6548\u7387\u548c\u5185\u5b58\u5229\u7528, \u5b83\u4eec\u7684\u6837\u672c\u8d28\u91cf\u8fd8\u6ca1\u6709\u8fbe\u5230\u81ea\u56de\u5f52\u548c\u57fa\u4e8e\u6d41\u6a21\u578b\u7684\u751f\u6210\u6a21\u578b\u7684\u6548\u679c. \u672c\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86 HiFi-GAN, \u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u548c\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u5408\u6210. \u7531\u4e8e\u8bed\u97f3\u97f3\u9891\u7531\u5177\u6709\u4e0d\u540c\u5468\u671f\u7684\u6b63\u5f26\u4fe1\u53f7\u7ec4\u6210, \u6211\u4eec\u6f14\u793a\u4e86\u5efa\u6a21\u97f3\u9891\u7684\u5468\u671f\u6a21\u5f0f\u5bf9\u4e8e\u589e\u5f3a\u6837\u672c\u8d28\u91cf\u5341\u5206\u91cd\u8981. \u5355\u4e2a\u8bf4\u8bdd\u4eba\u6570\u636e\u96c6\u7684\u4e3b\u89c2\u4eba\u7c7b\u8bc4\u4f30 MOS \u8868\u660e\u672c\u5de5\u4f5c\u63d0\u51fa\u7684\u65b9\u6cd5\u751f\u6210\u7684\u97f3\u9891\u548c\u4eba\u7c7b\u8d28\u91cf\u76f8\u8fd1, \u4e14\u5728\u5355\u4e2a V100 GPU \u4e0a\u751f\u6210 22.05 kHz \u9ad8\u8d28\u91cf\u97f3\u9891\u7684\u901f\u5ea6\u6bd4\u5b9e\u65f6\u901f\u5ea6\u5feb 167.9 \u500d. \u6211\u4eec\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86 HiFi-GAN \u7684\u6cdb\u5316\u6027, \u5373\u5bf9\u672a\u89c1\u8fc7\u7684\u53d1\u8a00\u4eba\u8fdb\u884c\u6885\u5c14\u9891\u8c31\u7684\u9006\u5411\u5de5\u7a0b, \u4ee5\u53ca\u7aef\u5230\u7aef\u7684\u8bed\u97f3\u5408\u6210. \u6700\u540e, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f53\u79ef\u5c0f\u7684 HiFi-GAN \u7248\u672c, \u5b83\u5728 CPU \u4e0a\u4ee5\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u5f53\u7684\u8d28\u91cf\u751f\u6210\u97f3\u9891, \u4e14\u751f\u6210\u901f\u5ea6\u6bd4\u5b9e\u65f6\u901f\u5ea6\u5feb 13.4 \u500d.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"\u539f\u6587  &gt; Voice is one of the most frequent and naturally used communication interfaces for humans. &gt; With recent developments in technology, voice is being used as a main interface in artificial intelligence (AI) voice assistant services such as Amazon Alexa, and it is also widely used in automobiles, smart homes and so forth. &gt; Accordingly, with the increase in demand for people to converse with machines, technology that synthesizes natural speech like human speech is being actively studied.  &gt; Recently, with the development of neural networks, speech synthesis technology has made a rapid progress. &gt; Most neural speech synthesis models use a two-stage pipeline:  &gt; 1. predicting a low resolution intermediate representation such as mel-spectrograms (Shen et al., 2018, Ping et al., 2017, Li et al., 2019) or linguistic features ([Oord et al.(WaveNet\u00b72016)](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md)) from text,  &gt; 2. synthesizing raw waveform audio from the intermediate representation ([Oord et al.(WaveNet\u00b72016)](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md), Prenger et al., 2019, Kumar et al., 2019). &gt; &gt; The first stage is to model low-level representations of human speech from text, whereas the second stage model synthesizes raw waveforms with up to 24,000 samples per second and up to 16 bit fidelity. &gt; In this work, we focus on designing a second stage model that efficiently synthesizes high fidelity waveforms from mel-spectrograms.  &gt; Various work have been conducted to improve the audio synthesis quality and efficiency of second stage models. &gt; [WaveNet (2016)](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md) is an autoregressive (AR) convolutional neural network that demonstrates the ability of neural network based methods to surpass conventional methods in quality..  &gt; However, because of the AR structure, WaveNet generates one sample at each forward operation; it is prohibitively slow in synthesizing high temporal resolution audio. &gt; Flow-based generative models are proposed to address this problem. &gt; Because of their ability to model raw waveforms by transforming noise sequences of the same size in parallel, flow-based generative models fully utilize modern parallel computing processors to speed up sampling. &gt; [Parallel WaveNet (2019)](2019.10.25_Parallel_WaveGAN.md) is an inverse autoregressive flow (IAF) that is trained to minimize its Kullback-Leibler divergence from a pre-trained WaveNet called a teacher. &gt; Compared to the teacher model, it improves the synthesis speed to 1,000 times or more, without quality degradation. &gt; [WaveGlow (2018)](2018.10.31_WaveGlow.md) eliminates the need for distilling a teacher model, and simplifies the learning process through maximum likelihood estimation by employing efficient bijective flows based on Glow (Kingma and Dhariwal, 2018). &gt; It also produces high-quality audio compared to WaveNet. &gt; However, it requires many parameters for its deep architecture with over 90 layers.  &gt; [Generative adversarial networks (GANs) (2014)](../_Basis/2014.06.10_GAN.md), which are one of the most dominant deep generative models, have also been applied to speech synthesis. &gt; Kumar et al. (2019) proposed a multi-scale architecture for discriminators operating on multiple scales of raw waveforms. &gt; With sophisticated architectural consideration, the MelGAN generator is compact enough to enable real-time synthesis on CPU. &gt; Yamamoto et al. (2020) proposed multi-resolution STFT loss function to improve and stabilize GAN training and achieved better parameter efficiency and less training time than an IAF model, ClariNet (Ping et al., 2018). &gt; Instead of mel-spectrograms, [GAN-TTS (2019)](2019.09.25_GAN-TTS.md) successfully generates high quality raw audio waveforms from linguistic features through multiple discriminators operating on different window sizes. &gt; The model also shows fewer FLOPs compared to Parallel WaveNet. &gt; Despite the advantages, there is still a gap in sample quality between the GAN models and AR or flow-based models.  &gt; We propose ***HiFi-GAN***, which achieves both higher computational efficiency and sample quality than AR or flow-based models. &gt; As speech audio consists of sinusoidal signals with various periods, modeling the periodic patterns matters to generate realistic speech audio. &gt; Therefore, we propose a discriminator which consists of small sub-discriminators, each of which obtains only a specific periodic parts of raw waveforms. &gt; This architecture is the very ground of our model successfully synthesizing realistic speech audio. &gt; As we extract different parts of audio for the discriminator, we also design a module that places multiple residual blocks each of which observes patterns of various lengths in parallel, and apply it to the generator.  &gt; ***HiFi-GAN*** achieves a higher MOS score than the best publicly available models, WaveNet and WaveGlow. &gt; It synthesizes human-quality speech audio at speed of 3.7 MHz on a single V100 GPU. &gt; We further show the generality of ***HiFi-GAN*** to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. &gt; Finally, the tiny footprint version of ***HiFi-GAN*** requires only 0.92M parameters while outperforming the best publicly available models and the fastest version of ***HiFi-GAN*** samples 13.44 times faster than real-time on CPU and 1,186 times faster than real-time on single V100 GPU with comparable quality to an autoregressive counterpart.  &gt; Our audio samples are available on the demo web-site, and we provide the implementation as open source for reproducibility and future work."},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#2methodology","title":"2\u00b7Methodology\u00b7\u65b9\u6cd5\u8bba","text":""},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#21overview","title":"2.1\u00b7Overview\u00b7\u6982\u89c8","text":"\u5c55\u5f00  &gt; ***HiFi-GAN*** consists of one generator and two discriminators: multi-scale and multi-period discriminators.  &gt; The generator and discriminators are trained adversarially, along with two additional losses for improving training stability and model performance.   <p>HiFi-GAN \u7531\u4e00\u4e2a\u751f\u6210\u5668\u548c\u4e24\u4e2a\u5224\u522b\u5668 (\u591a\u5c3a\u5ea6\u5224\u522b\u5668, \u591a\u5468\u671f\u5224\u522b\u5668) \u7ec4\u6210. \u751f\u6210\u5668\u548c\u5224\u522b\u5668\u4ee5\u5bf9\u6297\u7684\u65b9\u5f0f\u8bad\u7ec3, \u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u989d\u5916\u4e24\u79cd\u7684\u635f\u5931\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u7a33\u5b9a\u6027.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#22generator","title":"2.2\u00b7Generator\u00b7\u751f\u6210\u5668","text":"\u539f\u6587  &gt; The generator is a fully convolutional neural network.  &gt; It uses a mel-spectrogram as input and upsamples it through transposed convolutions until the length of the output sequence matches the temporal resolution of raw waveforms.  &gt; Every transposed convolution is followed by a multi-receptive field fusion (MRF) module, which we describe in the next paragraph.  &gt; Fig.01 shows the architecture of the generator.  &gt; As in previous work (Mathieu et al., 2015, Isola et al., 2017, [MelGAN (2019)](2019.10.08_MelGAN.md)), noise is not given to the generator as an additional input.   <p>\u751f\u6210\u5668\u91c7\u7528\u5168\u5377\u79ef\u795e\u7ecf\u7f51\u7edc. \u5b83\u63a5\u6536\u6885\u5c14\u9891\u8c31\u4f5c\u4e3a\u8f93\u5165, \u901a\u8fc7\u8f6c\u7f6e\u5377\u79ef\u4e0a\u91c7\u6837\u76f4\u5230\u8f93\u51fa\u5e8f\u5217\u7684\u957f\u5ea6\u4e0e\u539f\u59cb\u6ce2\u5f62\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u76f8\u5339\u914d. \u6bcf\u4e2a\u8f6c\u7f6e\u5377\u79ef\u540e\u8ddf\u968f\u4e00\u4e2a\u591a\u611f\u53d7\u91ce\u878d\u5408 (Multi-Receptive Field Fusion, MRF) \u6a21\u5757, \u5c06\u5728\u4e0b\u4e00\u6bb5\u4e2d\u8fdb\u884c\u4ecb\u7ecd. \u56fe 01 \u5c55\u793a\u4e86\u751f\u6210\u5668\u7684\u67b6\u6784.</p> <p></p> <ul> <li>\u7b2c\u4e00\u4e2a\u5b50\u56fe: \u751f\u6210\u5668\u5bf9\u6885\u5c14\u9891\u8c31\u4e0a\u91c7\u6837 $|k_u|$ \u6b21\u4ee5\u5339\u914d\u539f\u59cb\u6ce2\u5f62\u7684\u65f6\u95f4\u5206\u8fa8\u7387.</li> <li>\u7b2c\u4e8c\u4e2a\u5b50\u56fe: MRF \u6a21\u5757\u5c06\u6765\u81ea\u5177\u6709\u4e0d\u540c\u5377\u79ef\u6838\u5927\u5c0f\u548c\u81a8\u80c0\u7387\u7684 $|k_r|$ \u4e2a\u6b8b\u5dee\u5757\u7684\u8f93\u51fa\u7279\u5f81\u76f8\u52a0.</li> <li>\u7b2c\u4e09\u4e2a\u5b50\u56fe: MRF \u6a21\u5757\u4e2d\u5177\u6709\u5377\u79ef\u6838\u5927\u5c0f $k_{r}[n]$ \u548c\u81a8\u80c0\u7387 $D_{r}[n]$ \u7684\u7b2c $n$ \u4e2a\u6b8b\u5dee\u5757.</li> </ul> <p>\u548c\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u540c (\u5217\u4e3e\u5982\u4e0b), \u566a\u58f0\u4e0d\u4f5c\u4e3a\u989d\u5916\u7684\u8f93\u5165\u4f20\u7ed9\u751f\u6210\u5668.</p> <ul> <li>Deep Multi-Scale Video Prediction Beyond Mean Square Error</li> <li>Image-to-Image Translation with Conditional Adversarial Networks</li> <li>MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis</li> </ul> \u4ee3\u7801 <pre><code>class Generator(torch.nn.Module):\n    def __init__(self, h):\n        super(Generator, self).__init__()\n        self.h = h\n        self.num_kernels = len(h.resblock_kernel_sizes)\n        self.num_upsamples = len(h.upsample_rates)\n        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n            self.ups.append(weight_norm(\n                ConvTranspose1d(h.upsample_initial_channel//(2**i), h.upsample_initial_channel//(2**(i+1)),\n                                k, u, padding=(k-u)//2)))\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = h.upsample_initial_channel//(2**(i+1))\n            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n                self.resblocks.append(resblock(h, ch, k, d))\n\n        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n        self.ups.apply(init_weights)\n        self.conv_post.apply(init_weights)\n\n    def forward(self, x):\n        x = self.conv_pre(x)\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i*self.num_kernels+j](x)\n                else:\n                    xs += self.resblocks[i*self.num_kernels+j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()\n        remove_weight_norm(self.conv_pre)\n        remove_weight_norm(self.conv_post)\n</code></pre>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#multi-receptive-field-fusion-mrf-fusion","title":"Multi-Receptive Field Fusion (MRF) Fusion\u00b7\u591a\u611f\u53d7\u91ce\u878d\u5408","text":"\u539f\u6587  &gt; We design the multi-receptive field fusion (MRF) module for our generator, which observes patterns of various lengths in parallel.  &gt; Specifically, MRF module returns the sum of outputs from multiple residual blocks.  &gt; Different kernel sizes and dilation rates are selected for each residual block to form diverse receptive field patterns.  &gt; The architectures of MRF module and a residual block are shown in Fig.01.  &gt; We left some adjustable parameters in the generator;  &gt; the hidden dimension $h_{u}$, kernel sizes $k_{u}$ of the transposed convolutions, kernel sizes $k_{r}$, and dilation rates $D_{r}$ of MRF modules can be regulated to match one\u2019s own requirement in a trade-off between synthesis efficiency and sample quality.   <p>\u6211\u4eec\u4e3a\u751f\u6210\u5668\u8bbe\u8ba1\u4e86\u591a\u611f\u53d7\u91ce\u878d\u5408 (Multi-Receptive Field Fusion, MRF) \u6a21\u5757, \u5b83\u80fd\u591f\u5e76\u884c\u5730\u89c2\u5bdf\u4e0d\u540c\u957f\u5ea6\u7684\u6a21\u5f0f. \u5177\u4f53\u6765\u8bf4, MRF \u6a21\u5757\u8fd4\u56de\u591a\u4e2a\u6b8b\u5dee\u5757\u7684\u8f93\u51fa\u4e4b\u548c. \u6bcf\u4e2a\u6b8b\u5dee\u5757\u91c7\u7528\u4e0d\u540c\u7684\u5377\u79ef\u6838\u5927\u5c0f\u548c\u81a8\u80c0\u7387\u4ee5\u5f62\u6210\u591a\u79cd\u611f\u53d7\u91ce\u6a21\u5f0f. \u56fe 01 \u5c55\u793a\u4e86 MRF \u6a21\u5757\u548c\u6b8b\u5dee\u5757\u7684\u67b6\u6784. \u6211\u4eec\u5728\u751f\u6210\u5668\u4e2d\u7559\u4e0b\u4e00\u4e9b\u53ef\u8c03\u6574\u7684\u53c2\u6570: \u9690\u85cf\u5c42\u7ef4\u5ea6 $h_{u}$, \u8f6c\u7f6e\u5377\u79ef\u7684\u5377\u79ef\u6838\u5927\u5c0f $k_{u}$, MRF \u6a21\u5757\u4e2d\u7684\u5377\u79ef\u6838\u5927\u5c0f $k_{r}$, \u81a8\u80c0\u7387 $D_{r}$, \u8fd9\u4e9b\u53c2\u6570\u53ef\u4ee5\u6839\u636e\u4e2a\u4eba\u9700\u6c42\u8fdb\u884c\u8c03\u8282, \u4ee5\u8fbe\u5230\u5408\u7406\u7684\u5408\u6210\u6548\u7387\u548c\u97f3\u9891\u8d28\u91cf\u4e4b\u95f4\u7684\u5e73\u8861.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#23discriminator","title":"2.3\u00b7Discriminator\u00b7\u5224\u522b\u5668","text":"\u539f\u6587  &gt; Identifying long-term dependencies is the key for modeling realistic speech audio. &gt; For example, a phoneme duration can be longer than 100 ms, resulting in high correlation between more than 2,200 adjacent samples in the raw waveform. &gt; This problem has been addressed in the previous work (Donahue et al., 2018) by increasing receptive fields of the generator and discriminator.   <p>\u8bc6\u522b\u957f\u671f\u4f9d\u8d56\u6027\u662f\u5efa\u6a21\u771f\u5b9e\u8bed\u97f3\u97f3\u9891\u7684\u5173\u952e\u6240\u5728. \u4f8b\u5982, \u4e00\u4e2a\u97f3\u7d20\u6301\u7eed\u65f6\u95f4\u53ef\u4ee5\u8d85\u8fc7 100 \u6beb\u79d2, \u8fd9\u4f7f\u5f97\u539f\u59cb\u6ce2\u5f62\u4e2d\u8d85\u8fc7 2200 \u4e2a\u76f8\u90bb\u6837\u672c\u70b9\u95f4\u5177\u6709\u9ad8\u5ea6\u76f8\u5173\u6027. \u8fd9\u4e00\u95ee\u9898\u5728\u4e4b\u524d\u7684\u5de5\u4f5c \"Adversarial Audio Synthesis\" (2018) \u4e2d\u901a\u8fc7\u589e\u52a0\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u7684\u611f\u53d7\u91ce\u5f97\u5230\u4e86\u89e3\u51b3.</p> \u539f\u6587  &gt; We focus on another crucial problem that has yet been resolved; as speech audio consists of sinusoidal signals with various periods, the diverse periodic patterns underlying in the audio data need to be identified. &gt; To this end, we propose the multi-period discriminator (MPD) consisting of several sub-discriminators each handling a portion of periodic signals of input audio. &gt; Additionally, to capture consecutive patterns and long-term dependencies, we use the **multi-scale discriminator (MSD)** proposed in [MelGAN (2019)](2019.10.08_MelGAN.md), which consecutively evaluates audio samples at different levels. &gt; We conducted simple experiments to show the ability of MPD and MSD to capture periodic patterns, and the results can be found in Appendix B.   <p>\u6211\u4eec\u4e3b\u8981\u5173\u6ce8\u53e6\u4e00\u4e2a\u5c1a\u672a\u89e3\u51b3\u7684\u5173\u952e\u95ee\u9898. \u7531\u4e8e\u8bed\u97f3\u97f3\u9891\u7531\u5177\u6709\u5404\u79cd\u5468\u671f\u7684\u6b63\u5f26\u4fe1\u53f7\u7ec4\u6210, \u56e0\u6b64\u9700\u8981\u8bc6\u522b\u97f3\u9891\u6570\u636e\u4e2d\u5b58\u5728\u7684\u5404\u79cd\u4e0d\u540c\u5468\u671f\u6a21\u5f0f. \u4e3a\u6b64, \u6211\u4eec\u63d0\u51fa\u4e86\u7531\u6570\u4e2a\u5b50\u5224\u522b\u5668\u7ec4\u6210\u7684\u591a\u5468\u671f\u5224\u522b\u5668 (Multi-Period Discriminator, MPD), \u6bcf\u4e2a\u5b50\u5224\u522b\u5668\u53ea\u5904\u7406\u8f93\u5165\u97f3\u9891\u5468\u671f\u4fe1\u53f7\u7684\u4e00\u90e8\u5206. \u6b64\u5916, \u4e3a\u4e86\u6355\u83b7\u8fde\u7eed\u6a21\u5f0f\u548c\u957f\u671f\u4f9d\u8d56\u6027, \u6211\u4eec\u4f7f\u7528 MelGAN (2019) \u63d0\u51fa\u7684\u591a\u5c3a\u5ea6\u5224\u522b\u5668 (Multi-Scale Discriminator, MSD), \u5b83\u80fd\u5728\u4e0d\u540c\u7ea7\u522b\u8fde\u7eed\u5730\u8bc4\u4f30\u8f93\u5165\u97f3\u9891\u6837\u672c. \u6211\u4eec\u6784\u9020\u7b80\u5355\u7684\u5b9e\u9a8c\u7528\u4e8e\u5c55\u793a MPD \u548c MSD \u5728\u6355\u83b7\u5468\u671f\u6a21\u5f0f\u65b9\u9762\u7684\u80fd\u529b, \u7ed3\u679c\u53ef\u4ee5\u5728\u9644\u5f55 B \u4e2d\u627e\u5230.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#multi-period-discriminator","title":"Multi-Period Discriminator\u00b7\u591a\u5468\u671f\u5224\u522b\u5668","text":"\u539f\u6587  &gt; MPD is a mixture of sub-discriminators, each of which only accepts equally spaced samples of an input audio; the space is given as period p. &gt; The sub-discriminators are designed to capture different implicit structures from each other by looking at different parts of an input audio. &gt; We set the periods to [2, 3, 5, 7, 11] to avoid overlaps as much as possible. &gt; As shown in Fig.02b, we first reshape 1D raw audio of length $T$ into 2D data of height $T/p$ and width $p$ and then apply 2D convolutions to the reshaped data. &gt; In every convolutional layer of MPD, we restrict the kernel size in the width axis to be 1 to process the periodic samples independently. &gt; Each sub-discriminator is a stack of strided convolutional layers with leaky rectified linear unit (ReLU) activation. &gt; Subsequently, weight normalization (Salimans and Kingma, 2016) is applied to MPD. &gt; By reshaping the input audio into 2D data instead of sampling periodic signals of audio, gradients from MPD can be delivered to all time steps of the input audio.   <p>\u591a\u5468\u671f\u5224\u522b\u5668\u662f\u591a\u4e2a\u5b50\u5224\u522b\u5668\u7684\u6df7\u5408, \u6bcf\u4e2a\u5b50\u5224\u522b\u5668\u4ec5\u63a5\u53d7\u8f93\u5165\u97f3\u9891\u4e2d\u7b49\u95f4\u9694\u7684\u6837\u672c, \u95f4\u9694\u7531\u5468\u671f $p$ \u7ed9\u51fa. \u8fd9\u4e9b\u5b50\u5224\u522b\u5668\u88ab\u8bbe\u8ba1\u6210\u901a\u8fc7\u5173\u6ce8\u8f93\u5165\u97f3\u9891\u7684\u4e0d\u540c\u90e8\u5206\u4ee5\u6355\u6349\u5f7c\u6b64\u4e4b\u95f4\u4e0d\u540c\u7684\u9690\u5f0f\u7ed3\u6784. \u6211\u4eec\u8bbe\u7f6e\u5468\u671f\u4e3a <code>[2,3,5,7,11]</code> \u4ee5\u5c3d\u53ef\u80fd\u907f\u514d\u91cd\u53e0. \u5982\u56fe 02B \u6240\u793a, \u6211\u4eec\u9996\u5148\u5c06\u957f\u4e3a $T$ \u7684\u4e00\u7ef4\u539f\u59cb\u97f3\u9891\u91cd\u5851\u4e3a\u9ad8\u4e3a $T/p$ \u548c\u5bbd\u5ea6 $p$ \u7684\u4e8c\u7ef4\u6570\u636e, \u7136\u540e\u5e94\u7528\u4e8c\u7ef4\u5377\u79ef. \u6211\u4eec\u5728 MPD \u7684\u6bcf\u4e2a\u5377\u79ef\u5c42\u4e2d\u5c06\u5377\u79ef\u6838\u7684\u5bbd\u5ea6\u8bbe\u7f6e\u4e3a 1, \u4ee5\u72ec\u7acb\u5904\u7406\u5468\u671f\u6837\u672c. \u6bcf\u4e2a\u5b50\u5224\u522b\u5668\u662f\u4e00\u7cfb\u5217\u5e26 Leaky ReLU \u6fc0\u6d3b\u51fd\u6570\u548c\u6b65\u5e45\u7684\u5377\u79ef\u5c42\u7684\u5806\u53e0. \u968f\u540e, \u5bf9 MPD \u5e94\u7528\u6743\u91cd\u5f52\u4e00\u5316 (Weight Normalization). \u901a\u8fc7\u5c06\u8f93\u5165\u97f3\u9891\u91cd\u5851\u4e3a\u4e8c\u7ef4\u6570\u636e\u800c\u975e\u91c7\u6837\u97f3\u9891\u7684\u5468\u671f\u4fe1\u53f7, \u6765\u81ea MPD \u7684\u68af\u5ea6\u53ef\u4ee5\u4f20\u9012\u5230\u8f93\u5165\u97f3\u9891\u7684\u6240\u6709\u65f6\u95f4\u6b65\u4e0a.</p> \u4ee3\u7801 <pre><code>class DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList([\n            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(2, 0))),\n        ])\n        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n\n    def forward(self, x):\n        fmap = []\n\n        # 1d to 2d\n        b, c, t = x.shape\n        if t % self.period != 0: # pad first\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t = t + n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass MultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self):\n        super(MultiPeriodDiscriminator, self).__init__()\n        self.discriminators = nn.ModuleList([\n            DiscriminatorP(2),\n            DiscriminatorP(3),\n            DiscriminatorP(5),\n            DiscriminatorP(7),\n            DiscriminatorP(11),\n        ])\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            fmap_rs.append(fmap_r)\n            y_d_gs.append(y_d_g)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n</code></pre>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#multi-scale-discriminator","title":"Multi-Scale Discriminator\u00b7\u591a\u5c3a\u5ea6\u5224\u522b\u5668","text":"\u539f\u6587  &gt; Because each sub-discriminator in MPD only accepts disjoint samples, we add MSD to consecutively evaluate the audio sequence. &gt; The architecture of MSD is drawn from that of [MelGAN (2019)](2019.10.08_MelGAN.md). &gt; MSD is a mixture of three sub-discriminators operating on different input scales: raw audio,\u00d72 average-pooled audio, and \u00d74 average-pooled audio, as shown in Fig.02a. &gt; Each of the sub-discriminators in MSD is a stack of strided and grouped convolutional layers with leaky ReLU activation. &gt; The discriminator size is increased by reducing stride and adding more layers. &gt; Weight normalization is applied except for the first sub-discriminator, which operates on raw audio. &gt; Instead, spectral normalization (Miyato et al., 2018) is applied and stabilizes training as it reported. &gt; Note that MPD operates on disjoint samples of raw waveforms, whereas MSD operates on smoothed waveforms. &gt; For previous work using multi-discriminator architecture such as MPD and MSD, [Binkowski et al. (GAN-TTS\u00b72019)](2019.09.25_GAN-TTS.md)\u2019s work can also be referred. &gt; The discriminator architecture proposed in the work has resemblance to MPD and MSD in that it is a mixture of discriminators, but MPD and MSD are Markovian window-based fully unconditional discriminator, whereas it averages the output and has conditional discriminators. &gt; Also, the resemblance between MPD and RWD ([Binkowski et al.(GAN-TTS\u00b72019)](2019.09.25_GAN-TTS.md)) can be considered in the part of reshaping input audio, but MPD uses periods set to prime numbers to discriminate data of as many periods as possible, whereas RWD uses reshape factors of overlapped periods and does not handle each channel of the reshaped data separately, which are different from what MPD is aimed for. &gt; A variation of RWD can perform a similar operation to MPD, but it is also not the same as MPD in terms of parameter sharing and strided convolution to adjacent signals. &gt; More details of the architectural difference can be found in Appendix C.   <p>\u7531\u4e8e MPD \u4e2d\u6bcf\u4e2a\u5b50\u5224\u522b\u5668\u4ec5\u63a5\u53d7\u4e0d\u76f8\u4ea4\u7684\u6837\u672c, \u56e0\u6b64\u6211\u4eec\u6dfb\u52a0 MSD \u4ee5\u8fde\u7eed\u5730\u8bc4\u4f30\u97f3\u9891\u5e8f\u5217. MSD \u7684\u67b6\u6784\u6765\u81ea MelGAN. MSD \u662f\u7531\u4e09\u4e2a\u5b50\u5224\u522b\u5668\u7684\u6df7\u5408, \u6bcf\u4e2a\u5b50\u5224\u522b\u5668\u5728\u4e0d\u540c\u8f93\u5165\u5c3a\u5ea6\u4e0a\u64cd\u4f5c: \u539f\u59cb\u97f3\u9891, \u5e73\u5747\u6c60\u5316\u7684\u97f3\u9891 \u00d72, \u5e73\u5747\u6c60\u5316\u7684\u97f3\u9891 \u00d74, \u5982\u56fe 02A \u6240\u793a. MSD \u4e2d\u6bcf\u4e2a\u5b50\u5224\u522b\u5668\u662f\u5177\u6709 Leaky ReLU \u6fc0\u6d3b\u51fd\u6570, \u5e26\u6b65\u5e45\u548c\u5206\u7ec4\u5377\u79ef\u5c42\u7684\u5806\u53e0. \u5224\u522b\u5668\u7684\u5927\u5c0f\u901a\u8fc7\u51cf\u5c11\u6b65\u5e45\u548c\u589e\u52a0\u5c42\u6570\u6765\u589e\u52a0. MDP \u5728\u5904\u7406\u539f\u59cb\u97f3\u9891\u65f6\u9664\u4e86\u7b2c\u4e00\u4e2a\u5b50\u5224\u522b\u5668\u5916, \u90fd\u5e94\u7528\u4e86\u6743\u91cd\u5f52\u4e00\u5316, \u800c\u7b2c\u4e00\u5c42\u5e94\u7528\u4e86\u9891\u8c31\u5f52\u4e00\u5316, \u53ef\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b. \u6ce8\u610f\u5230 MPD \u5728\u539f\u59cb\u6ce2\u5f62\u7684\u4e0d\u76f8\u4ea4\u6837\u672c\u4e0a\u5904\u7406, \u800c MSD \u5728\u5e73\u6ed1\u6ce2\u5f62\u4e0a\u5904\u7406.</p> <p>\u5bf9\u4e8e\u4e4b\u524d\u4f7f\u7528\u591a\u5224\u522b\u5668\u7684\u67b6\u6784\u4f8b\u5982 MPD \u548c MSD, \u53ef\u4ee5\u53c2\u8003 GAN-TTS \u7684\u5de5\u4f5c. \u8be5\u5de5\u4f5c\u63d0\u51fa\u7684\u5224\u522b\u5668\u6846\u67b6\u548c MPD, MSD \u5177\u6709\u76f8\u4f3c\u4e4b\u5904, \u4e5f\u662f\u5224\u522b\u5668\u7684\u6df7\u5408, \u4f46 MPD \u548c MSD \u662f\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u7a97\u7684\u65e0\u6761\u4ef6\u5224\u522b\u5668, \u800c\u5b83\u5e73\u5747\u4e86\u8f93\u51fa\u4e14\u5177\u6709\u6761\u4ef6\u5316\u5224\u522b\u5668.</p> <p>\u6b64\u5916, MPD \u4e0e RWD \u7684\u76f8\u4f3c\u4e4b\u5904\u8fd8\u6709\u91cd\u5851\u8f93\u5165\u97f3\u9891\u7684\u5f62\u72b6\u90e8\u5206, \u4f46 MPD \u4f7f\u7528\u8d28\u6570\u8bbe\u7f6e\u7684\u5468\u671f\u6765\u5c3d\u53ef\u80fd\u591a\u5730\u5224\u522b\u4e0d\u540c\u5468\u671f\u7684\u6570\u636e, \u800c RWD \u4f7f\u7528\u91cd\u53e0\u5468\u671f\u7684\u91cd\u5851\u56e0\u5b50, \u4f46\u4e0d\u5355\u72ec\u5904\u7406\u6bcf\u4e2a\u901a\u9053, \u8fd9\u548c MPD \u7684\u76ee\u7684\u4e0d\u540c. RWD \u7684\u53d8\u4f53\u53ef\u4ee5\u5e94\u7528\u7c7b\u4f3c MPD \u7684\u64cd\u4f5c, \u4f46\u5b83\u5728\u53c2\u6570\u5171\u4eab\u548c\u76f8\u90bb\u4fe1\u53f7\u7684\u6b65\u5e45\u5377\u79ef\u65b9\u9762\u4e5f\u6709\u6240\u4e0d\u540c. \u5173\u4e8e\u67b6\u6784\u66f4\u591a\u7684\u4e0d\u540c\u4e4b\u5904\u53ef\u4ee5\u5728\u9644\u5f55 C \u4e2d\u627e\u5230.</p> \u4ee3\u7801 <pre><code>class DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList([\n            norm_f(Conv1d(1, 128, 15, 1, padding=7)),\n            norm_f(Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n            norm_f(Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n            norm_f(Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n            norm_f(Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n            norm_f(Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n        ])\n        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n\n    def forward(self, x):\n        fmap = []\n        for l in self.convs:\n            x = l(x)\n            x = F.leaky_relu(x, LRELU_SLOPE)\n            fmap.append(x)\n        x = self.conv_post(x)\n        fmap.append(x)\n        x = torch.flatten(x, 1, -1)\n\n        return x, fmap\n\n\nclass MultiScaleDiscriminator(torch.nn.Module):\n    def __init__(self):\n        super(MultiScaleDiscriminator, self).__init__()\n        self.discriminators = nn.ModuleList([\n            DiscriminatorS(use_spectral_norm=True),\n            DiscriminatorS(),\n            DiscriminatorS(),\n        ])\n        self.meanpools = nn.ModuleList([\n            AvgPool1d(4, 2, padding=2),\n            AvgPool1d(4, 2, padding=2)\n        ])\n\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []\n        fmap_rs = []\n        fmap_gs = []\n        for i, d in enumerate(self.discriminators):\n            if i != 0:\n                y = self.meanpools[i-1](y)\n                y_hat = self.meanpools[i-1](y_hat)\n            y_d_r, fmap_r = d(y)\n            y_d_g, fmap_g = d(y_hat)\n            y_d_rs.append(y_d_r)\n            fmap_rs.append(fmap_r)\n            y_d_gs.append(y_d_g)\n            fmap_gs.append(fmap_g)\n\n        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n</code></pre>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#24training-loss-terms","title":"2.4.Training Loss Terms\u00b7\u8bad\u7ec3\u635f\u5931\u9879","text":""},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#gan-lossgan","title":"GAN Loss\u00b7GAN \u635f\u5931","text":"\u539f\u6587  &gt; For brevity, we describe our discriminators, MSD and MPD, as one discriminator throughout Section 2.4. &gt; For the generator and discriminator, the training objectives follow LS GAN (Mao et al., 2017), which replace the binary cross-entropy terms of the [original GAN](../_Basis/2014.06.10_GAN.md) objectives with least squares loss functions for non-vanishing gradient flows. &gt; The discriminator is trained to classify ground truth samples to 1, and the samples synthesized from the generator to 0. &gt; The generator is trained to fake the discriminator by updating the sample quality to be classified to a value almost equal to 1. &gt; GAN losses for the generator $G$ and the discriminator $D$ are defined as  $$   Loss_{adv}(D;G) = E_{(x,s)} [(D(x)-1)^2+(D(G(s)))^2] $$  $$   Loss_{adv}(G;D) = E_{s} [(D(G(s))-1)^2] $$  &gt; where $x$ denotes the ground truth audio and $s$ denotes the input condition, the mel-spectrogram of the ground truth audio.   <p>\u4e3a\u4e86\u7b80\u4ecb\u8d77\u89c1, \u6211\u4eec\u5728\u6574\u4e2a 2.4 \u8282\u5c06 MSD \u548c MPD \u89c6\u4e3a\u4e00\u4e2a\u5224\u522b\u5668. \u5bf9\u4e8e\u751f\u6210\u5668\u548c\u5224\u522b\u5668, \u8bad\u7ec3\u76ee\u6807\u9075\u5faa LS GAN, \u5b83\u5c06\u539f\u59cb GAN \u76ee\u6807\u51fd\u6570\u7684\u4e8c\u5143\u4ea4\u53c9\u71b5\u9879\u66ff\u6362\u4e3a\u6700\u5c0f\u4e8c\u4e58\u635f\u5931\u51fd\u6570, \u4ee5\u4fdd\u6301\u68af\u5ea6\u6d41\u4e0d\u6d88\u5931. \u5224\u522b\u5668\u8bad\u7ec3\u6210\u5c06\u771f\u5b9e\u6837\u672c\u5206\u7c7b\u4e3a 1, \u751f\u6210\u5668\u5408\u6210\u7684\u6837\u672c\u5224\u522b\u4e3a 0. \u751f\u6210\u5668\u8bad\u7ec3\u4ee5\u6b3a\u9a97\u5224\u522b\u5668, \u63d0\u5347\u6837\u672c\u8d28\u91cf\u4ee5\u4fbf\u88ab\u5224\u522b\u5668\u5206\u7c7b\u4e3a\u63a5\u8fd1 1 \u7684\u503c. \u751f\u6210\u5668\u548c\u5224\u522b\u5668\u7684\u635f\u5931\u5982\u4e0b:</p> <p>$$   Loss_{adv}(D;G) = E_{(x,s)} [(D(x)-1)^2+(D(G(s)))^2] $$</p> <p>$$   Loss_{adv}(G;D) = E_{s} [(D(G(s))-1)^2] $$</p> <p>\u5176\u4e2d $x$ \u8868\u793a\u771f\u5b9e\u97f3\u9891, $s$ \u8868\u793a\u8f93\u5165\u6761\u4ef6, \u5373\u771f\u5b9e\u97f3\u9891\u7684\u6885\u5c14\u9891\u8c31.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#mel-spectrogram-loss","title":"Mel-Spectrogram Loss\u00b7\u6885\u5c14\u9891\u8c31\u635f\u5931","text":"\u539f\u6587  &gt; In addition to the GAN objective, we add a mel-spectrogram loss to improve the training efficiency of the generator and the fidelity of the generated audio. &gt; Referring to previous work (Isola et al., 2017), applying a reconstruction loss to GAN model helps to generate realistic results, and in Yamamoto et al. (2020)\u2019s work, time-frequency distribution is effectively captured by jointly optimizing multi-resolution spectrogram and adversarial loss functions. &gt; We used mel-spectrogram loss according to the input conditions, which can also be expected to have the effect of focusing more on improving the perceptual quality due to the characteristics of the human auditory system. &gt; The mel-spectrogram loss is the L1 distance between the mel-spectrogram of a waveform synthesized by the generator and that of a ground truth waveform. &gt; It is defined as  $$   Loss_{Mel}(G) = E_{(x,s)}[\\| \\phi(x)-\\phi(G(s))\\|_1] $$  &gt; where $\\phi$ is the function that transforms a waveform into the corresponding mel-spectrogram. &gt; The mel-spectrogram loss helps the generator to synthesize a realistic waveform corresponding to an input condition, and also stabilizes the adversarial training process from the early stages.   <p>\u9664\u4e86 GAN \u76ee\u6807\u51fd\u6570, \u6211\u4eec\u6dfb\u52a0\u4e86\u6885\u5c14\u9891\u8c31\u635f\u5931\u4ee5\u63d0\u5347\u751f\u6210\u5668\u7684\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u97f3\u9891\u7684\u4fdd\u771f\u5ea6. \u636e\u5148\u524d\u5de5\u4f5c, \u7ed9 GAN \u6a21\u578b\u5e94\u7528\u91cd\u6784\u635f\u5931\u6709\u52a9\u4e8e\u751f\u6210\u771f\u5b9e\u7ed3\u679c, \u4e14\u5728 Yamamoto \u7684\u5de5\u4f5c\u4e2d, \u901a\u8fc7\u8054\u5408\u4f18\u5316\u591a\u5206\u8fa8\u7387\u9891\u8c31\u548c\u5bf9\u6297\u635f\u5931\u51fd\u6570\u80fd\u591f\u6709\u6548\u6355\u6349\u65f6\u9891\u5206\u5e03. \u6211\u4eec\u6839\u636e\u8f93\u5165\u6761\u4ef6\u4f7f\u7528\u6885\u5c14\u9891\u8c31\u635f\u5931, \u7531\u4e8e\u4eba\u8033\u542c\u89c9\u7cfb\u7edf\u7684\u7279\u6027, \u5e0c\u671b\u6709\u80fd\u591f\u66f4\u4e13\u6ce8\u4e8e\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf\u7684\u6548\u679c. \u6885\u5c14\u9891\u8c31\u635f\u5931\u662f\u751f\u6210\u5668\u5408\u6210\u6ce2\u5f62\u7684\u6885\u5c14\u9891\u8c31\u548c\u771f\u5b9e\u6ce2\u5f62\u7684\u6885\u5c14\u9891\u8c31\u4e4b\u95f4\u7684 L1 \u8ddd\u79bb. \u5b83\u5b9a\u4e49\u5982\u4e0b:</p> <p>$$   Loss_{Mel}(G) = E_{(x,s)}[| \\phi(x)-\\phi(G(s))|_1] $$</p> <p>\u5176\u4e2d $\\phi$ \u662f\u5c06\u6ce2\u5f62\u8f6c\u6362\u4e3a\u5bf9\u5e94\u6885\u5c14\u9891\u8c31\u7684\u51fd\u6570.</p> <p>\u6885\u5c14\u9891\u8c31\u635f\u5931\u5e2e\u52a9\u751f\u6210\u5668\u6839\u636e\u8f93\u5165\u6761\u4ef6\u751f\u6210\u771f\u5b9e\u6ce2\u5f62, \u5e76\u4ece\u65e9\u671f\u9636\u6bb5\u7a33\u5b9a\u4e86\u5bf9\u6297\u8bad\u7ec3\u8fc7\u7a0b.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#feature-matching-loss","title":"Feature Matching Loss\u00b7\u7279\u5f81\u5339\u914d\u635f\u5931","text":"\u539f\u6587  &gt; The feature matching loss is a learned similarity metric measured by the difference in features of the discriminator between a ground truth sample and a generated sample (Larsen et al., 2016, [MelGAN (2019)](2019.10.08_MelGAN.md)). &gt; As it was successfully adopted to speech synthesis ([MelGAN (2019)](2019.10.08_MelGAN.md)), we use it as an additional loss to train the generator. &gt; Every intermediate feature of the discriminator is extracted, and the L1 distance between a ground truth sample and a conditionally generated sample in each feature space is calculated. &gt; The feature matching loss is defined as  $$   Loss_{FM}(G;D) = E_{(x,s)} [\\sum_{i=1}^T \\dfrac{1}{N_i} \\| D^i(x)-D^i(G(s))\\|_1] $$  &gt; where $T$ denotes the number of layers in the discriminator; $D_{i}$ and $N_{i}$ denote the features and the number of features in the $i$-th layer of the discriminator, respectively.   <p>\u7279\u5f81\u5339\u914d\u635f\u5931\u662f\u901a\u8fc7\u771f\u5b9e\u6837\u672c\u548c\u751f\u6210\u6837\u672c\u5bf9\u5e94\u7684\u5224\u522b\u5668\u7279\u5f81\u7684\u5dee\u5f02\u6765\u8861\u91cf\u76f8\u4f3c\u5ea6\u5ea6\u91cf. \u7531\u4e8e\u5b83\u5728\u8bed\u97f3\u5408\u6210\u4e2d\u7684\u6210\u529f, \u6211\u4eec\u5c06\u4e4b\u4f5c\u4e3a\u989d\u5916\u635f\u5931\u6765\u8bad\u7ec3\u751f\u6210\u5668. \u5224\u522b\u5668\u7684\u6bcf\u4e2a\u4e2d\u95f4\u7279\u5f81\u90fd\u88ab\u63d0\u53d6, \u7136\u540e\u771f\u5b9e\u6837\u672c\u548c\u6761\u4ef6\u751f\u6210\u6837\u672c\u5728\u6bcf\u4e2a\u7279\u5f81\u7a7a\u95f4\u4e0a\u8ba1\u7b97 L1 \u8ddd\u79bb. \u7279\u5f81\u5339\u914d\u635f\u5931\u5b9a\u4e49\u5982\u4e0b:</p> <p>$$   Loss_{FM}(G;D) = E_{(x,s)} [\\sum_{i=1}^T \\dfrac{1}{N_i} | D^i(x)-D^i(G(s))|_1] $$</p> <p>\u5176\u4e2d $T$ \u8868\u793a\u5224\u522b\u5668\u4e2d\u7684\u5c42\u6570; $D_{i}$ \u548c $N_{i}$ \u5206\u522b\u8868\u793a\u5224\u522b\u5668\u7b2c $i$ \u5c42\u7684\u7279\u5f81\u548c\u7279\u5f81\u6570.</p> \u4ee3\u7801 <pre><code>def feature_loss(fmap_r, fmap_g):\n    loss = 0\n    for dr, dg in zip(fmap_r, fmap_g):\n        for rl, gl in zip(dr, dg):\n            loss += torch.mean(torch.abs(rl - gl))\n\n    return loss*2\n</code></pre>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#final-loss","title":"Final Loss\u00b7\u6700\u7ec8\u635f\u5931","text":"\u539f\u6587  &gt; Our final objectives for the generator and discriminator are as  $$   Loss_{G} = Loss_{Adv}(G;D) + \\lambda_{fm} Loss_{FM}(G;D) + \\lambda_{mel}Loss_{Mel}(G) $$  $$   Loss_{D} = Loss_{Adv}(D;G) $$  &gt; where we set $\\lambda_{fm}= 2$ and $\\lambda_{mel}= 45$.  &gt; Because our discriminator is a set of sub-discriminators of MPD and MSD, Eq.05 and Eq.06 can be converted with respect to the sub-discriminators as  $$   Loss_G = \\sum_{k=1}^K [Loss_{Adv}(G;D_k)+\\lambda_{fm} Loss_{FM}(G;D_k)] + \\lambda_{mel}Loss_{Mel}(G) $$  $$   Loss_D = \\sum_{k=1}^K [Loss_{Adv}(D_k;G)] $$  &gt; where $D_{k}$ denotes the $k$-th sub-discriminator in MPD and MSD.   <p>\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u7684\u6700\u540e\u76ee\u6807\u51fd\u6570\u5982\u4e0b:</p> <p>$$   Loss_{G} = Loss_{Adv}(G;D) + \\lambda_{fm} Loss_{FM}(G;D) + \\lambda_{mel}Loss_{Mel}(G) $$</p> <p>$$   Loss_{D} = Loss_{Adv}(D;G) $$</p> <p>\u5176\u4e2d $\\lambda_{fm}= 2$ \u548c $\\lambda_{mel}= 45$.</p> <p>\u7531\u4e8e\u5224\u522b\u5668\u662f MPD \u548c MSD \u7684\u5b50\u5224\u522b\u5668\u96c6\u5408, \u8fd9\u4e24\u4e2a\u5f0f\u5b50\u53ef\u4ee5\u5206\u522b\u8f6c\u6362\u4e3a\u5173\u4e8e\u5b50\u5224\u522b\u5668\u7684\u5f62\u5f0f:</p> <p>$$   Loss_{G} = \\sum_{k=1}^K [Loss_{Adv}(G;D_k)+\\lambda_{fm} Loss_{FM}(G;D_k)] + \\lambda_{mel}Loss_{Mel}(G) $$</p> <p>$$   Loss_{D} = \\sum_{k=1}^K [Loss_{Adv}(D_k;G)] $$</p> <p>\u5176\u4e2d $D_{k}$ \u8868\u793a MPD \u548c MSD \u4e2d\u7684\u7b2c $k$ \u4e2a\u5b50\u5224\u522b\u5668.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#3experiments","title":"3\u00b7Experiments\u00b7\u5b9e\u9a8c","text":"\u539f\u6587  &gt; For fair and reproducible comparison with other models, we used the LJSpeech dataset (Ito, 2017) in which many speech synthesis models are trained. &gt; LJSpeech consists of 13,100 short audio clips of a single speaker with a total length of approximately 24 hours. &gt; The audio format is 16-bit PCM with a sample rate of 22 kHz; it was used without any manipulation. &gt; ***HiFi-GAN*** was compared against the best publicly available models: the popular mixture of logistics (MoL) WaveNet ([Parallel WaveNet (2019)](2019.10.25_Parallel_WaveGAN.md)). implementation (Yamamoto, 2018)3 and the official implementation of [WaveGlow (2018)](2018.10.31_WaveGlow.md) and [MelGAN (2019)](2019.10.08_MelGAN.md). &gt; We used the provided pretrained weights for all the models.  &gt; To evaluate the generality of ***HiFi-GAN*** to the mel-spectrogram inversion of unseen speakers, we used the VCTK multi-speaker dataset (Veaux et al., 2017), which consists of approximately 44,200 short audio clips uttered by 109 native English speakers with various accents. &gt; The total length of the audio clips is approximately 44 hours. &gt; The audio format is 16-bit PCM with a sample rate of 44 kHz. &gt; We reduced the sample rate to 22 kHz. &gt; We randomly selected nine speakers and excluded all their audio clips from the training set. &gt; We then trained MoL WaveNet, WaveGlow, and MelGAN with the same data settings; all the models were trained until 2.5M steps.  &gt; To evaluate the audio quality, we crowd-sourced 5-scale MOS tests via Amazon Mechanical Turk. &gt; The MOS scores were recorded with 95% confidence intervals (CI). &gt; Raters listened to the test samples randomly, where they were allowed to evaluate each audio sample once. &gt; All audio clips were normalized to prevent the influence of audio volume differences on the raters. &gt; All quality assessments in Section 4 were conducted in this manner, and were not sourced from other papers.  &gt; The synthesis speed was measured on GPU and CPU environments according to the recent research trends regarding the efficiency of neural networks ([MelGAN (2019)](2019.10.08_MelGAN.md), Zhai et al., 2020, Tan et al., 2020). &gt; The devices used are a single NVIDIA V100 GPU and a MacBook Pro laptop (Intel i7 CPU 2.6GHz). &gt; Additionally, we used 32-bit floating point operations for all the models without any optimization methods.  &gt; To confirm the trade-off between synthesis efficiency and sample quality, we conducted experiments based on the three variations of the generator, V1, V2, and V3 while maintaining the same discriminator configuration. &gt; For V1, we set hu= 512,ku= [16, 16, 4, 4],kr= [3, 7, 11], and `Dr= [[1, 1], [3, 1], [5, 1]] \u00d7 3]`. &gt; V2 is simply a smaller version of V1, which has a smaller hidden dimension hu= 128but with exactly the same receptive fields. &gt; To further reduce the number of layers while maintaining receptive fields wide, the kernel sizes and dilation rates of V3 were selected carefully. &gt; The detailed configurations of the models are listed in Appendix A.1. &gt; We used 80 bands mel-spectrograms as input conditions. &gt; The FFT, window, and hop size were set to 1024, 1024, and 256, respectively. &gt; The networks were trained using the AdamW optimizer (Loshchilov and Hutter, 2017) with \u03b21= 0.8,\u03b22= 0.99, and weight decay $\\lambda = 0.01$. &gt; The learning rate decay was scheduled by a 0.999 factor in every epoch with an initial learning rate of 2 \u00d7 10\u22124.."},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#4results","title":"4\u00b7Results\u00b7\u7ed3\u679c","text":""},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#41audio-quality-and-synthesis-speed","title":"4.1\u00b7Audio Quality and Synthesis Speed\u00b7\u97f3\u9891\u8d28\u91cf\u548c\u5408\u6210\u901f\u5ea6","text":"\u539f\u6587  &gt; To evaluate the performance of our models in terms of both quality and speed, we performed the MOS test for spectrogram inversion, and the speed measurement. &gt; For the MOS test, we randomly selected 50 utterances from the LJSpeech dataset and used the ground truth spectrograms of the utterances which were excluded from training as input conditions.  &gt; For easy comparison of audio quality, synthesis speed and model size, the results are compiled and presented in Table1. &gt; Remarkably, all variations of ***HiFi-GAN*** scored higher than the other models. &gt; V1 has 13.92M parameters and achieves the highest MOS with a gap of 0.09 compared to the ground truth audio; this implies that the synthesized audio is nearly indistinguishable from the human voice. &gt; In terms of synthesis speed, V1 is faster than WaveGlow and MoL WaveNet. &gt; V2 also demonstrates similarity to human quality with a MOS of 4.23 while significantly reducing the memory requirement and inference time, compared to V1. &gt; It only requires 0.92M parameters. &gt; Despite having the lowest MOS among our models, V3 can synthesize speech 13.44 times faster than real-time on CPU and 1,186 times faster than real-time on single V100 GPU while showing similar perceptual quality with MoL WaveNet. &gt; As V3 efficiently synthesizes speech on CPU, it can be well suited for on-device applications.."},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#42ablation-study","title":"4.2\u00b7Ablation Study\u00b7\u6d88\u878d\u5b9e\u9a8c","text":"\u539f\u6587  &gt; We performed an ablation study of MPD, MRF, and mel-spectrogram loss to verify the effect of each ***HiFi-GAN*** component on the quality of the synthesized audio. &gt; V3 that has the smallest expressive power among the three generator variations was used as a generator for the ablation study, and the network parameters were updated up to 500k steps for each configuration.  &gt; The results of the MOS evaluation are shown in Table 2, which show all three components contribute to the performance. &gt; RemovingMPDcauses a significant decrease in perceptual quality, whereas the absence of MSD shows a relatively small but noticeable degradation. &gt; To investigate the effect ofMRF, one residual block with the widest receptive field was retained in each MRF module. &gt; The result is also worse than the baseline. &gt; The experiment on the mel-spectrogram loss shows that it helps improve the quality, and we observed that the quality improves more stably when the loss is applied.  &gt; To verify the effect of MPD in the settings of other GAN models, we introduced MPD in MelGAN. &gt; MelGAN trained with MPD outperforms the original one by a gap of 0.47 MOS, which shows statistically significant improvement.  &gt; We experimented with periods of powers of 2 to verify the effect of periods set to prime numbers. &gt; While the period 2 allows signals to be processed closely, it results in statistically significant degradation with a difference of 0.20 MOS from the baseline.."},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#43generalization-to-unseen-speakers","title":"4.3\u00b7Generalization to Unseen Speakers\u00b7\u6cdb\u5316\u5230\u672a\u89c1\u8bf4\u8bdd\u4eba","text":"\u539f\u6587  &gt; We used 50 randomly selected utterances of nine unseen speakers in the VCTK dataset that were excluded from the training set for the MOS test. &gt; Table 3 shows the experimental results for the mel-spectrogram inversion of the unseen speakers. &gt; The three generator variations scored 3.77, 3.69, and 3.61. &gt; They were all better than AR and flow-based models, indicating that the proposed models generalize well to unseen speakers. &gt; Additionally, the tendency of difference in MOS scores of the proposed models is similar with the result shown in Section 4.1, which exhibits generalization across different datasets.."},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#44end-to-end-speech-synthesis","title":"4.4\u00b7End-to-End Speech Synthesis\u00b7\u7aef\u5230\u7aef\u8bed\u97f3\u5408\u6210","text":"\u539f\u6587  &gt; We conducted an additional experiment to examine the effectiveness of the proposed models when applied to an end-to-end speech synthesis pipeline, which consists of text to mel-spectrogram and mel spectrogram to waveform synthesis modules. &gt; We herein used Tacotron2 (Shen et al., 2018) to generate mel-spectrograms from text. &gt; Without any modification, we synthesized the mel-spectrograms using the most popular implementation of Tacotron2 (Valle, 2018a) with the provided pre-trained weights. &gt; We then fed them as input conditions into second stage models, including our models and WaveGlow used in Section 4.1.  &gt; (MelGAN and MoL WaveNet were excluded from comparison for their differences in pre-processing such as frequency clipping. Mismatch in the input representation led to producing low quality audio when combined with Tacotron2.)  &gt; The MOS scores are listed in Table 4. &gt; The results without fine-tuning show that all the proposed models outperform WaveGlow in the end-to-end setting, while the audio quality of all models are unsatisfactory compared to the ground truth audio. &gt; However, when the pixel-wise difference in the mel-spectrogram domain between generated waveforms and a mel-spectrogram from Tacotron2 are investigated as demonstrated in Fig.03, we found that the difference is insignificant, which means that the predicted mel-spectrogram from Tacotron2 was already noisy. &gt; To improve the audio quality in the end-to-end setting, we applied fine-tuning with predicted mel-spectrograms of Tacotron2 in teacher-forcing mode (Shen et al., 2018) to all the models up to 100k steps. &gt; MOS scores of all the fine-tuned proposed models over 4, whereas fine-tuned WaveGlow did not show quality improvement. &gt; We conclude that ***HiFi-GAN*** adapts well on the end-to-end setting with fine-tuning."},{"location":"TTS/Models/TTS3_Vocoder/2020.10.12_HiFi-GAN/#5conclusion","title":"5\u00b7Conclusion\u00b7\u7ed3\u8bba","text":"\u539f\u6587  &gt; In this work, we introduced ***HiFi-GAN***, which can efficiently synthesize high quality speech audio. &gt; Above all, our proposed model outperforms the best performing publicly available models in terms of synthesis quality, even comparable to human level. &gt; Moreover, it shows a significant improvement in terms of synthesis speed. &gt; We took inspiration from the characteristic of speech audio that consists of patterns with various periods and applied it to neural networks, and verified that the existence of the proposed discriminator greatly influences the quality of speech synthesis through the ablation study. &gt; Additionally, this work presents several experiments that are significant in speech synthesis applications. &gt; ***HiFi-GAN*** shows ability to generalize unseen speakers and synthesize speech audio comparable to human quality from noisy inputs in an end-to-end setting. &gt; In addition, our small footprint model demonstrates comparable sample quality with the best publicly available autoregressive counterpart, while generating samples in an order-of-magnitude faster than real-time on CPU. &gt; This shows progress towards on-device natural speech synthesis, which requires low latency and memory footprint. &gt; Finally, our experiments show that the generators of various configurations can be trained with the same discriminators and learning mechanism, which indicates the possibility of flexibly selecting a generator configuration according to the target specifications without the need for a time-consuming hyper-parameter search for the discriminators. &gt; We release ***HiFi-GAN*** as open source. &gt; We envisage that our work will serve as a basis for future speech synthesis studies..   <p>\u672c\u9879\u5de5\u4f5c\u4ecb\u7ecd\u4e86 HiFi-GAN, \u80fd\u591f\u9ad8\u6548\u5730\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u97f3\u9891. \u6211\u4eec\u7684\u6a21\u578b\u5728\u5408\u6210\u8d28\u91cf\u4e0a\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u516c\u5f00\u6a21\u578b, \u751a\u81f3\u53ef\u4ee5\u4e0e\u4eba\u7c7b\u6c34\u5e73\u76f8\u5f53. \u6b64\u5916, \u5b83\u5728\u5408\u6210\u901f\u5ea6\u65b9\u9762\u4e5f\u6709\u663e\u8457\u7684\u63d0\u5347. \u6211\u4eec\u501f\u9274\u4e86\u8bed\u97f3\u97f3\u9891\u7684\u7279\u6027\u2014\u2014\u5305\u542b\u4e0d\u540c\u5468\u671f\u7684\u6a21\u5f0f\u2014\u2014\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc, \u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u5224\u522b\u5668\u7684\u5b58\u5728\u5bf9\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u7684\u5f71\u54cd. \u6b64\u5916, \u672c\u9879\u5de5\u4f5c\u8fd8\u5c55\u793a\u4e86\u591a\u4e2a\u6709\u5173\u8bed\u97f3\u5408\u6210\u5e94\u7528\u7684\u5b9e\u9a8c. HiFi-GAN \u5728\u6cdb\u5316\u5230\u672a\u89c1\u8bf4\u8bdd\u4eba\u548c\u7aef\u5230\u7aef\u8bed\u97f3\u5408\u6210\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272, \u5e76\u80fd\u4ece\u566a\u58f0\u8f93\u5165\u4e2d\u5408\u6210\u8bed\u97f3\u97f3\u9891\u4e0e\u4eba\u7c7b\u6c34\u5e73\u76f8\u5f53. \u6b64\u5916, \u6211\u4eec\u5c0f\u578b\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u7684\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u6bd4, \u90fd\u80fd\u751f\u6210\u4e0e\u4eba\u7c7b\u6c34\u5e73\u76f8\u5f53\u7684\u97f3\u9891, \u4f46\u5728 CPU \u4e0a\u4ee5\u5b9e\u65f6\u901f\u5ea6\u751f\u6210. \u8fd9\u8868\u660e\u4e86\u8fc8\u5411\u7aef\u5230\u7aef\u8bed\u97f3\u5408\u6210\u7684\u8fc8\u8fdb, \u8981\u6c42\u4f4e\u5ef6\u8fdf\u548c\u5185\u5b58\u5360\u7528. \u6700\u540e, \u6211\u4eec\u5b9e\u9a8c\u8868\u660e, \u4e0d\u540c\u914d\u7f6e\u7684\u751f\u6210\u5668\u53ef\u4ee5\u4e0e\u5224\u522b\u5668\u4e00\u8d77\u8bad\u7ec3, \u8fd9\u8868\u660e\u4e86\u7075\u6d3b\u9009\u62e9\u751f\u6210\u5668\u914d\u7f6e\u7684\u53ef\u80fd\u6027, \u4e0d\u9700\u8981\u8017\u65f6\u9ad8\u6602\u7684\u8d85\u53c2\u6570\u641c\u7d22. \u6211\u4eec\u5c06 HiFi-GAN \u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\u53d1\u5e03. \u6211\u4eec\u671f\u5f85\u672a\u6765\u8bed\u97f3\u5408\u6210\u7814\u7a76\u5c06\u7ee7\u7eed\u6cbf\u7740\u8fd9\u4e00\u65b9\u5411\u53d1\u5c55.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/","title":"Vocos: Closing the Gap between Time-Domain and Fourier-Based Neural Vocoders for High-Quality Audio Synthesis","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: Vocos: Closing the Gap between Time-Domain and Fourier-Based Neural Vocoders for High-Quality Audio Synthesis - \u4f5c\u8005:   - [Hubert Siuzdak](../../Authors/Hubert_Siuzdak.md) - \u673a\u6784:   - [Gemelo AI](../../Institutions/Gemolo.AI.md) - \u65f6\u95f4:   - 2023.06.01 ArXiv v1   - 2023.10.03 ArXiv v2   - 2024.05.29 ArXiv v3 - \u53d1\u8868:   - 2024.01.16 [ICLR2024](../../Publications/ICLR.md) Accepted (Poster) 5/8/5/6 - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2306.00814)   - [Demo](https://gemelo-ai.github.io/vocos/)   - [Github](https://github.com/gemelo-ai/vocos) - \u6807\u7b7e:   - [\u58f0\u7801\u5668](../../Tags/Vocoder.md)   - [\u5f00\u6e90](../../Tags/OpenSource.md) - \u9875\u6570: 15 - \u5f15\u7528: ? - \u88ab\u5f15: 11"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"\u539f\u6587  &gt; Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. &gt; While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in redundant and computationally-intensive upsampling operations. &gt; Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. &gt; Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. &gt; This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. &gt; Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. &gt; The source code and model weights have been open-sourced at https://github.com/gemelo-ai/vocos.   <p>\u795e\u7ecf\u58f0\u7801\u5668\u7684\u8fd1\u671f\u8fdb\u5c55\u4e3b\u8981\u7531\u5728\u65f6\u57df\u4e2d\u8fd0\u884c\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u9a71\u52a8. \u5c3d\u7ba1\u6709\u6548, \u4f46\u8fd9\u4e00\u65b9\u6cd5\u5ffd\u7565\u4e86\u7531\u65f6\u9891\u8868\u793a\u63d0\u4f9b\u7684\u5f52\u7eb3\u504f\u7f6e, \u5bfc\u81f4\u5197\u4f59\u4e14\u8ba1\u7b97\u5bc6\u96c6\u7684\u4e0a\u91c7\u6837\u64cd\u4f5c. \u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u65f6\u9891\u8868\u793a\u662f\u4e00\u4e2a\u5438\u5f15\u4eba\u7684\u66ff\u4ee3\u65b9\u6848, \u5b83\u80fd\u66f4\u51c6\u786e\u5730\u4e0e\u4eba\u7c7b\u542c\u89c9\u611f\u77e5\u76f8\u543b\u5408, \u5e76\u53d7\u76ca\u4e8e\u8ba1\u7b97\u5176\u7684\u6210\u719f\u5feb\u901f\u7b97\u6cd5. \u7136\u800c, \u76f4\u63a5\u91cd\u5efa\u590d\u503c\u9891\u8c31\u56fe\u4e00\u76f4\u662f\u4e2a\u5386\u53f2\u95ee\u9898, \u4e3b\u8981\u662f\u56e0\u4e3a\u76f8\u4f4d\u6062\u590d\u7684\u96be\u9898. \u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u63d0\u51fa Vocos \u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d, Vocos \u662f\u4e00\u79cd\u76f4\u63a5\u751f\u6210\u5085\u91cc\u53f6\u8c31\u7cfb\u6570\u7684\u65b0\u6a21\u578b. Vocos \u4e0d\u4ec5\u5728\u8bc4\u4f30\u4e2d\u8bc1\u660e\u4e86\u5176\u97f3\u9891\u8d28\u91cf\u548c\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u76f8\u5ab2\u7f8e, \u800c\u4e14\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387, \u4e0e\u4e3b\u6d41\u7684\u65f6\u57df\u795e\u7ecf\u58f0\u7801\u5668\u65b9\u6cd5\u76f8\u6bd4, \u901f\u5ea6\u63d0\u9ad8\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7. \u6e90\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u5df2\u5728 https://github.com/gemelo-ai/vocos \u5f00\u653e.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#1introduction","title":"1.Introduction","text":"\u539f\u6587  &gt; Sound synthesis, the process of generating audio signals through electronic and computational means, has a long and rich history of innovation . &gt; Within the scope of text-to-speech (TTS), concatenative synthesis (Moulines &amp; Charpentier, 1990; Hunt &amp; Black, 1996) and statistical parametric synthesis (Yoshimura et al., 1999) were the prevailing approaches. &gt; The latter strategy relied on a source-filter theory of speech production, where the speech signal was seen as being produced by a source (the vocal cords) and then shaped by a filter (the vocal tract). &gt; In this framework, various parameters such as pitch, vocal tract shape, and voicing were estimated and then used to control a vocoder (Dudley, 1939) which would reconstruct the final audio signal. &gt; While vocoders evolved significantly (Kawahara et al., 1999; [WORLD](../../Models/TTS3_Vocoder/2016.07.01_WORLD.md)), they tended to oversimplify speech production, generating a distinctive \u201dbuzzy\u201d sound and thus compromising the naturalness of the speech.  &gt; A significant breakthrough in speech synthesis was achieved with the introduction of [WaveNet](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md), a deep generative model for raw audio waveforms. &gt; WaveNet proposed a novel approach to handle audio signals by modeling them autoregressively in the time-domain, using dilated convolutions to broaden receptive fields and consequently capture long-range temporal dependencies. &gt; In contrast to the traditional parametric vocoders which incorporate prior knowledge about audio signals, WaveNet solely depends on end-to-end learning.  &gt; Since the advent of WaveNet, modeling distribution of audio samples in the time-domain has become the most popular approach in the field of audio synthesis. &gt; The primary methods have fallen into two major categories: autoregressive models and non-autoregressive models. &gt; Autoregressive models, like WaveNet, generate audio samples sequentially, conditioning each new sample on all previously generated ones ([SampleRNN](../../Models/TTS3_Vocoder/2016.12.22_SampleRNN.md); [WaveRNN](../../Models/TTS3_Vocoder/2018.02.23_WaveRNN.md); [LPCNet](../../Models/TTS3_Vocoder/2018.10.28_LPCNet.md)). &gt; On the other hand, non-autoregressive models generate all samples independently, parallelizing the process and making it more computationally efficient ([Parallel WaveNet](../../Models/TTS3_Vocoder/2017.11.28_Parallel_WaveNet.md); [WaveGlow](../../Models/TTS3_Vocoder/2018.10.31_WaveGlow.md); Donahue et al., 2018).   <p>\u58f0\u97f3\u5408\u6210, \u5373\u901a\u8fc7\u7535\u5b50\u548c\u8ba1\u7b97\u624b\u6bb5\u751f\u6210\u97f3\u9891\u4fe1\u53f7\u7684\u8fc7\u7a0b, \u62e5\u6709\u4e00\u6bb5\u6f2b\u957f\u4e14\u4e30\u5bcc\u7684\u521b\u65b0\u5386\u53f2. \u5728\u6587\u672c\u5230\u8bed\u97f3 (TTS) \u7684\u9886\u57df\u5185, \u8fde\u63a5\u5408\u6210 (Moulines &amp; Charpentier, 1990; Hunt &amp; Black, 1996) \u548c\u7edf\u8ba1\u53c2\u6570\u5408\u6210 (Yoshimura et al., 1999) \u66fe\u662f\u4e3b\u6d41\u65b9\u6cd5. \u540e\u4e00\u79cd\u7b56\u7565\u4f9d\u8d56\u4e8e\u8bed\u97f3\u4ea7\u751f\u7684\u6e90\u6ee4\u6ce2\u7406\u8bba, \u5176\u4e2d\u8bed\u97f3\u4fe1\u53f7\u88ab\u89c6\u4e3a\u7531\u6e90 (\u58f0\u5e26) \u4ea7\u751f, \u7136\u540e\u901a\u8fc7\u6ee4\u6ce2\u5668 (\u58f0\u9053) \u5851\u5f62. \u5728\u8fd9\u4e00\u6846\u67b6\u4e2d, \u8bf8\u5982\u97f3\u9ad8, \u58f0\u9053\u5f62\u72b6\u548c\u53d1\u58f0\u7b49\u5404\u7c7b\u53c2\u6570\u88ab\u4f30\u7b97, \u5e76\u7528\u4e8e\u63a7\u5236\u58f0\u7801\u5668 (Dudley, 1939), \u540e\u8005\u5c06\u91cd\u6784\u6700\u7ec8\u7684\u97f3\u9891\u4fe1\u53f7. \u5c3d\u7ba1\u58f0\u7801\u5668\u7ecf\u5386\u4e86\u663e\u8457\u7684\u6f14\u8fdb (Kawahara et al., 1999; Morise et al., 2016), \u5b83\u4eec\u5f80\u5f80\u8fc7\u5ea6\u7b80\u5316\u8bed\u97f3\u4ea7\u751f\u8fc7\u7a0b, \u4ea7\u751f\u4e00\u79cd\u72ec\u7279\u7684\u201c\u55e1\u55e1\u201d\u58f0, \u4ece\u800c\u635f\u5bb3\u4e86\u8bed\u97f3\u7684\u81ea\u7136\u5ea6.</p> <p>\u8bed\u97f3\u5408\u6210\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u7a81\u7834\u662f WaveNet \u7684\u5f15\u5165, \u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u539f\u59cb\u97f3\u9891\u6ce2\u5f62\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b. WaveNet\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u5904\u7406\u97f3\u9891\u4fe1\u53f7, \u901a\u8fc7\u5728\u65f6\u57df\u4e2d\u81ea\u56de\u5f52\u5730\u5efa\u6a21\u5b83\u4eec, \u4f7f\u7528\u81a8\u80c0\u5377\u79ef\u6765\u6269\u5927\u611f\u53d7\u91ce, \u4ece\u800c\u6355\u6349\u957f\u8ddd\u79bb\u7684\u65f6\u95f4\u4f9d\u8d56\u6027. \u4e0e\u4f20\u7edf\u7684\u5305\u542b\u97f3\u9891\u4fe1\u53f7\u5148\u9a8c\u77e5\u8bc6\u7684\u53c2\u6570\u58f0\u7801\u5668\u4e0d\u540c, WaveNet\u5b8c\u5168\u4f9d\u8d56\u4e8e\u7aef\u5230\u7aef\u5b66\u4e60.</p> <p>\u81eaWaveNet\u95ee\u4e16\u4ee5\u6765, \u5728\u65f6\u57df\u4e2d\u5efa\u6a21\u97f3\u9891\u6837\u672c\u5206\u5e03\u5df2\u6210\u4e3a\u97f3\u9891\u5408\u6210\u9886\u57df\u6700\u6d41\u884c\u7684\u65b9\u6cd5. \u4e3b\u8981\u65b9\u6cd5\u53ef\u4ee5\u5206\u4e3a\u4e24\u5927\u7c7b\uff1a\u81ea\u56de\u5f52\u6a21\u578b\u548c\u975e\u81ea\u56de\u5f52\u6a21\u578b. \u81ea\u56de\u5f52\u6a21\u578b, \u5982WaveNet, \u6309\u987a\u5e8f\u751f\u6210\u97f3\u9891\u6837\u672c, \u6bcf\u4e2a\u65b0\u6837\u672c\u90fd\u4f9d\u8d56\u4e8e\u6240\u6709\u5148\u524d\u751f\u6210\u7684\u6837\u672c (Mehri et al., 2016; Kalchbrenner et al., 2018;) . \u53e6\u4e00\u65b9\u9762, \u975e\u81ea\u56de\u5f52\u6a21\u578b\u72ec\u7acb\u751f\u6210\u6240\u6709\u6837\u672c, \u5e76\u884c\u5316\u8fc7\u7a0b, \u4f7f\u5176\u5728\u8ba1\u7b97\u4e0a\u66f4\u9ad8\u6548 (Oord et al., 2018; Prenger et al., 2019; Donahue et al., 2018) .</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#11challenges-of-modeling-phase-spectrum","title":"1.1.Challenges of Modeling Phase Spectrum","text":"\u539f\u6587  &gt; Despite considerable advancements in time-domain audio synthesis, efforts to generate spectral representations of signals have been relatively limited. &gt; While it\u2019s possible to perfectly reconstruct the original signal from its Short-Time Fourier Transform (STFT), in many applications, only the magnitude of the STFT is utilized, leading to inherent information loss. &gt; The magnitude of the STFT provides a clear understanding of the signal by indicating the amplitude of different frequency components throughout its duration. &gt; In contrast, phase information is less intuitive and its manipulation can often yield unpredictable results.  &gt; Modeling the phase distribution presents challenges due to its intricate nature in the time-frequency domain. &gt; Phase spectrum exhibits a periodic structure causing wrapping around the principal values within the range of $(\u2212\\pi, \\pi]$ (Figure 1). &gt; Furthermore, the literature does not provide a definitive answer regarding the perceptual importance of phase-related information in speech (Wang &amp; Lim, 1982; Paliwal et al., 2011). &gt; However, improved phase spectrum estimates have been found to minimize perceptual impairments (Saratxaga et al., 2012). &gt; Researchers have explored the use of deep learning for directly modeling the phase spectrum, but this remains a challenging area (Williamson et al., 2015).   <p>\u5c3d\u7ba1\u5728\u65f6\u57df\u97f3\u9891\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55, \u4f46\u5bf9\u4e8e\u751f\u6210\u4fe1\u53f7\u9891\u8c31\u8868\u793a\u7684\u52aa\u529b\u76f8\u5bf9\u6709\u9650. \u867d\u7136\u53ef\u4ee5\u4ece\u5176\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362 (STFT) \u5b8c\u7f8e\u91cd\u5efa\u539f\u59cb\u4fe1\u53f7, \u4f46\u5728\u8bb8\u591a\u5e94\u7528\u4e2d, \u4ec5\u4f7f\u7528STFT\u7684\u5e45\u5ea6, \u5bfc\u81f4\u56fa\u6709\u4fe1\u606f\u4e22\u5931. STFT\u7684\u5e45\u5ea6\u901a\u8fc7\u6307\u793a\u4fe1\u53f7\u6301\u7eed\u65f6\u95f4\u5185\u4e0d\u540c\u9891\u7387\u6210\u5206\u7684\u632f\u5e45, \u63d0\u4f9b\u4e86\u5bf9\u4fe1\u53f7\u7684\u6e05\u6670\u7406\u89e3. \u76f8\u6bd4\u4e4b\u4e0b, \u76f8\u4f4d\u4fe1\u606f\u4e0d\u90a3\u4e48\u76f4\u89c2, \u5176\u64cd\u7eb5\u5f80\u5f80\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u7ed3\u679c.</p> <p>\u7531\u4e8e\u76f8\u4f4d\u5728\u65f6\u9891\u57df\u4e2d\u7684\u590d\u6742\u6027\u8d28, \u5efa\u6a21\u76f8\u4f4d\u5206\u5e03\u5b58\u5728\u6311\u6218. \u76f8\u4f4d\u8c31\u8868\u73b0\u51fa\u5468\u671f\u6027\u7ed3\u6784, \u5bfc\u81f4\u5728 $(\u2212\\pi, \\pi]$ \u8303\u56f4\u5185\u56f4\u7ed5\u4e3b\u503c\u7f20\u7ed5 (\u56fe 01) .</p> <p></p> <p>\u6b64\u5916, \u6587\u732e\u4e2d\u5bf9\u4e8e\u8bed\u97f3\u4e2d\u4e0e\u76f8\u4f4d\u76f8\u5173\u7684\u4fe1\u606f\u611f\u77e5\u91cd\u8981\u6027\u6ca1\u6709\u7ed9\u51fa\u660e\u786e\u7684\u7b54\u6848 (Wang &amp; Lim, 1982; Paliwal et al., 2011) . \u7136\u800c, \u6539\u8fdb\u7684\u76f8\u4f4d\u8c31\u4f30\u8ba1\u5df2\u88ab\u53d1\u73b0\u53ef\u4ee5\u6700\u5c0f\u5316\u611f\u77e5\u635f\u4f24 (Saratxaga et al., 2012) . \u7814\u7a76\u4eba\u5458\u5df2\u7ecf\u63a2\u7d22\u4e86\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u76f4\u63a5\u5efa\u6a21\u76f8\u4f4d\u8c31\u7684\u65b9\u6cd5, \u4f46\u8fd9\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u9886\u57df (Williamson et al., 2015) .</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#12contribution","title":"1.2.Contribution","text":"\u539f\u6587  &gt; Attempts to model Fourier-related coefficients with generative models have not achieved the same level of success as has been seen with modeling audio in the time-domain. &gt; This study focuses on bridging that gap with the following contributions: &gt; - We propose Vocos \u2013 a GAN-based vocoder, trained to produce complex STFT coefficients of an audio clip. &gt; Unlike conventional neural vocoder architectures that rely on transposed convolutions for upsampling, this work proposes maintaining the same feature temporal resolution across all layers. &gt; The upsampling to waveform is realized through the Inverse Fast Fourier Transform. &gt; - To estimate phase angles, we propose a simple activation function defined in terms of a unit circle. &gt; This approach naturally incorporates implicit phase wrapping, ensuring meaningful values across all phase angles. &gt; - As Vocos maintains a low temporal resolution throughout the network, we revisited the need to use dilated convolutions, typical to time-domain vocoders. &gt; Our results indicate that integrating ConvNeXt (Liu et al., 2022) blocks contributes to better performance. &gt; - Our extensive evaluation shows that Vocos matches the state-of-the-art in audio quality while demonstrating over an order of magnitude increase in speed compared to time-domain counterparts. &gt; The source code and model weights have been made open-source, enabling further exploration and potential advancements in the field of neural vocoding.   <p>\u5c1d\u8bd5\u4f7f\u7528\u751f\u6210\u6a21\u578b\u5bf9\u4e0e\u5085\u91cc\u53f6\u76f8\u5173\u7684\u7cfb\u6570\u8fdb\u884c\u5efa\u6a21\u7684\u52aa\u529b, \u5e76\u672a\u8fbe\u5230\u4e0e\u65f6\u57df\u4e2d\u5efa\u6a21\u97f3\u9891\u76f8\u540c\u7684\u6210\u529f\u6c34\u5e73. \u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4ee5\u4e0b\u8d21\u732e\u6765\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff1a - \u6211\u4eec\u63d0\u51fa\u4e86Vocos\u2014\u2014\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u58f0\u7801\u5668, \u8bad\u7ec3\u7528\u4e8e\u751f\u6210\u97f3\u9891\u7247\u6bb5\u7684\u590d\u6570STFT\u7cfb\u6570. \u4e0e\u4f9d\u8d56\u8f6c\u7f6e\u5377\u79ef\u8fdb\u884c\u4e0a\u91c7\u6837\u7684\u4f20\u7edf\u795e\u7ecf\u58f0\u7801\u5668\u67b6\u6784\u4e0d\u540c, \u672c\u5de5\u4f5c\u5efa\u8bae\u5728\u6240\u6709\u5c42\u4e2d\u4fdd\u6301\u76f8\u540c\u7684\u7279\u5f81\u65f6\u95f4\u5206\u8fa8\u7387. \u6ce2\u5f62\u7684\u4e0a\u91c7\u6837\u901a\u8fc7\u9006\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u5b9e\u73b0. - \u4e3a\u4e86\u4f30\u8ba1\u76f8\u4f4d\u89d2, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u6fc0\u6d3b\u51fd\u6570, \u5176\u5b9a\u4e49\u57fa\u4e8e\u5355\u4f4d\u5706. \u8fd9\u79cd\u65b9\u6cd5\u81ea\u7136\u5730\u5305\u542b\u4e86\u9690\u5f0f\u7684\u76f8\u4f4d\u7f20\u7ed5, \u786e\u4fdd\u6240\u6709\u76f8\u4f4d\u89d2\u90fd\u5177\u6709\u6709\u610f\u4e49\u7684\u503c. - \u7531\u4e8eVocos\u5728\u6574\u4e2a\u7f51\u7edc\u4e2d\u4fdd\u6301\u8f83\u4f4e\u7684\u65f6\u95f4\u5206\u8fa8\u7387, \u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u4f7f\u7528\u81a8\u80c0\u5377\u79ef\u7684\u9700\u6c42, \u8fd9\u662f\u65f6\u57df\u58f0\u7801\u5668\u7684\u5178\u578b\u7279\u5f81. \u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e, \u6574\u5408ConvNeXt (Liu et al., 2022) \u5757\u6709\u52a9\u4e8e\u63d0\u9ad8\u6027\u80fd. - \u6211\u4eec\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a, Vocos\u5728\u97f3\u9891\u8d28\u91cf\u4e0a\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u76f8\u5339\u914d, \u540c\u65f6\u4e0e\u65f6\u57df\u5bf9\u5e94\u65b9\u6cd5\u76f8\u6bd4, \u901f\u5ea6\u63d0\u9ad8\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7. \u6e90\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u5df2\u5f00\u6e90, \u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u548c\u6f5c\u5728\u8fdb\u5c55\u63d0\u4f9b\u4e86\u53ef\u80fd.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#2related-works","title":"2.Related Works","text":"\u539f\u6587  &gt; ### GAN-based vocoders  &gt; [Generative Adversarial Networks (GANs)](../_Basis/2014.06.10_GAN.md), have achieved significant success in image generation, sparking interest from audio researchers due to their ability for fast and parallel waveform generation (Donahue et al., 2018; Engel et al., 2018). &gt; Progress was made with the introduction of advanced critics, such as the multi-scale discriminator (MSD) (Kumar et al., 2019) and the multi-period discriminator (MPD) ([HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md)). &gt; These works also adopted a feature-matching loss to minimize the distance between the discriminator feature maps of real and synthetic audio. &gt; To discriminate between real and generated samples, also multi-resolution spectrograms (MRD) were employed (Jang et al., 2021).  &gt; At this point the standard practice involves using a stack of dilated convolutions to increase the receptive field, and transposed convolutions to sequentially upsample the feature sequence to the waveform. &gt; However, this design is known to be susceptible to aliasing artifacts, and there are works suggesting more specialized modules for both the discriminator (Bak et al., 2022) and generator (Lee et al., 2022). &gt; The historical jump in quality is largely attributed to discriminators that are able to capture implicit structures by examining input audio signal at various periods or scales. &gt; It has been argued (You et al., 2021) that the architectural details of the generators do not significantly affect the vocoded outcome, given a well-established multi-resolution discriminating framework. &gt; Contrary to these methods, Vocos presents a carefully designed, frequency-aware generator that models the distribution of Fourier spectral coefficients, rather than modeling waveforms in the time domain.   <p>\u57fa\u4e8eGAN\u7684\u58f0\u7801\u5668 \u751f\u6210\u5bf9\u6297\u7f51\u7edc (GANs) \u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f, \u7531\u4e8e\u5176\u80fd\u591f\u5feb\u901f\u5e76\u884c\u751f\u6210\u6ce2\u5f62, \u5f15\u8d77\u4e86\u97f3\u9891\u7814\u7a76\u4eba\u5458\u7684\u5174\u8da3 (Donahue et al., 2018; Engel et al., 2018) . \u968f\u7740\u5148\u8fdb\u6279\u8bc4\u5668\u7684\u5f15\u5165, \u5982\u591a\u5c3a\u5ea6\u5224\u522b\u5668 (MSD)  (Kumar et al., 2019) \u548c\u591a\u5468\u671f\u5224\u522b\u5668 (MPD), \u53d6\u5f97\u4e86\u8fdb\u5c55. \u8fd9\u4e9b\u5de5\u4f5c\u8fd8\u91c7\u7528\u4e86\u7279\u5f81\u5339\u914d\u635f\u5931\u6765\u6700\u5c0f\u5316\u771f\u5b9e\u97f3\u9891\u548c\u5408\u6210\u97f3\u9891\u7684\u5224\u522b\u5668\u7279\u5f81\u56fe\u4e4b\u95f4\u7684\u8ddd\u79bb. \u4e3a\u4e86\u533a\u5206\u771f\u5b9e\u548c\u751f\u6210\u7684\u6837\u672c, \u8fd8\u4f7f\u7528\u4e86\u591a\u5206\u8fa8\u7387\u9891\u8c31\u56fe (MRD)  (Jang et al., 2021) .</p> <p>\u76ee\u524d, \u6807\u51c6\u505a\u6cd5\u6d89\u53ca\u4f7f\u7528\u4e00\u53e0\u81a8\u80c0\u5377\u79ef\u6765\u589e\u52a0\u611f\u53d7\u91ce, \u5e76\u4f7f\u7528\u8f6c\u7f6e\u5377\u79ef\u6765\u987a\u5e8f\u4e0a\u91c7\u6837\u7279\u5f81\u5e8f\u5217\u5230\u6ce2\u5f62. \u7136\u800c, \u8fd9\u79cd\u8bbe\u8ba1\u5bb9\u6613\u53d7\u5230\u6df7\u53e0\u4f2a\u5f71\u7684\u5f71\u54cd, \u5e76\u4e14\u6709\u5de5\u4f5c\u5efa\u8bae\u4e3a\u5224\u522b\u5668 (Bak et al., 2022) \u548c\u751f\u6210\u5668 (Lee et al., 2022) \u4f7f\u7528\u66f4\u4e13\u95e8\u7684\u6a21\u5757. \u8d28\u91cf\u7684\u5386\u53f2\u6027\u98de\u8dc3\u4e3b\u8981\u5f52\u529f\u4e8e\u80fd\u591f\u901a\u8fc7\u5728\u4e0d\u540c\u5468\u671f\u6216\u5c3a\u5ea6\u4e0a\u68c0\u67e5\u8f93\u5165\u97f3\u9891\u4fe1\u53f7\u6765\u6355\u6349\u9690\u5f0f\u7ed3\u6784\u7684\u5224\u522b\u5668. \u6709\u4eba\u8ba4\u4e3a (You et al., 2021), \u5728\u5efa\u7acb\u7684\u591a\u5206\u8fa8\u7387\u5224\u522b\u6846\u67b6\u4e0b, \u751f\u6210\u5668\u7684\u67b6\u6784\u7ec6\u8282\u5bf9\u58f0\u7801\u7ed3\u679c\u7684\u5f71\u54cd\u4e0d\u5927. \u4e0e\u8fd9\u4e9b\u65b9\u6cd5\u76f8\u53cd, Vocos\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u5fc3\u8bbe\u8ba1\u7684, \u9891\u7387\u611f\u77e5\u7684\u751f\u6210\u5668, \u5b83\u5efa\u6a21\u5085\u91cc\u53f6\u8c31\u7cfb\u6570\u7684\u5206\u5e03, \u800c\u4e0d\u662f\u5728\u65f6\u57df\u4e2d\u5efa\u6a21\u6ce2\u5f62.</p> \u539f\u6587  &gt; ### Phase and Magnitude Estimation  &gt; Historically, the phase estimation problem has been at the core of audio signal reconstruction. &gt; Traditional methods usually rely on the Griffin-Lim algorithm (Griffin &amp; Lim, 1984), which iteratively estimate the phase by enforcing spectrogram consistency. &gt; However, the Griffin-Lim method introduces unnatural artifacts into synthesized speech. &gt; Several methods have been proposed for reconstructing phase using deep neural networks, including likelihood-based approaches (Takamichi et al., 2018) and GANs (Oyamada et al., 2018). &gt; Another line of work suggests perceptual phase quantization (Kim, 2003), which has proven promising in deep learning by treating the phase estimation problem as a classification problem (Takahashi et al., 2018).  &gt; Despite their effectiveness, these models assume the availability of a full-scale magnitude spectrogram, while modern audio synthesis pipelines often employ more compact representations, such as mel-spectrograms (Shen et al., 2018). &gt; Furthermore, recent research is focusing on leveraging latent features extracted by pretrained deep learning models (Polyak et al., 2021; Siuzdak et al., 2022).  &gt; Closer to this paper are studies that estimate both the magnitude and phase spectrum. &gt; This can be done either implicitly, by predicting the real and imaginary parts of the STFT, or explicitly, by parameterizing the model to generate the phase and magnitude components. &gt; In the former category, Gritsenko et al. (2020) presents a variant of a model trained to produce STFT coefficients. &gt; They recognized the significance of adversarial objective in preventing robotic sound quality, however they were unable to train it successfully due to its inherent instability. &gt; On the other hand, iSTFTNet (Kaneko et al., 2022) proposes modifications to HiFi-GAN, enabling it to return magnitude and phase spectrum. &gt; However, their optimal model only replaces the last two upsample blocks with inverse STFT, leaving the majority of the upsampling to be realized with transposed convolutions. &gt; They find that replacing more upsampling layers drastically degrades the quality. &gt; Pasini &amp; Schl\u00a8uter (2022) were able to successfully model the magnitude and phase spectrum of audio with higher frequency resolution, although it required multi-step training (Caillon &amp; Esling, 2021), because of the adversarial objective instability. &gt; Also, the initial studies using GANs to generate invertible spectrograms involved estimating instantaneous frequency (Engel et al., 2018). &gt; However, these were limited to a single dataset containing only individual musical instrument notes, with the assumption of a constant instantaneous frequency.   <p></p> <p>\u76f8\u4f4d\u548c\u5e45\u5ea6\u4f30\u8ba1 \u5386\u53f2\u4e0a, \u76f8\u4f4d\u4f30\u8ba1\u95ee\u9898\u4e00\u76f4\u662f\u97f3\u9891\u4fe1\u53f7\u91cd\u5efa\u7684\u6838\u5fc3. \u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8eGriffin-Lim\u7b97\u6cd5 (Griffin &amp; Lim, 1984), \u8be5\u7b97\u6cd5\u901a\u8fc7\u5f3a\u5236\u9891\u8c31\u56fe\u4e00\u81f4\u6027\u8fed\u4ee3\u4f30\u8ba1\u76f8\u4f4d. \u7136\u800c, Griffin-Lim\u65b9\u6cd5\u5728\u5408\u6210\u8bed\u97f3\u4e2d\u5f15\u5165\u4e86\u4e0d\u81ea\u7136\u7684\u4f2a\u5f71. \u5df2\u7ecf\u63d0\u51fa\u4e86\u51e0\u79cd\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u91cd\u5efa\u76f8\u4f4d\u7684\u65b9\u6cd5, \u5305\u62ec\u57fa\u4e8e\u4f3c\u7136\u7684\u65b9\u6cd5 (Takamichi et al., 2018) \u548cGANs (Oyamada et al., 2018) . \u53e6\u4e00\u7cfb\u5217\u5de5\u4f5c\u5efa\u8bae\u611f\u77e5\u76f8\u4f4d\u91cf\u5316 (Kim, 2003), \u901a\u8fc7\u5c06\u76f8\u4f4d\u4f30\u8ba1\u95ee\u9898\u89c6\u4e3a\u5206\u7c7b\u95ee\u9898, \u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5df2\u88ab\u8bc1\u660e\u662f\u6709\u524d\u9014\u7684 (Takahashi et al., 2018) .</p> <p>\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u6709\u6548, \u4f46\u5b83\u4eec\u5047\u8bbe\u53ef\u4ee5\u4f7f\u7528\u5168\u5c3a\u5ea6\u5e45\u5ea6\u9891\u8c31\u56fe, \u800c\u73b0\u4ee3\u97f3\u9891\u5408\u6210\u7ba1\u9053\u901a\u5e38\u91c7\u7528\u66f4\u7d27\u51d1\u7684\u8868\u793a, \u5982\u6885\u5c14\u9891\u8c31\u56fe (Shen et al., 2018) . \u6b64\u5916, \u6700\u8fd1\u7684\u7814\u7a76\u96c6\u4e2d\u5728\u5229\u7528\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u7684\u6f5c\u5728\u7279\u5f81\u4e0a (Polyak et al., 2021; Siuzdak et al., 2022) .</p> <p>\u4e0e\u672c\u6587\u66f4\u63a5\u8fd1\u7684\u662f\u4f30\u8ba1\u5e45\u5ea6\u548c\u76f8\u4f4d\u8c31\u7684\u7814\u7a76. \u8fd9\u53ef\u4ee5\u901a\u8fc7\u9690\u5f0f\u5730\u9884\u6d4bSTFT\u7684\u5b9e\u90e8\u548c\u865a\u90e8, \u6216\u8005\u663e\u5f0f\u5730\u53c2\u6570\u5316\u6a21\u578b\u4ee5\u751f\u6210\u76f8\u4f4d\u548c\u5e45\u5ea6\u7ec4\u4ef6\u6765\u5b8c\u6210. \u5728\u524d\u4e00\u7c7b\u4e2d, Gritsenko et al. (2020) \u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u53d8\u4f53, \u8bad\u7ec3\u7528\u4e8e\u751f\u6210STFT\u7cfb\u6570. \u4ed6\u4eec\u8ba4\u8bc6\u5230\u5bf9\u6297\u6027\u76ee\u6807\u5728\u9632\u6b62\u673a\u5668\u4eba\u58f0\u97f3\u8d28\u91cf\u65b9\u9762\u7684\u91cd\u8981\u6027, \u4f46\u7531\u4e8e\u5176\u56fa\u6709\u7684\u4e0d\u7a33\u5b9a\u6027, \u4ed6\u4eec\u65e0\u6cd5\u6210\u529f\u8bad\u7ec3\u5b83. \u53e6\u4e00\u65b9\u9762, iSTFTNet (Kaneko et al., 2022) \u63d0\u51fa\u4e86\u5bf9HiFi-GAN\u7684\u4fee\u6539, \u4f7f\u5176\u80fd\u591f\u8fd4\u56de\u5e45\u5ea6\u548c\u76f8\u4f4d\u8c31. \u7136\u800c, \u4ed6\u4eec\u7684\u6700\u4f73\u6a21\u578b\u4ec5\u7528\u9006STFT\u66ff\u6362\u4e86\u6700\u540e\u4e24\u4e2a\u4e0a\u91c7\u6837\u5757, \u5c06\u5927\u90e8\u5206\u4e0a\u91c7\u6837\u7559\u7ed9\u8f6c\u7f6e\u5377\u79ef\u5b9e\u73b0. \u4ed6\u4eec\u53d1\u73b0, \u66ff\u6362\u66f4\u591a\u7684\u4e0a\u91c7\u6837\u5c42\u4f1a\u5927\u5927\u964d\u4f4e\u8d28\u91cf. Pasini &amp; Schl\u00a8uter (2022) \u80fd\u591f\u6210\u529f\u5730\u5bf9\u5177\u6709\u66f4\u9ad8\u9891\u7387\u5206\u8fa8\u7387\u7684\u97f3\u9891\u7684\u5e45\u5ea6\u548c\u76f8\u4f4d\u8c31\u8fdb\u884c\u5efa\u6a21, \u5c3d\u7ba1\u7531\u4e8e\u5bf9\u6297\u6027\u76ee\u6807\u7684\u4e0d\u7a33\u5b9a\u6027, \u5b83\u9700\u8981\u591a\u6b65\u9aa4\u8bad\u7ec3 (Caillon &amp; Esling, 2021) . \u6b64\u5916, \u6700\u521d\u4f7f\u7528GAN\u751f\u6210\u53ef\u9006\u9891\u8c31\u56fe\u7684\u7814\u7a76\u6d89\u53ca\u4f30\u8ba1\u77ac\u65f6\u9891\u7387 (Engel et al., 2018) . \u7136\u800c, \u8fd9\u4e9b\u7814\u7a76\u4ec5\u9650\u4e8e\u5305\u542b\u5355\u4e2a\u4e50\u5668\u97f3\u7b26\u7684\u5355\u4e2a\u6570\u636e\u96c6, \u5e76\u5047\u8bbe\u77ac\u65f6\u9891\u7387\u6052\u5b9a.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#3methodology","title":"3.Methodology","text":""},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#31overview","title":"3.1.Overview","text":"\u539f\u6587  &gt; At its core, the proposed GAN model uses Fourier-based time-frequency representation as the target data distribution for the generator. &gt; Vocos is constructed without any transposed convolutions; instead, the upsample operation is realized solely through the fast inverse STFT. &gt; This approach permits a unique model design compared to time-domain vocoders, which typically employ a series of upsampling layers to inflate input features to the target waveform\u2019s resolution, often necessitating upscaling by several hundred times. &gt; In contrast, Vocos maintains the same temporal resolution throughout the network (Figure 2). &gt; This design, known as an isotropic architecture, has been found to work well in various settings, including [Transformer](../../Models/_Basis/2017.06.12_Transformer.md). &gt; This approach can also be particularly beneficial for audio synthesis. &gt; Traditional methods often use transposed convolutions that can introduce aliasing artifacts, necessitating additional measures to mitigate the issue (Karras et al., 2021; Lee et al., 2022). &gt; Vocos eliminates learnable upsampling layers, and instead employs the well-establish inverse Fourier transform to reconstruct the original-scale waveform. &gt; In the context of converting mel-spectrograms into audio signal, the temporal resolution is dictated by the hop size of the STFT.  &gt; Vocos uses the Short-Time Fourier Transform (STFT) to represent audio signals in the time-frequency domain:  $$   STFT_{x}[m,k]=\\sum_{n=0}^{N-1} x[n] w[n-m] e^{-j2\\pi kn/N} \\tag{01} $$  &gt; The STFT applies the Fourier transform to successive windowed sections of the signal. &gt; In practice, the STFT is computed by taking a sequence of Fast Fourier Transforms (FFTs) on overlapping, windowed frames of data, which are created as the window function advances or \u201chops\u201d through time.   <p>\u5728\u6838\u5fc3\u90e8\u5206, \u6240\u63d0\u51fa\u7684GAN\u6a21\u578b\u4f7f\u7528\u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u65f6\u9891\u8868\u793a\u4f5c\u4e3a\u751f\u6210\u5668\u7684\u76ee\u6807\u6570\u636e\u5206\u5e03. Vocos \u7684\u6784\u5efa\u4e0d\u5305\u542b\u4efb\u4f55\u8f6c\u7f6e\u5377\u79ef\uff1b\u76f8\u53cd, \u4e0a\u91c7\u6837\u64cd\u4f5c\u4ec5\u901a\u8fc7\u5feb\u901f\u9006STFT\u5b9e\u73b0. \u8fd9\u79cd\u65b9\u6cd5\u4e0e\u901a\u5e38\u91c7\u7528\u4e00\u7cfb\u5217\u4e0a\u91c7\u6837\u5c42\u5c06\u8f93\u5165\u7279\u5f81\u81a8\u80c0\u5230\u76ee\u6807\u6ce2\u5f62\u5206\u8fa8\u7387\u7684\u65f6\u57df\u58f0\u7801\u5668\u76f8\u6bd4, \u5141\u8bb8\u4e86\u4e00\u79cd\u72ec\u7279\u7684\u6a21\u578b\u8bbe\u8ba1, \u901a\u5e38\u9700\u8981\u6570\u767e\u500d\u7684\u4e0a\u91c7\u6837. \u76f8\u6bd4\u4e4b\u4e0b, Vocos \u5728\u6574\u4e2a\u7f51\u7edc\u4e2d\u4fdd\u6301\u76f8\u540c\u7684\u65f6\u95f4\u5206\u8fa8\u7387 (\u56fe 02) .</p> <p></p> <p>\u8fd9\u79cd\u8bbe\u8ba1, \u79f0\u4e3a\u5404\u5411\u540c\u6027\u67b6\u6784, \u5df2\u5728\u5305\u62ec Transformer \u5728\u5185\u7684\u5404\u79cd\u8bbe\u7f6e\u4e2d\u88ab\u53d1\u73b0\u6548\u679c\u826f\u597d. \u8fd9\u79cd\u65b9\u6cd5\u5bf9\u97f3\u9891\u5408\u6210\u4e5f\u53ef\u80fd\u7279\u522b\u6709\u76ca. \u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u8f6c\u7f6e\u5377\u79ef, \u8fd9\u53ef\u80fd\u5f15\u5165\u6df7\u53e0\u4f2a\u5f71, \u9700\u8981\u91c7\u53d6\u989d\u5916\u63aa\u65bd\u6765\u7f13\u89e3\u95ee\u9898 (Karras et al., 2021; Lee et al., 2022) . Vocos \u6d88\u9664\u4e86\u53ef\u5b66\u4e60\u7684\u4e0a\u91c7\u6837\u5c42, \u5e76\u6539\u7528\u5df2\u5efa\u7acb\u7684\u9006\u5085\u91cc\u53f6\u53d8\u6362\u6765\u91cd\u5efa\u539f\u59cb\u5c3a\u5ea6\u7684\u6ce2\u5f62. \u5728\u5c06\u6885\u5c14\u9891\u8c31\u56fe\u8f6c\u6362\u4e3a\u97f3\u9891\u4fe1\u53f7\u7684\u4e0a\u4e0b\u6587\u4e2d, \u65f6\u95f4\u5206\u8fa8\u7387\u7531STFT\u7684\u8df3\u8dc3\u5927\u5c0f\u51b3\u5b9a.</p> <p>Vocos \u4f7f\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362 (STFT) \u5728\u65f6\u9891\u57df\u4e2d\u8868\u793a\u97f3\u9891\u4fe1\u53f7\uff1a</p> <p>$$   STFT_{x}[m,k]=\\sum_{n=0}^{N-1} x[n] w[n-m] e^{-j2\\pi kn/N} \\tag{01} $$</p> <p>STFT\u5bf9\u4fe1\u53f7\u7684\u8fde\u7eed\u7a97\u53e3\u5316\u90e8\u5206\u5e94\u7528\u5085\u91cc\u53f6\u53d8\u6362. \u5b9e\u9645\u4e0a, STFT\u901a\u8fc7\u5bf9\u91cd\u53e0\u7684, \u7a97\u53e3\u5316\u7684\u6570\u636e\u5e27\u5e8f\u5217\u8fdb\u884c\u4e00\u7cfb\u5217\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362 (FFTs) \u6765\u8ba1\u7b97, \u8fd9\u4e9b\u6570\u636e\u5e27\u968f\u7740\u7a97\u53e3\u51fd\u6570\u5728\u65f6\u95f4\u4e0a\u63a8\u8fdb\u6216\u201c\u8df3\u8dc3\u201d\u800c\u521b\u5efa.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#32model","title":"3.2.Model","text":"\u539f\u6587  &gt; #### Backbone  &gt; Vocos adapts ConvNeXt (Liu et al., 2022) as the foundational backbone for the generator. &gt; It first embeds the input features into a hidden dimensionality and then applies a stack of 1D convolutional blocks. &gt; Each block consists of a depthwise convolution, followed by an inverted bottleneck that projects features into a higher dimensionality using pointwise convolution. &gt; GELU (Gaussian Error Linear Unit) activations are used within the bottleneck, and Layer Normalization is employed between the blocks."},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_1","title":"\u4e3b\u5e72\u7f51\u7edc","text":"<p>Vocos \u5c06 ConvNeXt \u4f5c\u4e3a\u751f\u6210\u5668\u7684\u57fa\u7840\u4e3b\u5e72. \u5b83\u9996\u5148\u5c06\u8f93\u5165\u7279\u5f81\u5d4c\u5165\u5230\u9690\u85cf\u7ef4\u5ea6\u4e2d, \u4e4b\u540e\u5e94\u7528\u4e00\u7ef4\u5377\u79ef\u5757\u7684\u5806\u53e0. \u6bcf\u4e2a\u5757\u7531\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u548c\u53cd\u5411\u74f6\u9888\u7ec4\u6210, \u540e\u8005\u4f7f\u7528\u9010\u70b9\u5377\u79ef\u5c06\u7279\u5f81\u6295\u5f71\u5230\u66f4\u9ad8\u7ef4\u5ea6. \u74f6\u9888\u5185\u4f7f\u7528\u9ad8\u65af\u8bef\u5dee\u7ebf\u6027\u5355\u5143 (Gaussian Error Linear Unit, GELU) \u6fc0\u6d3b\u51fd\u6570, \u5757\u4e4b\u95f4\u5e94\u7528\u5c42\u5f52\u4e00\u5316.</p> \u539f\u6587  &gt; #### Head  &gt; Fourier transform of real-valued signals is conjugate symmetric, so we use only a single side band spectrum, resulting in $n_{fft}/2 + 1$ coefficients per frame. &gt; As we parameterize the model to output phase and magnitude values, hidden-dim activations are projected into a tensor $h$ with $n_{fft}+2$ channels and splitted into:  $$   m, p = h[1:(n_{fft}/2+1)], h[(n_{fft}/2+2):n] $$  &gt; To represent the magnitude, we apply the exponential function to $m$: $M = \\exp(m)$.  &gt; We map $p$ onto the unit circle by calculating the cosine and sine of $p$ to obtain $x$ and $y$, respectively:  $$   x=\\cos(p), y=\\sin(p) $$  &gt; Finally, we represent complex-valued coefficients as: $STFT = M \u00b7 (x + jy)$.  &gt; Importantly, this simple formulation allows to express phase angle $\\phi = \\arctan2(y, x)$ for any real argument $p$, and it ensures that $\\phi$ is correctly wrapped into the desired range $(\u2212\\pi, \\pi]$.   <p></p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_2","title":"\u5934\u90e8","text":"<p>\u5b9e\u503c\u4fe1\u53f7\u7684\u5085\u91cc\u53f6\u53d8\u6362\u662f\u5171\u8f6d\u5bf9\u79f0\u7684, \u56e0\u6b64\u53ea\u4f7f\u7528\u5355\u4fa7\u5e26\u8c31, \u56e0\u6b64\u6bcf\u5e27\u6709 $n_{fft}/2 + 1$ \u4e2a\u7cfb\u6570. \u7531\u4e8e\u6211\u4eec\u53c2\u6570\u5316\u6a21\u578b\u4ee5\u8f93\u51fa\u76f8\u4f4d\u548c\u5e45\u5ea6\u503c, \u9690\u85cf\u7ef4\u5ea6\u7684\u6fc0\u6d3b\u503c\u88ab\u6295\u5f71\u5230\u5f20\u91cf $h$ \u4e2d, \u5176\u6709 $n_{fft}+2$ \u4e2a\u901a\u9053, \u5e76\u88ab\u5206\u5272\u4e3a:</p> <p>$$   m, p = h[1:(n_{fft}/2+1)], h[(n_{fft}/2+2):n] $$</p> <p>\u4e3a\u4e86\u8868\u793a\u5e45\u5ea6, \u6211\u4eec\u5bf9 $m$ \u5e94\u7528\u6307\u6570\u51fd\u6570: $M = \\exp(m)$.</p> <p>\u6211\u4eec\u5c06 $p$ \u6620\u5c04\u5230\u5355\u4f4d\u5706\u4e0a, \u901a\u8fc7\u8ba1\u7b97 $p$ \u7684\u4f59\u5f26\u548c\u6b63\u5f26\u6765\u83b7\u5f97 $x$ \u548c $y$:</p> <p>$$   x=\\cos(p), y=\\sin(p) $$</p> <p>\u6700\u540e, \u6211\u4eec\u7528\u590d\u6570\u503c\u7cfb\u6570\u8868\u793a: $STFT = M \u00b7 (x + jy)$.</p> <p>\u91cd\u8981\u7684\u662f, \u8fd9\u4e00\u7b80\u5355\u7684\u5f62\u5f0f\u80fd\u591f\u8868\u793a\u76f8\u4f4d\u89d2 $\\phi = \\arctan2(y, x)$, \u5bf9\u4e8e\u4efb\u4f55\u5b9e\u53c2\u6570 $p$, \u5e76\u786e\u4fdd $\\phi$ \u88ab\u6b63\u786e\u5305\u88c5\u5230\u6240\u9700\u8303\u56f4 $(\u2212\\pi, \\pi]$ \u4e2d.</p> \u539f\u6587  &gt; #### Discriminator  &gt; We employ the multi-period discriminator (MPD) as defined by Kong et al. (2020), and multi-resolution discriminator (MRD) (Jang et al., 2021).   <p></p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_3","title":"\u5224\u522b\u5668","text":"<p>\u6211\u4eec\u91c7\u7528\u4e86\u7531 Kong et al. (2020) \u5b9a\u4e49\u7684\u591a\u5468\u671f\u5224\u522b\u5668 (MPD) \u548c\u591a\u5206\u8fa8\u7387\u5224\u522b\u5668 (MRD) (Jang et al., 2021).</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#33loss","title":"3.3.Loss","text":"\u539f\u6587  &gt; Following the approach proposed by Kong et al. (2020), the training objective of Vocos consists of reconstruction loss, adversarial loss and feature matching loss. &gt; However, we adopt a hinge loss formulation instead of the least squares GAN objective, as suggested by Zeghidour et al. (2021):  $$   Loss_{G}(\\hat{x}) = \\dfrac{1}{K}\\sum_{k} \\max(0, 1-D_{k}(\\hat{x})) $$  $$   Loss_{D}(x,\\hat{x}) = \\dfrac{1}{K}\\sum_{k} \\max(0,1-D_{k}(x))+\\max(0,1+D_{k}(\\hat{x})) $$  &gt; where $D_{k}$ is the $k$-th subdiscriminator. &gt; The reconstruction loss, denoted as $Loss_{mel}$, is defined as the $L_1$ distance between the mel-scaled magnitude spectrograms of the ground truth sample $x$ and the synthesized sample: $\\hat{x}$:   $$   Loss_{mel} = \\|M(x) \u2212 M(\\hat{x})\\|_{1}. $$  &gt; The feature matching loss, denoted as $L_{feat}$ is calculated as the mean of the distances between the lth feature maps of the kth subdistriminator:   $$   Loss_{feat} = \\dfrac{1}{KL}\\sum_{k}\\sum_{l}\\|D_{k}^{l}(x)-D_{k}^{l}(\\hat{x})\\|_{1}. $$   <p>\u9075\u5faa Kong \u7b49\u4eba\u7684\u65b9\u6cd5, Vocos \u7684\u8bad\u7ec3\u76ee\u6807\u7531\u91cd\u6784\u635f\u5931, \u5bf9\u6297\u635f\u5931, \u7279\u5f81\u5339\u914d\u635f\u5931\u7ec4\u6210. \u7136\u800c, \u6211\u4eec\u4f7f\u7528\u5408\u9875\u635f\u5931\u5f62\u5f0f\u800c\u4e0d\u662f\u6700\u5c0f\u4e8c\u4e58 GAN \u76ee\u6807, \u5982 Zeghidour \u7b49\u4eba\u5efa\u8bae\u7684\u90a3\u6837:</p> <p>$$   Loss_{G}(\\hat{x}) = \\dfrac{1}{K}\\sum_{k} \\max(0, 1-D_{k}(\\hat{x})) $$</p> <p>$$   Loss_{D}(x,\\hat{x}) = \\dfrac{1}{K}\\sum_{k} \\max(0,1-D_{k}(x))+\\max(0,1+D_{k}(\\hat{x})) $$</p> <p>\u5176\u4e2d $D_{k}$ \u662f\u7b2c $k$ \u4e2a\u5b50\u5224\u522b\u5668.</p> <p>\u91cd\u6784\u635f\u5931, \u8bb0\u4e3a $Loss_{mel}$, \u5b9a\u4e49\u4e3a\u771f\u5b9e\u6837\u672c $x$ \u548c\u5408\u6210\u6837\u672c $\\hat{x}$ \u7684\u6885\u5c14\u9891\u8c31\u56fe\u7684 $L_1$ \u8ddd\u79bb:</p> <p>$$   Loss_{mel} = |M(x) \u2212 M(\\hat{x})|_{1}. $$</p> <p>\u7279\u5f81\u5339\u914d\u635f\u5931, \u8bb0\u4e3a $Loss_{feat}$, \u8ba1\u7b97\u4e3a\u7b2c $k$ \u4e2a\u5b50\u5224\u522b\u5668\u7684\u7b2c $l$ \u4e2a\u7279\u5f81\u56fe\u4e4b\u95f4\u7684\u8ddd\u79bb\u7684\u5747\u503c:</p> <p>$$   Loss_{feat} = \\dfrac{1}{KL}\\sum_{k}\\sum_{l}|D_{k}^{l}(x)-D_{k}^{l}(\\hat{x})|_{1}. $$</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#4results","title":"4.Results","text":""},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#41mel-spectrograms","title":"4.1.Mel-Spectrograms","text":"\u539f\u6587  &gt; Reconstructing audio waveforms from mel-spectrograms has become a fundamental task for vocoders in contemporary speech synthesis pipelines. &gt; In this section, we assess the performance of Vocos relative to established baseline methods.  &gt; #### Data  &gt; The models are trained on the [LibriTTS dataset](../../Datasets/2019.04.05_LibriTTS.md), from which we use the entire training subset (both train-clean and train-other). &gt; We maintain the original sampling rate of 24 kHz for the audio files. &gt; For each audio sample, we compute mel-scaled spectrograms using parameters: $n_{fft} = 1024$, $hop_{n} = 256$, and the number of Mel bins is set to $100$. &gt; A random gain is applied to the audio samples, resulting in a maximum level between $-1$ and $-6$ dBFS.  &gt; #### Training Details  &gt; We train our models up to 2 million iterations, with 1 million iterations per generator and discriminator. &gt; During training, we randomly crop the audio samples to $16384$ samples and use a batch size of $16$. &gt; The model is optimized using the AdamW optimizer with an initial learning rate of $2\\times 10^{-4}$ and betas set to $(0.9, 0.999)$. &gt; The learning rate is decayed following a cosine schedule.  &gt; ##### Baseline Methods  &gt; Our proposed model, Vocos, is compared to: iSTFTNet (Kaneko et al., 2022), BigVGAN (Lee et al., 2022), and [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md). &gt; These models are retrained on the same LibriTTS subset for up to 2 million iterations, following the original training details recommended by the authors. &gt; We use the official implementations of BigVGAN1 and HiFi-GAN2, and a community open-sourced version of iSTFTNet3.   <p>\u4ece\u6885\u5c14\u9891\u8c31\u56fe\u91cd\u5efa\u97f3\u9891\u6ce2\u5f62\u5df2\u6210\u4e3a\u5f53\u4ee3\u8bed\u97f3\u5408\u6210\u7ba1\u9053\u4e2d\u58f0\u7801\u5668\u7684\u57fa\u672c\u4efb\u52a1. \u5728\u672c\u8282\u4e2d, \u6211\u4eec\u8bc4\u4f30Vocos\u76f8\u5bf9\u4e8e\u5df2\u5efa\u7acb\u7684\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_4","title":"\u6570\u636e","text":"<p>\u6a21\u578b\u5728 LibriTTS\u6570\u636e\u96c6 \u4e0a\u8fdb\u884c\u8bad\u7ec3, \u6211\u4eec\u4f7f\u7528\u4e86\u6574\u4e2a\u8bad\u7ec3\u5b50\u96c6 (\u5305\u62ectrain-clean\u548ctrain-other) . \u6211\u4eec\u4fdd\u6301\u97f3\u9891\u6587\u4ef6\u7684\u539f\u59cb\u91c7\u6837\u7387\u4e3a 24 kHz. \u5bf9\u4e8e\u6bcf\u4e2a\u97f3\u9891\u6837\u672c, \u6211\u4eec\u4f7f\u7528\u53c2\u6570 $n_{fft} = 1024$, $hop_{n} = 256$ \u8ba1\u7b97\u6885\u5c14\u5c3a\u5ea6\u9891\u8c31\u56fe, \u6885\u5c14\u4ed3\u7684\u6570\u91cf\u8bbe\u7f6e\u4e3a $100$. \u5bf9\u97f3\u9891\u6837\u672c\u5e94\u7528\u968f\u673a\u589e\u76ca, \u5bfc\u81f4\u6700\u5927\u7535\u5e73\u5728 $-1$ \u548c $-6$ dBFS \u4e4b\u95f4.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_5","title":"\u8bad\u7ec3\u7ec6\u8282","text":"<p>\u6211\u4eec\u8bad\u7ec3\u6a21\u578b\u8fbe\u5230 200 \u4e07\u6b21\u8fed\u4ee3, \u5176\u4e2d\u6bcf\u4e2a\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u5404 100 \u4e07\u6b21\u8fed\u4ee3. \u5728\u8bad\u7ec3\u671f\u95f4, \u6211\u4eec\u968f\u673a\u5c06\u97f3\u9891\u6837\u672c\u88c1\u526a\u4e3a $16384$ \u4e2a\u6837\u672c, \u5e76\u4f7f\u7528\u6279\u91cf\u5927\u5c0f\u4e3a $16$. \u6a21\u578b\u4f7f\u7528 AdamW \u4f18\u5316\u5668\u8fdb\u884c\u4f18\u5316, \u521d\u59cb\u5b66\u4e60\u7387\u4e3a $2\\times 10^{-4}$, beta\u503c\u8bbe\u7f6e\u4e3a $(0.9, 0.999)$. \u5b66\u4e60\u7387\u9075\u5faa\u4f59\u5f26\u8c03\u5ea6\u8fdb\u884c\u8870\u51cf.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_6","title":"\u57fa\u7ebf\u65b9\u6cd5","text":"<p>\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b Vocos \u4e0e\u4ee5\u4e0b\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\uff1aiSTFTNet (Kaneko et al., 2022), BigVGAN (Lee et al., 2022), \u548c HiFi-GAN. \u8fd9\u4e9b\u6a21\u578b\u5728\u76f8\u540c\u7684 LibriTTS \u5b50\u96c6\u4e0a\u91cd\u65b0\u8bad\u7ec3, \u6700\u591a\u8fbe\u5230 200 \u4e07\u6b21\u8fed\u4ee3, \u9075\u5faa\u4f5c\u8005\u63a8\u8350\u7684\u539f\u59cb\u8bad\u7ec3\u7ec6\u8282. \u6211\u4eec\u4f7f\u7528\u4e86 BigVGAN \u548c HiFi-GAN \u7684\u5b98\u65b9\u5b9e\u73b0, \u4ee5\u53ca iSTFTNet \u7684\u793e\u533a\u5f00\u6e90\u7248\u672c.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#411evaluation","title":"4.1.1.Evaluation","text":"\u539f\u6587  &gt; ##### Objective Evaluation  &gt; For objective evaluation of our models, we employ the UTMOS (Saeki et al., 2022) automatic Mean Opinion Score (MOS) prediction system. &gt; Although UTMOS can yield scores highly correlated with human evaluations, it is restricted to 16 kHz sample rate. &gt; To assess perceptual quality, we also utilize ViSQOL (Chinen et al., 2020) in audio-mode, which operates in the full band. &gt; Our evaluation process also encompasses several other metrics, including the Perceptual Evaluation of Speech Quality (PESQ) (Rix et al., 2001), periodicity error, and the F1 score for voiced/unvoiced classification (V/UV F1), following the methodology proposed by Morrison et al. (2021). &gt; The results are presented in Table 1. &gt; Vocos achieves superior performance in most of the metrics compared to the other models. &gt; It obtains the highest scores in VISQOL and PESQ. &gt; Importantly, it also effectively mitigates the periodicity issues frequently associated with time-domain GANs. &gt; BigVGAN stands out as the closest competitor, especially in the UTMOS metric, where it slightly outperforms Vocos.  &gt; In our ablation study, we examined the impact of specific design decisions on Vocos\u2019s performance: &gt; - Vocos with absolute phase: In this variant, we predict phase angles using a tanh nonlinearity, scaled to fit within the range of $[\u2212\\pi, \\pi]$. &gt; This formulation does not give the model an inductive bias regarding the periodic nature of phase, and the results show it leads to degraded quality. &gt; This finding emphasizes the importance of implicit phase wrapping in the effectiveness of Vocos. &gt; - Vocos with Snake activation: Although Snake (Ziyin et al., 2020) has been shown to enhance time-domain vocoders such as BigVGAN, in our case, it did not result in performance gains; in fact, it showed a slight decline. &gt; The primary purpose of the Snake function is to induce periodicity, addressing the limitations of time-domain vocoders. &gt; Vocos, on the other hand, explicitly incorporates periodicity through the use of Fourier basis functions, eliminating the need for specialized modules like Snake. &gt; - Vocos without ConvNeXt: Replacing ConvNeXt blocks with traditional ResBlocks with dilated convolutions, slightly lowers scores across all evaluated metrics. &gt; This finding highlights the integral role of ConvNeXt blocks in Vocos, contributing significantly to its overall success."},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_7","title":"\u5ba2\u89c2\u8bc4\u4f30","text":"<p>\u4e3a\u4e86\u5ba2\u89c2\u8bc4\u4f30\u6211\u4eec\u7684\u6a21\u578b, \u6211\u4eec\u91c7\u7528\u4e86 UTMOS (Saeki et al., 2022) \u81ea\u52a8\u5e73\u5747\u610f\u89c1\u5f97\u5206 (MOS) \u9884\u6d4b\u7cfb\u7edf. \u5c3d\u7ba1 UTMOS \u53ef\u4ee5\u4ea7\u751f\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u76f8\u5173\u7684\u5206\u6570, \u4f46\u5b83\u4ec5\u9650\u4e8e 16 kHz \u91c7\u6837\u7387. \u4e3a\u4e86\u8bc4\u4f30\u611f\u77e5\u8d28\u91cf, \u6211\u4eec\u8fd8\u4f7f\u7528\u4e86\u5168\u9891\u5e26\u64cd\u4f5c\u7684 ViSQOL (Chinen et al., 2020) \u7684\u97f3\u9891\u6a21\u5f0f. \u6211\u4eec\u7684\u8bc4\u4f30\u8fc7\u7a0b\u8fd8\u5305\u62ec\u5176\u4ed6\u51e0\u4e2a\u6307\u6807, \u5305\u62ec\u8bed\u97f3\u8d28\u91cf\u611f\u77e5\u8bc4\u4f30 (PESQ)  (Rix et al., 2001), \u5468\u671f\u6027\u8bef\u5dee, \u4ee5\u53ca\u9075\u5faa Morrison \u7b49\u4eba (2021) \u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6d4a\u97f3/\u6e05\u97f3\u5206\u7c7b (V/UV F1) \u7684F1\u5206\u6570. \u7ed3\u679c\u663e\u793a\u5728\u8868 01 \u4e2d.</p> <p></p> <p>Vocos \u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4e0e\u5176\u4ed6\u6a21\u578b\u76f8\u6bd4\u8868\u73b0\u66f4\u4f18. \u5b83\u5728 VISQOL \u548c PESQ \u4e0a\u83b7\u5f97\u4e86\u6700\u9ad8\u5206\u6570. \u91cd\u8981\u7684\u662f, \u5b83\u8fd8\u6709\u6548\u5730\u7f13\u89e3\u4e86\u4e0e\u65f6\u57dfGANs\u76f8\u5173\u7684\u5468\u671f\u6027\u95ee\u9898. BigVGAN \u4f5c\u4e3a\u6700\u63a5\u8fd1\u7684\u7ade\u4e89\u5bf9\u624b\u8131\u9896\u800c\u51fa, \u7279\u522b\u662f\u5728 UTMOS \u6307\u6807\u4e0a, \u5b83\u7565\u5fae\u8d85\u8fc7\u4e86 Vocos.</p> <p>\u5728\u6211\u4eec\u7684\u6d88\u878d\u7814\u7a76\u4e2d, \u6211\u4eec\u68c0\u67e5\u4e86\u7279\u5b9a\u8bbe\u8ba1\u51b3\u7b56\u5bf9 Vocos \u6027\u80fd\u7684\u5f71\u54cd\uff1a - Vocos \u4e0e\u7edd\u5bf9\u76f8\u4f4d\uff1a\u5728\u8fd9\u4e2a\u53d8\u4f53\u4e2d, \u6211\u4eec\u4f7f\u7528 tanh \u975e\u7ebf\u6027\u9884\u6d4b\u76f8\u4f4d\u89d2, \u7f29\u653e\u5230\u8303\u56f4 $[\u2212\\pi, \\pi]$. \u8fd9\u79cd\u516c\u5f0f\u6ca1\u6709\u7ed9\u6a21\u578b\u5173\u4e8e\u76f8\u4f4d\u5468\u671f\u6027\u8d28\u7684\u5f52\u7eb3\u504f\u7f6e, \u7ed3\u679c\u663e\u793a\u5b83\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d. \u8fd9\u4e00\u53d1\u73b0\u5f3a\u8c03\u4e86 Vocos \u6709\u6548\u6027\u4e2d\u9690\u5f0f\u76f8\u4f4d\u7f20\u7ed5\u7684\u91cd\u8981\u6027. - Vocos \u4e0e Snake \u6fc0\u6d3b\uff1a\u5c3d\u7ba1 Snake (Ziyin et al., 2020) \u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u589e\u5f3a BigVGAN \u7b49\u65f6\u57df\u58f0\u7801\u5668, \u4f46\u5728\u6211\u4eec\u7684\u60c5\u51b5\u4e0b, \u5b83\u5e76\u6ca1\u6709\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\uff1b\u5b9e\u9645\u4e0a, \u5b83\u663e\u793a\u4e86\u8f7b\u5fae\u7684\u4e0b\u964d. Snake \u51fd\u6570\u7684\u4e3b\u8981\u76ee\u7684\u662f\u8bf1\u5bfc\u5468\u671f\u6027, \u89e3\u51b3\u65f6\u57df\u58f0\u7801\u5668\u7684\u5c40\u9650\u6027. \u53e6\u4e00\u65b9\u9762, Vocos \u901a\u8fc7\u4f7f\u7528\u5085\u91cc\u53f6\u57fa\u51fd\u6570\u660e\u786e\u5730\u5305\u542b\u4e86\u5468\u671f\u6027, \u6d88\u9664\u4e86\u5bf9 Snake \u7b49\u4e13\u7528\u6a21\u5757\u7684\u9700\u6c42. - Vocos \u4e0d\u5e26 ConvNeXt\uff1a\u7528\u5177\u6709\u81a8\u80c0\u5377\u79ef\u7684\u4f20\u7edf ResBlocks \u66ff\u6362 ConvNeXt \u5757, \u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u7565\u5fae\u964d\u4f4e\u4e86\u5206\u6570. \u8fd9\u4e00\u53d1\u73b0\u5f3a\u8c03\u4e86 ConvNeXt \u5757\u5728 Vocos \u4e2d\u7684\u6838\u5fc3\u4f5c\u7528, \u5bf9\u5b83\u7684\u6574\u4f53\u6210\u529f\u505a\u51fa\u4e86\u91cd\u5927\u8d21\u732e.</p> \u539f\u6587  &gt; ##### Subjective Evaluation  &gt; We conducted crowd-sourced subjective assessments, using a 5-point Mean Opinion Score (MOS) to evaluate the naturalness of the presented recordings. &gt; Participants rated speech samples on a scale from 1 (\u2019poor - completely unnatural speech\u2019) to 5 (\u2019excellent - completely natural speech\u2019). &gt; Following (Lee et al., 2022), we also conducted a 5-point Similarity Mean Opinion Score (SMOS) between the reproduced and ground-truth recordings. &gt; Participants were asked to assign a similarity score to pairs of audio files, with a rating of 5 indicating \u2019Extremely similar\u2019 and a rating of 1 representing \u2019Not at all similar\u2019.  &gt; To ensure the quality of responses, we carefully selected participants through a third-party crowdsourcing platform. &gt; Our criteria included the use of headphones, fluent English proficiency, and a declared interest in music listening as a hobby. &gt; A total of 1560 ratings were collected from 39 participants.  &gt; The results are detailed in Table 2. &gt; Vocos performs on par with the state-of-the-art in both perceived quality and similarity. &gt; Statistical tests show no significant differences between Vocos and BigVGAN in MOS and SMOS scores, with p-values greater than 0.05 from the Wilcoxon signed-rank test.   <p></p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_8","title":"\u4e3b\u89c2\u8bc4\u4f30","text":"<p>\u6211\u4eec\u8fdb\u884c\u4e86\u4f17\u5305\u4e3b\u89c2\u8bc4\u4f30, \u4f7f\u75285\u70b9\u5e73\u5747\u610f\u89c1\u5f97\u5206 (MOS) \u6765\u8bc4\u4f30\u5448\u73b0\u5f55\u97f3\u7684\u81ea\u7136\u5ea6. \u53c2\u4e0e\u8005\u6839\u636e 1 ('\u5dee - \u5b8c\u5168\u4e0d\u81ea\u7136\u7684\u8bed\u97f3') \u5230 5 ('\u4f18\u79c0 - \u5b8c\u5168\u81ea\u7136\u7684\u8bed\u97f3') \u7684\u8bc4\u5206\u6807\u51c6\u5bf9\u8bed\u97f3\u6837\u672c\u8fdb\u884c\u8bc4\u5206. \u9075\u5faa (Lee et al., 2022), \u6211\u4eec\u8fd8\u8fdb\u884c\u4e86 5 \u70b9\u76f8\u4f3c\u6027\u5e73\u5747\u610f\u89c1\u5f97\u5206 (SMOS), \u6bd4\u8f83\u91cd\u73b0\u5f55\u97f3\u548c\u771f\u5b9e\u5f55\u97f3\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027. \u53c2\u4e0e\u8005\u88ab\u8981\u6c42\u4e3a\u6210\u5bf9\u7684\u97f3\u9891\u6587\u4ef6\u5206\u914d\u76f8\u4f3c\u6027\u5f97\u5206, \u8bc4\u52065\u8868\u793a'\u6781\u5176\u76f8\u4f3c', \u8bc4\u52061\u8868\u793a'\u5b8c\u5168\u4e0d\u76f8\u4f3c'.</p> <p>\u4e3a\u4e86\u786e\u4fdd\u54cd\u5e94\u8d28\u91cf, \u6211\u4eec\u901a\u8fc7\u7b2c\u4e09\u65b9\u4f17\u5305\u5e73\u53f0\u4ed4\u7ec6\u9009\u62e9\u4e86\u53c2\u4e0e\u8005. \u6211\u4eec\u7684\u6807\u51c6\u5305\u62ec\u4f7f\u7528\u8033\u673a, \u6d41\u5229\u7684\u82f1\u8bed\u6c34\u5e73\u4ee5\u53ca\u58f0\u660e\u5bf9\u97f3\u4e50\u8046\u542c\u4f5c\u4e3a\u7231\u597d\u7684\u5174\u8da3. \u603b\u5171\u4ece 39 \u540d\u53c2\u4e0e\u8005\u90a3\u91cc\u6536\u96c6\u4e86 1560 \u4e2a\u8bc4\u5206.</p> <p>\u7ed3\u679c\u8be6\u89c1\u8868 02.</p> <p></p> <p>Vocos \u5728\u611f\u77e5\u8d28\u91cf\u548c\u76f8\u4f3c\u6027\u65b9\u9762\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u76f8\u5f53. \u7edf\u8ba1\u6d4b\u8bd5\u663e\u793a, \u5728 MOS \u548c SMOS \u5206\u6570\u4e0a, Vocos \u548c BigVGAN \u4e4b\u95f4\u6ca1\u6709\u663e\u8457\u5dee\u5f02, Wilcoxon \u7b26\u53f7\u79e9\u68c0\u9a8c\u7684 p \u503c\u5927\u4e8e 0.05.</p> \u539f\u6587  &gt; ##### Out-of-Distribution Data  &gt; A crucial aspect of a vocoder is its ability to generalize to unseen acoustic conditions. &gt; In this context, we evaluate the performance of Vocos with out-of-distribution audio using the MUSDB18 dataset (Rafii et al., 2017), which includes a variety of multi-track music audio like vocals, drums, bass, and other instruments, along with the original mixture. &gt; The VISQOL scores for this evaluation are provided in Table 3. &gt; From the table, Vocos consistently outperforms the other models, achieving the highest scores across all categories.  &gt; Figure 3 presents spectrogram visualization of an out-of-distribution singing voice sample, as reproduced by different models. &gt; Periodicity artifacts are commonly observed when employing time-domain GANs. &gt; BigVGAN, with its anti-aliasing filters, is able to recover some of the harmonics in the upper frequency ranges, marking an improvement over HiFi-GAN. &gt; Nonetheless, Vocos appears to provide a more accurate reconstruction of these harmonics, without the need for additional modules.   <p></p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_9","title":"\u5206\u5e03\u5916\u6570\u636e","text":"<p>\u58f0\u7801\u5668\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u9762\u662f\u5176\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u58f0\u5b66\u6761\u4ef6. \u5728\u6b64\u80cc\u666f\u4e0b, \u6211\u4eec\u4f7f\u7528 MUSDB18 \u6570\u636e\u96c6 (Rafii et al., 2017) \u8bc4\u4f30\u4e86 Vocos \u5728\u5206\u5e03\u5916\u97f3\u9891\u4e0a\u7684\u6027\u80fd, \u8be5\u6570\u636e\u96c6\u5305\u62ec\u5404\u79cd\u591a\u8f68\u97f3\u4e50\u97f3\u9891, \u5982\u4eba\u58f0, \u9f13, \u8d1d\u65af\u548c\u5176\u4ed6\u4e50\u5668, \u4ee5\u53ca\u539f\u59cb\u6df7\u5408\u97f3\u8f68. \u6b64\u8bc4\u4f30\u7684 VISQOL \u5206\u6570\u89c1\u8868 03.</p> <p></p> <p>\u4ece\u8868\u4e2d\u53ef\u4ee5\u770b\u51fa, Vocos \u5728\u6240\u6709\u7c7b\u522b\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b, \u53d6\u5f97\u4e86\u6700\u9ad8\u5206\u6570.</p> <p></p> <p>\u56fe 03 \u5c55\u793a\u4e86\u7531\u4e0d\u540c\u6a21\u578b\u91cd\u73b0\u7684\u5206\u5e03\u5916\u6b4c\u5531\u58f0\u97f3\u6837\u672c\u7684\u9891\u8c31\u56fe\u53ef\u89c6\u5316. \u5728\u4f7f\u7528\u65f6\u57df GAN \u65f6, \u5468\u671f\u6027\u4f2a\u5f71\u662f\u5e38\u89c1\u7684. BigVGAN \u901a\u8fc7\u5176\u6297\u6df7\u53e0\u6ee4\u6ce2\u5668\u80fd\u591f\u5728\u9ad8\u9891\u8303\u56f4\u5185\u6062\u590d\u4e00\u4e9b\u8c10\u6ce2, \u6807\u5fd7\u7740\u5bf9 HiFi-GAN \u7684\u6539\u8fdb. \u5c3d\u7ba1\u5982\u6b64, Vocos \u4f3c\u4e4e\u63d0\u4f9b\u4e86\u8fd9\u4e9b\u8c10\u6ce2\u7684\u66f4\u51c6\u786e\u91cd\u5efa, \u65e0\u9700\u989d\u5916\u7684\u6a21\u5757.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#42neural-audio-codec","title":"4.2.Neural Audio Codec","text":"\u539f\u6587  &gt; While traditionally, neural vocoders reconstruct the audio waveform from a mel-scaled spectrogram \u2013 an approach widely adopted in many speech synthesis pipelines \u2013 recent research has started to utilize learnt features (Siuzdak et al., 2022), often in a quantized form ([AudioLM](../../Models/Speech_LLM/2022.09.07_AudioLM.md)).  &gt; In this section, we draw a comparison with [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md), an open-source neural audio codec, which follows a typical time-domain GAN vocoder architecture and uses Residual Vector Quantization (RVQ) ([SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md)) to compress the latent space. &gt; RVQ cascades multiple layers of Vector Quantization, iteratively quantizing the residuals from the previous stage to form a multi-stage structure, thereby enabling support for multiple bandwidth targets. &gt; In [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md), dedicated discriminators are trained for each bandwidth. &gt; In contrast, we have adapted Vocos to be a conditional GAN with a projection discriminator (Miyato &amp; Koyama, 2018), and have incorporated adaptive layer normalization (Huang &amp; Belongie, 2017) into the generator.  &gt; #### Audio Reconstruction  &gt; We utilize the open-source model checkpoint of [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) operating at 24 kHz. &gt; To align with [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md), we scale down Vocos to match its parameter count (7.9M) and train it on clean speech segments sourced from the DNS Challenge (Dubey et al., 2022). &gt; Our evaluation, conducted on the DAPS dataset (Mysore, 2014) and detailed in Table 4, reveals that despite [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md)\u2019s reconstruction artifacts not significantly impacting PESQ and Periodicity scores, they are considerably reflected in the perceptual score, as denoted by UTMOS. &gt; In this regard, Vocos notably outperforms [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md). &gt; We also performed a crowd-sourced subjective assessment to evaluate the naturalness of these samples. &gt; The results, as shown in Table 5, indicate that Vocos consistently achieves better performance across a range of bandwidths, based on evaluations by human listeners.  &gt; #### End-to-End Text-to-Speech  &gt; Recent progress in text-to-speech (TTS) has been notably driven by language modeling architectures employing discrete audio tokens. &gt; Bark (Suno AI, 2023), a widely recognized open-source model, leverages a GPT-style, decoder-only architecture, with [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md)\u2019s 6kbps audio tokens serving as its vocabulary. &gt; Vocos trained to reconstruct [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) tokens can effectively serve as a drop-in replacement vocoder for Bark. &gt; We have provided text-to-speech samples from Bark and Vocos on our website and encourage readers to listen to them for a direct comparison.4.   <p>\u5c3d\u7ba1\u4f20\u7edf\u4e0a, \u795e\u7ecf\u58f0\u7801\u5668\u4ece\u6885\u5c14\u5c3a\u5ea6\u9891\u8c31\u56fe\u91cd\u5efa\u97f3\u9891\u6ce2\u5f62\u2014\u2014\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bb8\u591a\u8bed\u97f3\u5408\u6210\u7ba1\u9053\u4e2d\u88ab\u5e7f\u6cdb\u91c7\u7528\u2014\u2014\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u7ecf\u5f00\u59cb\u5229\u7528\u5b66\u4e60\u5230\u7684\u7279\u5f81 (Siuzdak et al., 2022), \u901a\u5e38\u4ee5\u91cf\u5316\u5f62\u5f0f (AudioLM).</p> <p>\u5728\u672c\u8282\u4e2d, \u6211\u4eec\u5c06\u4e0e EnCodec (D'efossez et al., 2022) \u8fdb\u884c\u6bd4\u8f83, EnCodec \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668, \u5b83\u9075\u5faa\u5178\u578b\u7684\u65f6\u57dfGAN\u58f0\u7801\u5668\u67b6\u6784, \u5e76\u4f7f\u7528\u6b8b\u5dee\u5411\u91cf\u91cf\u5316 (RVQ)  (SoundStream) \u6765\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4. RVQ\u901a\u8fc7\u591a\u5c42\u5411\u91cf\u91cf\u5316\u7ea7\u8054, \u8fed\u4ee3\u91cf\u5316\u524d\u4e00\u9636\u6bb5\u7684\u6b8b\u5dee, \u5f62\u6210\u591a\u7ea7\u7ed3\u6784, \u4ece\u800c\u652f\u6301\u591a\u4e2a\u5e26\u5bbd\u76ee\u6807. \u5728 EnCodec \u4e2d, \u4e3a\u6bcf\u4e2a\u5e26\u5bbd\u8bad\u7ec3\u4e13\u95e8\u7684\u5224\u522b\u5668. \u76f8\u6bd4\u4e4b\u4e0b, \u6211\u4eec\u5df2\u7ecf\u5c06 Vocos \u9002\u914d\u4e3a\u5177\u6709\u6295\u5f71\u5224\u522b\u5668 (Miyato &amp; Koyama, 2018) \u7684\u6761\u4ef6 GAN, \u5e76\u5728\u751f\u6210\u5668\u4e2d\u96c6\u6210\u4e86\u81ea\u9002\u5e94\u5c42\u5f52\u4e00\u5316 (Huang &amp; Belongie, 2017) .</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_10","title":"\u97f3\u9891\u91cd\u5efa","text":"<p>\u6211\u4eec\u4f7f\u7528\u8fd0\u884c\u5728 24 kHz \u7684 EnCodec \u7684\u5f00\u6e90\u6a21\u578b\u68c0\u67e5\u70b9. \u4e3a\u4e86\u4e0e EnCodec \u5bf9\u9f50, \u6211\u4eec\u5c06 Vocos \u7f29\u51cf\u5230\u4e0e\u5176\u53c2\u6570\u8ba1\u6570 (7.9M) \u5339\u914d, \u5e76\u4f7f\u7528\u6765\u81ea DNS \u6311\u6218 (Dubey et al., 2022) \u7684\u5e72\u51c0\u8bed\u97f3\u6bb5\u5bf9\u5176\u8fdb\u884c\u8bad\u7ec3. \u6211\u4eec\u7684\u8bc4\u4f30\u5728 DAPS \u6570\u636e\u96c6 (Mysore, 2014) \u4e0a\u8fdb\u884c, \u5e76\u5728\u8868 04 \u4e2d\u8be6\u7ec6\u8bf4\u660e, \u63ed\u793a\u5c3d\u7ba1 EnCodec \u7684\u91cd\u5efa\u4f2a\u5f71\u5bf9 PESQ \u548c\u5468\u671f\u6027\u5206\u6570\u7684\u5f71\u54cd\u4e0d\u5927, \u4f46\u5b83\u4eec\u5728\u611f\u77e5\u5206\u6570\u4e0a\u5f97\u5230\u4e86\u663e\u8457\u53cd\u6620, \u5982 UTMOS \u6240\u793a.</p> <p></p> <p>\u5728\u8fd9\u65b9\u9762, Vocos \u660e\u663e\u4f18\u4e8e EnCodec. \u6211\u4eec\u8fd8\u8fdb\u884c\u4e86\u4e00\u9879\u4f17\u5305\u4e3b\u89c2\u8bc4\u4f30, \u4ee5\u8bc4\u4f30\u8fd9\u4e9b\u6837\u672c\u7684\u81ea\u7136\u5ea6. \u7ed3\u679c\u5982\u8868 05 \u6240\u793a, \u8868\u660e\u6839\u636e\u4eba\u7c7b\u542c\u4f17\u7684\u8bc4\u4f30, Vocos \u5728\u5404\u79cd\u5e26\u5bbd\u4e0a\u59cb\u7ec8\u8868\u73b0\u66f4\u597d.</p> <p></p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#_11","title":"\u7aef\u5230\u7aef\u6587\u672c\u5230\u8bed\u97f3","text":"<p>\u6587\u672c\u5230\u8bed\u97f3 (TTS) \u7684\u6700\u65b0\u8fdb\u5c55\u663e\u8457\u5730\u7531\u91c7\u7528\u79bb\u6563\u97f3\u9891\u4ee4\u724c\u7684\u8bed\u8a00\u5efa\u6a21\u67b6\u6784\u63a8\u52a8. Bark (Suno AI, 2023) \u662f\u4e00\u4e2a\u5e7f\u6cdb\u8ba4\u53ef\u7684\u5f00\u6e90\u6a21\u578b, \u5b83\u5229\u7528\u4e86GPT\u98ce\u683c\u7684\u4ec5\u89e3\u7801\u5668\u67b6\u6784, \u5176\u4e2d EnCodec \u7684 6 kbps \u97f3\u9891\u4ee4\u724c\u4f5c\u4e3a\u5176\u8bcd\u6c47. \u8bad\u7ec3\u7528\u4e8e\u91cd\u5efa EnCodec \u4ee4\u724c\u7684 Vocos \u53ef\u4ee5\u6709\u6548\u5730\u4f5c\u4e3a Bark \u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u58f0\u7801\u5668. \u6211\u4eec\u5728\u6211\u4eec\u7684\u7f51\u7ad9\u4e0a\u63d0\u4f9b\u4e86\u6765\u81ea Bark \u548c Vocos \u7684\u6587\u672c\u5230\u8bed\u97f3\u6837\u672c, \u5e76\u9f13\u52b1\u8bfb\u8005\u76f4\u63a5\u6bd4\u8f83\u5b83\u4eec.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#43inference-speed","title":"4.3.Inference Speed","text":"\u539f\u6587  &gt; Our inference speed benchmarks were conducted using an Nvidia Tesla A100 GPU and an AMD EPYC 7542 CPU. &gt; The code was implemented in Pytorch, with no hardware-specific optimizations.  &gt; The forward pass was computed using a batch of 16 samples, each one second long. &gt; Table 6 presents the synthesis speed and model footprint of Vocos in comparison to other models.  &gt; Vocos showcases notable speed advantages compared to other models, operating approximately 13 times faster than HiFi-GAN and nearly 70 times faster than BigVGAN. &gt; This speed advantage is particularly pronounced when running without GPU acceleration. &gt; This is primarily due to the use of the Inverse Short-Time Fourier Transform (ISTFT) algorithm instead of transposed convolutions. &gt; We also evaluate a variant of Vocos that utilizes ResBlock\u2019s dilated convolutions instead of ConvNeXt blocks. &gt; Depthwise separable convolutions offer an additional speedup when executed on a GPU.   <p>\u6211\u4eec\u7684\u63a8\u7406\u901f\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u662f\u5728 Nvidia Tesla A100 GPU\u548cAMD EPYC 7542 CPU \u4e0a\u8fdb\u884c\u7684. \u4ee3\u7801\u662f\u7528 Pytorch \u5b9e\u73b0\u7684, \u6ca1\u6709\u8fdb\u884c\u786c\u4ef6\u7279\u5b9a\u7684\u4f18\u5316.</p> <p>\u6b63\u5411\u4f20\u9012\u8ba1\u7b97\u4f7f\u7528\u4e86\u4e00\u6279 16 \u4e2a\u6837\u672c, \u6bcf\u4e2a\u6837\u672c\u4e00\u79d2\u949f\u957f. \u8868 06 \u5c55\u793a\u4e86 Vocos \u4e0e\u5176\u4ed6\u6a21\u578b\u76f8\u6bd4\u7684\u5408\u6210\u901f\u5ea6\u548c\u6a21\u578b\u8db3\u8ff9.</p> <p></p> <p>Vocos \u5c55\u793a\u4e86\u4e0e\u5176\u4ed6\u6a21\u578b\u76f8\u6bd4\u663e\u8457\u7684\u901f\u5ea6\u4f18\u52bf, \u8fd0\u884c\u901f\u5ea6\u5927\u7ea6\u6bd4 HiFi-GAN \u5feb13\u500d, \u6bd4 BigVGAN \u5feb\u8fd170\u500d. \u5728\u6ca1\u6709 GPU \u52a0\u901f\u7684\u60c5\u51b5\u4e0b, \u8fd9\u79cd\u901f\u5ea6\u4f18\u52bf\u5c24\u5176\u660e\u663e. \u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u4f7f\u7528\u4e86\u9006\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362 (ISTFT) \u7b97\u6cd5\u800c\u4e0d\u662f\u8f6c\u7f6e\u5377\u79ef. \u6211\u4eec\u8fd8\u8bc4\u4f30\u4e86 Vocos \u7684\u4e00\u4e2a\u53d8\u4f53, \u8be5\u53d8\u4f53\u4f7f\u7528 ResBlock \u7684\u81a8\u80c0\u5377\u79ef\u800c\u4e0d\u662f ConvNeXt \u5757. \u5f53\u5728GPU\u4e0a\u6267\u884c\u65f6, \u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u52a0\u901f.</p>"},{"location":"TTS/Models/TTS3_Vocoder/2023.03.01_Vocos/#5conclusions","title":"5.Conclusions","text":"\u539f\u6587  &gt; This paper introduces Vocos, a novel neural vocoder that bridges the gap between time-domain and Fourier-based approaches. &gt; Vocos tackles the challenges associated with direct reconstruction of complex-valued spectrograms, with careful design of generator that correctly handle phase wrapping. &gt; It achieves accurate reconstruction of the coefficients in Fourier-based time-frequency representations.  &gt; The results demonstrate that the proposed vocoder matches state-of-the-art audio quality while effectively mitigating periodicity issues commonly observed in time-domain GANs. &gt; Importantly, Vocos provides a significant computational efficiency advantage over traditional time-domain methods by utilizing inverse fast Fourier transform for upsampling.  &gt; Overall, the findings of this study contribute to the advancement of neural vocoding techniques by incorporating the benefits of Fourier-based time-frequency representations. &gt; The open-sourcing of the source code and model weights allows for further exploration and application of the proposed vocoder in various audio processing tasks.   <p>\u672c\u6587\u4ecb\u7ecd\u4e86 Vocos, \u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u58f0\u7801\u5668, \u5b83\u5f25\u5408\u4e86\u65f6\u57df\u548c\u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd. Vocos \u89e3\u51b3\u4e86\u4e0e\u76f4\u63a5\u91cd\u5efa\u590d\u503c\u9891\u8c31\u56fe\u76f8\u5173\u7684\u6311\u6218, \u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u751f\u6210\u5668\u6b63\u786e\u5904\u7406\u76f8\u4f4d\u7f20\u7ed5. \u5b83\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u65f6\u9891\u8868\u793a\u4e2d\u7cfb\u6570\u7684\u51c6\u786e\u91cd\u5efa.</p> <p>\u7ed3\u679c\u8868\u660e, \u6240\u63d0\u51fa\u7684\u58f0\u7801\u5668\u5728\u97f3\u9891\u8d28\u91cf\u4e0a\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u76f8\u5339\u914d, \u540c\u65f6\u6709\u6548\u5730\u7f13\u89e3\u4e86\u5728\u65f6\u57df GANs \u4e2d\u5e38\u89c1\u7684\u5468\u671f\u6027\u95ee\u9898. \u91cd\u8981\u7684\u662f, Vocos \u901a\u8fc7\u4f7f\u7528\u9006\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u8fdb\u884c\u4e0a\u91c7\u6837, \u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u65f6\u57df\u65b9\u6cd5\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u4f18\u52bf.</p> <p>\u603b\u4f53\u800c\u8a00, \u672c\u7814\u7a76\u7684\u53d1\u73b0\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u65f6\u9891\u8868\u793a\u7684\u4f18\u70b9, \u4e3a\u795e\u7ecf\u58f0\u7801\u6280\u672f\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u8d21\u732e. \u6e90\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u7684\u5f00\u6e90\u5141\u8bb8\u5bf9\u6240\u63d0\u51fa\u7684\u58f0\u7801\u5668\u5728\u5404\u79cd\u97f3\u9891\u5904\u7406\u4efb\u52a1\u4e2d\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u63a2\u7d22\u548c\u5e94\u7528.</p> \u9644\u5f55  ## Appendix A.Modified Discrete Cosine Transform (MDCT)  &gt; While STFT is widely used in audio processing, there are other time-frequency representations with different properties. &gt; In audio coding applications, it is desirable to design the analysis/synthesis system in such a way that the overall rate at the output of the analysis stage equals the rate of the input signal. &gt; Such systems are described as being critically sampled. &gt; When we transform the signal via the DFT, even a slight overlap between adjacent blocks increases the data rate of the spectral representation of the signal. &gt; With 50% overlap between adjoining blocks, we end up doubling our data rate.  &gt; The Modified Discrete Cosine Transform (MDCT) with its corresponding Inverse Transform (IMDCT) have become a crucial tool in high-quality audio coding as they enable the implementation of a critically sampled analysis/synthesis filter bank. &gt; A key feature of these transforms is the Time-Domain Aliasing Cancellation (TDAC) property, which allows for the perfect reconstruction of overlapping segments from a source signal.  &gt; The MDCT is defined as follows:  &gt; for k = 0, 1, . . ., N \u2212 1 and N is the length of the window.  &gt; The MDCT is a lapped transform and thus produces N output coefficients from 2N input samples, allowing for a 50% overlap between blocks without increasing the data rate.  &gt; There is a relationship between the MDCT and the DFT through the Shifted Discrete Fourier Transform (SDFT) (Wang &amp; Vilermo, 2003). &gt; It can be leveraged to implement a fast version of the MDCT using FFT (Bosi &amp; Goldberg, 2002). &gt; See Appendix A.3.  ### A.1 VOCOS and MDCT  &gt; MDCT is attractive in audio coding because of its its efficiency and compact representation of audio signals. &gt; In the context of deep learning, this might be seen as reduced dimensionality, potentially advantageous as it requires fewer data points during generation.  &gt; While STFT coefficients can be conveniently expressed in polar form, providing a clear interpretation of both magnitude and phase, MDCT represents the signal only in a real subspace of the complex space needed to accurately convey spectral magnitude and phase. &gt; Naive approach would be to treat raw unnormalized hidden outputs of the network as MDCT coefficients and convert it back to time-domain with IMDCT. &gt; In our preliminary experiments we found that it led to slower convergence. &gt; However we can easily observe that the MDCT spectrum, similarly to the STFT, can be more perceptually meaningful on the logarithmic scale, which reflects the logarithmic nature of human auditory perception of sound intensity. &gt; But as the MDCT can take also negative values, they cannot be represented using the conventional logarithmic transformation.  &gt; One solution is to utilize a symmetric logarithmic function. &gt; In the context of deep learning, Hafner et al. (2023) introduces such function and its inverse, referred to as symlog and symexp respectively:  &gt; The symlog function compresses the magnitudes of large values, irrespective of their sign. &gt; Unlike the conventional logarithm, it is symmetric around the origin and retains the input sign. &gt; We note the correspondence with the \u00b5-law companding algorithm, a well-established method in telecommunication and signal processing.  &gt; An alternative approach involves parametrizing the model to output the absolute value of the MDCT coefficients and its corresponding sign. &gt; While the MDCT does not directly convey information about phase relationships, this strategy may offer advantages as the sign of the MDCT can potentially provide additional insights indirectly. &gt; For example, an opposite sign could imply a phase difference"},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/","title":"HyperTTS","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: HyperTTS: Parameter Efficient Adaptation in Text-to-Speech Using Hypernetworks - \u4f5c\u8005:   - [Yingting Li](../../Authors/Yingting_Li.md)   - [Rishabh Bhardwaj](../../Authors/Rishabh_Bhardwaj.md)   - [Ambuj Mehrish](../../Authors/Ambuj_Mehrish.md)   - [Bo Cheng](../../Authors/Bo_Cheng.md)   - [Soujanya Poria](../../Authors/Soujanya_Poria.md) - \u673a\u6784:   - \u673a\u6784  - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.04.06 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.06 - \u53d1\u8868:   - [\u5317\u4eac\u90ae\u7535\u5927\u5b66](../../Institutions/BUPT_\u5317\u4eac\u90ae\u7535\u5927\u5b66.md)   - [\u65b0\u52a0\u5761\u79d1\u6280\u8bbe\u8ba1\u5927\u5b66](../../Institutions/SUTD_\u65b0\u52a0\u5761\u79d1\u6280\u8bbe\u8ba1\u5927\u5b66.md) - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2404.04645)   - [DOI]()   - [Github](https://github.com/declare-lab/HyperTTS)   - [Demo](https://github.com/declare-lab/HyperTTS/tree/master/demo) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u5f00\u6e90](../../Tags/OpenSource.md) - \u9875\u6570: 12 - \u5f15\u7528: ? - \u88ab\u5f15: ?"},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#abstract","title":"Abstract","text":"<p>Neural speech synthesis, or text-to-speech (TTS), aims to transform a signal from the text domain to the speech domain. While developing TTS architectures that train and test on the same set of speakers has seen significant improvements, out-of-domain speaker performance still faces enormous limitations. Domain adaptation on a new set of speakers can be achieved by fine-tuning the whole model for each new domain, thus making it parameter-inefficient. This problem can be solved by Adapters that provide a parameter-efficient alternative to domain adaptation. Although famous in NLP, speech synthesis has not seen much improvement from Adapters. In this work, we present HyperTTS, which comprises a small learnable network, \"hypernetwork\", that generates parameters of the Adapter blocks, allowing us to condition Adapters on speaker representations and making them dynamic. Extensive evaluations of two domain adaptation settings demonstrate its effectiveness in achieving state-of-the-art performance in the parameter-efficient regime. We also compare different variants of HyperTTS, comparing them with baselines in different studies.Promising results on the dynamic adaptation of adapter parameters using hypernetworks open up new avenues for domain-generic multi-speaker TTS systems. The audio samples and code are available at https://github.com/declare-lab/HyperTTS.</p> <p>\u795e\u7ecf\u8bed\u97f3\u5408\u6210\u6216\u6587\u672c\u8f6c\u8bed\u97f3\u7684\u76ee\u7684\u662f\u5c06\u6587\u672c\u57df\u7684\u4fe1\u53f7\u8f6c\u6362\u4e3a\u8bed\u97f3\u57df. \u867d\u7136\u5f00\u53d1\u5728\u76f8\u540c\u7684\u8bf4\u8bdd\u4eba\u96c6\u5408\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7684\u8bed\u97f3\u5408\u6210\u67b6\u6784\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb, \u4f46\u5bf9\u4e8e\u57df\u5916\u8bf4\u8bdd\u4eba\u7684\u6027\u80fd\u4ecd\u7136\u9762\u4e34\u7740\u5de8\u5927\u7684\u9650\u5236. \u8fd9\u4e00\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7 Adapter, \u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u9886\u57df\u81ea\u9002\u5e94\u6765\u89e3\u51b3. \u5c3d\u7ba1\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5f88\u6709\u540d, \u4f46\u8bed\u97f3\u5408\u6210\u8fd8\u6ca1\u6709\u4ece Adapter \u4e2d\u83b7\u5f97\u592a\u591a\u7684\u6539\u8fdb. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86 HyperTTS, \u5b83\u7531\u4e00\u4e2a\u5c0f\u578b\u53ef\u5b66\u4e60\u7684\u7f51\u7edc, \"\u8d85\u7f51\u7edc\", \u751f\u6210 Adapter \u5757\u7684\u53c2\u6570, \u5141\u8bb8\u6211\u4eec\u6839\u636e\u8bf4\u8bdd\u4eba\u8868\u793a\u6765\u6761\u4ef6\u5316\u9002\u914d\u5668, \u5e76\u4f7f\u5176\u52a8\u6001. \u5728\u4e24\u4e2a\u9886\u57df\u81ea\u9002\u5e94\u8bbe\u7f6e\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d, \u6211\u4eec\u8bc1\u660e\u4e86 HyperTTS \u5728\u53c2\u6570\u9ad8\u6548\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd. \u6211\u4eec\u8fd8\u6bd4\u8f83\u4e86 HyperTTS \u7684\u4e0d\u540c\u53d8\u4f53, \u4e0e\u4e0d\u540c\u7814\u7a76\u4e2d\u7684\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83. \u901a\u8fc7\u4f7f\u7528\u8d85\u7f51\u7edc\u6765\u52a8\u6001\u9002\u914d\u5668\u53c2\u6570\u7684\u53ef\u884c\u6027, \u6211\u4eec\u5f00\u8f9f\u4e86\u65b0\u7684\u591a\u8bf4\u8bdd\u4eba TTS \u7cfb\u7edf\u7684\u9886\u57df\u901a\u7528\u9053\u8def.</p>"},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#1introduction","title":"1.Introduction","text":"<p>Neural text-to-speech (TTS) synthesis has trans formed our interactions with digital content by converting text into natural-sounding speech.Cur rent TTS systems are often limited to predefined speaker styles or specific sets of speaker IDs (Ren et al., 2019a), reducing their utility in multi-speaker environments with unseen speakers. To make TTS scalable and economical, parameter-efficient adaptation of such systems to new speakers is an important, but highly challenging problem (Li et al., 2023b). Zero-shot and few-shot speaker adaptation techniques (Shen et al., 2023; Li et al., 2023a; Casanova et al., 2021; Cooper et al., 2020; Casanova et al., 2022; Shen et al., 2023) have gained prominence in the domain of TTS, aiming at accommodating new speakers and styles with limited speaker-specific data. While these methods excel in scenarios with constrained data, it\u2019s important to note that when sufficient data is available, fine-tuning the model offers distinct advantages. Fine-tuning allows for highly personalized and tailored speech synthesis, precise control over the alignment of synthesized speech with the speaker\u2019s characteristics, and the production of higher-quality, more natural-sounding speech. In this paper, we assume sufficient availability of data from the adaptation domain.When adapting a multi-speaker TTS model (backbone model) to a target domain, the traditional approach involves complete fine-tuning of the entire back bone (Figure 1-Fine-tuning).However, this approach is resource-intensive, requiring separate copies of model parameters for each new target domain. To make the adaptation scalable, recent research has introduced parameter-efficient do main adaptation methods using Adapters, as seen in NLP (Houlsby et al., 2019) and speech (Li et al., 2023b). Adapters incorporate small blocks of learn able dense layers into each block of the backbone model, with the aim of learning additional parameters while keeping the main model parameters fixed (Figure 1-AdapterTTS). Despite the advantages demonstrated by adapters in various NLP tasks, their direct application in adapting a TTS backbone to a target domain has shown limited improvements (Li et al., 2023b) Since learning a generic TTS system that works well across different speaker styles is a more difficult problem than learning one network per speaker (Ren et al., 2019a, 2021), we hypothesize the same is the case with adapters. Forcing a static set of adapter parameters to perform well across multiple speakers of the adaptation domain can be challenging and potentially infeasible due to under parameterization (Mehrish et al., 2023a; Biadsy et al., 2022). In this paper, we present HyperTTS, a pioneer ing approach for the parameter-efficient adaptation of TTS models to new speakers.This method conditions adapters on speaker embeddings, expanding the learnable parameter space through a \"hypernetwork\". The main highlights of HyperTTS are:. 1. Dynamic Adapters: Instead of keeping the adapters static, for each speaker in the adaptation domain, HyperTTS learns speaker adaptive adapters.Adapter conditioning on speaker representations is observed to unlock adapter capabilities and make them performant which was a challenge with static adapters (Li et al., 2023b). 2. Parameter Sampling: A large set of speak ers makes it infeasible to keep the space of adapter parameters discrete. To facilitate this, we employ parameter sampling from a continuous distribution defined by a learnable hyper network. 3. ParameterEfficiency: Compared to parameter-expensive fine-tuning, it achieves competitive results with less than1% of the backbone parameters, making it highly practical and resource-friendly for scalable applications.</p> <p>We perform a comprehensive set of experiments to showcase HyperTTS\u2019s effectiveness (see Figure 1) compared to traditional methods like static bottleneck adapters (AdapterTTS) and full model fine-tuning (TTS-FT). Our experiments cover datasets from diverse environmental conditions, such as LibriTTS and VCTK, representing various accents from different regions. Results highlight HyperTTS\u2019s parameter-efficient performance advantages over the baselines across both objective and subjective metrics.Notably, HyperTTS can even surpass fine-tuning in performance with only a 20% increase in parameters (Table 6-HyperTTS<sub>e/v/d</sub>). A key strength of HyperTTS lies in its remarkable parameter efficiency: it achieves results within 1 point of fine-tuning while using less than 1% of the parameter count in the backbone. This practical and resource-friendly approach enables real-world applications.</p>"},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#2related-work","title":"2.Related Work","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#text-to-speech-models","title":"Text-to-Speech Models","text":"<p>The rise of deep learning has transformed TTS technology, with neural network-based architectures like Tacotron (2017); Tacotron2 (2017), FastSpeech2 (2020), and Transformer-TTS (2018) leading the way. These models represent significant progress in TTS, leveraging deep learning techniques.</p> <p>Autoregressive TTS models (Tacotron (2017); Flowtron (2020); FastSpeech; FastSpeech2 (2020); Glow-TTS (2020); FastPitch (2020)), while effective, face limitations in maintaining alignment in long utterances and exhibit slower training and inference speeds with longer sequences.</p> <p>In contrast, non-autoregressive (parallel) models separate phoneme duration estimation from decoding, reducing latency and enhancing training efficiency. These models typically rely on external aligners or pre-trained autoregressive models for phoneme duration. To achieve training efficiency and support end-to-end TTS, this paper focuses on a non-autoregressive TTS model with an alignment framework based on the RAD-TTS (2022) alignment learning objective.</p> <p>Recently, several speech models have been compared to GPT in natural language processing, with a focus on in-context learning for speech. Notably, VALL-E (2023) and SPEAR-TTS (2023) leverage emerging codecs to learn discrete speech tokens and employ a vocoder-like decodec to convert these tokens into waveforms. Meanwhile, Voicebox (2023), inspired by flow-matching and aligned with the FastSpeech (2019) framework, utilizes continuous features like Mel spectrogram and HiFi-GAN (2020).</p>"},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#speaker-adaptation-in-tts","title":"Speaker Adaptation in TTS","text":"<p>Speaker adaptation is a crucial aspect of TTS systems, aiming to personalize the synthesized speech by modify ing the voice characteristics to match those of a specific target speaker. Over the years, various techniques and approaches have been proposed to address the challenges associated with speaker adaptation in TTS (Jia et al., 2018; Chen et al.; Min et al., 2021; Hsieh et al., 2022; Gabry\u00b4s et al.2022). Furthermore, several studies have focused on exploring parameter-efficient methods for adapt ing TTS to new sets of speakers, addressing the need for effective adaptation in diverse speaker scenarios. These approaches aim to accommodate a wide range of linguistic variations (Pamisetty et al., 2023; Do et al., 2022), including diverse ac cents (Yang et al., 2023), speakers (Luo et al., 2021; Miao et al., 2021; Mehrish et al., 2023a), and low-resource scenarios introduced by the tar get domain (Azizah and Jatmiko, 2022; Mehrish et al., 2023a; Lux and Vu, 2022), while maintain ing the number of trainable parameters. HYPER TTS primarily focuses on contributing in the line of parameter-efficient domain adaptation of the back bone TTS model to a target set of speakers.</p>"},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#dynamic-parameters","title":"Dynamic Parameters","text":"<p>Parameter generation, although not popular in speech, has been used in various forms in other domains, such as Klein et al. (2015); Riegler et al. (2015) in NLP and Ha et al. (2017) in computer vision. Specific to adapters, Bhardwaj et al. (2022); Chen et al. (2020) make prompt tokens dynamic by conditioning their val ues on input text using a parameter prompt generator network, (\u00dcst\u00fcn et al., 2022; Mahabadi et al., 2021) used hypernetworks for generating adapter down and up-projection weights. Shared hypernetworks obviate the need to maintain a separate set of parameters for each task (or new setting) and generate weights for each block of the backbone network (Mahabadi et al., 2021). To the best of our knowledge, this is the first work that studies the utility of a parameter generator in the domain of speech (Mehrish et al., 2023b).</p>"},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#3methodology","title":"3.Methodology","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#31encoder","title":"3.1.Encoder","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#32variance-adaptor","title":"3.2.Variance Adaptor","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#duration-predictor","title":"Duration Predictor","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#pitch-predictor","title":"Pitch Predictor","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#energy-predictor","title":"Energy Predictor","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#33mel-decoder-and-postnet","title":"3.3.Mel-Decoder and Postnet","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#34hypernetwork","title":"3.4.Hypernetwork","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#implementation","title":"Implementation","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#4experiments","title":"4.Experiments","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#41baseline-models","title":"4.1.Baseline Models","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#tts-0","title":"TTS-0","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#reference-and-reference-voc","title":"Reference and Reference (Voc.)","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#tts-ft-full-fine-tuning","title":"TTS-FT (Full Fine-Tuning)","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#adaptertts","title":"AdapterTTS","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#hypertts_1","title":"HyperTTS","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#42datasets","title":"4.2.Datasets","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#43model-configuration","title":"4.3.Model Configuration","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#backbone-model-pre-training","title":"Backbone Model Pre-training","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#44evaluation-metrics","title":"4.4.Evaluation Metrics","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#objective-metrics","title":"Objective Metrics","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#subjective-metrics","title":"Subjective Metrics","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#5results-discussions","title":"5.Results &amp; Discussions","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#51subjective-evaluation","title":"5.1.Subjective Evaluation","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#52impact-of-parameter-efficiency","title":"5.2.Impact of Parameter Efficiency","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#53output-of-hypernetwork","title":"5.3.Output of Hypernetwork","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#54other-discussions","title":"5.4.Other Discussions","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#layernorms-standard-conditional","title":"Layernorms (Standard &amp; Conditional)","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#low-rank-adaptation","title":"Low-Rank Adaptation","text":""},{"location":"TTS/Models/Tricks/2024.04.06_HyperTTS/#6conclusion","title":"6.Conclusion","text":"<p>In this paper, we present HyperTTS, an approach that enhances the effectiveness of adapters by conditioning them on speaker embeddings. Utilizing a \"hypernetwork\" to customize adapter block weights for the TTS backbone network, we significantly expand the adapter parameter space. This dynamic method replaces the conventional static adapter parameter set, enabling input-conditioned parameter sampling. Additionally, the hypernetwork\u2019s continuous parameter space theoretically allows the generation of adapter parameters for numerous speakers without increasing hypernetwork parameters. This makes HyperTTS an excellent choice for multi-speaker TTS adaptation, surpassing traditional adapter limitations.</p> <p>Limitations  While hypernetworks exhibit promising enhancements in both adaptation domains, there are training challenges to address. Time and resource constraints may have led to potential underfitting, negatively impacting performance. Additionally, hypernetworks tend to overfit the backbone model on the adaptation domain, warranting further research to enhance their generalizability. Notably, the relatively higher number of parameters in hypernetworks poses potential inefficiency for low-resource training.</p>"},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/","title":"PAVITS: Exploring Prosody-Aware VITS for End-to-End Emotional Voice Conversion","text":"","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#abstract","title":"Abstract","text":"<p>In this paper, we propose Prosody-Aware VITS (PAVITS) for emotional voice conversion (EVC), aiming to achieve two major objectives of EVC: high content naturalness and high emotional naturalness, which are crucial for meeting the demands of human perception. To improve the content naturalness of converted audio, we have developed an end-to-end EVC architecture inspired by the high audio quality of VITS. By seamlessly integrating an acoustic converter and vocoder, we effectively address the common issue of mismatch between emotional prosody training and run-time conversion that is prevalent in existing EVC models. To further enhance the emotional naturalness, we introduce an emotion descriptor to model the subtle prosody variations of different speech emotions. Additionally, we propose a prosody predictor, which predicts prosody features from text based on the provided emotion label. Notably, we introduce a prosody alignment loss to establish a connection between latent prosody features from two distinct modalities, ensuring effective training. Experimental results show that the performance of PAVITS is superior to the state-of-the-art EVC methods. Speech Samples are available at https://jeremychee4.github.io/pavits4EVC/.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#1introduction","title":"1.Introduction","text":"<p>Emotional voice conversion (EVC) endeavors to transform the state of a spoken utterance from one emotion to another, while preserving the linguistic content and speaker identity [1]. It brings the capability to facilitate emotional communication between individuals [2], enhancing the user experience in human-computer interaction [3], and even achieving a seamless integration of human presence within the virtual world [4].</p> <p>There are two distinct challenges in EVC: one is low content naturalness, and the other is that the converted audio lacks the richness of emotion compared to human voice [1]. Previous studies were focused on frame-based solutions, such as CycleGAN [5] and StarGAN [6,7]. However, due to the fixed-length nature and poor training stability, the naturalness of converted audio is quite low to apply in practice. To address this challenge, autoencoder-based [8,9] especially for sequence-to-sequence (seq2seq) [10,11] frameworks raise much interests for its variable-length speech generation. It achieves an acceptable naturalness through the joint training with Text-to-speech (TTS) [12], which is used to capture linguistic information and avoid mispronunciation as well as skipping-words. Since speech emotion is inherently supra-segmental [13], it is difficult to learn emotional representation from the spectrogram. To tackle this, various pretraining methods, such as leveraging speech emotion recognition (SER) model [14] and 2-stage training strategy [15], are introduced to extract emotional feature for EVC system.</p> <p>Despite these works have achieved great success in EVC, the converted audio still falls short in meeting human\u2019s perceptual needs, which implies that these two challenges still remain to be effectively addressed. Remarkably, current EVC models generally operate in a cascade manner, i.e., the acoustic converter and the vocoder [1, 5, 7, 8], resulting in a mismatch between emotional prosody training and run-time conversion, ultimately leading to a degradation in audio quality, which is vital to evaluate content naturalness and impacts the perceptual experience of emotional utterance. However, there is no EVC model that attempt to bridge this gap, let alone models that aim to capture prosody variations at a finer granularity. To handle the similar issue, multiple solutions have been explored in TTS, including FastSpeech2s (2020), EATS [17], VITS (2021) [19], etc., seeking to alleviate the mismatch between acoustic feature generation and waveform reconstruction by integrating these two stages together.</p> <p>In this paper, inspired by the high audio quality of VITS (2021), we propose Prosody-Aware VITS (PAVITS) for EVC, a novel end-to-end system with implicit prosody modeling to enhance content naturalness and emotional naturalness. To our best knowledge, PAVITS is the first EVC method in solving the mismatch between acoustic feature conversion and waveform reconstruction. Compared to original VITS (2021), our approach involves several key innovations. In order to improve content naturalness with speech quality, we build upon VITS (2021) to solve the two-stage mismatch in EVC, and apply multi-task learning since TTS can significantly reduce the mispronunciation. To enhance emotional naturalness, we introduce an emotion descriptor to capture prosody differences associated with different emotional states in speech. By utilizing Valence-Arousal-Dominance values as condition, emotional representation at utterance-level is learned. Latent code is further refined by a prosody integrator, which incorporates with speaker identity and linguistic content to model finer-grained prosody variations. Then frame-level prosody features are obtained from normalizing flow. We also introduce a prosody predictor that leverages emotion labels and phoneme-level text embedding to predict frame-level emotional prosody features. Finally, we devise a prosody alignment loss to connect two modalities, aligning prosody features obtained from audio and text, respectively.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#2proposed-method","title":"2.Proposed Method","text":"<p>As shown in Fig.01, inspired by VITS (2021), the proposed model is constructed based on conditional variational autoencoder (CVAE), consisting of four parts: a textual prosody prediction module, an acoustic prosody modeling module, an information alignment module, and an emotional speech synthesis module.</p> <p>The textual prosody prediction (TPP) module predicts the prior distribution $p(z_1|c_1)$ as:</p> <p>$$     z_1= TPP (c_1) \\sim p (z_1| c_1)\\tag{1}  $$</p> <p>where $c_1$ including text $t$ and emotion label $e$.</p> <p>The acoustic prosody modeling (APM) module disentangles emotional features with intricate prosody variation, speaker identity, and linguistic content from the source audio given emotion label, forming the posterior distribution $q(z_2|c_2)$ as: </p> <p>$$     z_2= APM (c_2) \\sim q (z_2|c_2)\\tag{2}  $$</p> <p>where $c_2$ including audio $y$ and emotion label $e$.</p> <p>The information alignment module facilitates the alignment of text and speech, as well as the alignment of textual and acoustic prosody representations. In emotional speech synthesis (ESS) module, the decoder reconstructs waveform $\\hat{y}$ according to latent representation $z$.</p> <p>$$     \\hat{y} = Decoder (z) \\sim p (y | z)\\tag{3} $$</p> <p>where $z$ comes from $z_1$ or $z_2$.</p> <p>While the proposed model can perform both EVC and emotional TTS after training, EVC will be the main focus of this paper. In the following, we will introduce the details of the four modules.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#21textual-prosody-prediction-module","title":"2.1.Textual Prosody Prediction Module","text":"<p>Given condition $c_1$ including text $t$ and emotion label $e$, the textual prosody prediction module provides the prior distribution $p(z_1|c_1)$ of CVAE. The text encoder takes phonemes as input and extracts linguistic information $h_{text}$ at first. Considering the extensive prosody variation associated with each phoneme, we employ a prosody predictor to extend the representation to frame-level and predict the prosody variation (a fine-grained prior normal distribution with mean $\\mu_{\\theta}$ and variance $\\sigma_{\\theta}$ generated by a normalizing flow $f_{\\theta}$) based on emotion label. </p> <p>$$     p(z_1|c_1) = \\mathcal{N}(f_{\\theta}(z_1); \\mu_{\\theta}(c_1);\\sigma_{\\theta}(c_1))\\left|\\det\\dfrac{\\partial f_{\\theta}(z_1)}{\\partial z}\\right|\\tag{4} $$</p> <p>Text Encoder: Since the training process is constrained by the volume of textual content within parallel datasets, we initially convert text or characters into a phoneme sequence as a preprocessing step to maximize the utility of the available data, resulting in improved compatibility with the acoustic prosody modeling module. Similar to VITS (2021), text encoder comprises multiple Feed-Forward Transformer (FFT) blocks with a linear projection layer for representing linguistic information.</p> <p>Prosody Predictor: Prosody predictor leverages phoneme-level linguistic information extracted by the text encoder to anticipate frame-level prosody variation given discrete emotion label. It has been observed that simply increasing the depth of stacked flow does not yield satisfactory emotional prosody variations, unlike the prosody predictor. Therefore, the inclusion of the prosody predictor guarantees a continuous enhancement in prosody modeling for both the TPP and APM modules. The prosody predictor comprises multiple one-dimensional convolution layers and a linear projection layer. Furthermore, we integrate predicted emotional prosody information with linguistic information as input for the duration predictor, which significantly benefits the modeling of emotional speech duration.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#22acoustic-prosody-modeling-module","title":"2.2.Acoustic Prosody Modeling Module","text":"<p>The acoustic prosody modeling module provides emotional features with fine-grained prosody variation based on dimensional emotion representation, i.e., Valence-Arousal-Dominance values.Speaker identity and speech content information are also disentangled from the source audio and then complete feature fusion through the prosody integrator as the posterior distribution $q (z_2|c_2)$. </p> <p>$$     q(z_2|c_2) = \\mathcal{N}(f_{\\theta}(z_2); \\mu_{\\theta}(c_2);\\sigma_{\\theta}(c_2))\\tag{5} $$</p> <p>Speaker encoder: Considering the APM module\u2019s increased focus on understanding emotional prosody more thoroughly compared to previous models, it\u2019s apparent that speaker characteristics could unintentionally be overlooked during conversion.Recognizing the critical role of fundamental frequency (F0) in speaker modeling [20], we augment the F0 predictor of VISinger (2021) by adding multiple one-dimensional convolutional layers and a linear layer to construct the speaker encoder, which tackles the issue effectively.</p> <p>Emotion descriptor: To enhance PAVITS\u2019s emotional naturalness, we employ a specific SER system rooted in Russell\u2019s circumplex theory [22] to predict dimensional emotion representation, encompassing Valence-Arousal-Dominance values as a conditional input. This input guides the capture of nuanced prosody variations, which ensures that while satisfying human perception of emotions at utterance-level, natural prosody variations are retained from segment-level down to frame-level, preserving intricate details. It consists of a SER module [23] and a linear projection layer.</p> <p>Prosody Integrator: The prosody integrator incorporates a combination of speaker identity attributes, emotional prosody characteristics, and intrinsic content properties extracted from the linear spectrogram. It is constructed using multiple convolution layers, WaveNet residual blocks, and a linear projection layer.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#23information-alignment-module","title":"2.3.Information Alignment Module","text":"<p>In VITS (2021), the existing alignment mechanism, which is called Monotonic Alignment Search (MAS), solely relies on textual and acoustic features from parallel datasets. Thus, it is insufficient in capturing emotional prosody nuances, hindering effective linkage between the TPP and APM modules. To overcome this limitation, we propose an additional prosody alignment loss function based on Kullback-Leibler divergence, to facilitate joint training for frame-level prosody modeling across the TPP and APM modules, with the goal of enhancing prosody information integration and synchronization within our model.</p> <p>$$     L_{psd} = D_{KL}(q(z_2|c+2)| p(z_1|c_1))\\tag{6} $$</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#24emotional-speech-synthesis-module","title":"2.4.Emotional Speech Synthesis Module","text":"<p>In the emotional speech synthesis module, the decoder generates a waveform based on latent $z$, employing adversarial learning to continuously enhance naturalness in both content and emotion. To improve the naturalness of content, $L_{recon-cls}$ minimizes the $L_1$ distance between predicted and target spectrograms, $L_{recon-fm}$ minimizes the $L_1$ distance between feature maps extracted from intermediate layers in each discriminator, aimed at enhancing training stability. Since the former predominantly influences the early-to-mid stage, while the latter assumes a more prominent role in mid-to-late stage, we introduce two coefficients to balance their contributions as follows.</p> <p>$$     L_{recon}= \\gamma L_{recon-cls}+ \\beta L_{recon-fm}(G)\\tag{7} $$</p> <p>To enhance the perception of emotions, $L_{emo-cls}$ represents the loss function for emotional classification, while $L_{emo-fm}$ denotes the loss associated with feature mapping for emotion discrimination.</p> <p>$$     L_{emo}= L_{emo-cls}+ \\beta L_{emo-fm}(G)\\tag{8} $$</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#25final-loss","title":"2.5.Final Loss","text":"<p>By combining CVAE with adversarial training, we formulate the overall loss function as follows:</p> <p>$$ \\begin{align}     L &amp;= L_{recon}+ L_{adv}(G) + L_{emo}+ L_{psd}+ L_{F0}+ L_{dur}\\tag{9}\\     L(D) &amp;= L_{adv}(D)\\tag{10} \\end{align} $$</p> <p>where $L_{adv}(G)$ and $L_{adv}(D)$ represent the adversarial loss for the Generator and Discriminator respectively, $L_{F0}$ minimizes the $L_2$ distance between the predicted F0 and corresponding ground truth, $L_{dur}$ minimizes the $L_2$ distance between the predicted duration and ground truth which is obtained through estimated alignment.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#26run-time-conversion","title":"2.6.Run-Time Conversion","text":"<p>At runtime, there are two converting methods: a fixed-length approach (Audio-$z_2$-Audio, named PAVITS-FL) and a variable-length approach (Audio-Text-$z_1$-Audio, named PAVITS-VL). The former uses APM module for latent $z$ prediction from audio, ensuring robustness as it remains unaffected by text encoding, but is constrained by a fixed spectrum length due to Dynamic Time Warping (DTW) limitations. The latter employs TPP module to predict latent $z$ from corresponding text obtained through automatic speech recognition (ASR) technique, which is not bound by duration modeling and offers greater naturalness. Finally, the ESS module\u2019s decoder takes latent $z$ (either $z_1$ or $z_2$) as input and synthesizes the converted waveform without a separate vocoder.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#3experiments","title":"3.Experiments","text":"","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#31datasets","title":"3.1.Datasets","text":"<p>We perform emotional conversion on a Mandarin corpus belonged to Emotional Speech Dataset (ESD) [24] from neutral to angry, happy, sad, and surprise, denoted as Neu-Ang, Neu-Hap, Neu-Sad, Neu-Sur respectively. For each emotion pair, we use 300 utterances for training, 30 utterances for evaluation, and 20 utterances for test. The total duration of training data is around 80 minutes (16 minutes per emotion category), which is absolutely small compared to others.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#32experimental-setup","title":"3.2.Experimental Setup","text":"<p>We train the following models for comparison. - CycleGAN [25] (baseline): CycleGAN-based EVC model with WORLD vocoder. - StarGAN [26] (baseline): StarGAN-based EVC model with WORLD vocoder. - Seq2seq-WA2 [15] (baseline): Seq2seq-based EVC model employing 2-stage training strategy with WaveRNN vocoder. - VITS (2021) (baseline):EVC model constructed by original VITS, operating independently in both fixed-length and variable-length, take the average as the result. - PAVITS-FL (proposed): the proposed model based on VITS, incorporates all the contributions outlined in the paper, but operate within a fixed-length framework. - PAVITS-VL (proposed): the proposed model based on VITS, incorporates all the contributions outlined in the paper, but operate within a variable-length framework leveraging ASR to obtain text from source audio.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#33results-discussion","title":"3.3.Results &amp; Discussion","text":"<p>Mel-cepstral distortion (MCD) was calculated for objective evaluation, as depicted in Tab.01.In terms of subjective evaluation, Mean Opinion Score (MOS) tests were conducted to appraise both the quality and naturalness of speech as shown in Tab.02. The naturalness score was derived by averaging the scores for content naturalness and emotional prosody naturalness, as rated by 24 participants, each of whom assessed a total of 148 utterances. We further report emotional similarity results between converted audio and human voice to gauge emotional naturalness as illustrated in Fig.02.</p> <p>Through the above-mentioned metrics, it is obvious that the proposed PAVITS achieves competitive performance on both objective and subjective evaluation. From the perspective of objective MCD and subjective MOS, both original VITS and our proposed PAVITS models always outperform other models with traditional vocoder or neural vocoder, which proves that the integration of neural acoustic converter and vocoder is suitable for EVC task to enhance speech quality and naturalness. It is worth noting that even in the case of the fixed-length PAVITS-FL model, there is a reduction of over 0.4 in MCD when compared to the variable-length seq2seq model and the original VITS model. Furthermore, there has been an enhancement of 0.6 and 0.2 in MOS, respectively. To some extent, it reflects how human tend to be influenced by audio quality when assessing model naturalness, especially when there are significant differences in quality being compared.</p> <p>As depicted in Fig.02, our proposed PAVITS-VL (variable-length) model aligns more closely with human perception in the converted audio, which attributed to the model\u2019s capacity for fine-grained granularity in modeling speech emotion, incorporating implicit prosody cues.To further show the effectiveness of our method, we visualize the spectrogram of testing clips, as exemplified in Fig.03. It is readily apparent that the spectrogram converted by PAVITS exhibits finer details in prosody variations within the pertinent frequency bands, while simultaneously preserving descriptive information for other frequency bands. Consequently, the audio generated by PAVITS possesses a prosody naturalness and emotional accuracy that closely approximates the ground truth spectrogram.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#34ablation-study","title":"3.4.Ablation Study","text":"<p>We further conduct an ablation study to validate different contributions.We remove prosody predictor, prosody alignment, and prosody integrator in turn and let the subjects evaluate quality and naturalness of converted audio. From Tab.03, we can see that all scores are degraded with the removal of different components. When remove prosody predictor, the speech quality does not undergo significant changes, as the original VITS primarily relies on textual features as input. However, a significant decrease in naturalness is observed, attributed to the loss of explicit emotion label for TPP module as a conditioning factor. This highlights the importance of aligning with APM module on the basis of information asymmetry, which reflects the ingenious design of prosody modeling structure. Note that the performance of PAVITS is worse than VITS after deleting prosody alignment, it might be attributed the fact that latent prosody representations are not constrained during training, which damages the original MAS mechanism present in VITS. To further show the contribution from the prosody integrator, we replace it with a simple concatenation. Both speech quality and naturalness show a slight decrease, indicating that utilizing prosody integrator for information fusion is quite effective for APM module.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/Voice_Conversion/2024.03.03_PAVITS/#4conclusion","title":"4.Conclusion","text":"<p>In this paper, we propose Prosody-Aware VITS (PAVITS) for emotional voice conversion (EVC). By integrating acoustic prosody modeling (APM) module with textual prosody prediction (TPP) module through prosody alignment, the fine-grained emotional prosody features across various scales of emotional speech can be learned effectively. Experimental results on ESD corpus demonstrate the superiority of our proposed PAVITS for content naturalness and emotional naturalness, even when dealing with limited data scenarios. In the future, we will explore the controllable emotional prosody modeling to allow better interpretability of EVC.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Models/_tmp/2024.02.14_MobileSpeech/","title":"MobileSpeech","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech - \u4f5c\u8005:   - [Shengpeng Ji](../../Authors/Shengpeng_Ji.md)   - [Ziyue Jiang](../../Authors/Ziyue_Jiang.md)   - [Hanting Wang](../../Authors/Hanting_Wang.md)   - [Jialong Zuo](../../Authors/Jialong_Zuo.md)   - [Zhou Zhao](../../Authors/Zhou_Zhao_(\u8d75\u6d32).md) - \u673a\u6784:   - \u673a\u6784  - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.02.14 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.06 - \u53d1\u8868:   - \u671f\u520a/\u4f1a\u8bae  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2402.09378)   - [DOI]()   - [Github]()   - [Demo](https://mobilespeech.github.io) - \u6807\u7b7e:   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md)   - [\u79fb\u52a8\u8bbe\u5907](../../Tags/MobileDevice.md) - \u9875\u6570: 13 - \u5f15\u7528: ? - \u88ab\u5f15: 1"},{"location":"TTS/Models/_tmp/2024.02.14_MobileSpeech/#a","title":"A.\u6458\u8981","text":"<p>Zero-shot text-to-speech (TTS) has gained sig-nificant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been devel-oped for cloud-based systems.Taking au-toregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness.There-fore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging dis-crete codec, we design a parallel speech mask decoder module called SMD, which incorpo-rates hierarchical information from the speech codec and weight mechanisms across differ-ent codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of infor-mation flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mo-bile devices. Audio samples are available at https://mobilespeech.github.io/ .</p>"},{"location":"TTS/Models/_tmp/2024.02.14_MobileSpeech/#1","title":"1.\u5f15\u8a00","text":"<p>In recent years, remarkable progress has been made in the development of text-to-speech (TTS) technology (FastSpeech (2019); VITS (2021); Huang et al., 2022a; Li et al., 2023). With advancements in this field, current state-of-the-art TTS systems have demonstrated the ability to generate high-quality voices of unseen speakers while maintaining con-sistency in tone and rhythm (Chen et al., 2021; YourTTS (2021)). However, these systems are often trained on datasets of only a few hundred hours and often require fine-tuning with dozens of sentences during the inference stage, which signifi-cantly limits the generative capabilities of the mod-els and is time-consuming and not user-friendly.</p> <p>With the recent development of large-scale lan-guage models (Brown et al., 2020; Touvron et al.,2023) and the increase in training data for speech synthesis from hundreds of hours (Zen et al., 2019)to tens of thousands of hours (Kahn et al., 2020), current TTS models exhibit powerful zero-shot capabilities, enabling the cloning of unseen speak-ers\u2019 voices in just a few seconds (VALL-E (2023); VALL-E X (2023); NaturalSpeech2 (2023); VoiceBox (2023);Jiang et al., 2023b; SPEAR-TTS (2023); SoundStorm (2023); Huang et al., 2023; Yang et al., 2023b). However, these models primarily focus on in-context abilities and fail to address issues related to inference speed and model deployment parameters. Additionally, all existing zero-shot TTS models are cloud-based and lack a mobile-based deployment approach. MobileSpeech is the first zero-shot TTS synthesis system that can be deployed on mobile de-vices. Firstly we think deployable mobile zero-shot TTS models aim to achieve the following goals: - Fast: through our experiments, we have ob-served that the real-time factor (RTF) on a single mobile device often exceeds 8-10 times that of an A100 RTF for the same text. There-fore, the inference speed of the zero-shot TTS model must be significantly faster. - Lightweight: to enable deployment on mo-bile or edge devices, the model size should be small, and the runtime memory footprint should be minimal. - High similarity and diversity: zero-shot TTS system should be able to clone timbre and prosody with just a few seconds of prompts,yielding diverse speech even when provided with identical text. - High quality: to enhance the naturalness of synthesized speech, the model should pay attention to details such as frequency bins be-tween adjacent harmonics and possess power-ful duration modeling capabilities. - Robustness: a highly robust TTS system is essential. A zero-shot TTS system should minimize occurrences of missing or repeated words.</p> <p>To achieve these objectives, we have made our best efforts to reproduce several state-of-the-art generative models, which will be elaborated in the subsequent section on related work. We found cur-rent work is not applicable to mobile devices. Ul-timately, based on the Natspeech framework (FastSpeech (2019), FastSpeech2 (2020)) and the Mask generation module(MaskGIT (2022); SoundStorm (2023)), we addi-tionally design the SMD module and the speaker prompt component, taking into account the discrete codec architecture of speech. </p> <p>In summary, MobileSpeech contributes as follows: - MobileSpeech is the first zero-shot TTS syn-thesis system that can be deployed on mobile devices. MobileSpeech achieves a good bal-ance according to the above five evaluation metrics. - In MobileSpeech, we additionally design the SMD module based on the hierarchical token structure of discrete speech codecs and design the Speaker Prompt module to maintain high similarity, high quality, low latency. - Training MobileSpeech-english for 580 hours achieved SOTA inference speed and comparable audio quality. Training MobileSpeech-chinese for 40,000 hours resulted in SOTA performance in all aspects. - MobileSpeech has been successfully deployed on mobile phones and is expected to be used by several hundred thousand users.</p>"},{"location":"TTS/Models/_tmp/2024.02.14_MobileSpeech/#2","title":"2.\u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Models/_tmp/2024.02.14_MobileSpeech/#21zero-shot-tts","title":"2.1.Zero-Shot TTS","text":"<p>Zero-shot speech synthesis refers to the ability to synthesize the voice of an unseen speaker based solely on a few seconds of audio prompt, also known as voice cloning. Due to its impressive performance, it has gained significant attention re-cently. Previous approaches in the field of voice cloning can be classified into speaker adaptation (Wang et al., 2018; Chen et al., 2021; Huang et al.,2022b) and speaker encoding methods (YourTTS (2021); Arik et al., 2018; Kang et al., 2022).These models are often trained on smaller datasets and require a substantial amount of data for fine-tuning or employ highly complex encoders. These limitations greatly restrict the audio quality and further usage.</p> <p>In recent months, with the advancement of generative large-scale models, a plethora of outstand-ing works have emerged. VALL-E (2023) leverages discrete codec representations and combines autoregressive and non-autoregressive models in a cascaded manner, preserving the pow-erful contextual capabilities of language models. It can clone the voice of a target speaker with just a 3-second audio prompt. VALL-E X (2023) extends zero-shot TTS to multiple languages based on the cascaded structure of VALL-E. NaturalSpeech2 (2023) employs continu-ous vectors instead of discrete neural codec tokens and introduces in-context learning to a latent dif-fusion model. SPEAR-TTS (2023) and Make-a-Voice (Huang et al., 2023) utilize semantic tokens to reduce the gap between text and acoustic features. VoiceBox (2023) is a non-autoregressive flow-matching model trained to infill speech, given audio context and text. Mega-TTS (Jiang et al., 2023b,a) , on the other hand, utilizes traditional mel-spectrograms, decoupling timbre and prosody and further modeling the prosody us-ing an autoregressive approach.</p> <p>However, none of the aforementioned works take into account the model\u2019s speed and lightweight na-ture. It is well-known that autoregressive models often require large memory footprints and have slower iteration speeds, and diffusion models often require hundreds of sampling steps. Some work in-troduce a semantic token cascade structure, which will introduce more time overhead. However, com-pared to codecs, certain approaches based on mel-spectrogram exhibit lower generation diversity and audio quality (Siuzdak, 2023). MobileSpeech, on the other hand, is the first zero-shot TTS system that can be deployed on mobile devices, address-ing the fast speed,lightweight, high quality require-ments.</p>"},{"location":"TTS/Models/_tmp/2024.02.14_MobileSpeech/#22","title":"2.2.\u751f\u6210\u6a21\u578b","text":"<p>Generative models have been extensively utilized in various domains by employing techniques such as language models (AudioLM (2023); Kreuk et al., 2022; Ji et al., 2023; [VALL-E (20232023.01.05_VALL-E.md.md); VALL-E X (2023)), Variational Autoencoders (VAE) (Ren et al., 2021; Lee et al., 2022), Generative Adver-sarial Networks (GAN) (VITS (2021); Kong et al., 2020), Normalizing Flow (Glow-TTS (2020); Miao et al., 2020), Mask Autoencoders (MaskGIT (2022))and diffusion models (Huang et al.,2022a). Considering the challenge of balancing perceptual quality and inference speed in zero-shot TTS, as well as the recent success of parallel de-coding in text (Ghazvininejad et al., 2019), image (MaskGIT (2022)), audio (SoundStorm),and video (Villegas et al., 2022) generation tasks,MobileSpeech has adopted masked parallel gen-eration as the audio synthesis approach. Further-more, MobileSpeech has additionally designed the Speech Codec Mask Decoder (SMD) module and Speaker prompt based on the unique characteristics of acoustic tokens and the specific requirements of the zero-shot TTS task, aiming to address the afore-mentioned trade-off. In Appendix A, we present the detail of discrete acoustic codec and in Ap-pendix B we present the distinctions between mask-based generative models and other generative mod-els in modeling discrete codecs.</p>"},{"location":"TTS/Models/_tmp/2024.02.14_MobileSpeech/#3","title":"3.\u65b9\u6cd5","text":""},{"location":"TTS/Models/_tmp/2024.02.14_MobileSpeech/#4","title":"4.\u5b9e\u9a8c","text":""},{"location":"TTS/Models/_tmp/2024.02.14_MobileSpeech/#5","title":"5.\u7ed3\u8bba","text":"<p>In this paper, we propose MobileSpeech, a Fast and Lightweight Framework for Mobile zero-shot Text-to-Speech. MobileSpeech achieves a unique com-bination of audio naturalness and similarity perfor-mance, comparable to other zero-shot TTS systems on English corpora, while significantly enhancing inference speed through its distinctive SMD and Speaker modules. MobileSpeech stands as the first real-time zero-shot TTS system deployable at the edge, demonstrating state-of-the-art performance on Chinese corpora through extensive training on large-scale Chinese language data. We not only validate the significance of each module in Mo-bileSpeech through ablation experiments but also analyze the effects of different configurations and iterations to adapt to various deployment scenarios in mobile environments.</p>"},{"location":"TTS/Models/_tmp/2024.02.14_MobileSpeech/#6","title":"6.\u5c40\u9650\u6027\u4e0e\u672a\u6765\u5de5\u4f5c","text":"<p>In this section, we will analyze the limitations of MobileSpeech and discuss potential future work.</p> <p>Failure cases  During our experiments in real-world scenarios (Figure 2), we observed occasional instances of stuttering and unstable pitch in Mo-bileSpeech. This could be attributed to noisy input prompts or the possibility that the mask mechanism excessively focuses on local positions. In future work, we aim to enhance the model\u2019s noise robust-ness and design a more robust mask mechanism that considers a broader context.</p> <p>New Tasks The current zero-hhot TTS task it-self has significant potential for expansion. Most existing models primarily focus on similarity in timbre. However, future research can extend zero-shot TTS to include similarity in rhythm, emotion,and language. It could even involve using multiple audio segments to control different parts or combin-ing with text style control, representing meaningful new tasks.</p> <p>Broader impacts Since MobileSpeech could synthesize speech that maintains speaker identity,it may carry potential risks in misuse of the model,such as spoofing voice identification or impersonat-ing a specific speaker. To mitigate such risks, it is possible to build a detection model to discriminate whether an audio clip was synthesized by Mobile-Speech.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/","title":"ControlSpeech","text":"\u57fa\u672c\u4fe1\u606f \u6807\u9898: ControlSpeech: Towards Simultaneous Zero-shot Speaker Cloning and Zero-shot Language Style Control With Decoupled Codec \u4f5c\u8005:   - [Shengpeng Ji](../../Authors/Shengpeng_Ji.md)   - [Jialong Zuo](../../Authors/Jialong_Zuo.md)    - [Minghui Fang](../../Authors/Minghui_Fang.md)    - [Siqi Zheng](../../Authors/Siqi_Zheng.md)   - [Qian Chen](../../Authors/Qian_Chen.md)   - [Wen Wang](../../Authors/Wen_Wang.md)   - [Ziyue Jiang](../../Authors/Ziyue_Jiang.md)   - [Hai Huang](../../Authors/Hai_Huang.md)   - [Xize Cheng](../../Authors/Xize_Cheng_(\u6210\u66e6\u6cfd).md)   - [Rongjie Huang](../../Authors/Rongjie_Huang_(\u9ec4\u878d\u6770).md)    - [Zhou Zhao](../../Authors/Zhou_Zhao_(\u8d75\u6d32).md) - \u673a\u6784:   - \u6d59\u6c5f\u5927\u5b66 - \u65f6\u95f4:   - 2024.06.03 ArXiv v1 - \u94fe\u63a5:   - [ArXiv]()   - [DOI]()   - [Github](https://github.com/jishengpeng/ControlSpeech)   - [Demo]() - \u6807\u7b7e:   - ? - \u9875\u6570: 16 - \u5f15\u7528: 54 - \u88ab\u5f15: 0"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#abstract","title":"Abstract","text":"\u539f\u6587  &gt; In this paper, we present ***ControlSpeech***, a text-to-speech (TTS) system capable of fully cloning the speaker\u2019s voice and enabling arbitrary control and adjustment of speaking style, merely based on a few seconds of audio prompt and a simple textual style description prompt. &gt; Prior zero-shot TTS models and controllable TTS models either could only mimic the speaker\u2019s voice without further control and adjustment capabilities or were unrelated to speaker-specific voice generation. &gt; Therefore, ***ControlSpeech*** focuses on a more challenging new task\u2014a TTS system with controllable timbre, content, and style at the same time. &gt; ***ControlSpeech*** takes speech prompts, content prompts, and style prompts as inputs and utilizes bidirectional attention and mask-based parallel decoding to capture corresponding codec representations in a discrete decoupling codec space. &gt; Moreover, we discovered the issue of text style controllability in a many-to-many mapping fashion and proposed the ***Style Mixture Semantic Density (SMSD)*** model to resolve this problem. &gt; ***SMSD*** module which is based on Gaussian mixture density networks, is designed to enhance the fine-grained partitioning and sampling capabilities of style semantic information and generate speech with more diverse styles. &gt; In terms of experiments, we make available a controllable model toolkit called ***ControlToolkit*** with a new style controllable dataset, some replicated baseline models and propose new metrics to evaluate both the control capability and the quality of generated audio in ***ControlSpeech***. &gt; The relevant ablation studies validate the necessity of each component in ***ControlSpeech*** is necessary. &gt; We hope that ***ControlSpeech*** can establish the next foundation paradigm of controllable speech synthesis. &gt; The relevant code and demo are available at https://github.com/jishengpeng/ControlSpeech.   <p>\u672c\u9879\u5de5\u4f5c\u5c55\u793a\u4e86 ControlSpeech, \u4e00\u4e2a\u80fd\u591f\u4ec5\u57fa\u4e8e\u6570\u79d2\u7684\u97f3\u9891\u63d0\u793a\u548c\u7b80\u5355\u7684\u6587\u672c\u98ce\u683c\u63cf\u8ff0\u63d0\u793a\u4ece\u800c\u5b8c\u6574\u514b\u9686\u8bf4\u8bdd\u4eba\u58f0\u97f3\u4e14\u80fd\u4efb\u610f\u63a7\u5236\u548c\u8c03\u8282\u8bf4\u8bdd\u98ce\u683c\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf. \u4e4b\u524d\u7684\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u548c\u53ef\u63a7\u5236\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u8981\u4e48\u53ea\u80fd\u6a21\u4eff\u8bf4\u8bdd\u4eba\u58f0\u97f3\u800c\u6ca1\u6709\u8fdb\u4e00\u6b65\u63a7\u5236\u548c\u8c03\u8282\u7684\u80fd\u529b, \u8981\u4e48\u548c\u5177\u4f53\u8bf4\u8bdd\u4eba\u7684\u58f0\u97f3\u751f\u6210\u65e0\u5173. \u56e0\u6b64, ControlSpeech \u805a\u7126\u4e8e\u66f4\u5177\u6311\u6218\u6027\u7684\u65b0\u4efb\u52a1: \u97f3\u8272, \u5185\u5bb9, \u98ce\u683c\u80fd\u540c\u65f6\u53ef\u63a7\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf. ControlSpeech \u63a5\u53d7\u97f3\u9891\u63d0\u793a, \u5185\u5bb9\u63d0\u793a, \u98ce\u683c\u63d0\u793a\u4f5c\u4e3a\u8f93\u5165, \u5e76\u91c7\u7528\u53cc\u5411\u6ce8\u610f\u529b\u548c\u57fa\u4e8e\u63a9\u7801\u7684\u5e76\u884c\u89e3\u7801\u5668\u6765\u6355\u6349\u79bb\u6563\u89e3\u8026\u7f16\u89e3\u7801\u7a7a\u95f4\u5185\u7684\u76f8\u5e94\u7f16\u89e3\u7801\u8868\u793a. \u6b64\u5916, \u6211\u4eec\u53d1\u73b0\u5728\u591a\u5230\u591a\u6620\u5c04\u8303\u5f0f\u4e0b\u6587\u672c\u98ce\u683c\u53ef\u63a7\u6027\u95ee\u9898, \u5e76\u63d0\u51fa \u98ce\u683c\u6df7\u5408\u8bed\u97f3\u5bc6\u5ea6\u6a21\u578b (Style Mixture Semantic Density, SMSD) \u6a21\u5757\u6765\u89e3\u51b3\u6b64\u95ee\u9898. SMSD \u6a21\u5757\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc, \u88ab\u8bbe\u8ba1\u7528\u4e8e\u589e\u5f3a\u98ce\u683c\u8bed\u4e49\u4fe1\u606f\u7684\u7ec6\u7c92\u5ea6\u5212\u5206\u548c\u91c7\u6837\u80fd\u529b, \u5e76\u7528\u66f4\u591a\u6837\u6027\u7684\u98ce\u683c\u751f\u6210\u8bed\u97f3. \u5728\u5b9e\u9a8c\u65b9\u9762, \u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u540d\u4e3a ControlToolkit \u7684\u53ef\u63a7\u6a21\u578b\u5de5\u5177\u7bb1, \u4ee5\u53ca\u4e00\u4e2a\u65b0\u7684\u98ce\u683c\u53ef\u63a7\u6570\u636e\u96c6, \u4e00\u4e9b\u590d\u73b0\u7684\u57fa\u7ebf\u6a21\u578b, \u5e76\u63d0\u51fa\u65b0\u7684\u6307\u6807\u6765\u8bc4\u4f30 ControlSpeech \u7684\u63a7\u5236\u80fd\u529b\u548c\u751f\u6210\u97f3\u9891\u8d28\u91cf. \u76f8\u5173\u7684\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 ControlSpeech \u4e2d\u5404\u4e2a\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027. \u6211\u4eec\u5e0c\u671b ControlSpeech \u80fd\u591f\u6210\u4e3a\u53ef\u63a7\u8bed\u97f3\u5408\u6210\u7684\u4e0b\u4e00\u4ee3\u57fa\u7840\u8303\u5f0f. \u76f8\u5173\u4ee3\u7801\u548c\u793a\u4f8b\u53ef\u5728 https://github.com/jishengpeng/ControlSpeech \u83b7\u5f97.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#1introduction","title":"1.Introduction","text":"\u539f\u6587  &gt; Over the past decade, the field of speech synthesis has seen remarkable advancements ([FastSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md), [VITS](../../Models/E2E/2021.06.11_VITS.md), [FastDiff](../../Models/Diffusion/2022.04.21_FastDiff.md), [FastSpeech](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md)), achieving synthesized speech that rivals real human speech in terms of expressiveness and naturalness ([NaturalSpeech](../../Models/E2E/2022.05.09_NaturalSpeech.md)). &gt; Recently, with the development of large language models (GPT-3, GPT-4, LLaMA) and generative models (DDPM, [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md), [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md), [BVAE-TTS](../TTS2_Acoustic/BVAE-TTS.md)) in other domains, the tasks of zero-shot TTS ([VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md), [NaturalSpeech 2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md), [VoiceBox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md), [Mega-TTS](../../Models/_tmp/2023.06.06_Mega-TTS.md), [SoundStorm](../../Models/Speech_LLM/2023.05.16_SoundStorm.md)) and style-controllable speech synthesis ([PromptTTS](../../Models/_tmp/2022.11.22_PromptTTS.md), [PromptStyle](../../Models/E2E/2023.05.31_PromptStyle.md), [InstructTTS](../../Models/_tmp/2023.01.31_InstructTTS.md), [SALL-E](../../Datasets/2023.08.28_TextrolSpeech.md)) have garnered significant attention in the speech domain due to their powerful zero-shot generation and controllability capabilities. &gt; Zero-shot TTS ([VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md), [NaturalSpeech 2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md), [SPEAR-TTS](../../Models/Speech_LLM/2023.02.07_SPEAR-TTS.md)) refers to the ability to perfectly clone an unseen speaker\u2019s voice using only a few seconds of a speech prompt by significantly scaling up both the corpus and model sizes. &gt; Style-controllable TTS ([PromptTTS](../../Models/_tmp/2022.11.22_PromptTTS.md), [InstructTTS](../../Models/_tmp/2023.01.31_InstructTTS.md)), on the other hand, allows for the control of a speaker\u2019s style (prosody, accent, emotion, etc.) through textual descriptions.   <p>\u5728\u8fc7\u53bb\u7684\u5341\u5e74\u4e2d, \u8bed\u97f3\u5408\u6210\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65, \u4f8b\u5982 FastSpeech2, VITS, FastDiff, FastSpeech, \u5b9e\u73b0\u4e86\u5728\u8868\u8fbe\u6027\u548c\u81ea\u7136\u5ea6\u4e0a\u80fd\u4e0e\u771f\u4eba\u8bed\u97f3\u76f8\u5ab2\u7f8e\u7684\u5408\u6210\u8bed\u97f3. \u8fd1\u671f, \u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b (GPT-3, GPT-4, LLaMA) \u548c\u5728\u5176\u4ed6\u9886\u57df\u7684\u751f\u6210\u6a21\u578b (DDPM, HiFi-GAN, Glow-TTS, BVAE-TTS) \u7684\u53d1\u5c55, \u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u4efb\u52a1 (VALL-E, NaturalSpeech 2, VoiceBox, Mega-TTS, SoundStorm) \u548c\u98ce\u683c\u53ef\u63a7\u8bed\u97f3\u5408\u6210 (PromptTTS, PromptStyle, InstructTTS, SALL-E) \u5728\u8bed\u97f3\u9886\u57df\u53d7\u5230\u4e86\u6781\u5927\u7684\u5173\u6ce8, \u8fd9\u5f97\u76ca\u4e8e\u5b83\u4eec\u5f3a\u5927\u7684\u96f6\u6837\u672c\u751f\u6210\u548c\u53ef\u63a7\u6027\u80fd\u529b.</p> <ul> <li>\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3 (VALL-E, NaturalSpeech 2, SPEAR-TTS) \u6307\u7684\u662f\u901a\u8fc7\u663e\u8457\u6269\u6492\u8bed\u6599\u5e93\u548c\u6a21\u578b\u89c4\u6a21, \u4ec5\u4f7f\u7528\u51e0\u79d2\u949f\u7684\u8bed\u97f3\u63d0\u793a\u5c31\u80fd\u5b8c\u7f8e\u514b\u9686\u4e00\u4e2a\u672a\u89c1\u8fc7\u7684\u8bf4\u8bdd\u4eba\u7684\u58f0\u97f3.</li> <li>\u98ce\u683c\u53ef\u63a7\u6587\u672c\u8f6c\u8bed\u97f3 (PromptTTS, InstructTTS) \u5141\u8bb8\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u6765\u63a7\u5236\u8bf4\u8bdd\u4eba\u7684\u98ce\u683c (\u8bed\u8c03, \u53e3\u97f3, \u60c5\u7eea\u7b49).</li> </ul> \u539f\u6587  &gt; However, these two types of models have their own limitations. &gt; As illustrated in the right panel of Fig.01, zero-shot TTS can clone the voice of any speaker, but the style is fixed and cannot be further controlled or adjusted. &gt; Conversely, style-controllable TTS can synthesize speech in any desired style, but it cannot specify the timbre of the synthesized voice. &gt; Although some efforts ([InstructTTS](../../Models/_tmp/2023.01.31_InstructTTS.md), [PromptStyle](../../Models/E2E/2023.05.31_PromptStyle.md)) have been made to use speaker IDs for control, these approaches are limited to testing on constrained in-domain datasets. &gt; Therefore, we propose a novel model ***ControlSpeech***, which simultaneously controls timbre, content, and style, demonstrating powerful zero-shot cloning and control capabilities at the same time.   <p></p> <p>\u4f46\u662f, \u8fd9\u4e24\u79cd\u7c7b\u578b\u7684\u6a21\u578b\u6709\u7740\u5b83\u4eec\u7684\u5c40\u9650\u6027.</p> <p></p> <p>\u5982\u56fe 01 \u7684\u53f3\u4fa7, \u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u80fd\u591f\u514b\u9686\u4efb\u610f\u8bf4\u8bdd\u4eba\u7684\u58f0\u97f3, \u4f46\u98ce\u683c\u662f\u56fa\u5b9a\u7684, \u4e14\u65e0\u6cd5\u63a7\u5236\u6216\u8c03\u6574. \u800c\u98ce\u683c\u53ef\u63a7\u6587\u672c\u8f6c\u8bed\u97f3\u5219\u76f8\u53cd, \u80fd\u591f\u5408\u6210\u4efb\u4f55\u60f3\u8981\u7684\u98ce\u683c\u7684\u8bed\u97f3, \u4f46\u65e0\u6cd5\u6307\u5b9a\u5408\u6210\u58f0\u97f3\u7684\u97f3\u7d20. \u5c3d\u7ba1\u6709\u4e00\u4e9b\u5de5\u4f5c (InstructTTS, PromptStyle) \u8bd5\u56fe\u4f7f\u7528\u8bf4\u8bdd\u4eba ID \u6765\u63a7\u5236, \u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5728\u53d7\u9650\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5. \u56e0\u6b64, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b ControlSpeech, \u5b83\u80fd\u540c\u65f6\u63a7\u5236\u97f3\u8272, \u5185\u5bb9, \u98ce\u683c, \u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u514b\u9686\u548c\u63a7\u5236\u80fd\u529b.</p> \u539f\u6587  &gt; For this novel task, simply adding a style prompt control module or a speech prompt control module to previous model frameworks is evidently insufficient. &gt; The information from the two types of prompts can become entangled and interfere with each other. &gt; For instance, the speech prompt might contain a style different from that described by the text. &gt; Inspired by this observation, we attempted to incorporate the concept of an information bottleneck in ***ControlSpeech*** to achieve independent disentanglement of timbre, content, and style. &gt; Additionally, to attain robust zero-shot speaker cloning capabilities, a large-scale, multi-speaker training dataset ([MLS](../../Datasets/2020.12.07_MLS.md), [Libri-Light](../../Datasets/2019.12.17_Libri-Light.md)) is essential. &gt; Leveraging recent breakthroughs in the codec domain, we used [FACodec](../Diffusion/2024.03.05_NaturalSpeech3.md) which is pre-trained on 60,000 hours ([Libri-Light](../../Datasets/2019.12.17_Libri-Light.md)) speech as the speech tokenizer for ***ControlSpeech***. &gt; During the speech synthesis process, we adopted [FastSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md) as the base synthesis framework and employed an advanced non-autoregressive confidence-based codec generator [5, 2, [Phenaki](../../Models/_Basis_CV/2022.10.05_Phenaki.md)] as our decoder.   <p></p> <p>\u5bf9\u4e8e\u8fd9\u4e00\u65b0\u4efb\u52a1, \u4ec5\u4ec5\u5728\u4e4b\u524d\u7684\u6a21\u578b\u6846\u67b6\u4e0a\u7b80\u5355\u5730\u6dfb\u52a0\u98ce\u683c\u63d0\u793a\u63a7\u5236\u6a21\u5757\u6216\u8bed\u97f3\u63d0\u793a\u63a7\u5236\u6a21\u5757\u662f\u4e0d\u591f\u7684. \u8fd9\u4e24\u7c7b\u63d0\u793a\u5305\u542b\u7684\u4fe1\u606f\u53ef\u80fd\u4f1a\u4e92\u76f8\u7ea0\u7f20\u76f8\u4e92\u5e72\u6270. \u4f8b\u5982, \u8bed\u97f3\u63d0\u793a\u53ef\u80fd\u5305\u542b\u4e8e\u6587\u672c\u63cf\u8ff0\u4e0d\u540c\u7684\u98ce\u683c. \u53d7\u6b64\u89c2\u5bdf\u542f\u53d1, \u6211\u4eec\u5c1d\u8bd5\u5728 ControlSpeech \u4e2d\u5f15\u5165\u4fe1\u606f\u74f6\u9888\u7684\u6982\u5ff5, \u4ee5\u5b9e\u73b0\u97f3\u8272, \u5185\u5bb9\u548c\u98ce\u683c\u7684\u72ec\u7acb\u89e3\u8026. \u6b64\u5916, \u4e3a\u4e86\u83b7\u5f97\u5f3a\u5927\u7684\u96f6\u6837\u672c\u8bf4\u8bdd\u4eba\u514b\u9686\u80fd\u529b, \u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u591a\u8bf4\u8bdd\u4eba\u8bad\u7ec3\u6570\u636e\u96c6 (MLS, Libri-Light) \u662f\u5fc5\u8981\u7684. \u7ed3\u5408\u8fd1\u671f\u5728\u7f16\u89e3\u7801\u5668\u9886\u57df\u7684\u7a81\u7834, \u6211\u4eec\u4f7f\u7528\u5728 60,000 \u5c0f\u65f6\u7684 Libri-Light \u8bed\u97f3\u4e0a\u9884\u8bad\u7ec3\u7684 FACodec \u4f5c\u4e3a ControlSpeech \u7684\u8bed\u97f3\u5206\u8bcd\u5668. \u5728\u8bed\u97f3\u5408\u6210\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u91c7\u7528 FastSpeech2 \u4f5c\u4e3a\u57fa\u7840\u5408\u6210\u6846\u67b6, \u5e76\u91c7\u7528\u5148\u8fdb\u7684\u975e\u81ea\u56de\u5f52\u7f6e\u4fe1\u5ea6\u7f16\u89e3\u7801\u5668\u751f\u6210\u5668 (MaskGIT, SoundStorm, Phenaki) \u4f5c\u4e3a\u89e3\u7801\u5668.</p> \u539f\u6587  &gt; Moreover, during our experiments, we identified a many-to-many problem in style control: different style descriptions might correspond to the same audio, and a single style description might correspond to varying degrees of one style for the same speaker. &gt; Therefore, we designed a novel ***Style Mixture Semantic Density Sampling (SMSD)*** module to address the many-to-many issue in style control.  &gt; We incorporate the global semantic information of the style control and exploit sampling from a mixed distribution [53, 18] of style descriptions to achieve hierarchical control. &gt; Additionally, we also design a noise disturbance module in ***SMSD*** to further enhance the diversity of styles.   <p></p> <p>\u6b64\u5916, \u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u53d1\u73b0\u98ce\u683c\u63a7\u5236\u4e2d\u7684\u591a\u5bf9\u591a\u95ee\u9898: \u4e0d\u540c\u98ce\u683c\u63cf\u8ff0\u53ef\u80fd\u5bf9\u5e94\u540c\u4e00\u4e2a\u97f3\u9891, \u5355\u4e2a\u98ce\u683c\u63cf\u8ff0\u53ef\u80fd\u5bf9\u5e94\u540c\u4e00\u4e2a\u8bf4\u8bdd\u4eba\u5355\u4e2a\u98ce\u683c\u7684\u4e0d\u540c\u7a0b\u5ea6. \u56e0\u6b64, \u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684 \u98ce\u683c\u6df7\u5408\u8bed\u4e49\u5bc6\u5ea6\u91c7\u6837 (Style Mixture Semantic Density Sampling, SMSD) \u6a21\u5757\u6765\u5904\u7406\u8fd9\u4e00\u98ce\u683c\u63a7\u5236\u4e2d\u7684\u591a\u5bf9\u591a\u95ee\u9898. \u6211\u4eec\u5408\u5e76\u4e86\u98ce\u683c\u63a7\u5236\u7684\u5168\u5c40\u8bed\u8bed\u4e49\u4fe1\u606f\u5e76\u5229\u7528\u4ece\u98ce\u683c\u63cf\u8ff0\u7684\u6df7\u5408\u5206\u5e03\u4e2d\u91c7\u6837\u6765\u8fbe\u5230\u5c42\u6b21\u63a7\u5236\u7684\u6548\u679c. \u6b64\u5916, \u6211\u4eec\u5728 SMSD \u4e2d\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u566a\u58f0\u6270\u52a8\u6a21\u5757, \u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u98ce\u683c\u7684\u591a\u6837\u6027.</p> \u539f\u6587  &gt; To comprehensively evaluate ***ControlSpeech***\u2019s controllability, timbre similarity, audio quality, diversity, and generalization, we created a new dataset called ***VccmDataset*** and established new metrics based on [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md). &gt; Considering the lack of open-source text style-controllable TTS models, we have consolidated the re-implemented baselines, ***VccmDataset*** and some evaluation scripts into a toolkit named ***ControlToolkit*** to foster further advancements in the controllable TTS field.   <p></p> <p>\u4e3a\u4e86\u5168\u9762\u8bc4\u4f30 ControlSpeech \u7684\u53ef\u63a7\u6027, \u97f3\u8272\u76f8\u4f3c\u6027, \u97f3\u9891\u8d28\u91cf, \u591a\u6837\u6027, \u6cdb\u5316\u80fd\u529b, \u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6 VccmDataset, \u5e76\u57fa\u4e8e TextrolSpeech \u5efa\u7acb\u4e86\u65b0\u7684\u8bc4\u4ef7\u6307\u6807. \u8003\u8651\u5230\u7f3a\u4e4f\u5f00\u6e90\u7684\u6587\u672c\u98ce\u683c\u53ef\u63a7 TTS \u6a21\u578b, \u6211\u4eec\u5c06\u91cd\u65b0\u5b9e\u73b0\u7684\u57fa\u7ebf, VccmDataset \u4ee5\u53ca\u4e00\u4e9b\u8bc4\u4ef7\u811a\u672c\u6574\u5408\u5230\u540d\u4e3a ControlToolkit \u7684\u5de5\u5177\u7bb1\u4e2d, \u4ee5\u4fc3\u8fdb\u53ef\u63a7 TTS \u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55.</p> \u539f\u6587  &gt; In summary, our contributions are as follows: &gt; - We broadened the scope of controllable speech generation tasks by proposing the ***ControlSpeech***. &gt; To the best of our knowledge, ***ControlSpeech*** is the first TTS model capable of zero-shot control over both timbre and style simultaneously. &gt; - We designed a TTS technology pathway based on disentangled codec representations and validated the necessity of this disentanglement approach. &gt; Additionally, within ***ControlSpeech***, we introduced the novel ***Style Mixture Semantic Density (SMSD)*** module to address the many-to-many problem in style control. &gt; - Given the lack of baselines and datasets in the style-controllable TTS field, we compiled a new dataset called ***VccmDataset*** and established relevant new metrics. &gt; We also reproduced related baselines and open-sourced them along with ***VccmDataset*** in the ***ControlToolkit***. &gt; - We conducted comprehensive experiments, demonstrating that ***ControlSpeech*** exhibits comparable or SOTA performance in terms of controllability, timbre similarity, audio quality, robustness, and generalization.   <p></p> <p>\u7efc\u4e0a\u6240\u8ff0, \u6211\u4eec\u7684\u8d21\u732e\u5982\u4e0b: - \u6211\u4eec\u63d0\u51fa\u4e86 ControlSpeech, \u62d3\u5bbd\u4e86\u53ef\u63a7\u8bed\u97f3\u5408\u6210\u4efb\u52a1\u7684\u8303\u56f4;   \u636e\u6211\u4eec\u6240\u77e5, ControlSpeech \u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5bf9\u97f3\u8272\u548c\u98ce\u683c\u8fdb\u884c\u96f6\u6837\u672c\u63a7\u5236\u7684 TTS \u6a21\u578b. - \u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u5957\u57fa\u4e8e\u5206\u79bb\u7684\u7f16\u89e3\u7801\u5668\u8868\u793a\u7684 TTS \u6280\u672f\u8def\u7ebf, \u5e76\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u5206\u79bb\u65b9\u6cd5\u7684\u5fc5\u8981\u6027.   \u6b64\u5916, \u5728 ControlSpeech \u4e2d, \u6211\u4eec\u5f15\u5165\u4e86\u65b0\u7684 \u98ce\u683c\u6df7\u5408\u8bed\u4e49\u5bc6\u5ea6 (Style Mixture Semantic Density, SMSD) \u6a21\u5757\u6765\u89e3\u51b3\u98ce\u683c\u63a7\u5236\u4e2d\u7684\u591a\u5bf9\u591a\u95ee\u9898. - \u7531\u4e8e\u98ce\u683c\u53ef\u63a7 TTS \u9886\u57df\u7f3a\u4e4f\u57fa\u7ebf\u548c\u6570\u636e\u96c6, \u6211\u4eec\u7f16\u8bd1\u4e86\u4e00\u4efd\u540d\u4e3a VccmDataset \u7684\u65b0\u6570\u636e\u96c6, \u5e76\u5efa\u7acb\u4e86\u76f8\u5173\u7684\u65b0\u8bc4\u4ef7\u6307\u6807.   \u6211\u4eec\u8fd8\u5728 ControlToolkit \u4e2d\u590d\u73b0\u4e86\u76f8\u5173\u7684\u57fa\u7ebf\u5e76\u5f00\u6e90\u4e86 VccmDataset. - \u6211\u4eec\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c, \u8bc1\u660e\u4e86 ControlSpeech \u5728\u53ef\u63a7\u6027, \u97f3\u8272\u76f8\u4f3c\u6027, \u97f3\u9891\u8d28\u91cf, \u5065\u58ee\u6027, \u6cdb\u5316\u80fd\u529b\u65b9\u9762\u90fd\u5177\u6709\u4e0e SOTA \u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#2related-works","title":"2.Related Works","text":"\u539f\u6587  &gt; In this section, we will introduce zero-shot TTS, text prompt-based controllable TTS, and discrete codec tasks related to ***ControlSpeech***. &gt; Due to space constraints, the detailed discussion of related work is provided in appendix A. &gt; However, we encourage reviewers to closely examine the connections and distinctions between ***ControlSpeech*** and prior related work.   <p>\u672c\u8282\u6211\u4eec\u4ecb\u7ecd\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3, \u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u53ef\u63a7\u6587\u672c\u8f6c\u8bed\u97f3, \u4ee5\u53ca\u548c ControlSpeech \u76f8\u5173\u7684\u79bb\u6563\u7f16\u89e3\u7801\u4efb\u52a1. \u7531\u4e8e\u7bc7\u5e45\u9650\u5236, \u76f8\u5173\u5de5\u4f5c\u7684\u8be6\u7ec6\u8ba8\u8bba\u8bf7\u53c2\u89c1\u9644\u5f55 A. \u4f46\u662f, \u6211\u4eec\u9f13\u52b1\u8bc4\u5ba1\u8005\u4ed4\u7ec6\u5ba1\u67e5 ControlSpeech \u4e0e\u76f8\u5173\u5de5\u4f5c\u4e4b\u95f4\u7684\u8054\u7cfb\u548c\u533a\u522b. \u6ce8: \u7531\u9644\u5f55 A \u79fb\u52a8\u81f3\u6b64.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#a1discrete-codec-model","title":"A.1.Discrete Codec Model","text":"\u539f\u6587  &gt; In recent times, neural acoustic codecs ([SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md), [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md), [HiFi-Codec](../../Models/Speech_Neural_Codec/2023.05.04_HiFi-Codec.md), [Vocos](../../Models/TTS3_Vocoder/2023.03.01_Vocos.md), [SpeechTokenizer](../../Models/Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md), [FunCodec](../../Models/Speech_Neural_Codec/2023.09.14_FunCodec.md)) have demonstrated remarkable capabilities in reconstructing high-quality audio at extremely low bitrates. &gt; To elaborate, [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) utilizes a model architecture comprising a fully convolutional encoder/decoder network and a residual vector quantizer (RVQ) to effectively compress speech. &gt; [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) employs a streaming encoder-decoder architecture with a quantized latent space, trained in an end-to-end fashion. &gt; [HiFi-Codec](../../Models/Speech_Neural_Codec/2023.05.04_HiFi-Codec.md) introduces a group-residual vector quantization (GRVQ) technique to reduce the number of quantizers. &gt; [Vocos](../../Models/TTS3_Vocoder/2023.03.01_Vocos.md) aims to bridge the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis. &gt; In order to narrow the gaps between text and acoustic codec tokens, [SpeechTokenizer](../../Models/Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md) and [FunCodec](../../Models/Speech_Neural_Codec/2023.09.14_FunCodec.md) introduces the concept of using semantic tokens in the first channel of discrete codecs. &gt; This transition helps mitigate the disparity between text and acoustic tokens. &gt; [Language-Codec](../../Models/Speech_Neural_Codec/2024.02.19_Language-Codec.md) employs the MCRVQ mechanism to evenly distribute information from the first three channels, thereby reducing the gap between text and acoustic tokens. &gt; [DAC](../../Models/Speech_Neural_Codec/2023.06.11_Descript-Audio-Codec.md) greatly improves the quality of codec reconstruction by introducing multi-scale discriminators loss. &gt; [FACodec](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md) pioneered the concept of a disentangled codec, decomposing speech into timbre, content, prosody, and acoustic details within its discrete space, and demonstrated a certain degree of voice conversion capability. &gt; In an endeavor to enhance the disentangled codec, we explored modifying the parallel vector quantization structure to a residual vector quantization form and incorporating additional stylistic supervisory signals. &gt; However, our efforts did not yield substantial improvements compared to [FACodec](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md). &gt; Therefore, we utilize the pre-trained [FACodec](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md) to extract the corresponding codec representations in ***ControlSpeech***.   <p>\u8fd1\u6bb5\u65f6\u95f4, \u795e\u7ecf\u58f0\u5b66\u7f16\u89e3\u7801\u5668 (SoundStream, EnCodec, HiFi-Codec, Vocos, SpeechTokenizer, FunCodec) \u5df2\u7ecf\u5728\u4ee5\u76f8\u5f53\u4f4e\u6bd4\u7279\u7387\u91cd\u6784\u9ad8\u8d28\u91cf\u97f3\u9891\u65b9\u9762\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u80fd\u529b. \u4e0b\u9762\u505a\u8fdb\u4e00\u6b65\u7684\u8bf4\u660e: - SoundStream \u5229\u7528\u4e00\u4e2a\u7531\u5168\u5377\u79ef\u7f16\u7801\u5668/\u89e3\u7801\u5668\u7f51\u7edc\u548c\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u5668 (RVQ) \u7ec4\u6210\u7684\u6a21\u578b\u67b6\u6784, \u4ee5\u6709\u6548\u5730\u538b\u7f29\u8bed\u97f3. - EnCodec \u91c7\u7528\u5177\u6709\u91cf\u5316\u9690\u53d8\u91cf\u7684\u6d41\u5f0f\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784, \u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u8bad\u7ec3. - HiFi-Codec \u5f15\u5165\u4e86\u7ec4\u6b8b\u5dee\u5411\u91cf\u91cf\u5316 (GRVQ) \u6280\u672f, \u4ee5\u51cf\u5c11\u91cf\u5316\u5668\u7684\u6570\u91cf. - Vocos \u65e8\u5728\u5c06\u65f6\u57df\u548c\u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u795e\u7ecf\u8bed\u97f3\u5408\u6210\u5668\u4e4b\u95f4\u7684\u5dee\u8ddd\u7f29\u5c0f\u5230\u8db3\u4ee5\u4ea7\u751f\u9ad8\u8d28\u91cf\u97f3\u9891. - SpeechTokenizer \u548c FunCodec \u5728\u79bb\u6563\u7f16\u89e3\u7801\u7684\u7b2c\u4e00\u4e2a\u901a\u9053\u4e2d\u4f7f\u7528\u8bed\u4e49 Token \u4ee5\u51cf\u5c11\u6587\u672c\u548c\u97f3\u9891 Token \u7684\u5dee\u8ddd. - \u8fd9\u79cd\u8f6c\u53d8\u6709\u52a9\u4e8e\u7f13\u89e3\u6587\u672c\u548c\u97f3\u9891 Token \u4e4b\u95f4\u7684\u5dee\u8ddd. - Language-Codec \u91c7\u7528 MCRVQ \u673a\u5236, \u5c06\u4fe1\u606f\u5747\u5300\u5206\u914d\u5230\u524d\u4e09\u4e2a\u901a\u9053, \u4ece\u800c\u51cf\u5c11\u6587\u672c\u548c\u58f0\u5b66 Token \u4e4b\u95f4\u7684\u5dee\u8ddd. - DAC \u901a\u8fc7\u5f15\u5165\u591a\u5c3a\u5ea6\u5224\u522b\u5668\u635f\u5931\u6765\u5927\u5e45\u63d0\u9ad8\u7f16\u89e3\u7801\u5668\u91cd\u5efa\u7684\u8d28\u91cf. - FACodec \u5f00\u521b\u4e86\u89e3\u8026\u7f16\u89e3\u7801\u5668\u7684\u6982\u5ff5, \u5c06\u8bed\u97f3\u5206\u89e3\u4e3a\u97f3\u8272, \u5185\u5bb9, \u8bed\u8c03, \u548c\u58f0\u5b66\u7ec6\u8282, \u5e76\u5c55\u793a\u4e86\u4e00\u5b9a\u7a0b\u5ea6\u7684\u8bed\u97f3\u8f6c\u6362\u80fd\u529b.</p> <p>\u4e3a\u4e86\u589e\u5f3a\u89e3\u8026\u7f16\u89e3\u7801\u5668, \u6211\u4eec\u63a2\u7d22\u4e86\u4fee\u6539\u5e76\u884c\u5411\u91cf\u91cf\u5316\u7ed3\u6784\u4e3a\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u5f62\u5f0f, \u5e76\u5c06\u989d\u5916\u7684\u98ce\u683c\u76d1\u7763\u4fe1\u53f7\u7eb3\u5165\u5176\u4e2d. \u7136\u800c, \u4e0e FACodec \u76f8\u6bd4\u6211\u4eec\u7684\u52aa\u529b\u6ca1\u6709\u4ea7\u751f\u660e\u663e\u7684\u6539\u5584. \u56e0\u6b64, \u6211\u4eec\u4f7f\u7528\u9884\u8bad\u7ec3\u7684 FACodec \u63d0\u53d6 ControlSpeech \u4e2d\u7684\u76f8\u5e94\u7f16\u89e3\u7801\u5668\u8868\u793a.  </p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#a2zero-shot-tts","title":"A.2.Zero-shot TTS","text":"\u539f\u6587  &gt; Zero-shot speech synthesis refers to the ability to synthesize the voice of an unseen speaker based solely on a few seconds of audio prompt, also known as voice cloning. &gt; In recent months, with the advancement of generative large-scale models, a plethora of outstanding works have emerged. &gt; [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) leverages discrete codec representations and combines autoregressive and non-autoregressive models in a cascaded manner, preserving the powerful contextual capabilities of language models. &gt; [NaturalSpeech 2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md) employs continuous vectors instead of discrete neural codec tokens and introduces in-context learning to a latent diffusion model. &gt; [SPEAR-TTS](../../Models/Speech_LLM/2023.02.07_SPEAR-TTS.md) and [Make-A-Voice](../../Models/_tmp/2023.05.30_Make-A-Voice.md) utilize semantic tokens to reduce the gap between text and acoustic features. &gt; [VoiceBox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md) is a nonautoregressive flow-matching model trained to infill speech, given audio context and text. &gt; [Mega-TTS](../../Models/_tmp/2023.06.06_Mega-TTS.md), [Mega-TTS2](../../Models/_tmp/2023.07.14_Mega-TTS2.md), on the other hand, utilizes traditional mel-spectrograms, decoupling timbre and prosody and further modeling the prosody using an autoregressive approach. &gt; [VoiceBox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md) and [P-Flow](../../Models/_tmp/P-Flow.md) employ flowing models as generators, demonstrating robust generative performance. &gt; [SoundStorm](../../Models/Speech_LLM/2023.05.16_SoundStorm.md) and [MobileSpeech](../../Models/_tmp/2024.02.14_MobileSpeech.md) utilize a non-autoregressive and mask-based iterative generation method, achieving an excellent balance between inference speed and generation quality. &gt; Most relevant to our work is [NaturalSpeech 3](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md), which also leverages the latest disentangled codec but primarily aims to enhance zero-shot model performance. &gt; In contrast, we argue that a disentangled codec is inherently more suitable as the speech representation for controllable generative models. &gt; It is noteworthy that current zero-shot TTS models are unable to achieve arbitrary language style control. &gt; ***ControlSpeech*** is the first TTS model capable of simultaneously performing zero-shot timbre cloning and style control.   <p>\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u6307\u7684\u662f\u4ec5\u57fa\u4e8e\u6570\u79d2\u7684\u97f3\u9891\u63d0\u793a\u5408\u6210\u672a\u89c1\u8fc7\u7684\u8bf4\u8bdd\u4eba\u58f0\u97f3, \u4e5f\u79f0\u4e3a\u58f0\u97f3\u514b\u9686. \u8fd1\u51e0\u4e2a\u6708, \u968f\u7740\u751f\u6210\u5f0f\u5927\u89c4\u6a21\u6a21\u578b\u7684\u8fdb\u6b65, \u8d8a\u6765\u8d8a\u591a\u7684\u4f18\u79c0\u5de5\u4f5c\u51fa\u73b0. - VALL-E \u5229\u7528\u79bb\u6563\u7f16\u89e3\u7801\u5668\u8868\u793a, \u4ee5\u7ea7\u8054\u7684\u65b9\u5f0f\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u975e\u81ea\u56de\u5f52\u6a21\u578b, \u4fdd\u7559\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u4e0a\u4e0b\u6587\u80fd\u529b. - NaturalSpeech 2 \u4ee5\u8fde\u7eed\u5411\u91cf\u800c\u4e0d\u662f\u79bb\u6563\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u6807\u8bb0, \u5e76\u5f15\u5165\u9690\u53d8\u91cf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5185\u5728\u5b66\u4e60. - SPEAR-TTS \u548c Make-A-Voice \u5229\u7528\u8bed\u4e49\u6807\u8bb0\u4ee5\u51cf\u5c11\u6587\u672c\u548c\u97f3\u9891\u7279\u5f81\u4e4b\u95f4\u7684\u5dee\u8ddd. - Mega-TTS, Mega-TTS2 \u91c7\u7528\u4f20\u7edf\u7684\u6885\u5c14\u9891\u8c31, \u5206\u79bb\u97f3\u8272\u548c\u8bed\u8c03, \u5e76\u4f7f\u7528\u81ea\u56de\u5f52\u65b9\u6cd5\u8fdb\u4e00\u6b65\u5efa\u6a21\u8bed\u8c03. - VoiceBox \u548c P-Flow \u91c7\u7528\u6d41\u6a21\u578b\u4f5c\u4e3a\u751f\u6210\u5668, \u5c55\u793a\u51fa\u5065\u58ee\u7684\u751f\u6210\u6027\u80fd. - SoundStorm \u548c MobileSpeech \u91c7\u7528\u975e\u81ea\u56de\u5f52\u548c\u57fa\u4e8e\u63a9\u819c\u8fed\u4ee3\u751f\u6210\u65b9\u6cd5, \u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u7684\u826f\u597d\u5e73\u8861. - \u4e0e\u6211\u4eec\u7684\u5de5\u4f5c\u6700\u76f8\u5173\u7684\u662f NaturalSpeech 3, \u5b83\u4e5f\u91c7\u7528\u6700\u65b0\u89e3\u8026\u7f16\u89e3\u7801\u5668, \u4f46\u4e3b\u8981\u76ee\u6807\u662f\u63d0\u9ad8\u96f6\u6837\u672c\u6a21\u578b\u6027\u80fd.</p> <p>\u4f46\u6211\u4eec\u8ba4\u4e3a\u89e3\u8026\u7f16\u89e3\u7801\u5668\u5728\u63a7\u5236\u751f\u6210\u6a21\u578b\u4e2d\u66f4\u4e3a\u5408\u9002. \u503c\u5f97\u6ce8\u610f\u7684\u662f, \u5f53\u524d\u7684\u96f6\u6837\u672c TTS \u6a21\u578b\u65e0\u6cd5\u5b9e\u73b0\u4efb\u610f\u8bed\u8a00\u98ce\u683c\u63a7\u5236. ControlSpeech \u662f\u7b2c\u4e00\u4e2a TTS \u6a21\u578b, \u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u96f6\u6837\u672c\u97f3\u8272\u514b\u9686\u548c\u98ce\u683c\u63a7\u5236.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#a3text-prompt-based-controllable-tts","title":"A.3.Text prompt based controllable TTS","text":"\u539f\u6587  &gt; Some recent studies propose to control speech style through natural language prompts, providing a more interpretable and user-friendly approach of control. &gt; [PromptTTS](../../Models/_tmp/2022.11.22_PromptTTS.md) employs manually annotated text prompts to describe four to five attributes of speech (gender, pitch, speaking speed, energy, and emotion) and trains model on two synthesized speaker datasets and LibriTTS. &gt; [InstructTTS](../../Models/_tmp/2023.01.31_InstructTTS.md) is a three-stage training approach to capture semantic information from natural language style prompts as conditioning to the TTS system. &gt; [Textrolspeech](../../Datasets/2023.08.28_TextrolSpeech.md) introduces an efficient architecture which treats text controllable TTS as a language model task. &gt; [PromptStyle](../../Models/E2E/2023.05.31_PromptStyle.md) proposes a two-stage TTS approach for cross-speaker style transfer with natural language descriptions based on [VITS](../../Models/E2E/2021.06.11_VITS.md). &gt; [PromptTTS 2](../../Models/_tmp/2023.09.05_PromptTTS2.md) proposes an automatic description creation pipeline leveraging LLM [4] and adopts a diffusion model to capture the one-to-many relationship. &gt; It is noteworthy that existing style-controllable TTS models are either speaker-independent or can only control timbre using speaker IDs, without the capability for timbre cloning. &gt; The introduction of ***ControlSpeech*** significantly expands the boundaries of the controllable TTS task.   <p>\u4e00\u4e9b\u8fd1\u671f\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u63a7\u5236\u8bed\u97f3\u98ce\u683c, \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u7528\u6237\u53cb\u597d\u7684\u63a7\u5236\u65b9\u5f0f. - PromptTTS \u4ee5\u624b\u5de5\u6ce8\u91ca\u7684\u6587\u672c\u63d0\u793a\u63cf\u8ff0\u8bed\u97f3\u7684\u56db\u5230\u4e94\u4e2a\u5c5e\u6027 (\u6027\u522b, \u97f3\u9ad8, \u8bf4\u8bdd\u901f\u5ea6, \u80fd\u91cf, \u4ee5\u53ca\u60c5\u7eea), \u5e76\u5728\u4e24\u4e2a\u5408\u6210\u8bf4\u8bdd\u4eba\u6570\u636e\u96c6\u548c LibriTTS \u4e0a\u8bad\u7ec3\u6a21\u578b. - InstructTTS \u662f\u4e00\u79cd\u4e09\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5, \u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u98ce\u683c\u63d0\u793a\u4e2d\u6355\u83b7\u8bed\u4e49\u4fe1\u606f, \u5e76\u5c06\u5176\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u5230 TTS \u7cfb\u7edf. - Textrolspeech \u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u67b6\u6784, \u5c06\u6587\u672c\u53ef\u63a7 TTS \u89c6\u4e3a\u8bed\u8a00\u6a21\u578b\u4efb\u52a1. - PromptStyle \u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e VITS \u7684\u8de8\u8bf4\u8bdd\u4eba\u98ce\u683c\u8f6c\u79fb\u7684\u4e24\u9636\u6bb5 TTS \u65b9\u6cd5, \u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0. - PromptTTS 2 \u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528 LLM [4] \u7684\u81ea\u52a8\u63cf\u8ff0\u521b\u5efa\u7ba1\u9053, \u5e76\u91c7\u7528\u6269\u6563\u6a21\u578b\u6765\u6355\u83b7\u4e00\u5bf9\u591a\u5173\u7cfb.</p> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\u73b0\u6709\u98ce\u683c\u53ef\u63a7\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u8981\u4e48\u548c\u8bf4\u8bdd\u4eba\u65e0\u5173, \u8981\u4e48\u53ea\u80fd\u901a\u8fc7\u8bf4\u8bdd\u4eba ID \u63a7\u5236\u97f3\u8272, \u6ca1\u6709\u58f0\u97f3\u514b\u9686\u80fd\u529b. ControlSpeech \u5f15\u5165\u4e86\u8bed\u97f3\u98ce\u683c\u53ef\u63a7\u7684 TTS \u6a21\u578b, \u6269\u5c55\u4e86\u63a7\u5236 TTS \u4efb\u52a1\u7684\u8fb9\u754c.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#3controlspeech","title":"3.ControlSpeech","text":"\u539f\u6587  &gt; In this section, we will first introduce the motivation and overall design of ***ControlSpeech***. &gt; Next, we will provide a detailed explanation of the disentanglement and generation process of the codec, followed by the ***Style Mixture Semantic Density (SMSD)*** module. &gt; Finally, we will discuss the training loss and inference process of ***ControlSpeech***.   <p>\u672c\u8282, \u6211\u4eec\u9996\u5148\u4ecb\u7ecd ControlSpeech \u7684\u52a8\u673a\u548c\u603b\u4f53\u8bbe\u8ba1. \u7136\u540e, \u6211\u4eec\u5c06\u8be6\u7ec6\u89e3\u91ca\u7f16\u89e3\u7801\u5668\u7684\u89e3\u8026\u548c\u751f\u6210\u8fc7\u7a0b, \u5e76\u4ecb\u7ecd Style Mixture Semantic Density (SMSD) \u6a21\u5757. \u6700\u540e, \u6211\u4eec\u5c06\u8ba8\u8bba ControlSpeech \u7684\u8bad\u7ec3\u635f\u5931\u548c\u63a8\u7406\u8fc7\u7a0b.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#31overall","title":"3.1.Overall","text":"\u539f\u6587  &gt; To achieve simultaneous zero-shot timbre cloning and style cloning, one viable approach is to leverage a large-scale pre-trained disentangled codec space. &gt; On one hand, extensive pre-training on a diverse speaker dataset ensures fundamental zero-shot capabilities. &gt; On the other hand, disentangled codec representations effectively reduce the entanglement between timbre and style. &gt; As illustrated in Fig.02, ***ControlSpeech*** is fundamentally an encoder-decoder model based on [FastSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md) for codec generation. &gt; The dashed box represents frame-level features, while the solid box represents global features. &gt; The ***Style Mixture Semantic Density (SMSD)*** module samples style text to generate corresponding global style representations, which are then combined with text representations from the text encoder via a cross-attention module. &gt; These combined representations are fed into the duration prediction model and subsequently into the codec generator, which is a non-autoregressive [Conformer](../../Models/ASR/2020.05.16_Conformer.md) based on mask iteration and parallel generation. &gt; The timbre extractor is a Transformer encoder that converts the output of the speech encoder into a global vector, representing the timbre attributes. &gt; By inputting a style description $X_s$, a content text $X_c$, and a speech prompt $X_t$, ***ControlSpeech*** aims to sequentially generate the corresponding style codec $Y_s$, content codec $Y_c$, and timbre embedding $Y_t$. &gt; These representations are then concatenated and upsampled into speech through the [pre-trained codec decoder](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md).   <p>\u4e3a\u4e86\u5b9e\u73b0\u540c\u6b65\u7684\u96f6\u6837\u672c\u97f3\u8272\u514b\u9686\u548c\u98ce\u683c\u514b\u9686, \u4e00\u4e2a\u53ef\u884c\u7684\u65b9\u6cd5\u662f\u5229\u7528\u4e00\u4e2a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89e3\u8026\u7f16\u89e3\u7801\u7a7a\u95f4. \u4e00\u65b9\u9762, \u5728\u591a\u6837\u8bf4\u8bdd\u4eba\u6570\u636e\u96c6\u4e0a\u5e7f\u6cdb\u5730\u9884\u8bad\u7ec3\u80fd\u591f\u786e\u4fdd\u57fa\u672c\u7684\u96f6\u6837\u672c\u80fd\u529b. \u53e6\u4e00\u65b9\u9762, \u89e3\u8026\u7f16\u89e3\u7801\u8868\u793a\u80fd\u6709\u6548\u5730\u51cf\u5c11\u97f3\u8272\u548c\u98ce\u683c\u7684\u7ea0\u7f20. \u5982\u56fe 02 \u6240\u793a, ControlSpeech \u662f\u57fa\u4e8e FastSpeech2 \u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u7528\u4e8e\u7f16\u89e3\u7801\u751f\u6210. \u865a\u7ebf\u6846\u8868\u793a\u5e27\u7ea7\u7279\u5f81, \u5b9e\u7ebf\u6846\u8868\u793a\u5168\u5c40\u7279\u5f81.</p> <p></p> <p>\u98ce\u683c\u6df7\u5408\u8bed\u4e49\u5bc6\u5ea6 (Style Mixture Semantic Density, SMSD) \u6a21\u5757\u91c7\u6837\u98ce\u683c\u6587\u672c\u4ee5\u751f\u6210\u5bf9\u5e94\u7684\u5168\u5c40\u98ce\u683c\u8868\u793a, \u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u6a21\u5757\u4e0e\u6587\u672c\u7f16\u7801\u5668\u7684\u8868\u793a\u76f8\u7ed3\u5408. \u8fd9\u4e9b\u7ed3\u5408\u7684\u8868\u793a\u88ab\u8f93\u5165\u5230\u65f6\u957f\u9884\u6d4b\u6a21\u578b\u7136\u540e\u4f20\u5165\u7f16\u89e3\u7801\u751f\u6210\u5668, \u8be5\u7f16\u89e3\u7801\u751f\u6210\u5668\u662f\u57fa\u4e8e\u63a9\u7801\u8fed\u4ee3\u548c\u5e76\u884c\u751f\u6210\u7684\u975e\u81ea\u56de\u5f52 Conformer. \u97f3\u8272\u63d0\u53d6\u5668\u5219\u662f\u4e00\u4e2a Transformer \u7f16\u7801\u5668, \u5b83\u5c06\u8bed\u97f3\u7f16\u7801\u5668\u7684\u8f93\u51fa\u8f6c\u6362\u4e3a\u5168\u5c40\u5411\u91cf, \u4ee3\u8868\u97f3\u8272\u5c5e\u6027. \u901a\u8fc7\u8f93\u5165\u98ce\u683c\u63cf\u8ff0 $X_s$, \u5185\u5bb9\u6587\u672c $X_c$, \u8bed\u97f3\u63d0\u793a $X_t$, ControlSpeech \u8bd5\u56fe\u4f9d\u6b21\u751f\u6210\u5bf9\u5e94\u7684\u98ce\u683c\u7f16\u89e3\u7801 $Y_s$, \u5185\u5bb9\u7f16\u89e3\u7801 $Y_c$, \u97f3\u8272\u5d4c\u5165 $Y_t$. \u8fd9\u4e9b\u8868\u793a\u4e4b\u540e\u62fc\u63a5\u540e\u901a\u8fc7\u9884\u8bad\u7ec3\u7f16\u89e3\u7801\u5668\u89e3\u7801\u5668 FACodec \u8fdb\u884c\u4e0a\u91c7\u6837, \u751f\u6210\u8bed\u97f3.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#32decoupling-and-generation-of-codec","title":"3.2.Decoupling and Generation of Codec","text":"\u539f\u6587  &gt; We utilize [FACodec](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md) as our codec disentangler. &gt; During the training of ***ControlSpeech***, we freeze the corresponding codec encoder to obtain downsampled compressed audio frames $h$ from the speech $Y$. &gt; The frames $h$ are processed through the disentangling quantizer module and the timbre extractor module to derive the original content codec $Y_c$, prosody codec $Y_p$, acoustic codec $Y_a$, and timbre information $Y_t$. &gt; We exclude the content $Y_c$ and timbre information $Y_t$ of the representation collectively referred to as style $Y_s$. &gt; In practice, we concatenate the prosody codec $Y_p$ and the acoustic codec $Y_a$ along the channel dimension to obtain the corresponding style codec $Y_s$, as represented below:   <p>$$   Y_s = h-Y_c-Y_t = \\text{concat}(Y_p,Y_a)\\tag{01} $$</p> <p>\u6211\u4eec\u4f7f\u7528 FACodec \u4f5c\u4e3a\u6211\u4eec\u7684\u7f16\u89e3\u7801\u89e3\u8026\u5668. \u5728 ControlSpeech \u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u51bb\u7ed3\u76f8\u5e94\u7684\u7f16\u89e3\u7801\u5668\u7f16\u7801\u5668\u4ee5\u4ece\u8bed\u97f3 $Y$ \u83b7\u5f97\u4e0b\u91c7\u6837\u538b\u7f29\u7684\u97f3\u9891\u5e27 $h$. \u5e27 $h$ \u901a\u8fc7\u89e3\u8026\u91cf\u5316\u5668\u6a21\u5757\u548c\u97f3\u8272\u63d0\u53d6\u5668\u6a21\u5757, \u5bfc\u51fa\u539f\u59cb\u5185\u5bb9\u7f16\u89e3\u7801 $Y_c$, \u8bed\u8c03\u7f16\u89e3\u7801 $Y_p$, \u58f0\u5b66\u7f16\u89e3\u7801 $Y_a$, \u4ee5\u53ca\u97f3\u8272\u4fe1\u606f $Y_t$. \u6211\u4eec\u6392\u9664\u5185\u5bb9 $Y_c$ \u548c\u97f3\u8272\u4fe1\u606f $Y_t$ \u540e\u5e76\u5c06\u5176\u7edf\u79f0\u4e3a\u98ce\u683c $Y_s$. \u5b9e\u9645\u4e0a, \u6211\u4eec\u6cbf\u7740\u901a\u9053\u7ef4\u5ea6\u5c06\u8bed\u8c03\u7f16\u89e3\u7801 $Y_p$ \u548c\u58f0\u5b66\u7f16\u89e3\u7801 $Y_a$ \u8fde\u63a5\u8d77\u6765, \u5f97\u5230\u76f8\u5e94\u7684\u98ce\u683c\u7f16\u89e3\u7801 $Y_s$, \u5982\u516c\u5f0f 01 \u6240\u793a.</p> \u539f\u6587  &gt; The codec generation process comprises two stages. &gt; In the first stage, based on paired text-speech data $\\{X, Y_{codec}\\}$, where $X = \\{x_1, x_2, x_3, \\cdots, x_T \\}$ represents the fusion of global style and aligned text and $Y_{codec}$ denotes the representation of speech through vector quantization, formula as:  $$   Y_{codec} = Y_s+Y_c = C_{1:T,1:N}\\in \\mathbb{R}^{T\\times N}\\tag{02} $$  &gt; we consider $T$ as the downsampled utterance length, which is equal to the length of the text. &gt; $N$ represents the number of channels for every frame. &gt; The row vector of each acoustic code matrix $C_{t,:}$ represents the six codes for frame $t$, and the column vector of each acoustic code matrix $C_{:,i}$ represents the code sequence from the $j$-th codebook, where $i \\in \\{1, 2, \\cdots , N \\}$.   <p></p> <p>\u7f16\u89e3\u7801\u751f\u6210\u8fc7\u7a0b\u7531\u4e24\u4e2a\u9636\u6bb5\u7ec4\u6210. \u5728\u7b2c\u4e00\u4e2a\u9636\u6bb5, \u57fa\u4e8e\u6210\u5bf9\u7684\u6587\u672c-\u8bed\u97f3\u6570\u636e ${X, Y_{codec}}$, \u5176\u4e2d $X = {x_1, x_2, x_3, \\cdots, x_T }$ \u8868\u793a\u5168\u5c40\u98ce\u683c\u548c\u5bf9\u9f50\u6587\u672c\u7684\u878d\u5408, $Y_{codec}$ \u4ee3\u8868\u901a\u8fc7\u5411\u91cf\u91cf\u5316\u8868\u793a\u7684\u8bed\u97f3\u8868\u793a.</p> <p>$$   Y_{codec} = Y_s+Y_c = C_{1:T,1:N}\\in \\mathbb{R}^{T\\times N}\\tag{02} $$</p> <p>\u6211\u4eec\u8ba4\u4e3a $T$ \u4e3a\u4e0b\u91c7\u6837\u7684\u53d1\u8a00\u957f\u5ea6, \u7b49\u4e8e\u6587\u672c\u7684\u957f\u5ea6. $N$ \u8868\u793a\u6bcf\u5e27\u7684\u901a\u9053\u6570. \u6bcf\u4e00\u884c\u5411\u91cf $C_{t,:}$ \u8868\u793a\u7b2c $t$ \u5e27\u7684\u516d\u4e2a\u7f16\u7801, \u800c\u6bcf\u4e00\u5217\u5411\u91cf $C_{:,i}$ \u8868\u793a\u7b2c $j$ \u4e2a\u7801\u672c\u7684\u7801\u5e8f\u5217, \u5176\u4e2d $i \\in {1, 2, \\cdots , N }$.</p> \u539f\u6587  &gt; Follow [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md), in the training process of ***ControlSpeech***, we randomly select a certain channel of $C_{1:T,1:N}$ for training. &gt; For the generation of the $i$ channel $P(C_{1:T,i} | X_{1:T}; \\theta)$, we employ a mask-based generative model as our parallel decoder. &gt; We sample the mask $M_i \\in \\{0, 1\\}^T$ according to a cosine schedule [5] for codec level $i$, specifically sampling the masking ratio $p = \\cos(u')$ where ($u'\\sim U[0,\\dfrac{\\pi}{2}]$) and the mask $M_i \\sim Bernoulli(p)$. &gt; Here, $M_i$ represents the portion to be masked in the $i$-th level, while $M'_i$ denotes the unmasked portion in the $i$-th level. &gt; As shown in Fig.02 (c), the prediction of this portion is refined based on the prompt $j$ ($j &lt; i$) channels and the concatenation of target text and the unmasked portion of the $i$ channel. &gt; Therefore, the prediction for this part can be specified as follows:  $$   P(C_{1:T,i} | X_{1:T}; \\theta) = P(M_i C_{1:T,i}|C_{1:T,   \u9075\u5faa [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md), \u5728 ***ControlSpeech*** \u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u6211\u4eec\u968f\u673a\u9009\u62e9 $C_{1:T,1:N}$ \u4e2d\u7684\u67d0\u4e00\u901a\u9053\u7528\u4e8e\u8bad\u7ec3. \u5bf9\u4e8e\u7b2c $i$ \u4e2a\u901a\u9053\u7684\u751f\u6210 $P(C_{1:T,i} | X_{1:T}; \\theta)$, \u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u63a9\u7801\u7684\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u5e76\u884c\u89e3\u7801\u5668. \u6211\u4eec\u6839\u636e\u4f59\u5f26\u8c03\u5ea6 [5] \u91c7\u6837\u63a9\u7801 $M_i \\in \\{0, 1\\}^T$, \u5177\u4f53\u5730, \u6211\u4eec\u91c7\u6837\u63a9\u7801\u6bd4\u4f8b $p = \\cos(u')$ \u5176\u4e2d ($u'\\sim U[0,\\dfrac{\\pi}{2}]$) \u4ee5\u53ca\u63a9\u7801 $M_i \\sim Bernoulli(p)$. \u8fd9\u91cc, $M_i$ \u8868\u793a\u7b2c $i$ \u7ea7\u7684\u63a9\u7801\u90e8\u5206, \u800c $M'_i$ \u8868\u793a\u7b2c $i$ \u7ea7\u7684\u672a\u63a9\u7801\u90e8\u5206. \u5982\u56fe 02 C \u6240\u793a, \u8fd9\u90e8\u5206\u7684\u9884\u6d4b\u57fa\u4e8e\u63d0\u793a $j$ ($j &lt; i$) \u901a\u9053\u548c\u76ee\u6807\u6587\u672c\u548c\u672a\u63a9\u7801\u90e8\u5206\u7684\u62fc\u63a5. \u56e0\u6b64, \u5bf9\u4e8e\u8fd9\u4e00\u90e8\u5206\u7684\u9884\u6d4b, \u53ef\u4ee5\u5982\u4e0b\u5b9a\u4e49:   $$   P(C_{1:T,i} | X_{1:T}; \\theta) = P(M_i C_{1:T,i}|C_{1:T, \u539f\u6587  &gt; In the second stage, following [AdaSpeech](../TTS2_Acoustic/2021.03.01_AdaSpeech.md), we utilize a conditional normalization layer to fuse the previously obtained $Y_{codec} and $Y_t$, producing the input for the codec decoder. &gt; This input is then processed by the pre-trained decoder to generate the final speech output $Y$. &gt; Specifically, we first use two simple linear layers $W_{\\gamma}$ and $W_{\\beta}$, which take the speaker embedding $Y_s$ as input and output the scale vectors $\\gamma$ and bias vectors $\\beta$ respectively. &gt; These lightweight, learnable scale vectors $\\gamma$ and bias vectors $\\beta$ are then fused with $Y_{codec}$. &gt; This process can be described by the following formula:  $$   Y = \\text{CodecDecoder}(W_{\\gamma}Y_t\\dfrac{Y_{codec}-\\mu_c}{\\sigma_c^2} +W_{\\beta} Y_t)\\tag{04} $$  &gt; where $\\mu_c$ and $\\sigma_c^2$ are the mean and variance of the hidden representation of $Y_{codec}$.   <p></p> <p>\u7b2c\u4e8c\u4e2a\u9636\u6bb5, \u9075\u5faa AdaSpeech, \u6211\u4eec\u5229\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u5c42\u878d\u5408\u4e4b\u524d\u83b7\u5f97\u7684 $Y_{codec}$ \u548c $Y_t$, \u4ea7\u751f\u7f16\u89e3\u7801\u5668\u89e3\u7801\u5668\u7684\u8f93\u5165. \u8f93\u5165\u88ab\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u5904\u7406\u540e\u751f\u6210\u6700\u7ec8\u7684\u8bed\u97f3\u8f93\u51fa $Y$. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u9996\u5148\u4f7f\u7528\u4e24\u4e2a\u7b80\u5355\u7ebf\u6027\u5c42 $W_{\\gamma}$ \u548c $W_{\\beta}$, \u5b83\u4eec\u63a5\u6536\u8bf4\u8bdd\u8005\u5d4c\u5165 $Y_s$ \u4f5c\u4e3a\u8f93\u5165, \u8f93\u51fa\u5c3a\u5ea6\u5411\u91cf $\\gamma$ \u548c\u504f\u5dee\u5411\u91cf $\\beta$ \u5404\u81ea. \u8fd9\u4e9b\u8f7b\u91cf\u5316\u7684, \u53ef\u5b66\u4e60\u7684\u5c3a\u5ea6\u5411\u91cf $\\gamma$ \u548c\u504f\u5dee\u5411\u91cf $\\beta$ \u968f\u540e\u4e0e $Y_{codec}$ \u878d\u5408. \u8fd9\u4e00\u8fc7\u7a0b\u53ef\u4ee5\u7528\u4e0b\u9762\u7684\u516c\u5f0f\u8868\u793a:</p> <p>$$   Y = \\text{CodecDecoder}(W_{\\gamma}Y_t\\dfrac{Y_{codec}-\\mu_c}{\\sigma_c^2} +W_{\\beta} Y_t)\\tag{04} $$</p> <p>\u5176\u4e2d $\\mu_c$ \u548c $\\sigma_c^2$ \u662f $Y_{codec}$ \u7684\u9690\u85cf\u8868\u793a\u7684\u5747\u503c\u548c\u65b9\u5dee.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#33style-mixture-semantic-density-modules","title":"3.3.Style Mixture Semantic Density Modules","text":"\u539f\u6587  &gt; During our experiments, we identified a many-to-many problem in the text style control module. &gt; Specifically, different style texts can describe the same style of speech. &gt; Similar to previous approaches ([PromptTTS](../../Models/_tmp/2022.11.22_PromptTTS.md), [PromptStyle](../../Models/E2E/2023.05.31_PromptStyle.md)), we utilize a pre-trained BERT model to extract the semantic information of style descriptions, thereby enhancing the generalization of out-of-domain style descriptions. &gt; Moreover, it has been largely overlooked that a single style text can correspond to multiple speech instances from the same speaker. &gt; For example, when using a text describing a happy tone to control audio, various degrees of happiness can be attributed to that description. &gt; [PromptTTS 2](../../Models/_tmp/2023.09.05_PromptTTS2.md) recognized this issue and introduced a speech prompt to compensate for the details of the text description style, alleviating the one-to-many problem by incorporating additional information. &gt; However, in the task of simultaneously zero-shot cloning timbre and zero-shot controlling style in speech synthesis, this approach can cause interference between the speech prompt and the style prompt. &gt; Therefore, in ***ControlSpeech***, we designed a novel ***Style Mixture Semantic Density (SMSD)*** module to address the one-to-many problem of style representation. &gt; We hypothesize that the semantic representation of style $X'_s$ is a global mixture of Gaussian distributions, where different Gaussian distributions represent varying degrees of a particular style. &gt; During inference, we sample from the mixture of style semantic distributions to obtain an independent Gaussian distribution, with each sampled distribution reflecting different degrees of the same style. &gt; Additionally, to further enhance the diversity of style control, we incorporated a noise perturbation module within the MDN network of the ***SMSD*** in the ***ControlSpeech*** model.  &gt; **This module controls the isotropy of perturbations across different dimensions**.   <p>\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u53d1\u73b0\u6587\u672c\u98ce\u683c\u63a7\u5236\u6a21\u5757\u4e2d\u5b58\u5728\u591a\u5bf9\u591a\u95ee\u9898. \u5177\u4f53\u5730, \u4e0d\u540c\u98ce\u683c\u6587\u672c\u53ef\u4ee5\u63cf\u8ff0\u76f8\u540c\u98ce\u683c\u7684\u8bed\u97f3. \u7c7b\u4f3c\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5 (PromptTTS, PromptStyle), \u6211\u4eec\u4f7f\u7528\u9884\u8bad\u7ec3\u7684 BERT \u6a21\u578b\u7528\u4e8e\u63d0\u53d6\u98ce\u683c\u63cf\u8ff0\u7684\u8bed\u97f3\u4fe1\u606f, \u4ece\u800c\u589e\u5f3a\u9886\u57df\u5916\u98ce\u683c\u63cf\u8ff0\u7684\u6cdb\u5316. \u6b64\u5916, \u5355\u4e2a\u98ce\u683c\u6587\u672c\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2a\u6765\u81ea\u540c\u4e00\u8bf4\u8bdd\u4eba\u7684\u8bed\u97f3\u5b9e\u4f8b\u7684\u60c5\u51b5\u88ab\u6781\u5927\u5ffd\u89c6. \u4f8b\u5982, \u5f53\u4f7f\u7528\u6587\u672c\u63cf\u8ff0\u5f00\u5fc3\u7684\u8bed\u6c14\u6765\u63a7\u5236\u97f3\u9891\u65f6, \u4e0d\u540c\u7684\u5f00\u5fc3\u7a0b\u5ea6\u53ef\u4ee5\u5f52\u56e0\u4e8e\u8be5\u63cf\u8ff0. PromptTTS 2 \u53d1\u73b0\u4e86\u8fd9\u4e2a\u95ee\u9898, \u5e76\u5f15\u5165\u4e86\u8bed\u97f3\u63d0\u793a\u6765\u8865\u507f\u6587\u672c\u63cf\u8ff0\u98ce\u683c\u7684\u7ec6\u8282, \u901a\u8fc7\u5f15\u5165\u989d\u5916\u4fe1\u606f\u6765\u7f13\u89e3\u4e00\u5bf9\u591a\u95ee\u9898. \u7136\u800c, \u5728\u96f6\u6837\u672c\u514b\u9686\u97f3\u8272\u548c\u96f6\u6837\u672c\u63a7\u5236\u98ce\u683c\u7684\u8bed\u97f3\u5408\u6210\u4efb\u52a1\u4e2d, \u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8bed\u97f3\u63d0\u793a\u548c\u98ce\u683c\u63d0\u793a\u4e4b\u95f4\u7684\u5e72\u6270. \u56e0\u6b64, \u5728 ControlSpeech \u4e2d, \u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684 Style Mixture Semantic Density (SMSD) \u6a21\u5757\u6765\u89e3\u51b3\u98ce\u683c\u8868\u793a\u4e2d\u7684\u4e00\u5bf9\u591a\u95ee\u9898. \u6211\u4eec\u5047\u8bbe\u98ce\u683c\u8bed\u4e49\u8868\u793a $X'_s$ \u662f\u9ad8\u65af\u5206\u5e03\u7684\u5168\u5c40\u6df7\u5408, \u5176\u4e2d\u4e0d\u540c\u7684\u9ad8\u65af\u5206\u5e03\u4ee3\u8868\u4e0d\u540c\u7a0b\u5ea6\u7684\u7279\u5b9a\u98ce\u683c. \u5728\u63a8\u7406\u65f6, \u6211\u4eec\u4ece\u98ce\u683c\u8bed\u4e49\u5206\u5e03\u7684\u6df7\u5408\u91c7\u6837\u4ee5\u83b7\u5f97\u72ec\u7acb\u9ad8\u65af\u5206\u5e03, \u6bcf\u4e2a\u91c7\u6837\u7684\u5206\u5e03\u53cd\u6620\u51fa\u76f8\u540c\u98ce\u683c\u7684\u4e0d\u540c\u7a0b\u5ea6. \u6b64\u5916, \u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u98ce\u683c\u63a7\u5236\u7684\u591a\u6837\u6027, \u6211\u4eec\u5728 ControlSpeech \u6a21\u578b\u7684 SMSD \u7f51\u7edc\u4e2d\u5f15\u5165\u4e86\u566a\u58f0\u6270\u52a8\u6a21\u5757. \u8be5\u6a21\u5757\u63a7\u5236\u4e0d\u540c\u7ef4\u5ea6\u7684\u6270\u52a8\u7684\u540c\u8d28\u6027.</p> \u539f\u6587  &gt; Specifically, the raw style prompt sequence $X_s = [X_1, X_2, X_3, \\cdots, X_L]$ is prepended with a `[CLS]` token, converted into a word embedding, and fed into the BERT model, where $L$ refers to the length of style prompt. &gt; The hidden vector corresponding to the `[CLS]` token is regarded as the global style semantic representation $X_s$ to guide the generation and sampling of subsequent models.  &gt; Based on MDN network [53, 18, 10, 12], suppose we want to regress response target style representation $Y'_s\\in\\mathbb{R}^d$ by using covariates style semantic input representation $X'_s\\in\\mathbb{R}^{n}$.  &gt; We model the conditional distribution as a mixture of Gaussian distribution, The formula is as follows:  $$   P_{\\theta}(Y'_s|X'_s) = \\sum_{k=1}^K \\pi_k\\mathcal{N}(Y'_s|\\mu^{(k)}, \\sigma^{2(k)})\\tag{05} $$  &gt; where $K$ is a hyperparameter about the number of independent Gaussian distribution and other mixture distribution parameters $\\pi_k$, $\\mu^{(k)}$, $\\sigma^{2(k)}$ are output by a neural MDN network $f_{\\theta}$ dependent on input style semantic representation $X'_s$, formula as:  $$   \\pi\\in \\Delta^{K-1},\\quad \\mu^{(k)}\\in\\mathbb{R}^d,\\quad \\sigma^{2(k)}\\in S_{+}^{d}=f_{\\theta}(X'_s)\\tag{06} $$  &gt; It should be noted that the sum of the mixture weights is constrained to 1 during the training phase, which can be achieved by applying a softmax function on the corresponding neural network output $a_k$, formalized as:  $$   \\pi_{k} = \\dfrac{\\exp(a_k)}{\\sum_{k=1}^{K} \\exp(a_k)}\\tag{07} $$   <p></p> <p>\u5177\u4f53\u5730, \u539f\u59cb\u98ce\u683c\u63d0\u793a\u5e8f\u5217 $X_s = [X_1, X_2, X_3, \\cdots, X_L]$ \u7528 <code>[CLS]</code> \u6807\u8bc6\u7b26\u52a0\u5728\u524d\u9762, \u7136\u540e\u8f6c\u6362\u4e3a\u8bcd\u5d4c\u5165, \u5e76\u8f93\u5165 BERT \u6a21\u578b, \u5176\u4e2d $L$ \u8868\u793a\u98ce\u683c\u63d0\u793a\u7684\u957f\u5ea6. \u5bf9\u5e94 <code>[CLS]</code> \u6807\u8bc6\u7b26\u7684\u9690\u85cf\u5411\u91cf\u88ab\u89c6\u4e3a\u5168\u5c40\u98ce\u683c\u8bed\u4e49\u8868\u793a $X_s$ \u6765\u5f15\u5bfc\u540e\u7eed\u751f\u6210\u548c\u91c7\u6837\u6a21\u578b.</p> <p>\u57fa\u4e8e MDN \u7f51\u7edc, \u5047\u8bbe\u6211\u4eec\u60f3\u901a\u8fc7\u534f\u53d8\u91cf\u98ce\u683c\u8bed\u4e49\u8f93\u5165\u8868\u793a $X'_s\\in\\mathbb{R}^{n}$ \u6765\u56de\u5f52\u54cd\u5e94\u76ee\u6807\u98ce\u683c\u8868\u793a $Y'_s\\in\\mathbb{R}^d$. \u6211\u4eec\u5c06\u6761\u4ef6\u5206\u5e03\u5efa\u6a21\u4e3a\u9ad8\u65af\u6df7\u5408\u5206\u5e03, \u516c\u5f0f\u5982\u4e0b:</p> <p>$$   P_{\\theta}(Y's|X'_s) = \\sum{k=1}^K \\pi_k\\mathcal{N}(Y'_s|\\mu^{(k)}, \\sigma^{2(k)})\\tag{05} $$</p> <p>\u5176\u4e2d $K$ \u662f\u5173\u4e8e\u72ec\u7acb\u9ad8\u65af\u5206\u5e03\u6570\u91cf\u7684\u8d85\u53c2\u6570, \u5176\u4ed6\u6df7\u5408\u5206\u5e03\u53c2\u6570 $\\pi_k$, $\\mu^{(k)}$, $\\sigma^{2(k)}$ \u7531 MDN \u7f51\u7edc $f_\\theta$ \u8f93\u51fa, \u4f9d\u8d56\u4e8e\u8f93\u5165\u98ce\u683c\u8bed\u4e49\u8868\u793a $X'_s$, \u516c\u5f0f\u5982\u4e0b:</p> <p>$$   \\pi\\in \\Delta^{K-1},\\quad \\mu^{(k)}\\in\\mathbb{R}^d,\\quad \\sigma^{2(k)}\\in S_{+}^{d}=f_{\\theta}(X'_s)\\tag{06} $$</p> <p>\u5e94\u8be5\u6ce8\u610f\u5230\u8bad\u7ec3\u9636\u6bb5\u6df7\u5408\u6743\u91cd\u7684\u548c\u4e3a 1, \u8fd9\u53ef\u4ee5\u5bf9\u76f8\u5e94\u7f51\u7edc\u8f93\u51fa $a_k$ \u5e94\u7528 Softmax \u51fd\u6570, \u5373</p> <p>$$   \\pi_{k} = \\dfrac{\\exp(a_k)}{\\sum_{k=1}^{K} \\exp(a_k)}\\tag{07} $$</p> \u539f\u6587  &gt; To further enhance the diversity of style control, we designed a specialized noise perturbation module within the ***SMSD*** module to constrain the noise model. &gt; As illustrated by the circles of ***SMSD*** module in Fig.02 (b), this noise perturbation module regulates the isotropy of perturbations $\\varepsilon$ across different dimensions in variance $\\sigma^{2(k)}$. &gt; The four types of perturbations from left to right in Fig.02 (b) are as follows: &gt; &gt; - Fully factored, let $\\sigma^{2(k)}=f_{\\theta}(X'_s)+f_{\\theta}(\\varepsilon)=\\text{diag}(\\sigma^{2(k)})\\in \\mathbb{R}_{+}^{d}$ where the noise level for each dimension is predicted separately. &gt; - Isotropic, let $\\sigma^{2(k)}=f_{\\theta}(X'_s)+f_{\\theta}(\\varepsilon)=\\sigma^{2(k)}I\\in \\mathbb{R}_{+}$ which assumes the same noise level for each dimension over $d$. &gt; - Isotropic across clusters, let $\\sigma^{2(k)}=f_{\\theta}(X'_s)+f_{\\theta}(\\varepsilon)=\\sigma^{2}I\\in \\mathbb{R}_{+}$ which assumes the same noise level for each dimension over $d$ and cluster. &gt; - Fixed isotropic, same as above but do not learn $\\sigma^2$.   <p></p> <p>\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u98ce\u683c\u63a7\u5236\u7684\u591a\u6837\u6027, \u6211\u4eec\u5728 SMSD \u6a21\u5757\u5185\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u566a\u58f0\u6270\u52a8\u6a21\u5757, \u4ee5\u7ea6\u675f\u566a\u58f0\u6a21\u578b. \u5982\u56fe 02 (b) \u4e2d SMSD \u6a21\u5757\u7684\u5706\u5708\u6240\u793a, \u8fd9\u4e2a\u566a\u58f0\u6270\u52a8\u6a21\u5757\u8c03\u8282\u4e86\u6270\u52a8 $\\varepsilon$ \u5728\u4e0d\u540c\u7ef4\u5ea6\u4e0a\u7684\u5404\u5411\u540c\u6027, \u8fd9\u4e9b\u6270\u52a8\u4f4d\u4e8e\u65b9\u5dee $\\sigma^{2(k)}$ \u4e2d.  \u56fe 02 (b) \u4e2d\u4ece\u5de6\u81f3\u53f3\u7684\u56db\u79cd\u6270\u52a8\u7c7b\u578b\u5982\u4e0b: - \u5b8c\u5168\u5206\u89e3, \u4ee4 $\\sigma^{2(k)}=f_{\\theta}(X's)+f{\\theta}(\\varepsilon)=\\text{diag}(\\sigma^{2(k)})\\in \\mathbb{R}{+}^{d}$ \u5176\u4e2d, \u6bcf\u4e2a\u7ef4\u5ea6\u7684\u566a\u58f0\u6c34\u5e73\u662f\u5206\u522b\u9884\u6d4b\u7684. - \u5404\u5411\u540c\u6027, \u4ee4 $\\sigma^{2(k)}=f{\\theta}(X's)+f{\\theta}(\\varepsilon)=\\sigma^{2(k)}I\\in \\mathbb{R}{+}$ \u5176\u4e2d, \u6bcf\u4e2a\u7ef4\u5ea6\u7684\u566a\u58f0\u6c34\u5e73\u662f\u76f8\u540c\u7684. - \u5404\u5411\u540c\u6027\u7684\u7c07, \u4ee4 $\\sigma^{2(k)}=f{\\theta}(X's)+f{\\theta}(\\varepsilon)=\\sigma^{2}I\\in \\mathbb{R}_{+}$ \u5176\u4e2d, \u6bcf\u4e2a\u7ef4\u5ea6\u7684\u566a\u58f0\u6c34\u5e73\u662f\u76f8\u540c\u7684, \u4f46\u5728\u7c07\u4e2d. - \u56fa\u5b9a\u5404\u5411\u540c\u6027, \u4e0e\u4e0a\u8ff0\u76f8\u540c, \u4f46\u4e0d\u5b66\u4e60 $\\sigma^2$.</p> \u539f\u6587  &gt; Through the experimental analysis in the appendix G, isotropic across clusters is used as the mode of noise perturbation. &gt; Following the noise perturbation model, we obtain more robust mean, variance, and weight parameters for the mixture of Gaussian distributions. &gt; The criterion for training the ***SMSD*** module is the negative log-likelihood of the observation $Y_s$ given its input $X_s$. &gt; The loss function can be formulated as follows, the objective is clearly non-convex. &gt; Details about $Loss_{SMSD}$ are derived in the appendix B.   <p></p> <p>\u901a\u8fc7\u9644\u5f55 G \u7684\u5b9e\u9a8c\u5206\u6790, \u6211\u4eec\u91c7\u7528\u5404\u5411\u540c\u6027\u7684\u7c07\u4f5c\u4e3a\u566a\u58f0\u6270\u52a8\u6a21\u578b. \u57fa\u4e8e\u566a\u58f0\u6270\u52a8\u6a21\u578b, \u6211\u4eec\u83b7\u5f97\u4e86\u66f4\u52a0\u9c81\u68d2\u7684\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u7684\u5747\u503c, \u65b9\u5dee, \u6743\u91cd\u53c2\u6570. \u8bad\u7ec3 SMSD \u6a21\u5757\u7684\u6807\u51c6\u662f\u5bf9\u89c2\u6d4b\u503c $Y_s$ \u7ed9\u5b9a\u5176\u8f93\u5165 $X_s$ \u7684\u8d1f\u5bf9\u6570\u4f3c\u7136. \u635f\u5931\u51fd\u6570\u5b9a\u4e49\u5982\u4e0b, \u76ee\u6807\u51fd\u6570\u662f\u975e\u51f8\u7684. $Loss_{SMSD}$ \u7684\u7ec6\u8282\u5728\u9644\u5f55 B \u4e2d\u7ed9\u51fa.</p> <p>$$ \\begin{aligned}   \\mathcal{L}{SMSD   }&amp; =-\\log P{\\theta}(Y'{s}|X'{s})  \\   &amp;\\propto-\\sum_{k=1}^K(\\pi_k \\exp(-\\dfrac{1}{2}(Y'_s-\\mu^{(k)})^T (\\sigma^{2(k)})^{-1} (Y'_s-\\mu^{(k)})-\\frac{1}{2} \\text{logdet}\\sigma^{2(k)})) \\   &amp;=-\\text{logsumexp}_k(log\\pi_k-\\frac{1}{2}(Y_s^{'}-\\mu^{(k)})^T  (\\sigma^{2(k)})^{-1} (Y'_s-\\mu^{(k)})-\\frac{1}{2}\\text{logdet}\\sigma^{2(k)}) \\   &amp;=-\\text{logsumexp}_k(\\log\\pi_k-\\frac{1}{2}\\left|\\frac{Y'_s-\\mu^{(k)}}\\sigma\\right|^2) \\end{aligned}\\tag{08} $$</p> <p>\u6ce8: \u7531\u9644\u5f55 B \u79fb\u52a8\u81f3\u6b64.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#todo","title":"TODO","text":""},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#34training-and-inference","title":"3.4.Training and Inference","text":"\u539f\u6587  &gt; During the training process, the Duration Predictor is optimized using the mean square error loss, with the extracted duration serving as the training target. &gt; We employ the Montreal Forced Alignment (MFA) tool [39] to extract phoneme durations, and we denote the loss for the Duration Predictor as $Loss_{dur}$. &gt; The codec generator module is optimized using the cross-entropy loss function. &gt; We randomly select a channel for optimization and denote this loss as $Loss_{codec}$. &gt; In the ***SMSD*** module, the target style is the global style representation obtained by passing style codec $Y_{s}$ through the representation $Y_{s}$ style extractor. &gt; During training, we feed the ground truth style representation $Y_s$ and ground truth duration into the codec generator and duration predictor, respectively. &gt; Therefore, the overall loss $Loss$ for ***ControlSpeech*** can be expressed as follows:  $$   Loss = Loss_{codec} + Loss_{dur} + Loss_{SMSD}\\tag{09} $$  &gt; During the inference stage, we initiate the process by inputting the original stylistic descriptor $X_s$ into the BERT module to obtain the stylized semantic representation $X'_s$ into the ***SMSD*** subsequent module to obtain the corresponding $\\pi$, $\\mu$ and $\\sigma^2$. &gt; By directly sampling $X_s$, we can derive the predicted style distribution. &gt; Subsequently, we iteratively generate discrete acoustic tokens by incorporating the predicted style into the text state and employing the confidence based sampling scheme proposed by [5, 2]. &gt; Specifically, we perform multiple forward passes, and at each iteration $j$, we sample candidates for the masked positions. &gt; We then retain $P_j$ candidates based on their confidence scores, where $P_j$ follows a cosine schedule. &gt; Finally, by integrating the timbre prompt through the condition normalization layer and feeding it into the codec decoder, we generate the final speech output.   <p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u65f6\u957f\u9884\u6d4b\u5668\u901a\u8fc7\u5747\u65b9\u8bef\u5dee\u635f\u5931\u8fdb\u884c\u4f18\u5316, \u5176\u4e2d\u63d0\u53d6\u7684\u65f6\u957f\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807.  \u6211\u4eec\u91c7\u7528\u8499\u7279\u5229\u5c14\u5f3a\u5236\u5bf9\u9f50 (MFA) \u5de5\u5177 [39] \u6765\u63d0\u53d6\u97f3\u7d20\u65f6\u957f, \u5e76\u5c06\u65f6\u957f\u9884\u6d4b\u5668\u7684\u635f\u5931\u8868\u793a\u4e3a$Loss_{dur}$.  \u7f16\u89e3\u7801\u5668\u751f\u6210\u6a21\u5757\u5219\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4f18\u5316.  \u6211\u4eec\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u901a\u9053\u8fdb\u884c\u4f18\u5316, \u5e76\u5c06\u6b64\u635f\u5931\u8868\u793a\u4e3a $Loss_{codec}$.  \u5728 SMSD \u6a21\u5757\u4e2d, \u76ee\u6807\u98ce\u683c\u662f\u901a\u8fc7\u5c06\u98ce\u683c\u7f16\u89e3\u7801\u5668 $Y_{s}$ \u4f20\u9012\u7ed9\u8868\u793a $Y_{s}$ \u98ce\u683c\u63d0\u53d6\u5668\u83b7\u5f97\u7684\u5168\u7403\u98ce\u683c\u8868\u793a.  \u5728\u8bad\u7ec3\u671f\u95f4, \u6211\u4eec\u5c06\u771f\u5b9e\u98ce\u683c\u8868\u793a$Y_s$\u548c\u771f\u5b9e\u65f6\u957f\u5206\u522b\u8f93\u5165\u5230\u7f16\u89e3\u7801\u5668\u751f\u6210\u5668\u548c\u65f6\u957f\u9884\u6d4b\u5668\u4e2d.  \u56e0\u6b64, ControlSpeech \u7684\u6574\u4f53\u635f\u5931$Loss$\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> <p>$$   Loss = Loss_{codec} + Loss_{dur} + Loss_{SMSD}\\tag{09} $$</p> <p>\u5728\u63a8\u7406\u9636\u6bb5, \u6211\u4eec\u901a\u8fc7\u5c06\u539f\u59cb\u98ce\u683c\u63cf\u8ff0\u7b26$X_s$\u8f93\u5165\u5230BERT\u6a21\u5757\u4e2d, \u4ee5\u83b7\u5f97\u98ce\u683c\u5316\u7684\u8bed\u4e49\u8868\u793a $X'_s$, \u5e76\u5c06\u5176\u8f93\u5165\u5230SMSD\u540e\u7eed\u6a21\u5757\u4e2d, \u4ee5\u83b7\u5f97\u76f8\u5e94\u7684 $\\pi$, $\\mu$\u548c$\\sigma^2$.  \u901a\u8fc7\u76f4\u63a5\u91c7\u6837 $X_s$, \u6211\u4eec\u53ef\u4ee5\u63a8\u5bfc\u51fa\u9884\u6d4b\u7684\u98ce\u683c\u5206\u5e03.  \u968f\u540e, \u6211\u4eec\u901a\u8fc7\u5c06\u9884\u6d4b\u7684\u98ce\u683c\u878d\u5165\u6587\u672c\u72b6\u6001, \u5e76\u91c7\u7528[5, 2]\u63d0\u51fa\u7684\u57fa\u4e8e\u4fe1\u5fc3\u7684\u91c7\u6837\u65b9\u6848, \u8fed\u4ee3\u751f\u6210\u79bb\u6563\u7684\u58f0\u5b66\u6807\u8bb0.  \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u8fdb\u884c\u591a\u6b21\u524d\u5411\u4f20\u9012, \u5e76\u5728\u6bcf\u6b21\u8fed\u4ee3 $j$ \u65f6, \u5bf9\u63a9\u7801\u4f4d\u7f6e\u7684\u5019\u9009\u8005\u8fdb\u884c\u91c7\u6837.  \u7136\u540e, \u6211\u4eec\u6839\u636e\u5176\u7f6e\u4fe1\u5ea6\u5206\u6570\u4fdd\u7559 $P_j$ \u4e2a\u5019\u9009\u8005, \u5176\u4e2d $P_j$ \u9075\u5faa\u4f59\u5f26\u8c03\u5ea6.  \u6700\u540e, \u901a\u8fc7\u5c06\u97f3\u8272\u63d0\u793a\u901a\u8fc7\u6761\u4ef6\u5f52\u4e00\u5316\u5c42\u96c6\u6210, \u5e76\u5c06\u5176\u8f93\u5165\u5230\u7f16\u89e3\u7801\u5668\u89e3\u7801\u5668\u4e2d, \u6211\u4eec\u751f\u6210\u6700\u7ec8\u7684\u8bed\u97f3\u8f93\u51fa. </p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#4experiment","title":"4.Experiment","text":""},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#41controltoolkit","title":"4.1.ControlToolkit","text":"\u539f\u6587  &gt; In this section, we will introduce the ***VccmDataset***, related baselines, and evaluation metrics in ***ControlToolkit***. &gt; Detailed experimental settings for training and inference as well as model architecture specifics, are provided in appendix C and appendix D respectively.  &gt; ***VccmDataset*** To the best of our knowledge, there is no large-scale dataset that includes both text style prompts and speaker prompts. &gt; Building upon the existing [TextrolSpeech dataset](../../Datasets/2023.08.28_TextrolSpeech.md), we have developed the ***VccmDataset***. &gt; [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md) is currently the only open-source dataset in the field of text style control, containing a total of 330 hours of speech data along with 236203 style description texts. &gt; Based on [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md), we have optimized the pitch distribution, label boundaries, and the training and dataset splits. &gt; Specifically, we use the LibriTTS and emotional data from [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md) as the base databases, annotating each speech sample with five attribute labels: gender, volume, speed, pitch, and emotion. &gt; Considering the close proximity of data values between adjacent categories, we exclude the 5% of data at the boundaries of each interval for each attribute. &gt; This ensures greater distinctiveness for each label. &gt; Particularly, due to the significant difference in pitch distribution between male and female voices, we use gender-dependent thresholds to bin the pitch into three different levels. &gt; We randomly selected 1,500 audio samples as the ***ControlSpeech*** test set and matched the corresponding prompt voice based on speaker IDs. &gt; Additionally, to evaluate ***ControlSpeech***\u2019s performance on out-of-domain voices and styles, we further filtered an appropriate test set and enlisted language experts to compose style descriptions distinct from those in [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md). &gt; Further details on the creation of ***VccmDataset*** can be found in appendix E.  &gt; Baselines We have reproduced several state-of-the-art style-controllable models, including [PromptTTS](../../Models/_tmp/2022.11.22_PromptTTS.md), [PromptStyle](../../Models/E2E/2023.05.31_PromptStyle.md), [SALL-E](../../Datasets/2023.08.28_TextrolSpeech.md) and [InstructTTS](../../Models/_tmp/2023.01.31_InstructTTS.md), to serve as primary comparative models for evaluating the controllability of ***ControlSpeech***. &gt; For the comparison of voice cloning effectiveness, we have reproduced the [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) model and the [MobileSpeech](../../Models/_tmp/2024.02.14_MobileSpeech.md) model, representing the autoregressive paradigm and the parallel generation paradigm. &gt; The relevant baseline reproduction code has been integrated into the ***ControlToolkit***.  &gt; Evaluation metrics We have aligned our objective experiments with [[PromptTTS](../../Models/_tmp/2022.11.22_PromptTTS.md), [SALL-E](../../Datasets/2023.08.28_TextrolSpeech.md), [PromptTTS 2](../../Models/_tmp/2023.09.05_PromptTTS2.md)]. &gt; To evaluate the model\u2019s style controllability, we use accuracy to represent the evaluation metric, which measures the correspondence between the style factors in the output speech and those in the prompts. &gt; The accuracy of pitch, speaking speed, and volume is calculated using signal processing tools, while emotion classification accuracy is computed using a fine-tuned emotion classification model. &gt; We employ the official version of the [Emotion2Vec](../../Models/Speech_Representaion/2023.12.23_Emotion2Vec.md) model for the speech emotion recognition task, which is fine-tuned on the emotional dataset of ***VccmDataset***. &gt; To evaluate timbre similarity (Spk-sv) between the original prompt and synthesized speech, we utilize the base-plus-sv version of [WavLM](../../Models/_tmp/WavLM.md). &gt; Additionally, we conduct Automatic Speech Recognition (ASR) using the Whisper3 [41] model on the generated audio and calculate the word error rate (Wer) compared to the original transcriptions. &gt; For subjective testing, we conduct mean opinion score (MOS) evaluations on the test set to measure audio naturalness via crowdsourcing. &gt; We randomly select 30 samples from the test set of each dataset for subjective evaluation, and each audio sample is listened to by at least 10 testers. &gt; We analyze the MOS in two aspects: MOS-Q (Quality, assessing clarity and naturalness of duration and pitch) and MOS-S (Speaker similarity). &gt; Furthermore, for the evaluation of style-controllable many-to-many scenarios, we design specific metrics: MOS-TS (Timbre Similarity), MOS-SD (Style Diversity), and MOS-SA (Style Accuracy). &gt; We provide a detailed explanation of these metrics and their use cases in the many-to-many experiment section.   <p>\u5728\u672c\u8282\u4e2d, \u6211\u4eec\u5c06\u4ecb\u7ecd VccmDataset, \u76f8\u5173\u7684\u57fa\u7ebf\u6a21\u578b\u4ee5\u53ca ControlToolkit \u4e2d\u7684\u8bc4\u4f30\u6307\u6807.  \u8bad\u7ec3\u548c\u63a8\u7406\u7684\u8be6\u7ec6\u5b9e\u9a8c\u8bbe\u7f6e\u4ee5\u53ca\u6a21\u578b\u67b6\u6784\u7684\u5177\u4f53\u7ec6\u8282, \u5206\u522b\u5728\u9644\u5f55C\u548c\u9644\u5f55D\u4e2d\u63d0\u4f9b. </p> <p>VccmDataset \u636e\u6211\u4eec\u6240\u77e5, \u76ee\u524d\u6ca1\u6709\u5305\u542b\u6587\u672c\u98ce\u683c\u63d0\u793a\u548c\u8bf4\u8bdd\u8005\u63d0\u793a\u7684\u5927\u578b\u6570\u636e\u96c6.  \u6211\u4eec\u57fa\u4e8e\u73b0\u6709\u7684 TextrolSpeech\u6570\u636e\u96c6 \u5f00\u53d1\u4e86 VccmDataset.  TextrolSpeech \u662f\u76ee\u524d\u6587\u672c\u98ce\u683c\u63a7\u5236\u9886\u57df\u552f\u4e00\u7684\u5f00\u6e90\u6570\u636e\u96c6, \u5305\u542b\u603b\u5171330\u5c0f\u65f6\u7684\u8bed\u97f3\u6570\u636e\u4ee5\u53ca236203\u6761\u98ce\u683c\u63cf\u8ff0\u6587\u672c.  \u57fa\u4e8e TextrolSpeech, \u6211\u4eec\u4f18\u5316\u4e86\u97f3\u9ad8\u5206\u5e03, \u6807\u7b7e\u8fb9\u754c\u4ee5\u53ca\u8bad\u7ec3\u548c\u6570\u636e\u96c6\u5206\u5272.  \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u4f7f\u7528 LibriTTS \u548c\u6765\u81ea TextrolSpeech \u7684\u60c5\u611f\u6570\u636e\u4f5c\u4e3a\u57fa\u7840\u6570\u636e\u5e93, \u4e3a\u6bcf\u4e2a\u8bed\u97f3\u6837\u672c\u6807\u6ce8\u4e86\u4e94\u4e2a\u5c5e\u6027\u6807\u7b7e\uff1a\u6027\u522b, \u97f3\u91cf, \u901f\u5ea6, \u97f3\u9ad8\u548c\u60c5\u611f.  \u8003\u8651\u5230\u76f8\u90bb\u7c7b\u522b\u4e4b\u95f4\u6570\u636e\u503c\u7684\u7d27\u5bc6\u63a5\u8fd1, \u6211\u4eec\u6392\u9664\u4e86\u6bcf\u4e2a\u5c5e\u6027\u6bcf\u4e2a\u533a\u95f4\u8fb9\u754c\u4e0a\u76845%\u7684\u6570\u636e.  \u8fd9\u786e\u4fdd\u4e86\u6bcf\u4e2a\u6807\u7b7e\u7684\u66f4\u5927\u72ec\u7279\u6027.  \u7279\u522b\u662f, \u7531\u4e8e\u7537\u6027\u548c\u5973\u6027\u58f0\u97f3\u4e4b\u95f4\u7684\u97f3\u9ad8\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02, \u6211\u4eec\u4f7f\u7528\u6027\u522b\u4f9d\u8d56\u7684\u9608\u503c\u5c06\u97f3\u9ad8\u5206\u4e3a\u4e09\u4e2a\u4e0d\u540c\u7684\u7ea7\u522b.  \u6211\u4eec\u968f\u673a\u9009\u62e9\u4e861,500\u4e2a\u97f3\u9891\u6837\u672c\u4f5c\u4e3a ControlSpeech \u6d4b\u8bd5\u96c6, \u5e76\u6839\u636e\u8bf4\u8bdd\u8005ID\u5339\u914d\u76f8\u5e94\u7684\u63d0\u793a\u58f0\u97f3.  \u6b64\u5916, \u4e3a\u4e86\u8bc4\u4f30 ControlSpeech \u5728\u57df\u5916\u58f0\u97f3\u548c\u98ce\u683c\u4e0a\u7684\u8868\u73b0, \u6211\u4eec\u8fdb\u4e00\u6b65\u7b5b\u9009\u4e86\u4e00\u4e2a\u9002\u5f53\u7684\u6d4b\u8bd5\u96c6, \u5e76\u8bf7\u8bed\u8a00\u4e13\u5bb6\u7f16\u5199\u4e86\u4e0eTextrolSpeech \u4e2d\u4e0d\u540c\u7684\u98ce\u683c\u63cf\u8ff0.  \u5173\u4e8e VccmDataset \u521b\u5efa\u7684\u66f4\u591a\u7ec6\u8282\u53ef\u4ee5\u5728\u9644\u5f55 E \u4e2d\u627e\u5230. </p> <p>\u57fa\u7ebf\u6a21\u578b \u6211\u4eec\u91cd\u73b0\u4e86\u51e0\u4e2a\u6700\u5148\u8fdb\u7684\u98ce\u683c\u53ef\u63a7\u6a21\u578b, \u5305\u62ec PromptTTS, PromptStyle, SALL-E\u548cInstructTTS, \u4f5c\u4e3a\u8bc4\u4f30ControlSpeech\u53ef\u63a7\u6027\u7684\u4e3b\u8981\u6bd4\u8f83\u6a21\u578b.  \u4e3a\u4e86\u6bd4\u8f83\u58f0\u97f3\u514b\u9686\u7684\u6709\u6548\u6027, \u6211\u4eec\u91cd\u73b0\u4e86VALL-E\u6a21\u578b\u548cMobileSpeech\u6a21\u578b, \u5206\u522b\u4ee3\u8868\u4e86\u81ea\u56de\u5f52\u8303\u5f0f\u548c\u5e73\u884c\u751f\u6210\u8303\u5f0f.  \u76f8\u5173\u7684\u57fa\u7ebf\u91cd\u73b0\u4ee3\u7801\u5df2\u96c6\u6210\u5230ControlToolkit\u4e2d. </p> <p>\u8bc4\u4f30\u6307\u6807 \u6211\u4eec\u7684\u5ba2\u89c2\u5b9e\u9a8c\u4e0e (PromptTTS, SALL-E, PromptTTS 2) \u4fdd\u6301\u4e00\u81f4.  \u4e3a\u4e86\u8bc4\u4f30\u6a21\u578b\u7684\u98ce\u683c\u53ef\u63a7\u6027, \u6211\u4eec\u4f7f\u7528\u51c6\u786e\u5ea6\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807, \u8be5\u6307\u6807\u8861\u91cf\u8f93\u51fa\u8bed\u97f3\u4e2d\u7684\u98ce\u683c\u56e0\u7d20\u4e0e\u63d0\u793a\u4e2d\u7684\u98ce\u683c\u56e0\u7d20\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb.  \u97f3\u9ad8, \u8bf4\u8bdd\u901f\u5ea6\u548c\u97f3\u91cf\u7684\u51c6\u786e\u5ea6\u4f7f\u7528\u4fe1\u53f7\u5904\u7406\u5de5\u5177\u8ba1\u7b97, \u800c\u60c5\u611f\u5206\u7c7b\u51c6\u786e\u5ea6\u5219\u4f7f\u7528\u5728VccmDataset\u60c5\u611f\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u60c5\u611f\u5206\u7c7b\u6a21\u578b\u8ba1\u7b97.  \u6211\u4eec\u4f7f\u7528Emotion2Vec\u6a21\u578b\u7684\u5b98\u65b9\u7248\u672c\u6765\u8fdb\u884c\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1, \u8be5\u6a21\u578b\u5728VccmDataset\u7684\u60c5\u611f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03.  \u4e3a\u4e86\u8bc4\u4f30\u539f\u59cb\u63d0\u793a\u4e0e\u5408\u6210\u8bed\u97f3\u4e4b\u95f4\u7684\u97f3\u8272\u76f8\u4f3c\u5ea6(Spk-sv), \u6211\u4eec\u4f7f\u7528\u4e86WavLM\u7684base-plus-sv\u7248\u672c.  \u6b64\u5916, \u6211\u4eec\u4f7f\u7528Whisper3 [41]\u6a21\u578b\u5bf9\u751f\u6210\u7684\u97f3\u9891\u8fdb\u884c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR), \u5e76\u8ba1\u7b97\u4e0e\u539f\u59cb\u8f6c\u5f55\u76f8\u6bd4\u7684\u5b57\u9519\u8bef\u7387(Wer).  \u5bf9\u4e8e\u4e3b\u89c2\u6d4b\u8bd5, \u6211\u4eec\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u5e73\u5747\u610f\u89c1\u5206\u6570(MOS)\u8bc4\u4f30, \u901a\u8fc7\u4f17\u5305\u6765\u8861\u91cf\u97f3\u9891\u7684\u81ea\u7136\u5ea6.  \u6211\u4eec\u4ece\u6bcf\u4e2a\u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u96c6\u4e2d\u968f\u673a\u9009\u62e930\u4e2a\u6837\u672c\u8fdb\u884c\u4e3b\u89c2\u8bc4\u4f30, \u6bcf\u4e2a\u97f3\u9891\u6837\u672c\u81f3\u5c11\u753110\u540d\u6d4b\u8bd5\u8005\u542c\u53d6.  \u6211\u4eec\u5206\u6790MOS\u5728\u4e24\u4e2a\u65b9\u9762\uff1aMOS-Q (\u8d28\u91cf, \u8bc4\u4f30\u65f6\u957f\u548c\u97f3\u9ad8\u7684\u6e05\u6670\u5ea6\u548c\u81ea\u7136\u5ea6) \u548cMOS-S (\u8bf4\u8bdd\u8005\u76f8\u4f3c\u6027) .  \u6b64\u5916, \u4e3a\u4e86\u8bc4\u4f30\u98ce\u683c\u53ef\u63a7\u7684\u591a\u5bf9\u591a\u573a\u666f, \u6211\u4eec\u8bbe\u8ba1\u4e86\u7279\u5b9a\u7684\u6307\u6807\uff1aMOS-TS (\u97f3\u8272\u76f8\u4f3c\u5ea6) , MOS-SD (\u98ce\u683c\u591a\u6837\u6027) \u548cMOS-SA (\u98ce\u683c\u51c6\u786e\u5ea6) .  \u6211\u4eec\u5c06\u5728\u591a\u5bf9\u591a\u5b9e\u9a8c\u90e8\u5206\u8be6\u7ec6\u89e3\u91ca\u8fd9\u4e9b\u6307\u6807\u53ca\u5176\u7528\u4f8b. </p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#42evaluation-on-style-controllability","title":"4.2.Evaluation on style controllability","text":"<p>We first compared the performance of ControlSpeech with various models on the style controllability task. The evaluation was conducted on the 1,500-sample controllable test set from the VccmDataset. To eliminate the influence of timbre variations on the controllability results of ControlSpeech, we used the ground truth (GT) timbre as the prompt. We compared the controllability of the models using pitch accuracy, speed accuracy, energy accuracy, and emotion accuracy. Additionally, we measured the audio quality generated by the models using word error rate (Wer), timbre similarity (Spk-sv), and MOS-Q (Mean Opinion Score for Quality). The experimental results are shown in Table 1, and we drew the following conclusions: 1) The GTcodec version exhibits high reconstruction quality. However, it shows limitations in Wer, emotion classification accuracy, and speech speed classification accuracy. We believe this may be due to accumulated errors introduced by the test model. Additionally, the emotion classification experiment did not include neutral emotion classification results, which better highlights the model\u2019s emotion control capabilities and presents a more challenging task for all models. 2) Comparing ControlSpeech with other baseline models on controllability metrics, we found that, except for pitch accuracy, ControlSpeech achieved state-of-the-art results in energy, speed, and emotion classification accuracy. Upon analyzing the synthesized audio of ControlSpeech, we noticed some instances of unnatural pitch. We attribute this to possible conflicts arising from simultaneously controlling different timbres and styles. 3) In terms of Spk-sv, Wer, and MOS-Q metrics, the audio generated by ControlSpeech demonstrates good timbre similarity and audio quality. However, it falls slightly behind PromptStyle and InstructTTS in Wer. We believe this is due to the limitations imposed by the FACodec approach, which constrains the upper bound of ControlSpeech\u2019s performance. 4) We further attempted to replace the GT timbre in the test set with the timbre from the prompt test set. We found that while the audio quality-related metrics remained unchanged, the controllability-related metrics, except for speech speed accuracy, generally decreased by 5%-10%.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#43evaluation-on-the-timbre-cloning-task","title":"4.3.Evaluation on the timbre cloning task","text":"<p>To evaluate the timbre cloning capability of ControlSpeech in an out-of-domain speaker scenario, we compared the performance of ControlSpeech with models such as VALL-E and MobileSpeech on the out-of-domain speaker test set from the VccmDataset. The out-of-domain speaker test set consists of 1,086 test utterances, ensuring that none of the speakers were present in the training set. VALL-E and MobileSpeech used Encodec for feature extraction during training. The experimental results are shown in Table 2. We observed that in terms of robustness metrics, the zero-shot TTS systems trained on small datasets performed even lower than ControlSpeech. We attribute this to the performance gains from the pre-trained speaker prompt component in ControlSpeech, which was trained on 60,000 hours of speaker data. Additionally, in the MOS metric, we found that although ControlSpeech can further control styles, it still maintains performance comparable to zero-shot TTS systems in terms of timbre cloning.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#44evaluation-on-the-out-of-domain-style-control-task","title":"4.4.Evaluation on the out-of-domain style control task","text":"<p>We further tested the controllability of style-controllable models under out-of-domain style descriptions. We compared the performance of ControlSpeech with controllable baseline models on the out-of-domain style test set from the VccmDataset. The out-of-domain style test set comprises 100 test utterances, with style prompts rewritten by experts. The experimental results are shown in Table 3. We found that the generalization performance of ControlSpeech is significantly better than that of the baseline models. The accuracy of speech speed and energy is markedly higher for ControlSpeech, especially in terms of energy accuracy, where there is no significant difference between the main test set and the out-of-domain style test set. ControlSpeech exhibits a slightly higher wer, which may be due to the limitations imposed by the decoupled codec approach. Similar to the results of Table 1, the pitch accuracy of ControlSpeech is slightly lower. We believe this is due to pitch inconsistencies arising from the simultaneous control of style and timbre cloning.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#45evaluation-on-style-controlled-many-to-many-problems","title":"4.5.Evaluation on style controlled many-to-many problems","text":"<p>To better evaluate the performance of style-controllable models on many-to-many tasks, we compared the results of ControlSpeech with controllable baseline models on the many-to-many test set from the VccmDataset. Specifically, the MOS-TS metric was used to assess whether the timbre remains stable across 60 different style descriptions for four speakers. Additionally, we selected three pairs of six different style descriptions under 50 different timbre prompts, with pitch, speed, and energy labels set to normal, fast, normal; normal, slow, normal; high, normal, normal; low, normal, normal; normal, normal, high; and normal, normal, low, respectively. The MOS-SA and MOS-SD metrics represent the accuracy and diversity of style control for each pair respectively. The experimental results are shown in Table 4. We found that ControlSpeech significantly outperforms PromptStyle and InstructTTS in both the MOS-SA and MOS-SD metrics. This indicates that the unique SMSD module design in ControlSpeech enables it to synthesize speech that is both accurate and diverse.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#46ablation-experiment","title":"4.6.Ablation experiment","text":"<p>In this section, we validated the necessity of the decoupling scheme and the SMSD module. Additionally, in appendix F and appendix G, we examined the impact of hyperparameters for mixed distributions and various noise models.</p> <p>Decouple codec For the decouple codec experiment, we maintained the main framework of ControlSpeech but used a non-decoupled Encodec to represent discrete audio in the TTS model. Furthermore, during training, we input the speech prompt into the codec generator, allowing the model to learn corresponding timbre, content, and style representations from the speech prompt, content prompt, and style prompt, respectively. We refer to this experimental model as ControlSpeech w/o decoupling and evaluated it using the prompt version of the main test set from the VccmDataset. As shown in Table 1, ControlSpeech w/o decoupling performed significantly worse in controllability compared to ControlSpeech. This suggests that the speech prompt and style prompt may interfere with each other, making it difficult to simultaneously clone timbre and control style with this approach.</p> <p>SMSD module Regarding the SMSD module, we evaluated its effectiveness in addressing the many-to-many style control problem. Specifically, we replaced the SMSD module with a style encoder and referred to this experimental model as ControlSpeech w/o SMSD. As shown in Table 4, ControlSpeech w/o SMSD performed significantly worse in the MOS-SA and MOS-SD metrics compared to ControlSpeech, thus strongly validating that the SMSD module enables more fine-grained control of the model\u2019s style and increases style diversity through style sampling.</p>"},{"location":"TTS/Models/_tmp/2024.06.03_ControlSpeech/#5conclusion","title":"5.Conclusion","text":"\u539f\u6587  &gt; In this paper, we present ***ControlSpeech***, the first TTS system capable of simultaneously performing zero-shot timbre cloning and zero-shot style control. &gt; Leveraging large-scale pre-trained discrete codec representations, we disentangle style, content, and timbre, and generate the corresponding codec representations through a non-autoregressive, mask-based iterative codec generator. &gt; Additionally, we identified a many-to-many problem in style control and designed a unique ***Style Mixed Semantic Density (SMSD)*** module to mitigate this issue. &gt; We constructed a new dataset called ***VccmDataset***, and established relevant evaluation metrics and baselines for the new task, all of which are open-sourced in the ***ControlToolkit***. &gt; The limitations of ***ControlSpeech*** and directions for future work are discussed in the appendix H.   <p>\u5728\u672c\u6587\u4e2d, \u6211\u4eec\u4ecb\u7ecd\u4e86 ControlSpeech, \u8fd9\u662f\u9996\u4e2a\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u96f6\u6837\u672c\u97f3\u8272\u514b\u9686\u548c\u96f6\u6837\u672c\u98ce\u683c\u63a7\u5236\u7684\u6587\u672c\u5230\u8bed\u97f3 (TTS) \u7cfb\u7edf. \u6211\u4eec\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u79bb\u6563\u7f16\u89e3\u7801\u5668\u8868\u793a, \u5c06\u98ce\u683c\u3001\u5185\u5bb9\u548c\u97f3\u8272\u89e3\u8026, \u5e76\u901a\u8fc7\u975e\u81ea\u56de\u5f52\u3001\u57fa\u4e8e\u63a9\u7801\u7684\u8fed\u4ee3\u7f16\u89e3\u7801\u5668\u751f\u6210\u5668\u751f\u6210\u76f8\u5e94\u7684\u7f16\u89e3\u7801\u5668\u8868\u793a. \u6b64\u5916, \u6211\u4eec\u8bc6\u522b\u4e86\u98ce\u683c\u63a7\u5236\u4e2d\u7684\u591a\u5bf9\u591a\u95ee\u9898, \u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u72ec\u7279\u7684 \u98ce\u683c\u6df7\u5408\u8bed\u4e49\u5bc6\u5ea6 (SMSD) \u6a21\u5757\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898. \u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a VccmDataset \u7684\u65b0\u6570\u636e\u96c6, \u5e76\u4e3a\u65b0\u4efb\u52a1\u5efa\u7acb\u4e86\u76f8\u5173\u7684\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u7ebf, \u6240\u6709\u8fd9\u4e9b\u90fd\u5728 ControlToolkit \u4e2d\u5f00\u6e90. ControlSpeech \u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u5de5\u4f5c\u65b9\u5411\u5728\u9644\u5f55 H \u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba.</p> TODO  B ***SMSD*** Loss The loss function for the ***SMSD*** module represents the conditional probability of the input style \u2032 representation Xs . &gt; We further refine this into a maximum likelihood loss involving the style distribution parameters \u03c0k, \u00b5(k), \u03c32(k) derived through the MDN network and noise perturbation module. &gt; The detailed derivation of the loss function is as follows. C Training and inference settings ***ControlSpeech*** was trained on ***VccmDataset*** using 8 NVIDIA A100 40G GPUs with each batch accommodating 3500 frames of the discrete codec. &gt; We optimized the models using the AdamW optimizer with parameters \u03b21 = 0.9 and \u03b22 = 0.95. &gt; The learning rate was warmed up for the first 5k updates, reaching a peak of 5 \u00d7 10\u22124, and then linearly decayed. &gt; We utilized the open-source [FACodec](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md)\u2019s voice conversion version as the codec encoder and decoder for ***ControlSpeech***. &gt; The style-controllable baseline models were trained on the same sets ***VccmDataset*** to eliminate potential biases. &gt; We utilize a pre-trained [BERT](../../Models/_Basis/2018.10.11_BERT.md) model consisting of 12 hidden layers with 110M parameters. &gt; For the implementation of the basic MDN network model, we largely followed the approach described in [12]. D Model architecture in the ***ControlSpeech*** Follow [NaturalSpeech 3](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md), the basic architecture of codec encoder and codec decoder follows [DAC](../../Models/Speech_Neural_Codec/2023.06.11_Descript-Audio-Codec.md) and employs the SnakeBeta activation function ([BigVGAN](../../Models/TTS3_Vocoder/2022.06.09_BigVGAN.md)). &gt; The timbre extractor consists of several [Conformer](../../Models/ASR/2020.05.16_Conformer.md) blocks. &gt; We use Nqc = 2, Nqp = 1, Nqd = 3 as the number of quantizers for each of the three FVQ Qc, Qp, Qd, the codebook size for all the quantizers is 1024. &gt; Text encoder and variance adaptor share the similar architecture which is comprised of several FFT blocks or attention layers as used by [FastSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md). &gt; The codec generator is a decoder primarily based on [Conformer](../../Models/ASR/2020.05.16_Conformer.md) blocks, similar to [MobileSpeech](../../Models/_tmp/2024.02.14_MobileSpeech.md). &gt; However, we opted for fewer encoder layers and a smaller parameter count in the codec generator. E ***VccmDataset*** details We use the gender labels available in the online metadata. &gt; For the remaining style factors, such as pitch, speaking speed, and volume, we initially utilize the Montreal forced alignment (MFA) [39] tool to extract phoneme durations and silence segments. &gt; Subsequently, we calculate the average duration of each phoneme pronunciation within voiced segments for speaking speed. &gt; Regarding energy, we compute the L2-norm of the ampli tude of each short-time Fourier transform (STFT) frame. &gt; Then the Parselmouth 3 tool is employed to extract fundamental frequency (f0) and calculate the geometric mean across all voiced regions as pitch values. &gt; After obtaining the three style factors\u2019 values, we divide speech into 3 categories (high/normal/low) according to the proportion. &gt; To further validate ***ControlSpeech***\u2019s ability to simultaneously control style and clone speaker timbre, the ***VccmDataset*** includes four types of test sets: the main test set, the out-of-domain speaker test set, the out-ofdomain style test set, and the special case test set. &gt; Each test set corresponds to different experiments: style controllability experiments, out-of-domain speaker cloning experiments, out-of-domain style controllability experiments, and many-to-many style control experiments, respectively.  ## F.Ablation experiments about mixed distributions In this section, we investigate the impact of the number of mixtures in the ***SMSD*** module on model performance. &gt; We conducted ablation studies under the isotropic across clusters noise perturbation mode, examining the effects of using 3, 5, and 7 mixtures. &gt; As shown in Table 5, the differences in the MOS-SD metric were negligible. &gt; However, an increase in the number of mixtures led to a decline in the MOS-SA metric, indicating that an excessive number of mixtures may reduce the model\u2019s control accuracy. G Ablation experiments about various noise modes We analyzed the impact of different noise perturbation modes on the many-to-many style control problem, with the number of mixture distributions fixed at 5. &gt; As shown in Table 6, we found that the noise perturbation mode maintaining isotropy at the cluster centers achieved a balance between the MOS-SA and MOS-SD metrics. H Future work and limitations In this work, we introduced ***ControlSpeech***, the first TTS system capable of simultaneously cloning timbre and controlling style. &gt; While ***ControlSpeech*** has demonstrated notable controllability and cloning capabilities, there remains considerable scope for further research and improvement based on the current framework. &gt; - Decoupled Codec Optimization There is substantial room for performance enhancement within the existing decoupled codec model. &gt; Although we attempted further optimization based on [FACodec](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md), no significant improvements were observed. &gt; However, the decoupling method of [FACodec](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md) and the timbre conversion performance based on decoupled codecs still have potential for improvement, which might enhance the naturalness of synthesized speech pitch. &gt; Future work could explore more efficient forms of vector quantization and decoupled codec representations. &gt; Additionally, experimenting with a broader range of supervision signals and different methods of information fusion could provide further advancements. &gt; - Larger Training Datasets The field of style-controllable TTS urgently needs larger training datasets. &gt; Although [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md) and the ***VccmDataset*** have established a foundation, achieving more advanced speech controllability will require datasets comprising tens of thousands of hours of style descriptions. &gt; - Diversity in Style Descriptions Current style description texts predominantly include labels such as emotion, speed, intonation, energy, and gender. &gt; There is still a gap between these descriptions and the diversity of human-level style descriptions. &gt; - Exploration of Generative Models We experimented with decoupled codecs and non-autoregressive parallel generative models. &gt; Future research could explore a broader range of generative model architectures and additional audio representations."},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/","title":"Small-E","text":"\u57fa\u672c\u4fe1\u606f  - \u6807\u9898: Small-E: Small Language Model with Linear Attention for Efficient Speech Synthesis - \u4f5c\u8005:   - [Th\u00e9odor Lemerle](../../Authors/Th\u00e9odor_Lemerle.md)   - [Nicolas Obin](../../Authors/Nicolas_Obin.md)   - [Axel Roebel](../../Authors/Axel_Roebel.md) - \u673a\u6784:   - [\u7d22\u90a6\u5927\u5b66](../../Institutions/SorbonneU.md) - \u65f6\u95f4:   - \u9884\u5370\u65f6\u95f4: 2024.06.06 ArXiv v1   - \u66f4\u65b0\u7b14\u8bb0: 2024.06.10 - \u53d1\u8868:   - [InterSpeech](../../Publications/InterSpeech.md)  - \u94fe\u63a5:   - [ArXiv](https://arxiv.org/abs/2406.04467)   - [DOI]()   - [Github]()   - [Demo]() - \u6807\u7b7e:   - [\u8bed\u8a00\u6a21\u578b](../../Tags/LanguageModel.md)   - [\u8bed\u97f3\u5408\u6210](../../Tags/SpeechSynthesis.md)   - [\u96f6\u6837\u672c](../../Tags/Zero-Shot.md) - \u9875\u6570: 5 - \u5f15\u7528: 34 - \u88ab\u5f15: 0"},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>Recent advancements in text-to-speech (TTS) powered by language models have showcased remarkable capabilities in achieving naturalness and zero-shot voice cloning. Notably, the decoder-only transformer is the prominent architecture in this domain. However, transformers face challenges stemming from their quadratic complexity in sequence length, impeding training on lengthy sequences and resource-constrained hardware. Moreover they lack specific inductive bias with regards to the monotonic nature of TTS alignments. In response, we propose to replace transformers with emerging recurrent architectures and introduce specialized cross-attention mechanisms for reducing repeating and skipping issues. Consequently our architecture can be efficiently trained on long samples and achieve state-of-the-art zero-shot voice cloning against baselines of comparable size.</p>"},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#11context-related-works","title":"1.1.Context &amp; Related Works","text":"<p>Over the recent years, neural text-to-speech synthesis (TTS) has gained spectacular improvements in terms of quality with a diversity of approaches and paradigms [1, 2, 3, 4]. In particular, discrete speech and audio representations allowed immediate use of well-established decoder-only transformers such as GPT [5] in many state-of-the-art text-to-audio and text-to-speech model. However, transformers rely on the self-attention \u201ctime-mixing\u201d[6] operation which can be efficiently trained in parallel but suffers from quadratic complexity with respect to the sequence length. The challenge of designing sequence modeling architecture that can compete with transformers has sparked a resurgence in research on recurrent neural networks (RNNs). This work introduces the broad term \u201clinear attention\u201d to denote this emerging class of RNNs that replaces self-attention for linear complexity \u201ctime-mixing\u201d while keeping performances and high training throughput.</p> <p>This paper primarily relates to speech models formulated as language models (LMs) or employing discrete audio codecs through Residual Vector Quantization (RVQ). VALL-E [7] employs an autoregressive transformer to predict the first quantizer and a parallel transformer for the residuals. Before the rise of RVQ codecs, Tortoise [8] achieved a significant improvement through scaling up and leveraged a decoder-only transformer to predict a VQ representation of the mel Some other works introduce semantic codes as low frame rate audio latents, following advancements in self-supervised speech representations. For instance Bark [9] separately predicts semantic codes from text, first quantizers from semantic codes, and residuals with three decoder-only transformers. SoundStorm [10] predicts audio from semantic codes in parallel by leveraging a MaskGit [11] architecture. In contrast, NaturalSpeech2 [12] avoids the language model formulation by learning the continuous latents of an RVQ codec with a diffusion model, sidestepping autoregressive modeling or semantic encoding and instead relying on given durations and fundamental frequency.</p>"},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#12linear-surrogate-of-decoder-transformer","title":"1.2.Linear Surrogate of Decoder Transformer","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#13tts-as-a-conditional-codec-language-modeling","title":"1.3.TTS as A Conditional Codec Language Modeling","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#14positioning-contributions","title":"1.4.Positioning &amp; Contributions","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#2methodology","title":"2.Methodology\u00b7\u65b9\u6cd5","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#21model-architecture","title":"2.1.Model Architecture","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#3experiments","title":"3.Experiments\u00b7\u5b9e\u9a8c","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#31dataset","title":"3.1.Dataset","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#32implementation-details","title":"3.2.Implementation Details","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#33benchmark","title":"3.3.Benchmark","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#34methodology","title":"3.4.Methodology","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#341objective-evaluation","title":"3.4.1.Objective Evaluation","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#342subjective-evaluation","title":"3.4.2.Subjective Evaluation","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#4results-discussion","title":"4.Results &amp; Discussion\u00b7\u7ed3\u679c\u4e0e\u8ba8\u8bba","text":""},{"location":"TTS/Models/_tmp/2024.06.06_Small-E/#5conclusion","title":"5.Conclusion\u00b7\u7ed3\u8bba","text":"<p>In this paper we presented Small-E, a TTS model based on codec language model. The proposed model tackles limitations of current LM TTS models. Firstly, we introduced Linear Causal Language Model in place of the traditional decoder-only transformer. Secondly, we introduced a cross-attention mechanism designed specifically to handle text and speech modalities in the context of TTS, with the idea of preventing the skip and repetition problem of auto-regressive models. In contrast with existing work, we were able to show that training LM TTS model is interesting even on limited hardware and leads to state-of-the-art quality against model of the same size. Experimental evaluation demonstrated the efficiency of the proposed model either in terms of training throughput, skip and repetition reduction, as well as naturalness and similarity These to the reference speaker of the generated speech. observations constitute encouraging results opening the way for small and efficient generative TTS models. In future work we are interested in streaming TTS, taking advantage of the linear complexity of LCLM for very long or embedded synthesis.</p>"},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/","title":"Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech","text":"<p>Jaehyeon Kim Jungil Kong Juhee Son </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS com parable to ground truth.</p> <p>\u8fd1\u671f\u51e0\u9879\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u4f7f\u5f97\u5355\u9636\u6bb5\u8bad\u7ec3\u548c\u5e76\u884c\u91c7\u6837\u6210\u4e3a\u53ef\u80fd, \u4f46\u5b83\u4eec\u7684\u91c7\u6837\u8d28\u91cf\u4e0e\u4e24\u9636\u6bb5\u7cfb\u7edf\u7684\u8d28\u91cf\u76f8\u6bd4\u4ecd\u6709\u5dee\u8ddd. \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e76\u884c\u7684\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u65b9\u6cd5, \u5176\u751f\u6210\u7684\u97f3\u9891\u66f4\u81ea\u7136. \u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u6807\u51c6\u5316\u6d41\u548c\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u7684\u53d8\u5206\u63a8\u65ad, \u8fd9\u53ef\u4ee5\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u8868\u73b0\u529b. \u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7528\u4e8e\u4ece\u8f93\u5165\u6587\u672c\u751f\u6210\u591a\u79cd\u8282\u594f\u7684\u8bed\u97f3. \u901a\u8fc7\u5bf9\u9690\u53d8\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u8868\u8fbe\u81ea\u7136\u7684\u4e00\u5bf9\u591a\u5173\u7cfb, \u5373\u4e00\u4e2a\u6587\u672c\u8f93\u5165\u53ef\u4ee5\u88ab\u8bf4\u6210\u591a\u79cd\u97f3\u9ad8\u548c\u8282\u594f. \u5728 LJ Speech \u6570\u636e\u96c6\u7684\u4e3b\u89c2\u8bc4\u4ef7 (MOS) \u4e2d, \u6211\u4eec\u5bf9\u6bd4\u4e86\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u7684\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf, \u5e76\u8fbe\u5230\u4e86\u53ef\u6bd4\u7684MOS.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Text-to-speech (TTS) systems synthesize raw speech waveforms from given text through several components. With the rapid development of deep neural networks, TTS system pipelines have been simplified to two-stage generative modeling apart from text preprocessing such as text normalization and phonemization. The first stage is to produce intermediate speech representations such as mel-spectrograms (Tacotron2 (2017)) or linguistic features (Oord et al., 2016) from the preprocessed text (Although there is a text preprocessing step in TTS systems, We herein use preprocessed text interchangeably with the word \"text\".), and the second stage is to generate raw waveforms conditioned on the intermediate representations (Oord et al., 2016; WaveRNN (2018)). Models at each of the two-stage pipelines have been developed independently.</p> <p>\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u901a\u8fc7\u6570\u4e2a\u7ec4\u4ef6\u4ece\u7ed9\u5b9a\u7684\u6587\u672c\u751f\u6210\u539f\u59cb\u8bed\u97f3\u6ce2\u5f62. \u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8fc5\u901f\u53d1\u5c55, \u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u5df2\u7ecf\u7b80\u5316\u4e3a\u9664\u4e86\u6587\u672c\u9884\u5904\u7406 (\u6587\u672c\u89c4\u8303\u5316\u548c\u97f3\u7d20\u5316) \u4e4b\u5916\u7684\u4e24\u9636\u6bb5\u751f\u6210\u6a21\u578b. \u7b2c\u4e00\u9636\u6bb5\u662f\u751f\u6210\u4e2d\u95f4\u8bed\u97f3\u8868\u793a\u5982\u6885\u5c14\u9891\u8c31\u56fe\u6216\u6765\u81ea\u9884\u5904\u7406\u6587\u672c\u7684\u8bed\u8a00\u7279\u5f81, \u7b2c\u4e8c\u9636\u6bb5\u662f\u6839\u636e\u4e2d\u95f4\u8868\u793a\u751f\u6210\u539f\u59cb\u6ce2\u5f62. \u6bcf\u4e2a\u9636\u6bb5\u7684\u6a21\u578b\u90fd\u5355\u72ec\u5efa\u7acb.</p> <p>Neural network-based autoregressive TTS systems have shown the capability of synthesizing realistic speech (Tacotron2 (2017); Transformer-TTS (2018)), but their sequential generative process makes it difficult to fully utilize modern parallel processors. To overcome this limitation and improve synthesis speed, several non-autoregressive methods have been proposed. In the text-to-spectrogram generation step, extracting attention maps from pre-trained autoregressive teacher networks (FastSpeech (2019); ParaNet) is attempted to decrease the difficulty of learning alignments between text and spectrograms. More recently, likelihood-based methods further eliminate the dependency on external aligners by estimating or learning alignments that maximize the likelihood of target mel-spectrograms (AlignTTS (2020); Flow-TTS (2020); Glow-TTS (2020)). Meanwhile, Generative Adversarial Networks (GANs) have been explored in second stage models. GAN-based feed-forward networks with multiple discriminators, each distinguishing samples at different scales or periods, achieve high-quality raw waveform synthesis (MelGAN (2019); GAN-TTS (2019); HiFi-GAN (2020)).</p> <p>\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u5df2\u7ecf\u5c55\u793a\u4e86\u5408\u6210\u771f\u5b9e\u8bed\u97f3\u7684\u80fd\u529b, \u4f46\u5b83\u4eec\u7684\u987a\u5e8f\u751f\u6210\u8fc7\u7a0b\u4f7f\u5f97\u5b83\u4eec\u96be\u4ee5\u5145\u5206\u5229\u7528\u73b0\u4ee3\u5e76\u884c\u5904\u7406\u5668. \u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\u5e76\u63d0\u5347\u5408\u6210\u901f\u5ea6, \u5df2\u7ecf\u6709\u6570\u79cd\u975e\u81ea\u56de\u5f52\u65b9\u6cd5\u88ab\u63d0\u51fa. \u5728\u6587\u672c\u5230\u9891\u8c31\u56fe\u751f\u6210\u6b65\u9aa4\u4e2d, \u4ece\u9884\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u6559\u5e08\u7f51\u7edc\u4e2d\u63d0\u53d6\u6ce8\u610f\u529b\u7279\u5f81\u56fe\u8bd5\u56fe\u51cf\u5c11\u5b66\u4e60\u6587\u672c\u548c\u9891\u8c31\u56fe\u4e4b\u95f4\u5bf9\u9f50\u7684\u96be\u5ea6. \u8fd1\u671f, \u57fa\u4e8e\u4f3c\u7136\u7684\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6d88\u9664\u4e86\u5bf9\u5916\u90e8\u5bf9\u9f50\u5668\u7684\u4f9d\u8d56, \u901a\u8fc7\u6700\u5927\u5316\u76ee\u6807\u9891\u8c31\u56fe\u7684\u4f3c\u7136\u6765\u4f30\u8ba1\u6216\u5b66\u4e60\u5bf9\u9f50. \u540c\u65f6, \u751f\u6210\u5bf9\u6297\u7f51\u7edc\u88ab\u7528\u4e8e\u7b2c\u4e8c\u9636\u6a21\u578b. \u57fa\u4e8e GAN \u7684\u524d\u9988\u7f51\u7edc\u5177\u6709\u591a\u4e2a\u5224\u522b\u5668, \u6bcf\u4e2a\u5224\u522b\u4e0d\u540c\u5c3a\u5ea6\u6216\u5468\u671f\u7684\u6837\u672c, \u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u539f\u59cb\u6ce2\u5f62\u5408\u6210.</p> <p>Despite the progress of parallel TTS systems, two-stage pipelines remain problematic because they require sequential training or fine-tuning (Tacotron2 (2017); Wave-Tacotron (2020)) for high-quality production wherein latter stage models are trained with the generated samples of earlier stage models. In addition, their dependency on predefined intermediate features precludes applying learned hidden representations to obtain further improvements in performance. Recently, several works, i.e., FastSpeech 2s (2020) and EATS (2020), have proposed efficient end-to-end training methods such as training over short audio clips rather than entire waveforms, leveraging a mel-spectrogram decoder to aid text representation learning, and designing a specialized spectrogram loss to relax length-mismatch between target and generated speech.  However, despite potentially improving performance by utilizing the learned representations, their synthesis quality lags behind two-stage systems.</p> <p>\u5c3d\u7ba1\u5e76\u884c\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u53d6\u5f97\u4e86\u8fdb\u5c55, \u4f46\u4e24\u9636\u6bb5\u65b9\u6848\u4ecd\u7136\u5b58\u5728\u95ee\u9898. \u56e0\u4e3a\u5b83\u4eec\u8981\u6c42\u987a\u5e8f\u8bad\u7ec3\u6216\u5fae\u8c03\u4ee5\u83b7\u53d6\u9ad8\u8d28\u91cf\u7ed3\u679c, \u5373\u540e\u4e00\u9636\u6bb5\u7684\u6a21\u578b\u9700\u8981\u5728\u524d\u4e00\u9636\u6bb5\u6a21\u578b\u7684\u751f\u6210\u6837\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3. \u6b64\u5916, \u4ed6\u4eec\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u4e2d\u95f4\u7279\u5f81\u9650\u5236\u4e86\u5c06\u5b66\u4e60\u5230\u7684\u9690\u85cf\u8868\u793a\u4ee5\u83b7\u5f97\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347. \u8fd1\u671f, \u6709\u51e0\u9879\u5de5\u4f5c\u5982 FastSpeech 2s (2020) \u548c EATS \u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5, \u5982\u8bad\u7ec3\u4e8e\u8f83\u77ed\u7684\u97f3\u9891\u7247\u6bb5\u800c\u4e0d\u662f\u5b8c\u6574\u6ce2\u5f62, \u5229\u7528\u6885\u5c14\u9891\u8c31\u56fe\u89e3\u7801\u5668\u8f85\u52a9\u6587\u672c\u8868\u793a\u5b66\u4e60, \u5e76\u8bbe\u8ba1\u4e13\u95e8\u7684\u9891\u8c31\u56fe\u635f\u5931\u4ee5\u677e\u5f1b\u76ee\u6807\u548c\u751f\u6210\u8bed\u97f3\u7684\u957f\u5ea6\u4e0d\u5339\u914d. \u7136\u800c, \u5c3d\u7ba1\u5229\u7528\u5b66\u4e60\u5230\u7684\u8868\u793a\u53ef\u4ee5\u63d0\u5347\u6027\u80fd, \u4f46\u5b83\u4eec\u7684\u5408\u6210\u8d28\u91cf\u4ecd\u7136\u843d\u540e\u4e8e\u4e24\u9636\u6bb5\u7cfb\u7edf.</p> <p>In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models.  Using a Variational AutoEncoder (VAE), we connect two modules of TTS systems through latent variables to enable efficient end-to-end learning.  To improve the expressive power of our method so that high-quality speech waveforms can be synthesized, we apply normalizing flows to our conditional prior distribution and adversarial training on the waveform domain.  In addition to generating fine-grained audio, it is important for TTS systems to express the one-to-many relationship in which text input can be spoken in multiple ways with different variations (e.g., pitch and duration).  To tackle the one-to-many problem, we also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method captures speech variations that cannot be represented by text. </p> <p>\u672c\u9879\u5de5\u4f5c\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e76\u884c\u7684\u7aef\u5230\u7aef\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u65b9\u6cd5, \u5176\u751f\u6210\u7684\u97f3\u9891\u66f4\u81ea\u7136. \u6211\u4eec\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAE) \u5c06\u4e24\u4e2a\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u7684\u6a21\u5757\u8fde\u63a5\u8d77\u6765, \u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u5b66\u4e60. \u4e3a\u4e86\u63d0\u9ad8\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u8868\u73b0\u529b, \u6211\u4eec\u5c06\u6807\u51c6\u5316\u6d41\u5e94\u7528\u5230\u6761\u4ef6\u5148\u9a8c\u5206\u5e03, \u5e76\u5728\u6ce2\u5f62\u57df\u4e0a\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3. \u9664\u4e86\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u97f3\u9891\u4e4b\u5916, \u5bf9\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u6765\u8bf4, \u8868\u8fbe\u4e00\u5bf9\u591a\u5173\u7cfb\u662f\u81f3\u5173\u91cd\u8981\u7684, \u5373\u8f93\u5165\u6587\u672c\u53ef\u4ee5\u4ee5\u4e0d\u540c\u7684\u97f3\u9ad8\u548c\u8282\u594f\u88ab\u8bf4\u51fa\u6765. \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898, \u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u7528\u4e8e\u4ece\u8f93\u5165\u6587\u672c\u751f\u6210\u591a\u79cd\u8282\u594f\u7684\u8bed\u97f3. \u901a\u8fc7\u5bf9\u9690\u53d8\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u6355\u6349\u5230\u4e0d\u80fd\u7528\u6587\u672c\u8868\u793a\u7684\u8bed\u97f3\u53d8\u4f53.</p> <p>Our method obtains more natural sounding speech and higher sampling efficiency than the best publicly available TTS system, Glow-TTS (2020) with HiFi-GAN (2020).  We make both our demo page and source-code publicly available.</p> <p>\u6211\u4eec\u7684\u65b9\u6cd5\u83b7\u5f97\u66f4\u81ea\u7136\u7684\u8bed\u97f3\u548c\u66f4\u9ad8\u7684\u91c7\u6837\u6548\u7387.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#2related-works","title":"2.Related Works\u00b7\u76f8\u5173\u5de5\u4f5c","text":"<p>\u6ce8: \u539f\u6587\u7b2c\u4e94\u8282 Section 5 in Original.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#21end-to-end-text-to-speech","title":"2.1.End-to-End Text-to-Speech\u00b7\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3","text":"<p>Currently, neural TTS models with a two-stage pipeline can synthesize human-like speech (WaveNet (2016); DeepVoice3; Tacotron2 (2017)). However, they typically require vocoders trained or fine-tuned with first stage model output, which causes training and deployment inefficiency. They are also unable to reap the potential benefits of an end-to-end approach that can use learned hidden representations rather than predefined intermediate features.</p> <p>\u5f53\u524d\u4e24\u9636\u6bb5\u7684\u795e\u7ecf\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u53ef\u4ee5\u5408\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8bed\u97f3. \u7136\u800c\u4ed6\u4eec\u540c\u6837\u8981\u6c42\u4f7f\u7528\u7b2c\u4e00\u9636\u6bb5\u6a21\u578b\u7684\u8f93\u51fa\u7528\u4e8e\u8bad\u7ec3\u6216\u5fae\u8c03\u58f0\u7801\u5668, \u8fd9\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u548c\u90e8\u7f72\u6548\u7387\u4f4e\u4e0b. \u4ed6\u4eec\u8fd8\u65e0\u6cd5\u5229\u7528\u7aef\u5230\u7aef\u65b9\u6cd5\u7684\u6f5c\u5728\u4f18\u52bf, \u5373\u53ef\u4ee5\u5229\u7528\u5b66\u4e60\u5230\u7684\u9690\u85cf\u8868\u793a\u800c\u975e\u9884\u5b9a\u4e49\u7684\u4e2d\u95f4\u7279\u5f81.</p> <p>Recently, single-stage end-to-end TTS models have been proposed to tackle the more challenging task of generating raw waveforms, which contain richer information (e.g., high-frequency response and phase) than mel-spectrograms, directly from text. FastSpeech 2s (2020) is an extension of FastSpeech 2 that enables end-to-end parallel generation by adopting adversarial training and an auxiliary mel-spectrogram decoder that helps learn text representations. However, to resolve the one-to-many problem, FastSpeech 2s must extract phoneme duration, pitch, and energy from speech used as input conditions in training. EATS (2020) employs adversarial training as well and a differentiable alignment scheme. To resolve the length mismatch problem between generated and target speech, EATS adopts soft dynamic time warping loss that is calculated by dynamic programming. Wave-Tacotron (2020) combines normalizing flows with Tacotron 2 for an end-to-end structure but remains autoregressive. The audio quality of all the aforementioned end-to-end TTS models is less than that of two-stage models.</p> <p>\u6700\u8fd1\u5355\u9636\u6bb5\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u88ab\u63d0\u51fa\u7528\u4e8e\u89e3\u51b3\u751f\u6210\u539f\u59cb\u6ce2\u5f62\u7684\u66f4\u5177\u6311\u6218\u6027\u4efb\u52a1, \u5b83\u5305\u542b\u6bd4\u6885\u5c14\u9891\u8c31\u56fe\u66f4\u591a\u7684\u4fe1\u606f (\u5982\u9ad8\u9891\u54cd\u5e94\u548c\u76f8\u4f4d), \u76f4\u63a5\u4ece\u6587\u672c\u751f\u6210. FastSpeech 2s \u662f FastSpeech 2 \u7684\u6269\u5c55, \u5b83\u901a\u8fc7\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u548c\u4e00\u4e2a\u8f85\u52a9\u7684\u6885\u5c14\u9891\u8c31\u56fe\u89e3\u7801\u5668\u6765\u5b9e\u73b0\u7aef\u5230\u7aef\u5e76\u884c\u751f\u6210. \u7136\u800c, \u4e3a\u4e86\u89e3\u51b3\u4e00\u5bf9\u591a\u95ee\u9898, FastSpeech 2s \u5fc5\u987b\u4ece\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u8bed\u97f3\u4e2d\u63d0\u53d6\u97f3\u7d20\u65f6\u957f, \u97f3\u9ad8\u548c\u80fd\u91cf. EATS \u4e5f\u662f\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u540c\u7684\u5bf9\u9f50\u65b9\u6848. \u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u548c\u76ee\u6807\u8bed\u97f3\u4e4b\u95f4\u7684\u957f\u5ea6\u4e0d\u5339\u914d\u95ee\u9898, EATS \u91c7\u7528\u8f6f\u52a8\u6001\u65f6\u95f4\u7a97\u635f\u5931, \u5b83\u662f\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u8ba1\u7b97\u7684. Wave Tacotron \u662f\u5c06\u6807\u51c6\u5316\u6d41\u4e0e Tacotron 2 \u7ed3\u5408\u7528\u4e8e\u7aef\u5230\u7aef\u7ed3\u6784, \u4f46\u4ecd\u7136\u662f\u81ea\u56de\u5f52\u7684. \u6240\u6709\u8fd9\u4e9b\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u7684\u8bed\u97f3\u8d28\u91cf\u90fd\u4e0d\u5982\u4e24\u9636\u6bb5\u6a21\u578b.</p> <p>Unlike the aforementioned end-to-end models, by utilizing a conditional VAE, our model </p> <ol> <li>learns to synthesize raw waveforms directly from text without requiring additional input conditions,</li> <li>uses a dynamic programming method, MAS, to search the optimal alignment rather than to calculate loss, </li> <li>generates samples in parallel, </li> <li>outperforms the best publicly available two-stage models.</li> </ol> <p>\u548c\u524d\u9762\u63d0\u53ca\u7684\u7aef\u5230\u7aef\u6a21\u578b\u4e0d\u540c, \u901a\u8fc7\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668, \u6211\u4eec\u7684\u6a21\u578b: 1. \u76f4\u63a5\u4ece\u6587\u672c\u751f\u6210\u539f\u59cb\u6ce2\u5f62\u800c\u4e0d\u9700\u8981\u989d\u5916\u7684\u8f93\u5165\u6761\u4ef6, 2. \u4f7f\u7528\u52a8\u6001\u89c4\u5212\u65b9\u6cd5 MAS \u641c\u7d22\u6700\u4f18\u5bf9\u9f50\u800c\u4e0d\u662f\u8ba1\u7b97\u635f\u5931, 3. \u4ee5\u5e76\u884c\u7684\u65b9\u5f0f\u751f\u6210\u6837\u672c, 4. \u6027\u80fd\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u516c\u5f00\u53ef\u7528\u7684\u4e24\u9636\u6bb5\u6a21\u578b.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#22variational-autoencoders","title":"2.2.Variational Autoencoders\u00b7\u53d8\u5206\u81ea\u7f16\u7801\u5668","text":"<p>VAEs are one of the most widely used likelihood-based deep generative models. We adopt a conditional VAE to a TTS system. A conditional VAE is a conditional generative model where the observed conditions modulate the prior distribution of latent variables used to generate outputs. In speech synthesis, GMVAE-Tacotron (2018) and Learning latent representations for style control and transfer in end-to-end speech synthesis combine Tacotron 2 and VAEs to learn speech style and prosody. BVAE-TTS (2020) generates mel-spectrograms in parallel based on a bidirectional VAE (IAF (2016)). Unlike the previous works that applied VAEs to first stage models, we adopt a VAE to a parallel end-to-end TTS system. Variational inference with normalizing flows, Variational Lossy Autoencoder and Latent normalizing flows for discrete sequences improve VAE performance by enhancing the expressive power of prior and posterior distribution with normalizing flows. To improve the representation power of the prior distribution, we add normalizing flows to our conditional prior network, leading to the generation of more realistic samples.</p> <p>\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAEs) \u662f\u6700\u5e38\u7528\u7684\u57fa\u4e8e\u4f3c\u7136\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e4b\u4e00. \u6211\u4eec\u5c06\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf. \u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u662f\u4e00\u79cd\u6761\u4ef6\u751f\u6210\u6a21\u578b, \u5176\u4e2d\u89c2\u5bdf\u5230\u7684\u6761\u4ef6\u4f1a\u5f71\u54cd\u7528\u4e8e\u751f\u6210\u8f93\u51fa\u7684\u9690\u53d8\u91cf\u7684\u5148\u9a8c\u5206\u5e03. \u5728\u8bed\u97f3\u5408\u6210\u4e2d, Hsu \u548c Zhang \u5c06 Tacotron2 \u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7ed3\u5408\u4ee5\u5b66\u4e60\u8bed\u97f3\u98ce\u683c\u548c\u97f5\u5f8b. BVAE-TTS \u57fa\u4e8e\u53cc\u5411\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ee5\u5e76\u884c\u751f\u6210\u6885\u5c14\u9891\u8c31. \u548c\u8fd9\u4e9b\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u7b2c\u4e00\u9636\u6bb5\u6a21\u578b\u7684\u5de5\u4f5c\u4e0d\u540c, \u6211\u4eec\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u5e76\u884c\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf. Rezende, Chen, Ziegler \u901a\u8fc7\u4f7f\u7528\u6807\u51c6\u5316\u6d41\u589e\u5f3a\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\u8868\u73b0\u6027\u4ee5\u63d0\u9ad8\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6027\u80fd. \u4e3a\u4e86\u63d0\u5347\u5148\u9a8c\u5206\u5e03\u7684\u8868\u793a\u80fd\u529b, \u6211\u4eec\u5728\u6761\u4ef6\u5148\u9a8c\u7f51\u7edc\u4e2d\u52a0\u5165\u4e86\u6807\u51c6\u5316\u6d41, \u4ece\u800c\u80fd\u591f\u751f\u6210\u66f4\u771f\u5b9e\u7684\u6837\u672c.</p> <p>Similar to our work, Flowseq proposed a conditional VAE with normalizing flows in a conditional prior network for non-autoregressive neural machine translation, FlowSeq. However, the fact that our model can explicitly align a latent sequence with the source sequence differs from FlowSeq, which needs to learn implicit alignment through attention mechanisms. Our model removes the burden of transforming the latent sequence into standard normal random variables by matching the latent sequence with the time-aligned source sequence via MAS, which allows for simpler architecture of normalizing flows.</p> <p>\u548c\u6211\u4eec\u7684\u5de5\u4f5c\u7c7b\u4f3c, Ma \u7b49\u4eba\u63d0\u51fa\u4e86 FlowSeq \u4e00\u4e2a\u5e26\u6709\u6807\u51c6\u5316\u6d41\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668, \u7528\u4e8e\u975e\u81ea\u56de\u5f52\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1. \u7136\u800c, \u7531\u4e8e\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u663e\u5f0f\u5730\u5bf9\u9f50\u9690\u53d8\u91cf\u5e8f\u5217\u548c\u6e90\u5e8f\u5217, \u4e0e FlowSeq \u4e0d\u540c, \u5b83\u9700\u8981\u901a\u8fc7\u6ce8\u610f\u673a\u5236\u5b66\u4e60\u9690\u5f0f\u5bf9\u9f50. \u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7 MAS \u5339\u914d\u9690\u53d8\u91cf\u5e8f\u5217\u548c\u65f6\u95f4\u5bf9\u9f50\u7684\u6e90\u5e8f\u5217, \u4ece\u800c\u6d88\u9664\u4e86\u5c06\u9690\u53d8\u91cf\u5e8f\u5217\u8f6c\u6362\u4e3a\u6807\u51c6\u6b63\u6001\u968f\u673a\u53d8\u91cf\u7684\u8d1f\u62c5, \u8fd9\u4f7f\u5f97\u6807\u51c6\u5316\u6d41\u7684\u67b6\u6784\u66f4\u7b80\u5355.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#23duration-prediction-in-non-autoregressive-text-to-speech","title":"2.3.Duration Prediction in Non-Autoregressive Text-to-Speech\u00b7\u975e\u81ea\u56de\u5f52\u6587\u672c\u8f6c\u8bed\u97f3\u4e2d\u7684\u65f6\u957f\u9884\u6d4b","text":"<p>Autoregressive TTS models (VoiceLoop (2017.07); Tacotron2 (2017); Flowtron (2021)) generate diverse speech with different rhythms through their autoregressive structure and several tricks including maintaining dropout probability during inference and priming (Generating sequences with recurrent neural networks). Parallel TTS models (FastSpeech (2019); ParaNet (2020); Glow-TTS (2020); FastSpeech 2s (2020); BVAE-TTS (2020)), on the other hand, have been relied on deterministic duration prediction. It is because parallel models have to predict target phoneme duration or the total length of target speech in one feed-forward path, which makes it hard to capture the correlated joint distribution of speech rhythms. In this work, we suggest a flow-based stochastic duration predictor that learns the joint distribution of the estimated phoneme duration, resulting in the generation of diverse speech rhythms in parallel.</p> <p>\u81ea\u56de\u5f52\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u901a\u8fc7\u4ed6\u4eec\u7684\u81ea\u56de\u5f52\u7ed3\u6784\u548c\u4e00\u4e9b\u6280\u5de7\u5305\u62ec\u5728\u63a8\u7406\u7ef4\u6301\u968f\u673a\u5931\u6d3b\u7387\u7b49\u7b49\u751f\u6210\u5177\u6709\u4e0d\u540c\u8282\u594f\u7684\u591a\u6837\u8bed\u97f3. \u5e76\u884c\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u5219\u4f9d\u8d56\u4e8e\u786e\u5b9a\u6027\u7684\u65f6\u957f\u9884\u6d4b, \u8fd9\u662f\u56e0\u4e3a\u5e76\u884c\u6a21\u578b\u5fc5\u987b\u5728\u4e00\u4e2a\u524d\u9988\u8def\u5f84\u4e2d\u9884\u6d4b\u76ee\u6807\u97f3\u7d20\u65f6\u957f\u6216\u76ee\u6807\u8bed\u97f3\u7684\u603b\u957f\u5ea6, \u8fd9\u4f7f\u5f97\u96be\u4ee5\u6355\u6349\u8bed\u97f3\u8282\u594f\u7684\u76f8\u5173\u8054\u5408\u5206\u5e03. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6d41\u7684\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u5b83\u5b66\u4e60\u4f30\u8ba1\u7684\u97f3\u7d20\u65f6\u957f\u7684\u8054\u5408\u5206\u5e03, \u4ece\u800c\u5728\u5e76\u884c\u751f\u6210\u4e2d\u751f\u6210\u5177\u6709\u4e0d\u540c\u8282\u594f\u7684\u591a\u6837\u8bed\u97f3.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#3method","title":"3.Method","text":"<p>In this section, we explain our proposed method and the architecture of it. The proposed method is mostly described in the first three subsections: a conditional VAE formulation; alignment estimation derived from variational inference; adversarial training for improving synthesis quality. The overall architecture is described at the end of this section. Fig.01a and Fig.01b show the training and inference procedures of our method, respectively. From now on, we will refer to our method as Variational Inference with adversarial learning for end-to-end Text-to-Speech (VITS).</p> <p></p> <p>\u672c\u8282\u5c06\u89e3\u91ca\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ca\u5176\u67b6\u6784. \u5728\u524d\u4e09\u4e2a\u5c0f\u8282\u4e2d, \u6211\u4eec\u5c06\u4ecb\u7ecd\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u4e3b\u8981\u5185\u5bb9: \u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5f62\u5f0f; \u7531\u53d8\u5206\u63a8\u65ad\u5f97\u5230\u7684\u5bf9\u9f50\u4f30\u8ba1; \u5bf9\u6297\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u5408\u6210\u8d28\u91cf. \u6700\u540e\u4e00\u5c0f\u8282\u5c06\u4ecb\u7ecd\u6574\u4f53\u67b6\u6784. \u56fe 01a \u548c\u56fe 01b \u5206\u522b\u5c55\u793a\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b. \u4e4b\u540e, \u6211\u4eec\u5c06\u4f7f\u7528 VITS \u6765\u6307\u4ee3\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#31variational-inference","title":"3.1.Variational Inference\u00b7\u53d8\u5206\u63a8\u65ad","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#311overview","title":"3.1.1.Overview\u00b7\u6982\u89c8","text":"<p>VITS can be expressed as a conditional VAE with the objective of maximizing the variational lower bound, also called the evidence lower bound (ELBO), of the intractable marginal log-likelihood of data $\\log p_{\\theta}(x|c)$:</p> <p>VITS \u53ef\u4ee5\u8868\u793a\u4e3a\u4e00\u4e2a\u6761\u4ef6\u81ea\u7f16\u7801\u5668, \u5176\u76ee\u6807\u662f\u6700\u5927\u5316\u96be\u4ee5\u5904\u7406\u7684\u6570\u636e\u7684\u8fb9\u9645\u5206\u5e03\u5bf9\u6570\u4f3c\u7136 $\\log p_{\\theta}(x|c)$ \u7684\u53d8\u5206\u4e0b\u754c, \u4e5f\u79f0\u4e3a\u8bc1\u636e\u4e0b\u754c (Evidence Lower BOund, ELBO).</p> <p>$$   \\log p_{\\theta}(x|c) \\geq \\mathbb{E}{q{\\phi}(z|x)}\\left[\\log p_{\\theta}(x|z)-\\log\\dfrac{q_{\\phi}(z|x)}{p_{\\theta}(z|c)}\\right] \\tag{1} $$</p> <p>where $p_{\\theta}(z|c)$ denotes a prior distribution of the latent variables $z$ given condition $c$,  $p_{\\theta}(x|z)$ is the likelihood function of a data point $x$, and $q_{\\phi}(z|x)$ is an approximate posterior distribution.</p> <ul> <li>$p_{\\theta}(z|c)$: \u7ed9\u5b9a\u6761\u4ef6 $c$ \u7684\u9690\u53d8\u91cf $z$ \u7684\u5148\u9a8c\u5206\u5e03.</li> <li>$p_{\\theta}(x|z)$: \u6570\u636e\u70b9 $x$ \u7684\u4f3c\u7136\u51fd\u6570.</li> <li>$q_{\\phi}(z|x)$: \u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03.</li> </ul> <p>The training loss is then the negative ELBO, which can be viewed as the sum of reconstruction loss $-\\log p_{\\theta}(x|z)$ and KL-divergence $\\log q_{\\phi}(z|x) -\\log | p_{\\theta}(z|c)$, where $z\\sim q_{\\phi}(z|x)$.</p> <p>\u8bad\u7ec3\u635f\u5931\u4e3a\u8d1f\u7684 ELBO, \u5b83\u53ef\u4ee5\u88ab\u89c6\u4e3a\u91cd\u6784\u635f\u5931 $-\\log p_{\\theta}(x|z)$ \u548c KL \u6563\u5ea6 $\\log q_{\\phi}(z|x) -\\log | p_{\\theta}(z|c)$ \u7684\u603b\u548c, \u5176\u4e2d $z\\sim q_{\\phi}(z|x)$.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#312reconstruction-loss","title":"3.1.2.Reconstruction Loss\u00b7\u91cd\u6784\u635f\u5931","text":"<p>As a target data point in the reconstruction loss, we use a mel-spectrogram instead of a raw waveform, denoted by $x_{mel}$.  We upsample the latent variables $z$ to the waveform domain $\\hat{y}$ through a decoder and transform $\\hat{y}$ to the mel-spectrogram domain $\\hat{x}_{mel}$.  Then the $L_1$ loss between the predicted and target mel-spectrogram is used as the reconstruction loss:</p> <p>\u91cd\u6784\u635f\u5931\u4e2d\u4f7f\u7528\u7684\u76ee\u6807\u6570\u636e\u70b9\u662f\u6885\u5c14\u9891\u8c31\u800c\u4e0d\u662f\u539f\u59cb\u6ce2\u5f62, \u8bb0\u4e3a $x_{mel}$. \u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u89e3\u7801\u5668\u5c06\u9690\u53d8\u91cf $z$ \u4e0a\u91c7\u6837\u5230\u6ce2\u5f62\u57df $\\hat{y}$, \u5e76\u5c06 $\\hat{y}$ \u8f6c\u6362\u5230\u6885\u5c14\u9891\u8c31\u57df $\\hat{x}_{mel}$. \u7136\u540e, \u6211\u4eec\u4f7f\u7528\u9884\u6d4b\u7684\u548c\u76ee\u6807\u7684\u6885\u5c14\u9891\u8c31\u4e4b\u95f4\u7684 $L_1$ \u635f\u5931\u4f5c\u4e3a\u91cd\u6784\u635f\u5931:</p> <p>$$   L_{recon}= |x_{mel}\u2212\\hat{x}_{mel}|_1 \\tag{2} $$</p> <p>This can be viewed as maximum likelihood estimation assuming a Laplace distribution for the data distribution and ignoring constant terms. </p> <p>\u8fd9\u53ef\u4ee5\u89c6\u4e3a\u5047\u8bbe\u6570\u636e\u5206\u5e03\u4e3a Laplace \u5206\u5e03, \u5e76\u5ffd\u7565\u5e38\u6570\u9879\u540e\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1.</p> <p>We define the reconstruction loss in the mel-spectrogram domain to improve the perceptual quality by using a mel-scale that approximates the response of the human auditory system.  Note that the mel-spectrogram estimation from a raw waveform does not require trainable parameters as it only uses STFT and linear projection onto the mel-scale.  Furthermore, the estimation is only employed during training, not inference.  In practice, we do not upsample the whole latent variables $z$ but use partial sequences as an input for the decoder, which is the windowed generator training used for efficient end-to-end training (FastSpeech 2s (2020); EATS (2020)).</p> <p>\u6211\u4eec\u5728\u6885\u5c14\u9891\u8c31\u57df\u4e2d\u5b9a\u4e49\u4e86\u91cd\u6784\u635f\u5931, \u4ee5\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf, \u56e0\u4e3a\u5b83\u4f7f\u7528\u4e00\u4e2a\u8fd1\u4f3c\u4eba\u7c7b\u542c\u89c9\u7cfb\u7edf\u54cd\u5e94\u7684\u6885\u5c14\u523b\u5ea6. \u6ce8\u610f, \u4ece\u539f\u59cb\u6ce2\u5f62\u4f30\u8ba1\u6885\u5c14\u9891\u8c31\u4e0d\u9700\u8981\u53ef\u8bad\u7ec3\u53c2\u6570, \u56e0\u4e3a\u5b83\u53ea\u4f7f\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\u548c\u7ebf\u6027\u6295\u5f71\u5230\u6885\u5c14\u523b\u5ea6. \u6b64\u5916, \u4f30\u8ba1\u4ec5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528, \u800c\u63a8\u7406\u65f6\u4e0d\u4f7f\u7528. \u5b9e\u9645\u4e0a, \u6211\u4eec\u4e0d\u4e0a\u91c7\u6837\u6574\u4e2a\u9690\u53d8\u91cf $z$, \u800c\u662f\u4f7f\u7528\u90e8\u5206\u5e8f\u5217\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165, \u8fd9\u4e0e\u7528\u4e8e\u9ad8\u6548\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u7a97\u53e3\u751f\u6210\u5668\u8bad\u7ec3\u76f8\u540c.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#313kl-divergencekl","title":"3.1.3.KL-Divergence\u00b7KL \u6563\u5ea6","text":"<p>The input condition of the prior encoder $c$ is composed of phonemes $c_{text}$ extracted from text and an alignment $A$ between phonemes and latent variables. </p> <p>\u5148\u9a8c\u7f16\u7801\u5668\u7684\u8f93\u5165\u6761\u4ef6 $c$ \u7531\u6587\u672c\u4e2d\u63d0\u53d6\u7684\u97f3\u7d20 $c_{text}$, \u97f3\u7d20\u548c\u9690\u53d8\u91cf\u4e4b\u95f4\u7684\u5bf9\u9f50 $A$ \u7ec4\u6210.</p> <p>The alignment is a hard monotonic attention matrix with $|c_{text}|\\times|z|$ dimensions representing how long each input phoneme expands to be time-aligned with the target speech. Because there are no ground truth labels for the alignment, we must estimate the alignment at each training iteration, which we will discuss in Section 2.2.1. </p> <p>\u8fd9\u4e2a\u5bf9\u9f50\u662f\u4e00\u4e2a\u7ef4\u5ea6\u4e3a $|c_{text}|\\times|z|$ \u786c\u6027\u5355\u8c03\u6ce8\u610f\u529b\u77e9\u9635, \u8868\u793a\u6bcf\u4e2a\u8f93\u5165\u97f3\u7d20\u6269\u5c55\u5230\u4e0e\u76ee\u6807\u8bed\u97f3\u65f6\u95f4\u5bf9\u9f50\u7684\u65f6\u95f4\u957f\u5ea6. \u56e0\u4e3a\u5bf9\u9f50\u6ca1\u6709\u771f\u5b9e\u6807\u7b7e, \u6211\u4eec\u5fc5\u987b\u5728\u6bcf\u4e00\u6b21\u8bad\u7ec3\u8fed\u4ee3\u4e2d\u4f30\u8ba1\u5bf9\u9f50, \u6211\u4eec\u5c06\u5728\u7b2c 3.2.1 \u8282\u4e2d\u8ba8\u8bba. </p> <p>In our problem setting, we aim to provide more high-resolution information for the posterior encoder. We, therefore, use the linear-scale spectrogram of target speech $x_{lin}$ as input rather than the mel-spectrogram.  Note that the modified input does not violate the properties of variational inference. </p> <p>\u5728\u6211\u4eec\u7684\u95ee\u9898\u8bbe\u7f6e\u4e2d, \u6211\u4eec\u5e0c\u671b\u4e3a\u540e\u9a8c\u7f16\u7801\u5668\u63d0\u4f9b\u66f4\u591a\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f. \u56e0\u6b64, \u6211\u4eec\u4f7f\u7528\u76ee\u6807\u8bed\u97f3\u7684\u7ebf\u6027\u9891\u8c31 $x_{lin}$ \u4f5c\u4e3a\u8f93\u5165, \u800c\u4e0d\u662f\u6885\u5c14\u9891\u8c31. \u6ce8\u610f, \u6211\u4eec\u4fee\u6539\u7684\u8f93\u5165\u5e76\u4e0d\u8fdd\u53cd\u53d8\u5206\u63a8\u65ad\u7684\u6027\u8d28.</p> <p>The KL divergence is then:</p> <p>KL \u6563\u5ea6\u4e3a:</p> <p>$$   L_{kl} = \\log q_{\\phi}(z|x) -\\log p_{\\theta}(z|c_{text},A) \\tag{3} $$</p> <p>$$   z\\sim q_{\\phi}(z|x) =\\mathcal{N}(z;\\mu_{\\phi}(x_{lin}), \\sigma_{\\phi}(x_{lin})) $$</p> <p>The factorized normal distribution is used to parameterize our prior and posterior encoders. </p> <p>\u6211\u4eec\u4f7f\u7528\u56e0\u5b50\u6b63\u6001\u5206\u5e03\u6765\u53c2\u6570\u5316\u6211\u4eec\u7684\u5148\u9a8c\u548c\u540e\u9a8c\u7f16\u7801\u5668.</p> <p>We found that increasing the expressiveness of the prior distribution is important for generating realistic samples.  We, therefore, apply a normalizing flow $f_{\\theta}$ (Variational inference with normalizing flows), which allows an invertible transformation of a simple distribution into a more complex distribution following the rule of change-of-variables, on top of the factorized normal prior distribution:</p> <p>\u6211\u4eec\u53d1\u73b0\u589e\u52a0\u5148\u9a8c\u5206\u5e03\u7684\u8868\u73b0\u6027\u5bf9\u4e8e\u751f\u6210\u771f\u5b9e\u6837\u672c\u81f3\u5173\u91cd\u8981. \u56e0\u6b64, \u6211\u4eec\u5728\u56e0\u5b50\u6b63\u6001\u5148\u9a8c\u5206\u5e03\u4e0a\u5e94\u7528\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u6d41 $f_{\\theta}$, \u5b83\u80fd\u591f\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u5206\u5e03\u901a\u8fc7\u53d8\u91cf\u53d8\u6362\u89c4\u5219\u8f6c\u6362\u4e3a\u5230\u4e00\u4e2a\u66f4\u590d\u6742\u5206\u5e03\u7684\u53ef\u9006\u53d8\u6362:</p> <p>$$   p_{\\theta}(z|c) = \\mathcal{N}(f_{\\theta}(z);\\mu_{\\theta}(c),\\sigma_{\\theta}(c))|\\det\\dfrac{\\partial f_{\\theta}(z)}{\\partial z}| \\tag{4} $$</p> <p>\u5176\u4e2d $c=[c_{text}, A]$</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#32alignment-estimation","title":"3.2.Alignment Estimation\u00b7\u5bf9\u9f50\u4f30\u8ba1","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#321monotonic-alignment-search","title":"3.2.1.Monotonic Alignment Search\u00b7\u5355\u8c03\u5bf9\u9f50\u641c\u7d22","text":"<p>To estimate an alignment $A$ between input text and target speech, we adopt Monotonic Alignment Search (MAS), a method to search an alignment that maximizes the likelihood of data parameterized by a normalizing flow $f$:</p> <p>\u4e3a\u4e86\u4f30\u8ba1\u6587\u672c\u548c\u76ee\u6807\u8bed\u97f3\u4e4b\u95f4\u7684\u5bf9\u9f50 $A$, \u6211\u4eec\u91c7\u7528\u5355\u8c03\u5bf9\u9f50\u641c\u7d22 (Monotonic Alignment Search, MAS), \u8be5\u65b9\u6cd5\u641c\u7d22\u4e00\u4e2a\u5bf9\u9f50, \u4f7f\u5f97\u7531\u6807\u51c6\u5316\u6d41 $f$ \u53c2\u6570\u5316\u7684\u6570\u636e\u7684\u4f3c\u7136\u6700\u5927\u5316:</p> <p>$$ \\begin{aligned}   A &amp;= \\arg\\max_{\\hat{A}}\\log p_{\\theta}(x_{mel}|c_{text},\\hat{A}) \\   &amp;= \\arg\\max_{\\hat{A}}\\log\\mathcal{N}(f(x);\\mu(c_{text},\\hat{A}),\\sigma(c_{text},\\hat{A})) \\end{aligned}\\tag{5} $$</p> <p>where the candidate alignments are restricted to be monotonic and non-skipping following the fact that humans read text in order without skipping any words.</p> <p>\u5176\u4e2d\u5019\u9009\u5bf9\u9f50\u88ab\u9650\u5236\u4e3a\u5355\u8c03\u4e14\u4e0d\u8df3\u8fc7\u4efb\u4f55\u5355\u8bcd, \u56e0\u4e3a\u4eba\u7c7b\u9605\u8bfb\u6587\u672c\u662f\u6709\u5e8f\u7684\u4e14\u4e0d\u4f1a\u8df3\u8fc7\u4efb\u4f55\u5355\u8bcd.</p> <p>To find the optimal alignment, Glow-TTS (2020) use dynamic programming.  Applying MAS directly in our setting is difficult because our objective is the ELBO, not the exact log-likelihood.  We, therefore, redefine MAS to find an alignment that maximizes the ELBO, which reduces to finding an alignment that maximizes the log-likelihood of the latent variables $z$:</p> <p>\u4e3a\u4e86\u627e\u5230\u6700\u4f18\u5bf9\u9f50, Glow-TTS (2020) \u4f7f\u7528\u52a8\u6001\u89c4\u5212. \u5728\u6211\u4eec\u7684\u8bbe\u7f6e\u4e2d\u76f4\u63a5\u5e94\u7528 MAS \u56f0\u96be, \u56e0\u4e3a\u6211\u4eec\u7684\u76ee\u6807\u662f ELBO, \u800c\u4e0d\u662f\u7cbe\u786e\u7684\u5bf9\u6570\u4f3c\u7136. \u56e0\u6b64, \u6211\u4eec\u91cd\u65b0\u5b9a\u4e49 MAS, \u5bfb\u627e\u4e00\u4e2a\u5bf9\u9f50, \u4f7f\u5f97 ELBO \u6700\u5927\u5316, \u8fd9\u7b49\u4ef7\u4e8e\u5bfb\u627e\u4e00\u4e2a\u5bf9\u9f50, \u4f7f\u5f97\u9690\u53d8\u91cf $z$ \u7684\u5bf9\u6570\u4f3c\u7136\u6700\u5927\u5316:</p> <p>$$ \\begin{aligned}   &amp;\\arg\\max_{\\hat{A}}\\log p_{\\theta}(x_{mel}|z)-\\log\\frac{q_{\\phi}(z|x_{lin})}{p_{\\theta}(z|c_{text},\\hat{A})} \\   &amp;=\\arg\\max_{\\hat{A}}\\log p_{\\theta}(z|c_{text},\\hat{A}) \\   &amp;=\\arg\\max_{\\hat{A}}\\log \\mathcal{N}(f_\\theta(z);\\mu_\\theta(c_{text},\\hat{A}),\\sigma_\\theta(c_{text},\\hat{A}))  \\end{aligned} $$</p> <p>Due to the resemblance of Eq.05 to Eq.06, we can use the original MAS implementation without modification. Appendix A includes pseudocode for MAS. Although we search the alignment which maximizes the ELBO not the exact log-likelihood of data, we can use the MAS implementation of Glow-TTS as described in Section 2.2.1.</p> <p>\u7531\u4e8e\u65b9\u7a0b 05 \u548c\u65b9\u7a0b 06 \u7684\u76f8\u4f3c\u6027, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u539f\u59cb\u7684 MAS \u5b9e\u73b0\u800c\u65e0\u9700\u4fee\u6539. \u9644\u5f55 A \u5305\u542b MAS \u7684\u4f2a\u4ee3\u7801. \u5c3d\u7ba1\u6211\u4eec\u641c\u7d22 ELBO \u6700\u5927\u7684\u5bf9\u9f50, \u800c\u4e0d\u662f\u6570\u636e\u7cbe\u786e\u7684\u5bf9\u6570\u4f3c\u7136, \u4f46\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 Glow-TTS \u7684 MAS \u5b9e\u73b0.</p> <pre><code>def monotonic_alignment_search(value):\n    \"\"\"Returns the most likely alignment for the given log-likelihood matrix.\n    Args:\n        value: the log-likelihood matrix. Its (i, j)-th entry contains\n        the log-likelihood of the j-th latent variable\n        for the given i-th prior mean and variance:\n        .. math::\n            value_{i,j} = log N(f(z)_{j}; \\mu_{i}, \\sigma_{i})\n        (dtype=float, shape=[text_length, latent_variable_length])\n    Returns:\n        path: the most likely alignment.\n        (dtype=float, shape=[text_length, latent_variable_length])\n    \"\"\"\n    t_x, t_y = value.shape # [text_length, letent_variable_length]\n    path = zeros([t_x, t_y])\n    # A cache to store the log-likelihood for the most likely alignment so far.\n    Q = -INFINITY * ones([t_x, t_y])\n    for y in range(t_y):\n        for x in range(max(0, t_x + y - t_y), min(t_x, y + 1)):\n            if y == 0: # Base case. If y is 0, the possible x value is only 0.\n                Q[x, 0] = value[x, 0]\n            else:\n                if x == 0:\n                    v_prev = -INFINITY\n                else:\n                    v_prev = Q[x-1, y-1]\n                v_cur = Q[x, y-1]\n                Q[x, y] = value[x, y] + max(v_prev, v_cur)\n    # Backtrack from last observation.\n    index = t_x - 1\n    for y in range(t_y - 1, -1, -1):\n        path[index, y] = 1\n        if index != 0 and (index == y or Q[index, y-1] &lt; Q[index-1, y-1]):\n            index = index - 1\nreturn path\n</code></pre> <p>\u6e90\u4ee3\u7801\u91c7\u7528 Cython \u7f16\u5199: - \u5b9a\u4e49\u51fd\u6570 <code>maximum_path_each()</code>, \u63a5\u53d7\u4e00\u4e2a\u4e8c\u7ef4\u6570\u7ec4 <code>path</code>, \u4e8c\u7ef4\u6570\u7ec4 <code>value</code>, \u4e24\u4e2a\u6574\u6570 <code>t_x</code>, <code>t_y</code>, \u4ee5\u53ca\u4e00\u4e2a\u6d6e\u70b9\u6570 <code>max_neg_val</code>. \u7528\u4e8e\u8ba1\u7b97\u6bcf\u4e2a\u8def\u5f84\u7684\u6700\u5927\u503c. - \u5b9a\u4e49\u51fd\u6570 <code>maximum_path_c()</code>, \u63a5\u53d7\u4e09\u4e2a\u4e09\u7ef4\u6570\u7ec4 <code>paths</code>, <code>values</code> \u548c\u4e24\u4e2a\u4e00\u7ef4\u6570\u7ec4 <code>t_xs</code> \u548c <code>t_ys</code>, \u4ee5\u53ca\u4e00\u4e2a\u6d6e\u70b9\u6570 <code>max_neg_val</code>. \u7528\u4e8e\u8ba1\u7b97\u6240\u6709\u8def\u5f84\u7684\u6700\u5927\u503c.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#322duration-prediction-from-text","title":"3.2.2.Duration Prediction from Text\u00b7\u6587\u672c\u4e2d\u7684\u65f6\u957f\u9884\u6d4b","text":"<p>We can calculate the duration of each input token $d_i$ by summing all the columns in each row of the estimated alignment $\\sum_j A_{i,j}$.  The duration could be used to train a deterministic duration predictor, as proposed in previous work (HiFi-GAN (2020)), but it cannot express the way a person utters at different speaking rates each time.  To generate human-like rhythms of speech, we design a stochastic duration predictor so that its samples follow the duration distribution of given phonemes.  The stochastic duration predictor is a flow-based generative model that is typically trained via maximum likelihood estimation. The direct application of maximum likelihood estimation, however, is difficult because the duration of each input phoneme is  1. a discrete integer, which needs to be dequantized for using continuous normalizing flows, 2. a scalar, which prevents high-dimensional transformation due to invertibility. </p> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5bf9\u4f30\u8ba1\u7684\u5bf9\u9f50\u7684\u6bcf\u4e00\u884c\u4e2d\u6240\u6709\u5217\u6c42\u548c\u4ee5\u8ba1\u7b97\u6bcf\u4e2a\u8f93\u5165 Token $d_i$ \u7684\u65f6\u957f, \u5373 $\\sum_j A_{i,j}$. \u65f6\u957f\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3\u4e00\u4e2a\u786e\u5b9a\u6027\u7684\u65f6\u957f\u9884\u6d4b\u5668, \u5982 HiFi-GAN (2020) \u6240\u63d0\u51fa\u7684, \u4f46\u5b83\u4e0d\u80fd\u8868\u8fbe\u4eba\u7c7b\u5728\u4e0d\u540c\u8bed\u901f\u4e0b\u8bf4\u8bdd\u7684\u65b9\u5f0f. \u4e3a\u4e86\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u8bed\u8c03\u7684\u8bed\u97f3, \u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u4f7f\u5f97\u5176\u91c7\u6837\u7b26\u5408\u7ed9\u5b9a\u97f3\u7d20\u7684\u65f6\u957f\u5206\u5e03. \u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b, \u901a\u5e38\u901a\u8fc7\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u8fdb\u884c\u8bad\u7ec3. \u7136\u800c, \u76f4\u63a5\u5e94\u7528\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u662f\u56f0\u96be\u7684, \u56e0\u4e3a\u6bcf\u4e2a\u8f93\u5165\u97f3\u7d20\u7684\u65f6\u957f\u662f: 1. \u4e00\u4e2a\u79bb\u6563\u6574\u6570, \u8fd9\u9700\u8981\u5bf9\u5176\u53bb\u91cf\u5316\u4ee5\u4fbf\u4f7f\u7528\u8fde\u7eed\u6807\u51c6\u5316\u6d41, 2. \u4e00\u4e2a\u6807\u91cf, \u7531\u4e8e\u53ef\u9006\u6027\u800c\u963b\u788d\u4e86\u9ad8\u7ef4\u53d8\u6362.</p> <p>We apply variational dequantization (Flow++ (2019)) and variational data augmentation (VFlow (2020)) to solve these problems.  To be specific, we introduce two random variables $u$ and $\u03bd$, which have the same time resolution and dimension as that of the duration sequenced, for variational dequantization and variational data augmentation, respectively.  We restrict the support of $u$ to be $[0, 1)$ so that the difference $d\u2212u$ becomes a sequence of positive real numbers, and we concatenate $\u03bd$ and $d$ channel-wise to make a higher dimensional latent representation.  We sample the two variables through an approximate posterior distribution $q_{\\phi}(u, \u03bd|d, c_{text})$.  The resulting objective is a variational lower bound of the log-likelihood of the phoneme duration:</p> <p>\u6211\u4eec\u5e94\u7528\u53d8\u5206\u53bb\u91cf\u5316\u548c\u53d8\u5206\u6570\u636e\u589e\u5f3a\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898. \u5177\u4f53\u5730, \u6211\u4eec\u5f15\u5165\u4e24\u4e2a\u968f\u673a\u53d8\u91cf $u$ \u548c $v$, \u62e5\u6709\u548c\u65f6\u957f\u5e8f\u5217\u76f8\u540c\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7ef4\u5ea6, \u5206\u522b\u7528\u4e8e\u53d8\u5206\u53bb\u91cf\u5316\u548c\u53d8\u5206\u6570\u636e\u589e\u5f3a. \u6211\u4eec\u5c06 $u$ \u7684\u652f\u6301\u9650\u5236\u4e3a $[0, 1)$, \u8fd9\u6837 $d-u$ \u53d8\u6210\u4e00\u7cfb\u5217\u6b63\u5b9e\u6570, \u5e76\u5c06 $\u03bd$ \u548c $d$ \u9010\u901a\u9053\u62fc\u63a5\u4ee5\u751f\u6210\u66f4\u9ad8\u7ef4\u7684\u6f5c\u5728\u8868\u793a. \u6211\u4eec\u901a\u8fc7\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03 $q_{\\phi}(u, v|d, c_{text})$ \u91c7\u6837\u8fd9\u4e24\u4e2a\u53d8\u91cf. \u6700\u540e\u5f97\u5230\u7684\u76ee\u6807\u662f\u97f3\u7d20\u65f6\u957f\u7684\u5bf9\u6570\u4f3c\u7136\u7684\u53d8\u5206\u4e0b\u754c:</p> <p>$$   \\log p_{\\theta}(d|c_{text}) \\geq \\mathbb{E}{q{\\phi}(u, \u03bd|d, c_{text})} \\left[\\log\\dfrac{p_{\\theta}(d-u,v|c_{text})}{q_{\\phi}(u,v|d,c_{text})}\\right]\\tag{7} $$</p> <p>The training loss $L_{dur}$ is then the negative variational lower bound.  We apply the stop gradient operator (Neural discrete representation learning), which prevents back-propagating the gradient of inputs, to the input conditions so that the training of the duration predictor does not affect that of other modules.</p> <p>\u8bad\u7ec3\u635f\u5931 $L_{dur}$ \u5219\u662f\u8d1f\u53d8\u5206\u4e0b\u754c. \u6211\u4eec\u5e94\u7528\u5bf9\u8f93\u5165\u6761\u4ef6\u5e94\u7528\u505c\u6b62\u68af\u5ea6\u7b97\u5b50 (\u9632\u6b62\u68af\u5ea6\u53cd\u5411\u4f20\u64ad) \u4ee5\u4fbf\u8bad\u7ec3\u65f6\u957f\u9884\u6d4b\u5668\u4e0d\u5f71\u54cd\u5176\u4ed6\u6a21\u5757.</p> <p>The sampling procedure is relatively simple; the phoneme duration is sampled from random noise through the inverse transformation of the stochastic duration predictor, and then it is converted to integers.</p> <p>\u91c7\u6837\u8fc7\u7a0b\u76f8\u5bf9\u7b80\u5355; \u97f3\u7d20\u65f6\u957f\u901a\u8fc7\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7684\u9006\u53d8\u6362\u4ece\u968f\u673a\u566a\u58f0\u4e2d\u91c7\u6837, \u7136\u540e\u5c06\u5176\u8f6c\u6362\u4e3a\u6574\u6570.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#33adversarial-training","title":"3.3.Adversarial Training\u00b7\u5bf9\u6297\u8bad\u7ec3","text":"<p>To adopt adversarial training in our learning system, we add a discriminator $D$ that distinguishes between the output generated by the decoder $G$ and the ground truth waveform $y$. In this work, we use two types of loss successfully applied in speech synthesis; the least-squares loss function (LSGAN (2016)) for adversarial training, and the additional feature-matching loss (Autoencoding beyond pixels using a learned similarity metric) for training the generator:</p> <p>\u4e3a\u4e86\u5728\u6211\u4eec\u7684\u5b66\u4e60\u7cfb\u7edf\u4e2d\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3, \u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e2a\u5224\u522b\u5668 $D$ \u6765\u533a\u5206\u7531\u89e3\u7801\u5668 $G$ \u751f\u6210\u7684\u8f93\u51fa\u548c\u771f\u5b9e\u6ce2\u5f62 $y$. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u6210\u529f\u5730\u5e94\u7528\u4e86\u4e24\u79cd\u635f\u5931, \u5373\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u6700\u5c0f\u4e8c\u4e58\u635f\u5931\u548c\u989d\u5916\u7684\u7279\u5f81\u5339\u914d\u635f\u5931\u7528\u4e8e\u8bad\u7ec3\u751f\u6210\u5668:</p> <p>$$ \\begin{align}   L_{adv}(D) &amp;= \\mathbb{E}{(y,z)} [(D(y)-1)^2 + (D(G(z)))^2]\\tag{8}\\   L{adv}(G) &amp;= \\mathbb{E}{z} [(D(G(z)) - 1)^2]\\tag{9}\\   L{fm}(G) &amp;= \\mathbb{E}{(y,z)} \\left[\\sum{l=1}^T \\dfrac{1}{N_l}| D^l(y)-D^l(G(z))|_1\\right]\\tag{10} \\end{align} $$</p> <p>where $T$ denotes the total number of layers in the discriminator and $D^l$ outputs the feature map of the $l$-th layer of the discriminator with $N_l$ number of features.</p> <p>\u5176\u4e2d $T$ \u8868\u793a\u5224\u522b\u5668\u4e2d\u7684\u5c42\u6570, $D^l$ \u8f93\u51fa\u7b2c $l$ \u5c42\u5224\u522b\u5668\u7684\u7279\u5f81\u56fe, \u5176\u6709 $N_l$ \u4e2a\u7279\u5f81.</p> <p>Notably, the feature matching loss can be seen as reconstruction loss that is measured in the hidden layers of the discriminator suggested as an alternative to the element-wise reconstruction loss of VAEs (Autoencoding beyond pixels using a learned similarity metric).</p> <p>\u7279\u5f81\u5339\u914d\u635f\u5931\u53ef\u4ee5\u89c6\u4e3a\u91cd\u5efa\u635f\u5931, \u5b83\u5728\u5224\u522b\u5668\u7684\u9690\u85cf\u5c42\u4e0a\u8fdb\u884c\u5ea6\u91cf, \u88ab\u5efa\u8bae\u4f5c\u4e3a VAE \u7684\u9010\u5143\u7d20\u91cd\u5efa\u635f\u5931\u7684\u66ff\u4ee3.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#34final-loss","title":"3.4.Final Loss\u00b7\u6700\u7ec8\u635f\u5931","text":"<p>With the combination of VAE and GAN training, the total loss for training our conditional VAE can be expressed as follows:</p> <p>\u901a\u8fc7\u5bf9 VAE \u548c GAN \u8bad\u7ec3\u7684\u7ec4\u5408, \u6211\u4eec\u7684\u6761\u4ef6 VAE \u7684\u603b\u635f\u5931\u53ef\u4ee5\u8868\u793a\u5982\u4e0b:</p> <p>$$   L_{vae} = L_{recon} + L_{kl} + L_{dur} + L_{adv}(G) + L_{fm}(G) \\tag{11} $$</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#35model-architecture","title":"3.5.Model Architecture\u00b7\u6a21\u578b\u67b6\u6784","text":"<p>The overall architecture of the proposed model consists of a posterior encoder, prior encoder, decoder, discriminator, and stochastic duration predictor.  The posterior encoder and discriminator are only used for training, not for inference. Architectural details are available in Appendix B.</p> <p>\u6240\u63d0\u65b9\u6cd5\u7684\u6574\u4f53\u67b6\u6784\u7531\u540e\u9a8c\u7f16\u7801\u5668, \u5148\u9a8c\u7f16\u7801\u5668, \u89e3\u7801\u5668, \u5224\u522b\u5668\u548c\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7ec4\u6210. \u540e\u9a8c\u7f16\u7801\u5668\u548c\u5224\u522b\u5668\u4ec5\u7528\u4e8e\u8bad\u7ec3, \u800c\u4e0d\u7528\u4e8e\u63a8\u65ad.</p> <p>In this section, we mainly describe the newly added parts of VITS as we followed configurations of Glow-TTS and HiFi-GAN for several parts of our model:  we use the same transformer encoder and WaveNet residual blocks as those of Glow-TTS;  our decoder and the multi-period discriminator is the same as the generator and multi-period discriminator of HiFi-GAN, respectively, except that we use different input dimension for the decoder and append a sub-discriminator.</p> <p>\u672c\u8282\u4e3b\u8981\u63cf\u8ff0\u4e86\u6240\u63d0\u65b9\u6cd5\u4e2d\u65b0\u589e\u7684 VITS \u7684\u90e8\u5206, \u6211\u4eec\u9075\u5faa Glow-TTS \u548c HiFi-GAN \u7684\u914d\u7f6e, \u5e76\u5bf9\u6a21\u578b\u7684\u4e0d\u540c\u90e8\u5206\u8fdb\u884c\u4e86\u63cf\u8ff0: - \u6211\u4eec\u4f7f\u7528\u4e0e Glow-TTS \u76f8\u540c\u7684 Transformer \u7f16\u7801\u5668\u548c WaveNet \u6b8b\u5dee\u5757; - \u6211\u4eec\u4f7f\u7528\u4e0e HiFi-GAN \u76f8\u540c\u7684\u751f\u6210\u5668\u548c\u591a\u5468\u671f\u5224\u522b\u5668, \u9664\u4e86\u5bf9\u89e3\u7801\u5668\u4f7f\u7528\u4e0d\u540c\u7684\u8f93\u5165\u7ef4\u5ea6\u5e76\u6dfb\u52a0\u4e86\u4e00\u4e2a\u5b50\u5224\u522b\u5668.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#351posterior-encoder","title":"3.5.1.Posterior Encoder\u00b7\u540e\u9a8c\u7f16\u7801\u5668","text":"<p>For the posterior encoder, we use the non-causal WaveNet residual blocks used in WaveGlow (2018) and Glow-TTS (2020).  A WaveNet residual block consists of layers of dilated convolutions with a gated activation unit and skip connection.  The linear projection layer above the blocks produces the mean and variance of the normal posterior distribution.  For the multi-speaker case, we use global conditioning (Oord et al., 2016) in residual blocks to add speaker embedding.</p> <p>\u5bf9\u4e8e\u540e\u9a8c\u7f16\u7801\u5668, \u6211\u4eec\u91c7\u7528 WaveGlow \u548c Glow-TTS \u4e2d\u4f7f\u7528\u7684\u975e\u56e0\u679c\u7684 WaveNet \u6b8b\u5dee\u5757. WaveNet \u6b8b\u5dee\u5757\u7531\u5e26\u6709\u95e8\u63a7\u6fc0\u6d3b\u5355\u5143\u7684\u7a7a\u6d1e\u5377\u79ef\u5c42\u548c\u8df3\u8dc3\u8fde\u63a5\u7ec4\u6210. \u5757\u4e0a\u7684\u7ebf\u6027\u6620\u5c04\u5c42\u4ea7\u751f\u6b63\u6001\u540e\u9a8c\u5206\u5e03\u7684\u5747\u503c\u548c\u65b9\u5dee. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u60c5\u5f62, \u6211\u4eec\u5728\u6b8b\u5dee\u5757\u4e2d\u4f7f\u7528\u5168\u5c40\u6761\u4ef6\u5316\u4ee5\u6dfb\u52a0\u8bf4\u8bdd\u4eba\u5d4c\u5165.</p> <p>The posterior encoder, consisting of 16 WaveNet residual blocks, takes linear-scale log magnitude spectrograms and produce latent variables with 192 channels.</p> <p>\u540e\u9a8c\u7f16\u7801\u5668\u7531 16 \u4e2a WaveNet \u6b8b\u5dee\u5757\u7ec4\u6210, \u5b83\u63a5\u53d7\u7ebf\u6027\u7684\u5bf9\u6570\u5e45\u5ea6\u8c31\u56fe\u5e76\u751f\u6210\u5177\u6709 192 \u901a\u9053\u7684\u9690\u53d8\u91cf.</p> <pre><code>class PosteriorEncoder(nn.Module):\n    def __init__(self,\n        in_channels,\n        out_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        gin_channels=0):\n        super().__init__()\n        self.in_channels     = in_channels\n        self.out_channels    = out_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size     = kernel_size\n        self.dilation_rate   = dilation_rate\n        self.n_layers        = n_layers\n        self.gin_channels    = gin_channels\n\n        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n        self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\n        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, x, x_lengths, g=None):\n        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n        x = self.pre(x) * x_mask\n        x = self.enc(x, x_mask, g=g)\n        stats = self.proj(x) * x_mask\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n        return z, m, logs, x_mask\n</code></pre> <p>\u9996\u5148\u5bf9\u8f93\u5165 x \u751f\u6210\u76f8\u5e94\u7684 x_mask \u4ee5\u53bb\u6389\u65e0\u6548\u90e8\u5206. \u4f7f\u7528\u4e00\u7ef4\u5377\u79ef\u5bf9\u8f93\u5165 x \u7684\u901a\u9053\u8fdb\u884c\u4fee\u6539. \u7136\u540e\u5c06\u4fee\u6539\u540e\u7684 x \u548c\u8bf4\u8bdd\u4eba\u5d4c\u5165\u4e00\u540c\u8f93\u5165\u5230 WaveNet \u4e2d. \u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u7ebf\u6027\u6620\u5c04\u8c03\u6574\u901a\u9053\u6570\u4e3a\u9690\u85cf\u7ef4\u5ea6\u7684\u4e24\u500d, \u5207\u5206\u540e\u5f97\u5230\u5747\u503c\u548c\u5bf9\u6570\u6807\u51c6\u5dee. \u7136\u540e\u91c7\u6837\u9690\u53d8\u91cf z \u5e76\u8fd4\u56de.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#352prior-encoder","title":"3.5.2.Prior Encoder\u00b7\u5148\u9a8c\u7f16\u7801\u5668","text":"<p>The prior encoder consists of a text encoder that processes the input phonemes $c_{text}$ and a normalizing flow $f_{\\theta}$ that improves the flexibility of the prior distribution. The text encoder is a transformer encoder  that uses relative positional representation instead of absolute positional encoding. We can obtain the hidden representation $h_{text}$ from $c_{text}$ through the text encoder and a linear projection layer above the text encoder that produces the mean and variance used for constructing the prior distribution.</p> <p>\u5148\u9a8c\u7f16\u7801\u5668\u7531\u4e00\u4e2a\u7528\u4e8e\u5904\u7406\u8f93\u5165\u97f3\u7d20\u7684\u6587\u672c\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u5148\u9a8c\u5206\u5e03\u7075\u6d3b\u6027\u7684\u6807\u51c6\u5316\u6d41\u7ec4\u6210. \u6587\u672c\u7f16\u7801\u5668\u662f\u4e00\u4e2a Transformer \u7f16\u7801\u5668, \u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u8868\u793a\u800c\u4e0d\u662f\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801. \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u7f16\u7801\u5668\u548c\u5176\u4e0a\u7684\u7ebf\u6027\u6620\u5c04\u5c42\u6765\u83b7\u5f97\u9690\u85cf\u8868\u793a $h_{text}$ \u5e76\u7528\u4e8e\u6784\u9020\u5148\u9a8c\u5206\u5e03\u7684\u5747\u503c\u548c\u65b9\u5dee.</p> <pre><code>class TextEncoder(nn.Module):\n    def __init__(self,\n        n_vocab,\n        out_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout):\n        super().__init__()\n        self.n_vocab = n_vocab\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n\n        self.emb = nn.Embedding(n_vocab, hidden_channels)\n        nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n\n        self.encoder = attentions.Encoder(\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout)\n        self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, x, x_lengths):\n        x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\n        x = torch.transpose(x, 1, -1) # [b, h, t]\n        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n\n        x = self.encoder(x * x_mask, x_mask)\n        stats = self.proj(x) * x_mask\n\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        return x, m, logs, x_mask\n</code></pre> <p>\u6587\u672c\u7f16\u7801\u5668\u5bf9\u8f93\u5165 x \u8fdb\u884c\u5d4c\u5165\u67e5\u627e\u540e, \u4e58\u4ee5\u6839\u53f7\u4e0b\u9690\u85cf\u901a\u9053\u6570, \u7136\u540e\u8f6c\u7f6e. \u6784\u9020 mask \u5bf9\u8f93\u5165 x \u8fdb\u884c\u65e0\u6548\u4f4d\u7f6e\u63a9\u819c. \u8f93\u5165\u5230 Transformer \u7f16\u7801\u5668\u4e2d, \u901a\u8fc7\u4e00\u7ef4\u5377\u79ef\u5c42\u8c03\u6574\u901a\u9053\u6570\u4e3a\u9690\u85cf\u7ef4\u5ea6\u7684\u4e24\u500d, \u5207\u5206\u540e\u5f97\u5230\u5747\u503c\u548c\u5bf9\u6570\u6807\u51c6\u5dee.</p> <p>The normalizing flow is a stack of affine coupling layers (Density estimation using Real NVP) consisting of a stack of WaveNet residual blocks. For simplicity, we design the normalizing flow to be a volume-preserving transformation with the Jacobian determinant of one. For the multi-speaker setting, we add speaker embedding to the residual blocks in the normalizing flow through global conditioning.</p> <p>\u6807\u51c6\u5316\u6d41\u662f\u7531\u4eff\u5c04\u8026\u5408\u5c42\u5806\u53e0\u800c\u6210, \u6bcf\u4e2a\u4eff\u5c04\u8026\u5408\u5c42\u7531 WaveNet \u6b8b\u5dee\u5757\u5806\u53e0\u800c\u6210. \u4e3a\u4e86\u7b80\u5316, \u6211\u4eec\u8bbe\u8ba1\u6807\u51c6\u5316\u6d41\u4e3a\u5177\u6709\u5355\u4f4d\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u7684\u4f53\u79ef\u4fdd\u6301\u53d8\u6362. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u8bbe\u7f6e, \u6211\u4eec\u901a\u8fc7\u5168\u5c40\u6761\u4ef6\u5316\u5728\u6807\u51c6\u5316\u6d41\u7684\u6b8b\u5dee\u5757\u4e2d\u6dfb\u52a0\u8bf4\u8bdd\u4eba\u5d4c\u5165.</p> <p>The normalizing flow in the prior encoder is a stack of four affine coupling layers, each coupling layer consisting of four WaveNet residual blocks.  As we restrict the affine coupling layers to be volume-preserving transformations, the coupling layers do not produce scale parameters.</p> <p>\u5177\u4f53\u5730, \u5148\u9a8c\u7f16\u7801\u5668\u4e2d\u7684\u6807\u51c6\u5316\u6d41\u7531\u56db\u4e2a\u4eff\u5c04\u8026\u5408\u5c42\u7ec4\u6210, \u6bcf\u4e2a\u4eff\u5c04\u8026\u5408\u5c42\u7531\u56db\u4e2a WaveNet \u6b8b\u5dee\u5757\u7ec4\u6210. \u7531\u4e8e\u6211\u4eec\u9650\u5236\u4eff\u5c04\u8026\u5408\u5c42\u4e3a\u4f53\u79ef\u4fdd\u6301\u53d8\u6362, \u56e0\u6b64\u8026\u5408\u5c42\u4e0d\u4ea7\u751f\u5c3a\u5ea6\u53c2\u6570.</p> <pre><code>class ResidualCouplingBlock(nn.Module):\n    def __init__(self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        n_flows=4,\n        gin_channels=0):\n        super().__init__()\n        self.channels = channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.n_flows = n_flows\n        self.gin_channels = gin_channels\n\n        self.flows = nn.ModuleList()\n        for i in range(n_flows):\n            self.flows.append(modules.ResidualCouplingLayer(channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels, mean_only=True))\n            self.flows.append(modules.Flip())\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        if not reverse:\n            for flow in self.flows:\n                x, _ = flow(x, x_mask, g=g, reverse=reverse)\n        else:\n            for flow in reversed(self.flows):\n                x = flow(x, x_mask, g=g, reverse=reverse)\n        return x\n</code></pre>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#353decoder","title":"3.5.3.Decoder\u00b7\u89e3\u7801\u5668","text":"<p>The decoder is essentially the HiFi-GAN (2020) V1 generator. It is composed of a stack of transposed convolutions, each of which is followed by a multi-receptive field fusion module (MRF). The output of the MRF is the sum of the output of residual blocks that have different receptive field sizes. For the multi-speaker setting, we add a linear layer that transforms speaker embedding and add it to the input latent variables $z$.</p> <p>\u89e3\u7801\u5668\u4e3a HiFi-GAN V1 \u751f\u6210\u5668\u7684\u57fa\u672c\u7ed3\u6784. \u5b83\u7531\u8f6c\u7f6e\u5377\u79ef\u5c42\u548c\u591a\u611f\u53d7\u91ce\u878d\u5408\u6a21\u5757 (MRF) \u5806\u53e0\u800c\u6210. \u6bcf\u4e2a MRF \u7684\u8f93\u51fa\u662f\u4e0d\u540c\u611f\u53d7\u91ce\u5927\u5c0f\u7684\u6b8b\u5dee\u5757\u8f93\u51fa\u7684\u603b\u548c. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u8bbe\u7f6e, \u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e2a\u7ebf\u6027\u5c42, \u7528\u4e8e\u8f6c\u6362\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u5165\u9690\u53d8\u91cf $z$ \u4e2d.</p> <p>The input of our decoder is latent variables generated from the prior or posterior encoders, so the input channel size of the decoder is 192.  For the last convolutional layer of the decoder, we remove a bias parameter, as it causes unstable gradient scales during mixed precision training.</p> <p>\u89e3\u7801\u5668\u7684\u8f93\u5165\u662f\u4ece\u5148\u9a8c\u6216\u540e\u9a8c\u7f16\u7801\u5668\u751f\u6210\u7684\u56e0\u53d8\u91cf\u6240\u4ee5\u7f16\u7801\u5668\u7684\u8f93\u5165\u901a\u9053\u6570\u4e3a 192. \u5bf9\u4e8e\u89e3\u7801\u5668\u7684\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42, \u6211\u4eec\u79fb\u9664\u504f\u7f6e\u53c2\u6570, \u56e0\u4e3a\u5b83\u4f1a\u5bfc\u81f4\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u65f6\u7684\u4e0d\u7a33\u5b9a\u68af\u5ea6\u5c3a\u5ea6.</p> <pre><code>class Generator(torch.nn.Module):\n    def __init__(self, initial_channel, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):\n        super(Generator, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\n        resblock = modules.ResBlock1 if resblock == '1' else modules.ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(weight_norm(\n                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n                                k, u, padding=(k-u)//2)))\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel//(2**(i+1))\n            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n    def forward(self, x, g=None):\n        x = self.conv_pre(x)\n        if g is not None:\n          x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i*self.num_kernels+j](x)\n                else:\n                    xs += self.resblocks[i*self.num_kernels+j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()\n</code></pre>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#354discriminator","title":"3.5.4.Discriminator\u00b7\u5224\u522b\u5668","text":"<p>We follow the discriminator architecture of the multi-period discriminator proposed in HiFi-GAN (2020). The multi-period discriminator is a mixture of Markovian window-based sub-discriminators (MelGAN (2019)), each of which operates on different periodic patterns of input waveforms.</p> <p>\u6211\u4eec\u9075\u5faa HiFi-GAN \u63d0\u51fa\u7684\u591a\u5468\u671f\u5224\u522b\u5668\u7684\u67b6\u6784. \u591a\u5468\u671f\u5224\u522b\u5668\u662f\u4e00\u4e2a\u7531\u9a6c\u5c14\u53ef\u592b\u7a97\u53e3\u5b50\u5224\u522b\u5668\u7ec4\u6210\u7684\u6df7\u5408\u6a21\u578b, \u5b83\u4eec\u5206\u522b\u64cd\u4f5c\u4e8e\u8f93\u5165\u6ce2\u5f62\u7684\u4e0d\u540c\u5468\u671f\u6a21\u5f0f.</p> <p>For the discriminator, HiFi-GAN uses the multi-period discriminator containing five sub-discriminators with periods [2, 3, 5, 7, 11] and the multi-scale discriminator containing three sub-discriminators.  To improve training efficiency, we leave only the first sub-discriminator of the multi-scale discriminator that operates on raw waveforms and discard two sub-discriminators operating on average-pooled waveforms.  The resultant discriminator can be seen as the multi-period discriminator with periods [1, 2, 3, 5, 7, 11].</p> <p>\u5bf9\u4e8e\u5224\u522b\u5668, HiFi-GAN \u4f7f\u7528\u5305\u542b\u4e94\u4e2a\u5b50\u5224\u522b\u5668\u7684\u591a\u5468\u671f\u5224\u522b\u5668\u548c\u5305\u542b\u4e09\u4e2a\u5b50\u5224\u522b\u5668\u7684\u591a\u5c3a\u5ea6\u5224\u522b\u5668. \u4e3a\u4e86\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387, \u6211\u4eec\u4ec5\u4fdd\u7559\u591a\u5c3a\u5ea6\u5224\u522b\u5668\u7684\u7b2c\u4e00\u4e2a\u5b50\u5224\u522b\u5668, \u5e76\u4e22\u5f03\u4e24\u4e2a\u64cd\u4f5c\u4e8e\u5e73\u5747\u6c60\u5316\u6ce2\u5f62\u7684\u5b50\u5224\u522b\u5668. \u6700\u7ec8\u7684\u5224\u522b\u5668\u53ef\u4ee5\u770b\u4f5c\u662f\u5177\u6709\u5468\u671f [1, 2, 3, 5, 7, 11] \u7684\u591a\u5468\u671f\u5224\u522b\u5668.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#355stochastic-duration-predictor","title":"3.5.5.Stochastic Duration Predictor\u00b7\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668","text":"<p>The stochastic duration predictor estimates the distribution of phoneme duration from a conditional input $h_{text}$. For the efficient parameterization of the stochastic duration predictor, we stack residual blocks with dilated and depth-separable convolutional layers. We also apply Neural Spline Flows, which take the form of invertible nonlinear transformations by using monotonic rational-quadratic splines, to coupling layers. Neural spline flows improve transformation expressiveness with a similar number of parameters compared to commonly used affine coupling layers. For the multi-speaker setting, we add a linear layer that transforms speaker embedding and add it to the input $h_{text}$.</p> <p>\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u4ece\u6761\u4ef6\u8f93\u5165 $h_{text}$ \u4f30\u8ba1\u97f3\u7d20\u65f6\u957f\u7684\u5206\u5e03. \u4e3a\u4e86\u6709\u6548\u5730\u53c2\u6570\u5316\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u6211\u4eec\u5806\u53e0\u5177\u6709\u81a8\u80c0\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5c42\u7684\u6b8b\u5dee\u5757. \u6211\u4eec\u8fd8\u5e94\u7528\u4e86\u795e\u7ecf\u6837\u6761\u6d41, \u5b83\u662f\u4e00\u79cd\u4f7f\u7528\u5355\u8c03\u5206\u6bb5\u4e8c\u6b21\u6837\u6761\u7684\u975e\u7ebf\u6027\u53d8\u6362, \u7528\u4e8e\u8026\u5408\u5c42. \u795e\u7ecf\u6837\u6761\u6d41\u7528\u548c\u5e38\u7528\u7684\u4eff\u5c04\u8026\u5408\u5c42\u53c2\u6570\u6570\u91cf\u76f8\u5f53\u7684\u53c2\u6570\u6765\u63d0\u9ad8\u53d8\u6362\u7684\u8868\u8fbe\u80fd\u529b. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u8bbe\u7f6e, \u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e2a\u7ebf\u6027\u5c42, \u7528\u4e8e\u8f6c\u6362\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u5165 $h_{text}$.</p> <p>Fig.05a and Fig.05b show the training and inference procedures of the stochastic duration predictor, respectively.  The main building block of the stochastic duration predictor is the dilated and depth-wise separable convolutional (DDSConv) residual block as in Fig.05c.  Each convolutional layer in DDSConv blocks is followed by a layer normalization layer and GELU activation function.  We choose to use dilated and depth-wise separable convolutional layers for improving parameter efficiency while maintaining large receptive field size.</p> <p></p> <p>\u56fe 05a \u548c\u56fe 05b \u5206\u522b\u5c55\u793a\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b. \u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7684\u4e3b\u8981\u6784\u5efa\u5757\u662f\u5177\u6709\u81a8\u80c0\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7684\u6b8b\u5dee\u5757 (DDSConv), \u5982\u56fe 05c \u6240\u793a. DDSConv \u4e2d\u7684\u6bcf\u4e2a\u5377\u79ef\u5c42\u540e\u8ddf\u968f\u4e00\u4e2a\u5c42\u5f52\u4e00\u5316\u5c42\u548c GELU \u6fc0\u6d3b\u51fd\u6570. \u6211\u4eec\u9009\u62e9\u4f7f\u7528\u81a8\u80c0\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5c42\u6765\u63d0\u9ad8\u53c2\u6570\u6548\u7387, \u540c\u65f6\u4fdd\u6301\u5927\u7684\u611f\u53d7\u91ce\u5c3a\u5bf8.</p> <p>The posterior encoder and normalizing flow module in the duration predictor are flow-based neural networks and have the similar architecture.  The difference is that the posterior encoder transforms a Gaussian noise sequence into two random variables $\u03bd$ and $u$ to express the approximate posterior distribution $q_{\\phi}(u, \u03bd|d, c_{text})$, and the normalizing flow module transforms $d\u2212u$ and $\u03bd$ into a Gaussian noise sequence to express the log-likelihood of the augmented and dequantized data $\\log p_{\\theta}(d \u2212 u, \u03bd|c_{text})$ as described in Section 2.2.2.</p> <p>\u65f6\u957f\u9884\u6d4b\u5668\u4e2d\u7684\u540e\u9a8c\u7f16\u7801\u5668\u548c\u6807\u51c6\u5316\u6d41\u6a21\u5757\u90fd\u662f\u57fa\u4e8e\u6d41\u7684\u795e\u7ecf\u7f51\u7edc, \u4e14\u5177\u6709\u76f8\u4f3c\u67b6\u6784. \u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u540e\u9a8c\u7f16\u7801\u5668\u5c06\u9ad8\u65af\u566a\u58f0\u5e8f\u5217\u8f6c\u6362\u4e3a\u4e24\u4e2a\u968f\u673a\u53d8\u91cf $\u03bd$ \u548c $u$, \u7528\u4e8e\u8868\u793a\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03 $q_{\\phi}(u, \u03bd|d, c_{text})$, \u800c\u6807\u51c6\u5316\u6d41\u6a21\u5757\u5c06 $d\u2212u$ \u548c $\u03bd$ \u8f6c\u6362\u4e3a\u9ad8\u65af\u566a\u58f0\u5e8f\u5217, \u7528\u4e8e\u8868\u793a\u589e\u5f3a\u548c\u53bb\u91cf\u5316\u6570\u636e\u7684\u5bf9\u6570\u4f3c\u7136 $\\log p_{\\theta}(d \u2212 u, \u03bd|c_{text})$, \u5982\u7b2c 2.2.2 \u8282\u6240\u8ff0.</p> <p>All input conditions are processed through condition encoders, each consisting of two 1x1 convolutional layers and a DDSConv residual block.  The posterior encoder and normalizing flow module have four coupling layers of neural spline flows.  Each coupling layer first processes input and input conditions through a DDSConv block and produces 29-channel parameters that are used to construct 10 rational-quadratic functions.  We set the hidden dimension of all coupling layers and condition encoders to 192.  Fig.06a and Fig.6b show the architecture of a condition encoder and a coupling layer used in the stochastic duration predictor.</p> <p></p> <p>\u6240\u6709\u7684\u8f93\u5165\u6761\u4ef6\u901a\u8fc7\u6761\u4ef6\u7f16\u7801\u5668\u5904\u7406, \u6bcf\u4e00\u4e2a\u7531\u4e24\u4e2a 1x1 \u5377\u79ef\u5c42\u548c DDSConv \u6b8b\u5dee\u5757\u7ec4\u6210. \u540e\u9a8c\u7f16\u7801\u5668\u548c\u6807\u51c6\u5316\u6d41\u6a21\u5757\u90fd\u6709\u56db\u4e2a\u8026\u5408\u5c42\u7684\u795e\u7ecf\u6837\u6761\u6d41. \u6bcf\u4e2a\u8026\u5408\u5c42\u9996\u5148\u901a\u8fc7 DDSConv \u5757\u5904\u7406\u8f93\u5165\u548c\u8f93\u5165\u6761\u4ef6, \u5e76\u4ea7\u751f 29 \u901a\u9053\u7684\u53c2\u6570, \u7528\u4e8e\u6784\u9020 10 \u4e2a\u5206\u6bb5\u4e8c\u6b21\u51fd\u6570. \u6211\u4eec\u5c06\u6240\u6709\u8026\u5408\u5c42\u548c\u6761\u4ef6\u7f16\u7801\u5668\u7684\u9690\u85cf\u7ef4\u5ea6\u8bbe\u7f6e\u4e3a 192. \u56fe 06a \u548c\u56fe 06b \u5c55\u793a\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u4e2d\u6761\u4ef6\u7f16\u7801\u5668\u548c\u8026\u5408\u5c42\u7684\u67b6\u6784.</p> <pre><code>class StochasticDurationPredictor(nn.Module):\n        def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\n        super().__init__()\n        filter_channels = in_channels # it needs to be removed from future version.\n        self.in_channels = in_channels\n        self.filter_channels = filter_channels\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.n_flows = n_flows\n        self.gin_channels = gin_channels\n\n        self.log_flow = modules.Log()\n        self.flows = nn.ModuleList()\n        self.flows.append(modules.ElementwiseAffine(2))\n        for i in range(n_flows):\n            self.flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n            self.flows.append(modules.Flip())\n\n        self.post_pre = nn.Conv1d(1, filter_channels, 1)\n        self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\n        self.post_convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n        self.post_flows = nn.ModuleList()\n        self.post_flows.append(modules.ElementwiseAffine(2))\n        for i in range(4):\n            self.post_flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n            self.post_flows.append(modules.Flip())\n\n        self.pre = nn.Conv1d(in_channels, filter_channels, 1)\n        self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\n        self.convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\n\n        def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\n        x = torch.detach(x)\n        x = self.pre(x)\n        if g is not None:\n            g = torch.detach(g)\n            x = x + self.cond(g)\n        x = self.convs(x, x_mask)\n        x = self.proj(x) * x_mask\n\n        if not reverse:\n            flows = self.flows\n            assert w is not None\n\n            logdet_tot_q = 0 \n            h_w = self.post_pre(w)\n            h_w = self.post_convs(h_w, x_mask)\n            h_w = self.post_proj(h_w) * x_mask\n            e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n            z_q = e_q\n            for flow in self.post_flows:\n            z_q, logdet_q = flow(z_q, x_mask, g=(x + h_w))\n            logdet_tot_q += logdet_q\n            z_u, z1 = torch.split(z_q, [1, 1], 1) \n            u = torch.sigmoid(z_u) * x_mask\n            z0 = (w - u) * x_mask\n            logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1,2])\n            logq = torch.sum(-0.5 * (math.log(2*math.pi) + (e_q**2)) * x_mask, [1,2]) - logdet_tot_q\n\n            logdet_tot = 0\n            z0, logdet = self.log_flow(z0, x_mask)\n            logdet_tot += logdet\n            z = torch.cat([z0, z1], 1)\n            for flow in flows:\n            z, logdet = flow(z, x_mask, g=x, reverse=reverse)\n            logdet_tot = logdet_tot + logdet\n            nll = torch.sum(0.5 * (math.log(2*math.pi) + (z**2)) * x_mask, [1,2]) - logdet_tot\n            return nll + logq # [b]\n        else:\n            flows = list(reversed(self.flows))\n            flows = flows[:-2] + [flows[-1]] # remove a useless vflow\n            z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\n            for flow in flows:\n            z = flow(z, x_mask, g=x, reverse=reverse)\n            z0, z1 = torch.split(z, [1, 1], 1)\n            logw = z0\n            return logw\n</code></pre>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#4experiments","title":"4.Experiments\u00b7\u5b9e\u9a8c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#41datasets","title":"4.1.Datasets\u00b7\u6570\u636e\u96c6","text":"<p>We conducted experiments on two different datasets. We used the LJ Speech dataset for comparison with other publicly available models and the VCTK dataset to verify whether our model can learn and express diverse speech characteristics. The LJ Speech dataset consists of 13,100 short audio clips of a single speaker with a total length of approximately 24 hours. The audio format is 16-bit PCM with a sample rate of 22 kHz, and we used it without any manipulation. We randomly split the dataset into a training set (12,500 samples), validation set (100 samples), and test set (500 samples). The VCTK dataset consists of approximately 44,000 short audio clips uttered by 109 native English speakers with various accents. The total length of the audio clips is approximately 44 hours. The audio format is 16-bit PCM with a sample rate of 44 kHz. We reduced the sample rate to 22 kHz. We randomly split the dataset into a training set (43,470 samples), validation set (100 samples), and test set (500 samples).</p> <p>\u6211\u4eec\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c. \u6211\u4eec\u4f7f\u7528 LJ Speech \u6570\u636e\u96c6\u7528\u4e8e\u548c\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4, \u7136\u540e\u7528 VCTK \u6570\u636e\u96c6\u9a8c\u8bc1\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u5b66\u4e60\u548c\u8868\u8fbe\u591a\u6837\u5316\u7684\u8bed\u97f3\u7279\u5f81.</p> <p>LJ Speech \u6570\u636e\u96c6\u7531\u5355\u4e2a\u8bf4\u8bdd\u4eba\u7684 13,100 \u77ed\u97f3\u9891\u7247\u6bb5\u7ec4\u6210, \u603b\u65f6\u957f\u7ea6\u4e3a 24 \u5c0f\u65f6. \u97f3\u9891\u683c\u5f0f\u4e3a 16 \u4f4d PCM, \u91c7\u6837\u7387\u4e3a 22 kHz, \u6211\u4eec\u6ca1\u6709\u5bf9\u5176\u8fdb\u884c\u4efb\u4f55\u5904\u7406. \u6211\u4eec\u968f\u673a\u5c06\u6570\u636e\u96c6\u5206\u4e3a\u8bad\u7ec3\u96c6 (12,500 \u4e2a\u6837\u672c), \u9a8c\u8bc1\u96c6 (100 \u4e2a\u6837\u672c), \u548c\u6d4b\u8bd5\u96c6 (500 \u4e2a\u6837\u672c).</p> <p>VCTK \u6570\u636e\u96c6\u7531 109 \u540d\u82f1\u8bed\u6bcd\u8bed\u53d1\u8a00\u4eba\u53d1\u51fa\u7684\u7ea6 44,000 \u77ed\u97f3\u9891\u7247\u6bb5\u7ec4\u6210, \u603b\u65f6\u957f\u7ea6\u4e3a 44 \u5c0f\u65f6. \u97f3\u9891\u683c\u5f0f\u4e3a 16 \u4f4d PCM, \u91c7\u6837\u7387\u4e3a 44 kHz. \u6211\u4eec\u5c06\u91c7\u6837\u7387\u964d\u4f4e\u5230 22 kHz. \u6211\u4eec\u968f\u673a\u5c06\u6570\u636e\u96c6\u5206\u4e3a\u8bad\u7ec3\u96c6 (43,470 \u4e2a\u6837\u672c), \u9a8c\u8bc1\u96c6 (100 \u4e2a\u6837\u672c), \u548c\u6d4b\u8bd5\u96c6 (500 \u4e2a\u6837\u672c).</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#42preprocessing","title":"4.2.Preprocessing\u00b7\u9884\u5904\u7406","text":"<p>We use linear spectrograms which can be obtained from raw waveforms through the Short-time Fourier transform (STFT), as input of the posterior encoder. The FFT size, window size and hop size of the transform are set to 1024, 1024 and 256, respectively. We use 80 bands mel-scale spectrograms for reconstruction loss, which is obtained by applying a mel-filter bank to linear spectrograms. We use International Phonetic Alphabet (IPA) sequences as input to the prior encoder. We convert text sequences to IPA phoneme sequences using open-source software (Bernard, 2021), and the converted sequences are interspersed with a blank token following the implementation of Glow-TTS.</p> <p>\u6211\u4eec\u4f7f\u7528\u7ebf\u6027\u9891\u8c31\u56fe, \u5b83\u53ef\u4ee5\u4ece\u539f\u59cb\u6ce2\u5f62\u901a\u8fc7\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362 (STFT) \u83b7\u5f97. FFT \u5927\u5c0f, \u7a97\u53e3\u5927\u5c0f\u548c\u8df3\u8dc3\u5927\u5c0f\u5206\u522b\u8bbe\u7f6e\u4e3a 1024, 1024 \u548c 256. \u6211\u4eec\u4f7f\u7528 80 bands \u6885\u5c14\u9891\u8c31\u56fe\u7528\u4e8e\u8ba1\u7b97\u91cd\u6784\u635f\u5931, \u5b83\u662f\u901a\u8fc7\u5bf9\u7ebf\u6027\u9891\u8c31\u56fe\u5e94\u7528\u6885\u5c14\u6ee4\u6ce2\u5668\u83b7\u5f97\u7684. \u6211\u4eec\u4f7f\u7528 IPA \u5e8f\u5217\u4f5c\u4e3a\u5148\u9a8c\u7f16\u7801\u5668\u7684\u8f93\u5165. \u6211\u4eec\u4f7f\u7528\u5f00\u6e90\u8f6f\u4ef6\u5c06\u6587\u672c\u5e8f\u5217\u8f6c\u6362\u4e3a IPA \u97f3\u7d20\u5e8f\u5217, \u5e76\u9075\u5faa Glow-TTS \u7684\u5b9e\u73b0, \u5728\u97f3\u7d20\u5e8f\u5217\u4e4b\u95f4\u63d2\u5165\u7a7a\u767d Token.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#43training","title":"4.3.Training\u00b7\u8bad\u7ec3","text":"<p>The networks are trained using the AdamW optimizer with $\\beta_1= 0.8$, $\\beta_2=0.99$ and weight decay $\\lambda = 0.01$. The learning rate decay is scheduled by a 0.9991/8 factor in every epoch with an initial learning rate of $2\\times 10^{\u22124}$. Following previous work (FastSpeech 2s (2020); EATS (2020)), we adopt the windowed generator training, a method of generating only a part of raw waveforms to reduce the training time and memory usage during training. We randomly extract segments of latent representations with a window size of 32 to feed to the decoder instead of feeding entire latent representations and also extract the corresponding audio segments from the ground truth raw waveforms as training targets. We use mixed precision training on 4 NVIDIA V100 GPUs. The batch size is set to 64 per GPU and the model is trained up to 800k steps.</p> <p>\u7f51\u7edc\u91c7\u7528 AdamW \u4f18\u5316\u5668\u8fdb\u884c\u8bad\u7ec3, \u5176\u53c2\u6570\u4e3a $\\beta_1= 0.8$, $\\beta_2=0.99$ \u548c\u6743\u91cd\u8870\u51cf $\\lambda = 0.01$. \u5b66\u4e60\u7387\u8870\u51cf\u662f\u6bcf\u4e2a Epoch \u6309 0.9991/8 \u56e0\u5b50\u8fdb\u884c\u8c03\u5ea6, \u521d\u59cb\u5b66\u4e60\u7387\u4e3a $2\\times 10^{\u22124}$. \u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u7c7b\u4f3c, \u6211\u4eec\u91c7\u7528\u7a97\u53e3\u751f\u6210\u5668\u8bad\u7ec3, \u4e00\u79cd\u53ea\u751f\u6210\u90e8\u5206\u539f\u59cb\u6ce2\u5f62\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u5360\u7528. \u6211\u4eec\u968f\u673a\u63d0\u53d6 32 \u957f\u5ea6\u7684\u9690\u8868\u793a\u7247\u6bb5, \u5e76\u5c06\u5176\u8f93\u5165\u5230\u89e3\u7801\u5668, \u800c\u4e0d\u662f\u8f93\u5165\u6574\u4e2a\u9690\u8868\u793a, \u5e76\u4e14\u63d0\u53d6\u76f8\u5e94\u7684\u771f\u5b9e\u97f3\u9891\u7247\u6bb5\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807. \u6211\u4eec\u5728 4 \u5757 NVIDIA V100 GPU \u4e0a\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3. \u6bcf\u5757 GPU \u7684\u6279\u91cf\u5927\u5c0f\u8bbe\u7f6e\u4e3a 64, \u6a21\u578b\u8bad\u7ec3 800k \u6b65.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#44experiment-setup-for-comparison","title":"4.4.Experiment Setup for Comparison\u00b7\u5bf9\u6bd4\u5b9e\u9a8c\u8bbe\u7f6e","text":"<p>We compared our model with the best publicly available models. We used Tacotron 2, an autoregressive model, and Glow-TTS, a flow-based non-autoregressive model, as first stage models and HiFi-GAN as a second stage model. We used their public implementations and pre-trained weights. Since a two-stage TTS system can theoretically achieve higher synthesis quality through sequential training, we included the fine-tuned HiFi-GAN up to 100k steps with the predicted outputs from the first stage models. We empirically found that fine-tuning HiFi-GAN with the generated mel-spectrograms from Tacotron 2 under teacher-forcing mode, led to better quality for both Tacotron 2 and Glow-TTS than fine-tuning with the generated mel-spectrograms from Glow-TTS, so we appended the better fine-tuned HiFi-GAN to both Tacotron 2 and Glow-TTS. As each model has a degree of randomness during sampling, we fixed hyper-parameters that controls the randomness of each model throughout our experiments. The probability of dropout in the pre-net of Tactron 2 was set to 0.5. For Glow-TTS, the standard deviation of the prior distribution was set to 0.333. For VITS, the standard deviation of input noise of the stochastic duration predictor was set to 0.8 and we multiplied a scale factor of 0.667 to the standard deviation of the prior distribution.</p> <p>\u6211\u4eec\u5c06\u6a21\u578b\u548c\u6700\u4f73\u7684\u516c\u5f00\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83. \u6211\u4eec\u4f7f\u7528\u81ea\u56de\u5f52\u6a21\u578b Tacotron2, \u57fa\u4e8e\u6d41\u7684\u975e\u81ea\u56de\u5f52\u6a21\u578b Glow-TTS \u4f5c\u4e3a\u4e00\u9636\u6bb5\u6a21\u578b, HiFi-GAN \u4f5c\u4e3a\u4e8c\u9636\u6bb5\u6a21\u578b. \u6211\u4eec\u4f7f\u7528\u5b83\u4eec\u7684\u516c\u5f00\u5b9e\u73b0\u548c\u9884\u8bad\u7ec3\u6743\u91cd. \u7531\u4e8e\u4e24\u9636\u6bb5 TTS \u7cfb\u7edf\u7406\u8bba\u4e0a\u53ef\u4ee5\u901a\u8fc7\u4e32\u884c\u8bad\u7ec3\u5b9e\u73b0\u66f4\u9ad8\u7684\u5408\u6210\u8d28\u91cf, \u6211\u4eec\u5c06 HiFi-GAN \u4f7f\u7528 Tacotron2 \u548c Glow-TTS \u7684\u9884\u6d4b\u8f93\u51fa\u5fae\u8c03\u5230 100k \u6b65. \u6211\u4eec\u7ecf\u9a8c\u6027\u5730\u53d1\u73b0, \u4f7f\u7528 Tacotron2 \u751f\u6210\u7684\u6885\u5c14\u9891\u8c31\u56fe\u4f5c\u4e3a\u6559\u5e08\u5f3a\u5236\u6a21\u5f0f, \u5fae\u8c03 HiFi-GAN \u80fd\u4ea7\u751f\u66f4\u597d\u7684\u8d28\u91cf, \u56e0\u6b64\u6211\u4eec\u5c06\u66f4\u597d\u7684\u5fae\u8c03 HiFi-GAN \u8ffd\u52a0\u5230 Tacotron2 \u548c Glow-TTS. \u7531\u4e8e\u6bcf\u4e2a\u6a21\u578b\u5728\u91c7\u6837\u65f6\u90fd\u6709\u4e00\u5b9a\u7684\u968f\u673a\u6027, \u6211\u4eec\u5728\u5b9e\u9a8c\u4e2d\u56fa\u5b9a\u4e86\u6bcf\u4e2a\u6a21\u578b\u7684\u8d85\u53c2\u6570, \u4ee5\u63a7\u5236\u968f\u673a\u6027. Tacotron2 \u7684\u9884-\u7f51\u7edc\u7684\u4e22\u5f03\u6982\u7387\u8bbe\u7f6e\u4e3a 0.5. Glow-TTS \u7684\u5148\u9a8c\u5206\u5e03\u7684\u6807\u51c6\u5dee\u8bbe\u7f6e\u4e3a 0.333. VITS \u7684\u8f93\u5165\u566a\u58f0\u7684\u6807\u51c6\u5dee\u8bbe\u7f6e\u4e3a 0.8, \u5e76\u5c06\u5148\u9a8c\u5206\u5e03\u7684\u6807\u51c6\u5dee\u4e58\u4ee5 0.667.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#5results","title":"5.Results\u00b7\u7ed3\u679c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#51speech-synthesis-quality","title":"5.1.Speech Synthesis Quality\u00b7\u8bed\u97f3\u5408\u6210\u8d28\u91cf","text":"<p>We conducted crowd-sourced MOS tests to evaluate the quality. Raters listened to randomly selected audio samples, and rated their naturalness on a 5 point scale from 1 to 5. Raters were allowed to evaluate each audio sample once, and we normalized all the audio clips to avoid the effect of amplitude differences on the score. All of the quality assessments in this work were conducted in this manner. The evaluation results are shown in Tab.01.</p> <p>\u6211\u4eec\u8fdb\u884c\u4e86\u4f17\u5305 MOS \u6d4b\u8bd5, \u4ee5\u8bc4\u4f30\u8d28\u91cf. \u8bc4\u5206\u5458\u968f\u673a\u9009\u62e9\u4e86\u97f3\u9891\u6837\u672c, \u5e76\u5bf9\u5176\u81ea\u7136\u5ea6\u8fdb\u884c\u4e86 5 \u7ea7\u8bc4\u5206, 1 \u5230 5 \u4e4b\u95f4. \u8bc4\u5206\u5458\u4ec5\u5141\u8bb8\u5bf9\u6bcf\u4e2a\u97f3\u9891\u6837\u672c\u8fdb\u884c\u4e00\u6b21\u8bc4\u5206, \u6211\u4eec\u5bf9\u6240\u6709\u97f3\u9891\u7247\u6bb5\u8fdb\u884c\u4e86\u5f52\u4e00\u5316, \u4ee5\u907f\u514d\u632f\u5e45\u5dee\u5f02\u5bf9\u5f97\u5206\u7684\u5f71\u54cd. \u672c\u6587\u4e2d\u6240\u6709\u8d28\u91cf\u8bc4\u4f30\u90fd\u91c7\u7528\u4e86\u8fd9\u79cd\u65b9\u5f0f. \u8868 01 \u663e\u793a\u4e86\u8bc4\u4f30\u7ed3\u679c.</p> <p>Tab.01.Comparison of evaluated MOS with 95% confidence intervals on the LJ Speech dataset.</p> Model MOS (CI) Ground Truth 4.46 (\u00b10.06) Tacotron 2 + HiFi-GAN 3.77 (\u00b10.08) Tacotron 2 + HiFi-GAN (Fine-tuned) 4.25 (\u00b10.07) Glow-TTS + HiFi-GAN 4.14 (\u00b10.07) Glow-TTS + HiFi-GAN (Fine-tuned) 4.32 (\u00b10.07) VITS (DDP) 4.39 (\u00b10.06) VITS 4.43 (\u00b10.06) <p>VITS outperforms other TTS systems and achieves a similar MOS to that of ground truth. The VITS (DDP), which employs the same deterministic duration predictor architecture used in Glow-TTS rather than the stochastic duration predictor, scores the second-highest among TTS systems in the MOS evaluation. These results imply that 1) the stochastic duration predictor generates more realistic phoneme duration than the deterministic duration predictor and 2) our end-to-end training method is an effective way to make better samples than other TTS models even if maintaining the similar duration predictor architecture.  </p> <p>VITS \u4f18\u4e8e\u5176\u4ed6 TTS \u7cfb\u7edf, \u5e76\u8fbe\u5230\u548c\u771f\u5b9e\u503c\u76f8\u4f3c\u7684 MOS. VITS (DDP) \u4f7f\u7528\u548c Glow-TTS \u76f8\u540c\u7684\u786e\u5b9a\u6027\u65f6\u957f\u9884\u6d4b\u5668\u67b6\u6784\u800c\u4e0d\u662f\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u5728 MOS \u8bc4\u4f30\u4e2d\u6392\u540d\u7b2c\u4e8c. \u8fd9\u4e9b\u7ed3\u679c\u8868\u660e: 1. \u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u6bd4\u786e\u5b9a\u6027\u65f6\u957f\u9884\u6d4b\u5668\u751f\u6210\u66f4\u771f\u5b9e\u7684\u97f3\u7d20\u65f6\u957f, 2. \u6211\u4eec\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5, \u5373\u4f7f\u4fdd\u6301\u76f8\u4f3c\u7684\u65f6\u957f\u9884\u6d4b\u5668\u67b6\u6784, \u4e5f\u80fd\u4ea7\u751f\u66f4\u597d\u7684\u6837\u672c.</p> <p>We conducted an ablation study to demonstrate the effectiveness of our methods, including the normalized flow in the prior encoder and linear-scale spectrogram posterior input. All models in the ablation study were trained up to 300k steps. The results are shown in Tab.02.</p> <p>\u6211\u4eec\u8fdb\u884c\u4e86\u6d88\u878d\u5b9e\u9a8c\u4ee5\u8bf4\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027, \u5305\u62ec\u5148\u9a8c\u7f16\u7801\u5668\u4e2d\u7684\u5f52\u4e00\u5316\u6d41\u548c\u7ebf\u6027\u9891\u8c31\u56fe\u540e\u9a8c\u8f93\u5165. \u6d88\u878d\u5b9e\u9a8c\u4e2d\u6240\u6709\u6a21\u578b\u90fd\u8bad\u7ec3\u4e86 300k \u6b65. \u8868 02 \u663e\u793a\u4e86\u7ed3\u679c.</p> <p>Tab.02. MOS comparison in the ablation studies.</p> Model MOS (CI) Ground Truth 4.50 (\u00b10.06) Baseline 4.50 (\u00b10.06) without Normalizing Flow 2.98 (\u00b10.08) with Mel-spectrogram 4.31 (\u00b10.08) <p>Removing the normalizing flow in the prior encoder results in a 1.52 MOS decrease from the baseline, demonstrating that the prior distribution\u2019s flexibility significantly influences the synthesis quality. Replacing the linear-scale spectrogram for posterior input with the mel-spectrogram results in a quality degradation (-0.19 MOS), indicating that the high-resolution information is effective for VITS in improving the synthesis quality.</p> <p>\u79fb\u9664\u5148\u9a8c\u7f16\u7801\u5668\u4e2d\u7684\u6807\u51c6\u5316\u6d41\u4f1a\u964d\u4f4e 1.52 \u7684 MOS \u5f97\u5206, \u8bf4\u660e\u5148\u9a8c\u5206\u5e03\u7684\u7075\u6d3b\u6027\u5bf9\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd. \u7528\u6885\u5c14\u9891\u8c31\u56fe\u66ff\u6362\u7ebf\u6027\u9891\u8c31\u56fe\u540e\u9a8c\u8f93\u5165\u4f1a\u5bfc\u81f4\u8d28\u91cf\u964d\u4f4e (-0.19 MOS), \u8868\u660e\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\u5bf9\u4e8e VITS \u5728\u63d0\u5347\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u6709\u4f5c\u7528.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#52generalization-to-multi-speaker-text-to-speech","title":"5.2.Generalization to Multi-Speaker Text-to-Speech\u00b7\u591a\u8bf4\u8bdd\u4eba\u6587\u672c\u8f6c\u8bed\u97f3","text":"<p>To verify that our model can learn and express diverse speech characteristics, we compared our model to Tacotron 2, Glow-TTS and HiFi-GAN, which showed the ability to extend to multi-speaker speech synthesis (Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis; Glow-TTS (2020); HiFi-GAN (2020)). We trained the models on the VCTK dataset. We added speaker embedding to our model as described in Section 2.5. For Tacotron 2, we broadcasted speaker embedding and concatenated it with the encoder output, and for Glow-TTS, we applied the global conditioning following the previous work. The evaluation method is the same as that described in Section 4.1. As shown in Tab.03, our model achieves a higher MOS than the other models.</p> <p>\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u548c\u8868\u793a\u591a\u6837\u7684\u8bed\u97f3\u7279\u6027, \u6211\u4eec\u5c06\u6a21\u578b\u4e0e Tacotron 2, Glow-TTS \u548c HiFi-GAN \u8fdb\u884c\u4e86\u6bd4\u8f83, \u5b83\u4eec\u90fd\u5177\u6709\u6269\u5c55\u5230\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u5408\u6210\u80fd\u529b. \u6211\u4eec\u5728 VCTK \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u6a21\u578b. \u6211\u4eec\u5728\u6a21\u578b\u4e2d\u52a0\u5165\u4e86\u8bf4\u8bdd\u4eba\u5d4c\u5165, \u5982\u7b2c 2.5 \u8282\u6240\u8ff0. \u5bf9\u4e8e Tacotron 2, \u6211\u4eec\u5e7f\u64ad\u4e86\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5e76\u5c06\u5176\u4e0e\u7f16\u7801\u5668\u8f93\u51fa\u8fde\u63a5\u8d77\u6765, \u5bf9\u4e8e Glow-TTS, \u6211\u4eec\u9075\u5faa\u4e4b\u524d\u7684\u5de5\u4f5c, \u5e94\u7528\u5168\u5c40\u6761\u4ef6. \u8bc4\u4f30\u65b9\u6cd5\u4e0e\u7b2c 4.1 \u8282\u4e2d\u63cf\u8ff0\u7684\u76f8\u540c. \u5982\u8868 3 \u6240\u793a, \u6211\u4eec\u7684\u6a21\u578b\u8d85\u8fc7\u4e86\u5176\u4ed6\u6a21\u578b.</p> <p>Table 3.Comparison of evaluated MOS with 95% confidence intervals on the VCTK dataset.</p> Model MOS (CI) Ground Truth 4.38 (\u00b10.07) Tacotron 2 + HiFi-GAN 3.14 (\u00b10.09) Tacotron 2 + HiFi-GAN (Fine-tuned) 3.19 (\u00b10.09) Glow-TTS + HiFi-GAN 3.76 (\u00b10.07) Glow-TTS + HiFi-GAN (Fine-tuned) 3.82 (\u00b10.07) VITS 4.38 (\u00b10.06) <p>This demonstrates that our model learns and expresses various speech characteristics in an end-to-end manner.  </p> <p>\u8fd9\u8868\u660e, \u6211\u4eec\u7684\u6a21\u578b\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u5b66\u4e60\u548c\u8868\u793a\u5404\u79cd\u8bed\u97f3\u7279\u6027.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#53speech-variation","title":"5.3.Speech Variation\u00b7\u8bed\u97f3\u53d8\u5316","text":"<p>We verified how many different lengths of speech the stochastic duration predictor produces, and how many different speech characteristics the synthesized samples have. Similar to Flowtron (2020), all samples here were generated from a sentence \u201cHow much variation is there?\u201d. Fig.02a shows histograms of the lengths of 100 generated utterances from each model. While Glow-TTS generates only fixed-length utterances due to the deterministic duration predictor, samples from our model follow a similar length distribution to that of Tacotron 2. Fig.02b shows the lengths of 100 utterances generated with each of five speaker identities from our model in the multi-speaker setting, implying that the model learns the speaker-dependent phoneme duration. F0 contours of 10 utterances extracted with the YIN algorithm (a fundamental frequency estimator for speech and music) in Fig.03 shows that our model generates speech with diverse pitches and rhythms, and five samples generated with each of different speaker identities in Fig.03d demonstrates our model expresses very different lengths and pitches of speech for each speaker identity. Note that Glow-TTS could increase the diversity of pitch by increasing the standard deviation of the prior distribution, but on the contrary, it could lower the synthesis quality.</p> <p>\u6211\u4eec\u9a8c\u8bc1\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u751f\u6210\u7684\u8bed\u97f3\u957f\u5ea6\u6709\u591a\u5c11\u79cd, \u5408\u6210\u7684\u6837\u672c\u6709\u591a\u5c11\u79cd\u4e0d\u540c\u7684\u8bed\u97f3\u7279\u6027. \u4e0e Flowtron (2020) \u7c7b\u4f3c, \u8fd9\u91cc\u6240\u6709\u7684\u6837\u672c\u90fd\u6765\u81ea\u4e8e\u4e00\u53e5\u8bdd \"How much variation is there?\". \u56fe 2a \u663e\u793a\u4e86\u6765\u81ea\u6bcf\u4e2a\u6a21\u578b\u7684 100 \u4e2a\u5408\u6210\u53e5\u5b50\u7684\u957f\u5ea6\u76f4\u65b9\u56fe. \u867d\u7136 Glow-TTS \u7531\u4e8e\u786e\u5b9a\u6027\u65f6\u957f\u9884\u6d4b\u5668\u800c\u4ea7\u751f\u56fa\u5b9a\u957f\u5ea6\u7684\u53e5\u5b50, \u4f46\u6765\u81ea\u6211\u4eec\u7684\u6a21\u578b\u7684\u6837\u672c\u4e0e Tacotron 2 \u7684\u6837\u672c\u6709\u7740\u76f8\u4f3c\u7684\u957f\u5ea6\u5206\u5e03. \u56fe 2b \u663e\u793a\u4e86\u6765\u81ea\u6211\u4eec\u7684\u6a21\u578b\u7684 100 \u4e2a\u53e5\u5b50\u7684\u957f\u5ea6, \u8fd9\u8868\u660e\u6a21\u578b\u5b66\u4e60\u4e86\u8bf4\u8bdd\u4eba\u76f8\u5173\u7684\u97f3\u7d20\u65f6\u957f. \u56fe 3 \u663e\u793a\u4e86\u4f7f\u7528 YIN \u7b97\u6cd5\u63d0\u53d6\u7684 10 \u4e2a\u53e5\u5b50\u7684 F0 \u8f6e\u5ed3, \u8fd9\u8868\u660e\u6211\u4eec\u7684\u6a21\u578b\u751f\u6210\u4e86\u5177\u6709\u4e0d\u540c\u97f3\u9ad8\u548c\u97f5\u5f8b\u7684\u8bed\u97f3, \u56fe 3d \u663e\u793a\u4e86\u6765\u81ea\u4e94\u4e2a\u4e0d\u540c\u8bf4\u8bdd\u4eba\u6807\u8bc6\u7684 10 \u4e2a\u53e5\u5b50, \u8fd9\u8868\u660e\u6a21\u578b\u4e3a\u6bcf\u4e2a\u8bf4\u8bdd\u4eba\u6807\u8bc6\u751f\u6210\u4e86\u4e0d\u540c\u7684\u8bed\u97f3\u957f\u5ea6\u548c\u97f3\u9ad8. \u6ce8\u610f\u5230 Glow-TTS \u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u5148\u9a8c\u5206\u5e03\u7684\u6807\u51c6\u5dee\u6765\u589e\u52a0\u97f3\u9ad8\u7684\u591a\u6837\u6027, \u4f46\u76f8\u53cd, \u5b83\u4f1a\u964d\u4f4e\u5408\u6210\u8d28\u91cf.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#54synthesis-speed","title":"5.4.Synthesis Speed\u00b7\u5408\u6210\u901f\u5ea6","text":"<p>We compared the synthesis speed of our model with a parallel two-stage TTS system, Glow-TTS and HiFi-GAN. We measured the synchronized elapsed time over the entire process to generate raw waveforms from phoneme sequences with 100 sentences randomly selected from the test set of the LJ Speech dataset. We used a single NVIDIA V100 GPU with a batch size of 1. The results are shown in Table 4. Since our model does not require modules for generating predefined intermediate representations, its sampling efficiency and speed are greatly improved.</p> <p>\u6211\u4eec\u6bd4\u8f83\u4e86\u6211\u4eec\u7684\u6a21\u578b\u4e0e Glow-TTS \u548c HiFi-GAN \u7684\u5408\u6210\u901f\u5ea6. \u6211\u4eec\u6d4b\u91cf\u4e86\u4ece LJ Speech \u6d4b\u8bd5\u96c6\u4e2d\u968f\u673a\u9009\u62e9 100 \u4e2a\u53e5\u5b50\u7684\u97f3\u7d20\u5e8f\u5217\u751f\u6210\u539f\u59cb\u6ce2\u5f62\u6240\u9700\u7684\u65f6\u95f4. \u6211\u4eec\u4f7f\u7528\u5355\u4e2a NVIDIA V100 GPU, \u6279\u5927\u5c0f\u4e3a 1. \u7ed3\u679c\u5982\u8868 4 \u6240\u793a. \u7531\u4e8e\u6211\u4eec\u7684\u6a21\u578b\u4e0d\u9700\u8981\u751f\u6210\u9884\u5b9a\u4e49\u7684\u4e2d\u95f4\u8868\u793a, \u5176\u91c7\u6837\u6548\u7387\u548c\u901f\u5ea6\u5927\u5e45\u63d0\u9ad8.</p> <p>Table 4.Comparison of the synthesis speed.  Speed of n kHz means that the model can generate n\u00d71000 raw audio samples per second.  Real-time means the synthesis speed over real-time.</p> Model Speed (kHz) Real-time Glow-TTS + HiFi-GAN 606.05 \u00d727.48 VITS 1480.15 \u00d767.12 VITS (DDP) 2005.03 \u00d790.93","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#6conclusion","title":"6.Conclusion\u00b7\u7ed3\u8bba","text":"<p>In this work, we proposed a parallel TTS system, VITS, that can learn and generate in an end-to-end manner. We further introduced the stochastic duration predictor to express diverse rhythms of speech. The resulting system synthesizes natural sounding speech waveforms directly from text, without having to go through predefined intermediate speech representations. Our experimental results show that our method outperforms two-stage TTS systems and achieves close to human quality. We hope the proposed method will be used in many speech synthesis tasks, where two-stage TTS systems have been used, to achieve performance improvement and enjoy the simplified training procedure. We also want to point out that even though our method integrates two separated generative pipelines in TTS systems, there remains a problem of text preprocessing. Investigating self-supervised learning of language representations could be a possible direction for removing the text preprocessing step. We will release our source-code and pre-trained models to facilitate research in plenty of future directions.</p> <p>\u672c\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e76\u884c\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf, VITS, \u80fd\u591f\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u5b66\u4e60\u548c\u751f\u6210\u8bed\u97f3. \u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u4ee5\u751f\u6210\u5177\u6709\u591a\u6837\u97f5\u5f8b\u7684\u8bed\u97f3. \u8be5\u7cfb\u7edf\u80fd\u76f4\u63a5\u4ece\u6587\u672c\u5408\u6210\u81ea\u7136\u7684\u8bed\u97f3\u6ce2\u5f62, \u800c\u4e0d\u9700\u8981\u7ecf\u8fc7\u9884\u5148\u5b9a\u4e49\u7684\u4e2d\u95f4\u8bed\u97f3\u8868\u793a. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, \u6211\u4eec\u7684\u6a21\u578b\u4f18\u4e8e\u4e24\u9636\u6bb5\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf, \u5e76\u8fbe\u5230\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u8d28\u91cf. \u6211\u4eec\u5e0c\u671b\u8be5\u65b9\u6cd5\u80fd\u591f\u7528\u4e8e\u8bb8\u591a\u4e4b\u524d\u5df2\u7ecf\u4f7f\u7528\u4e24\u9636\u6bb5\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u7684\u8bed\u97f3\u5408\u6210\u4efb\u52a1, \u4ee5\u63d0\u5347\u6027\u80fd\u5e76\u7b80\u5316\u8bad\u7ec3\u8fc7\u7a0b. \u6211\u4eec\u8fd8\u9700\u8981\u6307\u51fa\u5c3d\u7ba1\u6211\u4eec\u7684\u65b9\u6cd5\u5408\u5e76\u4e86\u4e24\u79cd\u5355\u72ec\u7684\u751f\u6210 pipelines, \u4f46\u4ecd\u7136\u5b58\u5728\u6587\u672c\u9884\u5904\u7406\u7684\u95ee\u9898. \u63a2\u7d22\u81ea\u76d1\u7763\u5b66\u4e60\u8bed\u8a00\u8868\u5f81\u53ef\u80fd\u662f\u6d88\u9664\u6587\u672c\u9884\u5904\u7406\u6b65\u9aa4\u7684\u4e00\u79cd\u53ef\u80fd\u65b9\u5411. \u6211\u4eec\u5c06\u4f1a\u53d1\u5e03\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b, \u4ee5\u4fc3\u8fdb\u7814\u7a76\u7684\u5e7f\u9614\u524d\u666f.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/","title":"StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Text-to-speech (TTS) synthesis has seen significant advancements in recent years, with numerous applications such as virtual assistants, audiobooks, and voice-over narration benefiting from increasingly natural and expressive synthetic speech [1,2]. Some previous works have made significant progress towards human-level performance [3,4,5]. However, the quest for robust and accessible human-level TTS synthesis remains an ongoing challenge because there is still room for improvement in terms of diverse and expressive speech [5,6], robustness for out-of-distribution (OOD) texts [7], and the requirements of massive datasets for high-performing zero-shot TTS systems [8].</p> <p>In this paper, we introduce StyleTTS 2, an innovative TTS model that builds upon the style-based generative model StyleTTS [6] to present the next step towards human-level TTS systems. We model speech styles as a latent random variable and sample them with a probabilistic diffusion model, allowing the model to efficiently synthesize highly realistic speech without the need for reference audio. Since it only needs to sample a style vector instead of the entire speech as a latent variable, StyleTTS 2 is faster than other diffusion TTS models while still benefiting from the diverse speech synthesis enabled by diffusion models. One of the key contributions of StyleTTS 2 is the use of large pre-trained speech language models (SLMs) like Wav2Vec 2.0 [9], HuBERT [10], and WavLM [11] as discriminators, in conjunction with a novel differentiable duration modeling approach. This end-to-end (E2E) training setup leverages SLM representations to enhance the naturalness of the synthesized speech, transferring knowledge from large SLMs for speech generation tasks. </p> <p>Our evaluations suggest that speech generated by StyleTTS 2 surpasses human recordings as judged by native English speakers on the benchmark LJSpeech [12] dataset with statistically significant comparative mean opinion scores (CMOS) of+0.28 (p &lt; 0.05). Additionally, StyleTTS 2 advances the state-of-the-art by achieving a CMOS of+1.07 (p \u226a 0.01)compared to NaturalSpeech [5]. Furthermore, it attains human-level performance on the multispeaker VCTK dataset [13] in terms of naturalness (CMOS =\u22120.02,p \u226b 0.05) and similarity (CMOS =+0.30,p &lt; 0.1) to the reference speaker. When trained on a large number of speakers like the LibriTTS dataset [14], StyleTTS 2 demonstrates potential for speaker adaptation. It surpasses previous publicly available models in this task and outperforms Vall-E [8] in naturalness. Moreover, it achieves slightly worse similarity to the target speaker with only a 3-second reference speech, despite using around 250 times less data compared to Vall-E, making it a data-efficient alternative for large pre-training in the zero-shot speaker adaptation task. As the first model to achieve human-level performance on publicly available single and multispeaker datasets, StyleTTS 2 sets a new benchmark for TTS synthesis, highlighting the potential of style diffusion and adversarial training with SLMs for human-level TTS synthesis. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#2related-work","title":"2.Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#diffusion-models-for-speech-synthesis","title":"Diffusion Models for Speech Synthesis","text":"<p>Diffusion models have gained traction in speech synthesis due to their potential for diverse speech sampling and fine-grained speech control [15]. They have been applied to mel-based text-to-speech [16,17,18,19,20], mel-to-waveform vocoder [21,22, 23,24,25,26], and end-to-end speech generation [27,28,29]. However, their efficiency is limited compared to non-iterative methods, like GAN-based models [30,31,32], due to the need to iteratively sample mel-spectrograms, waveforms, or other latent representations proportional to the target speech duration [15]. Furthermore, recent works suggest that state-of-the-art GAN-based models still perform better than diffusion models in speech synthesis [26,33]. To address these limitations, we introduce style diffusion, where a fixed-length style vector is sampled by a diffusion model conditioned on the input text. This approach significantly improves model speed and enables end-to-end training. Notably, StyleTTS 2 synthesizes speech using GAN-based models, with only the style vector dictating the diversity of speech sampled. This unique combination allows StyleTTS 2 to achieve high-quality synthesis with fast inference speed while maintaining the benefits of diverse speech generation, further advancing the capabilities of diffusion models in speech synthesis. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#text-to-speech-with-large-speech-language-models","title":"Text-to-Speech with Large Speech Language Models","text":"<p>Recent advancements have proven the effectiveness of large-scale self-supervised speech language models (SLMs) in enhancing text-to-speech (TTS) quality [34,35,36,37] and speaker adaptation [8,38,29,39]. These works typically convert text input into either continuous or quantized representations derived from pre-trained SLMs for speech reconstruction. However, SLM features are not directly optimized for speech synthesis, while tuning SLMs as a neural codec [34,35,8,29] involves two-stage training. In contrast, our model benefits from the knowledge of large SLMs via adversarial training using SLM features without latent space mapping, thus directly learning a latent space optimized for speech synthesis like other end-to-end (E2E) TTS models. This innovative approach signifies a new direction in TTS with SLMs. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#human-level-text-to-speech","title":"Human-Level Text-to-Speech","text":"<p>Several recent works have advanced towards human-level TTS [3,4,5] using techniques like BERT pre-training [4,40,7] and E2E training [32,5] with differentiable duration modeling [41,42]. VITS [3] demonstrates MOS comparable to human recordings on the LJSpeech and VCTK datasets, while PnG-BERT [4] obtains human-level results on a proprietary dataset. NaturalSpeech [5], in particular, achieves both MOS and CMOS on LJSpeech statistically indistinguishable from human recordings. However, we find that there is still room for improvement in speech quality beyond these state-of-the-art models, as we attain higher performance and set a new standard for human-level TTS synthesis. Furthermore, recent work shows the necessity for disclosing the details of evaluation procedures for TTS research [43]. Our evaluation procedures are detailed in Appendix E, which can be used for reproducible future research toward human-level TTS. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#3method","title":"3.Method\u00b7\u65b9\u6cd5","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#31styletts-overview","title":"3.1.StyleTTS Overview","text":"<p>StyleTTS [6] is a non-autoregressive TTS framework using a style encoder to derive a style vector from reference audio, enabling natural and expressive speech generation. The style vector is incorporated into the decoder and duration and prosody predictors using adaptive instance normalization (AdaIN) [44], allowing the model to generate speech with varying duration, prosody, and emotions. The model comprises eight modules, organized into three categories: (1) a speech generation system (acoustic modules) with a text encoder, style encoder, and speech decoder; (2) a TTS prediction system with duration and prosody predictors; and (3) a utility system for training, including a discriminator, text aligner, and pitch extractor. It undergoes a two-stage training process: the first stage trains the acoustic modules for mel-spectrogram reconstruction, and the second trains TTS prediction modules using the fixed acoustic modules trained in the first stage.</p> <p>In the first stage, the text encoder $T$ encodes input phonemes $t$ into phoneme representations $h_{text}= T(t)$, while the text aligner $A$ extracts speech-phoneme alignment $a_{algn}= A(x, t)$ from input speech $x$ and phonemes $t$ to produce aligned phoneme representations $h_{algn}= h_{text}\\cdot a_{algn}$ via dot product. Concurrently, the style encoder $E$ obtains the style vector $s = E(x)$, and the pitch extractor $F$ extracts the pitch curve $p_x= F(x)$ along with its energy $n_x= |x|$. Lastly, the speech decoderG reconstructs $\\hat{x}=G(h_{algn}, s, p_x, n_x)$, which is trained to match input $x$ using a $L_1$ reconstruction loss $L_{mel}$ and adversarial objectives $L_{adv}$, $L_{fm}$ with a discriminator $D$. Transferable monotonic aligner (TMA) objectives are also applied to learn optimal alignments (see Appendix G for details).</p> <p>In the second stage, all components except the discriminatorDare fixed, with only the duration and prosody predictors being trained. The duration predictor $S$ predicts the phoneme duration with $d_{pred}= S(h_{text}, s)$, whereas the prosody predictor $P$ predicts pitch and energy as $\\hat{p}{x}, \\hat{n}{x}= P(h_{text}, s)$. The predicted duration is trained to match the ground truth duration $d_{gt}$ derived from the summed monotonic version of the alignment $a_{algn}$ along the time axis with an $L_1$ loss $L_{dur}$. The predicted pitch $\\hat{p}x$ and energy $\\hat{n}_x$ are trained to match the ground truth pitch $p_x$ and energy $n_x$ derived from pitch extractor $F$ with $L_1$ loss $L{f0}$ and $L_n$. During inference, $d_{pred}$ is used to upsample $h_{text}$ through $a_{pred}$, the predicted alignment, obtained by repeating the value 1 for $d_{pred[i]}$ times at $l_{i-1}$, where $l_i$ is the end position of the $i$-th phoneme $t_i$ calculated by summing $d_{pred}[k]$ for $k \\in {1, \\cdots, i}$, and $d_{pred}[i]$ are the predicted duration of $t_i$. The mel-spectrogram is synthesized by $x_{pred}= G(h_{text}\\cdot a_{pred}, E(\\tilde{x}), \\hat{p}{\\tilde{x}}, \\hat{n}{\\tilde{x}}$) with $\\tilde{x}$ an arbitrary reference audio that influences the style of $x_{pred}$, which is then converted into a waveform using a pre-trained vocoder.</p> <p>Despite its state-of-the-art performance in synthesizing diverse and controllable speech, StyleTTS has several drawbacks, such as a two-stage training process with an additional vocoding stage that degrades sample quality, limited expressiveness due to deterministic generation, and reliance on reference speech hindering real-time applications. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#32styletts2","title":"3.2.StyleTTS2","text":"<p>StyleTTS 2 improves upon the StyleTTS framework, resulting in a more expressive text-to-speech (TTS) synthesis model with human-level quality and improved out-of-distribution performance. We introduce an end-to-end (E2E) training process that jointly optimizes all components, along with direct waveform synthesis and adversarial training with large speech language models (SLMs) enabled by our innovative differentiable duration modeling. The speech style is modeled as a latent variable sampled through diffusion models, allowing diverse speech generation without reference audio. We outline these important changes in the following sections with an overview in Figure 1. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#321end-to-end-training","title":"3.2.1.End-to-End Training","text":"<p>E2E training optimizes all TTS system components for inference without relying on any fixed components like pre-trained vocoders that convert mel-spectrograms into waveforms [3,32]. To achieve this, we modify the decoderGto directly generate the waveform from the style vector, aligned phoneme representations, and pitch and energy curves. We remove the last projection layer for mel-spectrograms of the decoder and append a waveform decoder after it. We propose two types of decoders: HiFiGAN-based and iSTFTNet-based. The first is based on HiFi-GAN [30], which directly generates the waveform. In contrast, the iSTFTNet-based decoder [45] produces magnitude and phase, which are converted into waveforms using inverse short-time Fourier transform for faster training and inference. We employ the snake activation function [46], proven effective for waveform generation in [31]. An AdaIN module [44] is added after each activation function to model the style dependence of the speech, similar to the original StyleTTS decoder. We replace the mel-discriminator in [6] with the multi-period discriminator (MPD) [30] and multi-resolution discriminator (MRD) [47] along with the LSGAN loss functions [48] for decoder training, and incorporate the truncated point-wise relativistic loss function [49] to enhance sound quality (see Appendix F and G for details).</p> <p>We found that well-trained acoustic modules, especially the style encoder, can accelerate the training process for TTS prediction modules. Therefore, before jointly optimizing all components, we first pre-train the acoustic modules along with the pitch extractor and text aligner via $L_{mel}$, $L_{adv}$, $L_{fm}$ and TMA objectives for $N$ epochs where $N$ depends on the size of the training set, in the same way as the first training stage of [6]. However, we note that this pre-training is not an absolute necessity: despite being slower, starting joint training directly from scratch also leads to model convergence.</p> <p>After acoustic module pre-training, we jointly optimize $L_{mel}$, $L_{adv}$, $L_{fm}$, $L_{dur}$, $L_{f0}$ and $L_n$, where $L_{mel}$ is modified to match the mel-spectrograms of waveforms reconstructed from predicted pitch $\\hat{p}x$ and energy $\\hat{n}_x$ (Fig 1a). During joint training, stability issues emerge from diverging gradients, as the style encoder must encode both acoustic and prosodic information. To address this inconsistency, we introduce a prosodic style encoder $E_p$ alongside the original acoustic style encoder $E_a$, previously denoted as $E$ in section 3.1. Instead of using $s_a= E_a(x)$, predictors $S$ and $P$ take $s_p= E_p(x)$ as the input style vector. The style diffusion model generates the augmented style vector $s = [sp, sa]$. This modification effectively improves sample quality (see section 5.3). To further decouple the acoustic modules and predictors, we replace the phoneme representations $h{text}$ from $T$, now referred to as acoustic text encoder, with $h_{bert}$ from another text encoder $B$ based on BERT transformers, denoted as prosodic text encoder. Specifically, we employ a phoneme-level BERT [7] pre-trained on extensive corpora of Wikipedia articles as the prosodic text encoder. This approach has been shown to enhance the naturalness of StyleTTS in the second stage [7], similar to our proposed usage here. </p> <p>With differentiable upsampling and fast style diffusion, we can generate speech samples during training in a fully differentiable manner, just as during inference. These samples are used to optimize $L_{slm}$ (eq. 5) during joint training to update the parameters of all components for inference (Fig 1b). </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#322style-diffusion","title":"3.2.2.Style Diffusion","text":"<p>In StyleTTS 2, we model the speech $x$ as a conditional distribution $p(x|t) =\\int p(x|t, s)p(s|t) ds$ through a latent variable $s$ that follows the distribution $p(s|t)$. We call this variable the generalized speech style, representing any characteristic in speech beyond phonetic content $t$, including but not limited to prosody, lexical stress, formant transitions, and speaking rate [6]. We sample $s$ by EDM [50] that follows the combined probability flow [51] and time-varying Langevin dynamics [52]: </p> <p>Unlike [50] that uses 2nd-order Heun, we solve eq. 4 with the ancestral DPM-2 solver [54] for fast and diverse sampling as we demand speed more than accuracy. On the other hand, we use the same scheduler as in [50] with\u03c3min= 0.0001, \u03c3max= 3and\u03c1 = 9. This combination allows us to sample a style vector for high-quality speech synthesis with only three steps, equivalent to running a 9-layer transformer model, minimally impacting the inference speed (see Appendix B for more discussions). Vconditions ontthroughhbertconcatenated with the noisy inputE(x) + \u03c3\u03be, and\u03c3is conditioned via sinusoidal positional embeddings [53]. In the multispeaker setting, we modelp(s|t, c)by K(s; t, c, \u03c3)with an additional speaker embeddingc = E(xref)wherexrefis the reference audio of the target speaker. The speaker embedding c is injected into V by adaptive layer normalization [6]. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#323slm-discriminators","title":"3.2.3.SLM Discriminators","text":"<p>Speech language models (SLMs) encode valuable information ranging from acoustic to semantic aspects [55], and SLM representations are shown to mimic human perception for evaluating synthesized speech quality [45]. We uniquely transfer knowledge from SLM encoders to generative tasks via adversarial training by employing a 12-layer WavLM [11] pre-trained on 94k hours of data1as the discriminator. As the number of parameters of WavLM is greater than StyleTTS 2, to avoid discriminator overpowering, we fix the pre-trained WavLM modelWand append a convolutional neural network (CNN)Cas the discriminative head. We denote the SLM discriminatorDSLM= C \u25e6 W. The input audios are downsampled to 16 kHz before being fed intoDSLMto match that of WavLM. Cpools featureshSLM= W(x)from all layers with a linear map from13 \u00d7 768to 256 channels. We train the generator components (T, B, G, S, P, V , denoted as G) and DSLMto optimize: </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#324differentiable-duration-modeling","title":"3.2.4.Differentiable Duration Modeling","text":"<p>The duration predictor produces phoneme durationsdpred, but the upsampling method described in section 3.1 to obtainapredis not differentiable, blocking gradient flow for E2E training. NaturalSpeech [5] employs an attention-based upsampler [42] for human-level TTS. However, we find this approach unstable during adversarial training because we train our model using differentiable upsampling with only the adversarial objective described in eq. 5 and without extra loss terms due to the length mismatch caused by deviations ofdpredfromdgt. Although this mismatch can be mitigated with soft dynamic time warping as used in [42,5], we find this approach both computationally expensive and unstable with mel-reconstruction and adversarial objectives. To achieve human-level performance with adversarial training, a non-parametric upsampling method is preferred for stable training. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#4experiments","title":"4.Experiments","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2022.05_StyleTTS/2023.06_StyleTTS2/#5conclusions","title":"5.Conclusions","text":"<p>In this study, we present StyleTTS 2, a novel text-to-speech (TTS) model with human-level performance via style diffusion and speech language model discriminators. In particular, it exceeds the ground truth on LJSpeech and performs on par with it on the VCTK dataset. StyleTTS 2 also shows potential for zero-shot speaker adaption, with remarkable performance even on limited training data compared to large-scale models like Vall-E. With our innovative style diffusion method, StyleTTS 2 generates expressive and diverse speech of superior quality while ensuring fast inference time. While StyleTTS 2 excels in several areas, our results indicate room for improvement in handling large-scale datasets such as LibriTTS, which contain thousands of or more speakers, acoustic environments, accents, and other various aspects of speaking styles. The speaker similarity in the aforementioned zero-shot adaptation speaker task could also benefit from further improvements. However, zero-shot speaker adaptation has the potential for misuse and deception by mimicking the voices of individuals as a potential source of misinformation or disinformation. This could lead to harmful, deceptive interactions such as theft, fraud, harassment, or impersonations of public figures that may influence political processes or trust in institutions. In order to manage the potential for harm, we will require users of our model to adhere to a code of conduct that will be clearly displayed as conditions for using the publicly available code and models. In particular, we will require users to inform those listening to samples synthesized by StyleTTS 2 that they are listening to synthesized speech or to obtain informed consent regarding the use of samples synthesized by StyleTTS 2 in experiments. Users will also be required to use reference speakers who have given consent to have their voice adapted, either directly or by license. Finally, we will make the source code publicly available for further research in speaker fraud and impersonation detection. In addition, while human evaluators have favored StyleTTS 2 over ground truth with statistical significance on the LJSpeech dataset, this preference may be context-dependent. Original audio segments from larger contexts like audiobooks could inherently differ in naturalness when isolated, potentially skewing the evaluations in favor of synthesized speech. Additionally, the inherent variability in human speech, which is context-independent, might lead to lower ratings when compared to the more uniform output from StyleTTS 2. Future research should aim to improve evaluation methods to address these limitations and develop more natural and human-like speech synthesis models with longer context dependencies. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/","title":"VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers","text":"\u4f5c\u8005 \u673a\u6784 \u4f5c\u8005 \u673a\u6784 \u738b\u7a0b\u4e00 Microsoft \u9648\u4e09\u5143 Microsoft \u5434\u4fe3 Microsoft \u5f20\u81ea\u5f3a Microsoft \u5468\u9f99 Microsoft \u5218\u6811\u6770 Microsoft Zhuo Chen Microsoft Yanqing Liu Microsoft Huaming Wang Microsoft \u674e\u52b2\u5b87 Microsoft \u4f55\u78ca Microsoft \u8d75\u80dc Microsoft \u97e6\u798f\u5982 Microsoft","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#abstract","title":"Abstract","text":"<p>We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find that VALL-E could preserve the speaker\u2019s emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.</p> <p>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u8bed\u97f3\u5408\u6210\u7684\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u4f7f\u7528\u4ece\u73b0\u6210\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u5bfc\u51fa\u7684\u79bb\u6563\u7f16\u7801\u8bad\u7ec3\u4e86\u4e00\u4e2a\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b (\u79f0\u4e3a VALL-E), \u5e76\u5c06 TTS \u89c6\u4e3a\u4e00\u4e2a\u6761\u4ef6\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1, \u800c\u4e0d\u662f\u50cf\u4ee5\u524d\u7684\u5de5\u4f5c\u90a3\u6837\u89c6\u4e3a\u8fde\u7eed\u4fe1\u53f7\u56de\u5f52\u4efb\u52a1. \u5728\u9884\u8bad\u7ec3\u9636\u6bb5, \u6211\u4eec\u5c06 TTS \u8bad\u7ec3\u6570\u636e\u6269\u5c55\u5230 60K \u5c0f\u65f6\u7684\u82f1\u8bed\u8bed\u97f3, \u8fd9\u6bd4\u73b0\u6709\u7cfb\u7edf\u5927\u6570\u767e\u500d. VALL-E \u5c55\u73b0\u51fa\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b, \u5e76\u53ef\u7528\u4e8e\u4f7f\u7528\u4ec5 3 \u79d2\u7684\u672a\u89c1\u8bf4\u8bdd\u8005\u7684\u8f93\u5165\u5f55\u97f3\u4f5c\u4e3a\u58f0\u5b66\u63d0\u793a\u6765\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u8bed\u97f3. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e VALL-E \u5728\u8bed\u97f3\u81ea\u7136\u5ea6\u548c\u8bf4\u8bdd\u8005\u76f8\u4f3c\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u6b21\u8bed\u8a00\u5408\u6210\u7cfb\u7edf. \u6b64\u5916, \u6211\u4eec\u53d1\u73b0 VALL-E \u53ef\u4ee5\u5728\u5408\u6210\u4e2d\u4fdd\u7559\u58f0\u5b66\u63d0\u793a\u4e2d\u7684\u8bf4\u8bdd\u8005\u7684\u60c5\u611f\u548c\u58f0\u5b66\u73af\u5883 \u8bf7\u8bbf\u95ee https://aka.ms/valle \u67e5\u770b\u672c\u9879\u5de5\u4f5c\u7684\u793a\u4f8b.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#1introduction","title":"1.Introduction","text":"<p>The last decade has yielded dramatic breakthroughs in speech synthesis through the development of neural networks and end-to-end modeling. Currently, cascaded text-to-speech (TTS) systems (Tacotron2 (2017), FastSpeech (2019), Transformer TTS (2018)) usually leverage a pipeline with an acoustic model and a vocoder using mel spectrograms as the intermediate representations. While advanced TTS systems can synthesize high-quality speech from single or multiple speakers (DelightfulTTS2 (2022), VITS (2021)), it still requires high-quality clean data from the recording studio. Large-scale data crawled from the Internet cannot meet the requirement, and always lead to performance degradation. Because the training data is relatively small, current TTS systems still suffer from poor generalization. Speaker similarity and speech naturalness decline dramatically for unseen speakers in the zero-shot scenario.</p> <p>To tackle the zero-shot TTS problem, existing work leverages speaker adaptation [Chen et al., 2019, Wang et al., 2020] and speaker encoding [Arik et al., 2018, YourTTS (2021)] methods, requiring additional fine-tuning, complex pre-designed features, or heavy structure engineering.</p> <p>Instead of designing a complex and specific network for this problem, the ultimate solution is to train a model with large and diverse data as much as possible, motivated by success in the field of text synthesis [Brown et al., 2020, Chowdhery et al., 2022]. Recent years have witnessed notable performance improvement for data increase in the text language model, from 16GB of uncompressed text [Devlin et al., 2019], to 160GB [Liu et al., 2019], to 570GB [Brown et al., 2020], and finally, around 1TB [Chowdhery et al., 2022]. Transferring this success to the field of speech synthesis, we introduce VALL-E, the first language model-based TTS framework leveraging large, diverse, and multi-speaker speech data.</p> <p></p> <p>As shown in Fig.01, to synthesize personalized speech (e.g., zero-shot TTS), VALL-E generates the corresponding acoustic tokens conditioned on the acoustic tokens of the 3-second enrolled recording and the phoneme prompt, which constrain the speaker and content information respectively. Finally, the generated acoustic tokens are used to synthesize the final waveform with the corresponding neural codec decoder [D\u00e9fossez et al., 2022]. The discrete acoustic tokens derived from an audio codec model enable us to treat TTS as conditional codec language modeling and advanced prompting-based large-model techniques (as in GPTs [Brown et al., 2020])can be leveraged for the TTS tasks. The acoustic tokens also allow us to generate diverse synthesized results in TTS by using different sampling strategies during inference.</p> <p>We train VALL-E with LibriLight [Kahn et al., 2020], a corpus consisting of 60K hours of English speech with over 7000 unique speakers. The original data is audio-only, so we employ a speech recognition model to generate the transcriptions. Compared to previous TTS training datasets, such as LibriTTS [Zen et al., 2019], our data contain more noisy speech and inaccurate transcriptions but provide diverse speakers and prosodies. We believe the proposed approach is robust to the noise and generalize well by leveraging large data. It is worth noting that existing TTS systems are always trained with dozens of hours of single-speaker data or hundreds of hours of multi-speaker data, which is over hundreds of times smaller than VALL-E. Tab.01 summarizes the innovation of VALL-E, a language model approach for TTS, using audio codec codes as intermediate representations, leveraging large and diverse data, leading to strong in-context learning capabilities.</p> Table 1 Current Systems VALL-E Intermediate Representation Mel Spectrogram Audio Codec Code Objective Function Continuous Signal Regression Language Model Training Data \u2264600 Hours 60K Hours In-Context Language \u00d7 \u221a <p>We evaluate VALL-E on LibriSpeech [Panayotov et al., 2015] and VCTK [Veaux et al., 2016]datasets, where all test speakers are unseen in the training corpus. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system (YourTTS (2021)) in terms of speech naturalness and speaker similarity, with +0.12 comparative mean option score (CMOS) and +0.93 similarity mean option score (SMOS) improvement on LibriSpeech. VALL-E also beats the baseline on VCTK with+0.11 SMOS and +0.23 CMOS improvements. It even achieves a +0.04 CMOS score against ground truth, showing the synthesized speech of unseen speakers is as natural as human recordings on VCTK. Moreover, the qualitative analysis shows that VALL-E is able to synthesize diverse outputs with the same text and target speaker, which could benefit pseudo-data creation for the speech recognition task. We also find that VALL-E could keep the acoustic environment (e.g., reverberation) and emotion (e.g. anger) of the acoustic prompt.</p> <p>In summary, we make the following contributions. - We propose VALL-E, the first TTS framework with strong in-context learning capabilities as GPT-3, which treats TTS as a language model task with audio codec codes as an intermediate representation to replace the traditional mel spectrogram. It has in-context learning capability and enables prompt-based approaches for zero-shot TTS, which does not require additional structure engineering, pre-designed acoustic features, and fine-tuning as in previous work. - We build a generalized TTS system in the speaker dimension by leveraging a huge amount of semi-supervised data, suggesting that simple scaling up semi-supervised data has been underestimated for TTS. - VALL-E is able to provide diverse outputs with the same input text and keep the acoustic environment and speaker\u2019s emotion of the acoustic prompt. - We verify that VALL-E synthesizes natural speech with high speaker similarity by prompt-ing in the zero-shot scenario. Evaluation results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system on LibriSpeech and VCTK.</p> <p>We encourage the reader to listen to our samples on the demo page https://aka.ms/valle.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#2related-work","title":"2.Related Work","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#21zero-shot-tts","title":"2.1.Zero-Shot TTS","text":"<p>Current TTS methods can be categorized into cascaded and end-to-end methods. Cascaded TTS systems (Tacotron2 (2017), FastSpeech (2019), Transformer TTS (2018)) usually leverage a pipeline with an acoustic model and a vocoder using mel spectrograms as the intermediate representations. To tackle the drawbacks of the vocoder, end-to-end TTS models (VITS (2021), DelightfulTTS2 (2022)) are proposed to jointly optimize the acoustic model and vocoder. In real scenarios, it is highly desirable to customize a TTS system to an arbitrary voice with rare enrolled recordings. Therefore, there is growing interest in the zero-shot multi-speaker TTS techniques, and most of work is done in the context of cascaded TTS systems. As the pioneers, Arik et al.2018 proposes speaker adaptation and speaker encoding approaches. In the line of speaker adaptation, the following work [Chen et al., 2019, Wang et al., 2020, Chen et al., 2021] tries to improve the adaptation efficiency with less target speaker data and speaker-specific parameters. Huang et al.[2022] applies meta-learning on speaker adaptation, which only requires 5-shot to build a well-performed system. In parallel, speaker encoding-based methods achieved great progress in recent years. A speaker encoding based system contains a speaker encoder and a TTS component, where the speaker encoder could be pre-trained on the speaker verification task [Jia et al., 2018]. In Jia et al.[2018] and Arik et al.[2018], the experiments show that the model is able to generate high-quality outputs with 3 seconds enrolled recordings for in-domain speakers. To improve the quality of unseen speakers, advanced speaker embedding models [Cai et al., 2018] can be employed, but it is still undesirable according to Tan et al.[2021]. Another way is to design advanced but complex speaker encoder [Wu et al., 2022].Diffusion model based TTS [Popov et al., 2021, Kim et al., 2022] is also extended to zero-shot TTS [Kang et al., 2022] and achieved good results. Compared to previous work [FastSpeech (2019), Du et al.,2022], our work follows the line of cascaded TTS but first uses audio codec code as intermediate representations. It is the first one that has strong in-context learning capabilities as GPT-3, which does not require fine-tuning, pre-designed features, or a complex speaker encoder.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#22spoken-generative-pre-trained-models","title":"2.2.Spoken Generative Pre-Trained Models","text":"<p>Self-supervised learning is widely investigated in the field of speech understanding [Wav2Vec2.0 (2020), HuBERT (2021), Chen et al., 2022] and speech-to-speech generation [Lakhotia et al., 2021, AudioLM (2022)]. In the context of speech-to-speech generation, a hot topic is how to synthesize speech in a textless setting. GSLM [Lakhotia et al.,2021] proposes to synthesize speech based on HuBERT (2021) codes, and Polyak et al.[2021] improves the performance by combining HuBERT codes with codes of VQVAE and a speaker encoder. AudioLM (2022) follows a similar way but use audio codecs [Zeghidour et al.,2022] to synthesize speech, together with semantic codes. It should be noted that AudioLM is able to synthesize speech based on audio codecs without training an additional vocoder such as HifiGAN (2020). AudioLM is a speech-to-speech model, whereas VALL-E is a TTS model, so we can explicitly control the content in speech synthesis. Another direction is to apply pre-training to the neural TTS. Chung et al.[2018] pre-trains speech decoder in TTS through autoregressive mel-spectrogram prediction. In Ao et al.[2022], the authors propose a unified-modal encoder-decoder framework SpeechT5, which can leverage unlabeled speech and text data to pre-train all components of TTS model. Tjandra et al.[2019] quantizes unlabeled speech into discrete tokens by a VQVAE model [van den Oord et al., 2017], and train a model with the token-to-speech sequence. They demonstrate that the pre-trained model only requires a small amount of real data for fine-tuning. Bai et al.[2022] proposes mask and reconstruction on mel spectrogram and showing better performance on speech editing and synthesis. Previous TTS pre-training work leverages less than 1K hours of data, whereas VALL-E is pre-trained with 60K hours of data. Furthermore, VALL-E is the first to use audio codec codes as intermediate representations, and emerge in-context learning capability in zero-shot TTS.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#3background-speech-quantization","title":"3.Background: Speech Quantization","text":"<p>Since audio is typically stored as a sequence of 16-bit integer values, a generative model is required to output $2^{16}=65,536$ probabilities per timestep to synthesize the raw audio. In addition, the audio sample rate exceeding ten thousand leads to an extraordinarily long sequence length, making it more intractable for raw audio synthesis. To this end, speech quantization is required to compress integer values and sequence length.  $\\mu$-law transformation can quantize each timestep to 256 values and reconstruct high-quality raw audio. It is widely used in speech generative models, such as WaveNet [van den Oord et al., 2016], but the inference speed is still slow since the sequence length is not reduced. Recently, vector quantization is widely applied in self-supervised speech models for feature extraction, such as vq-wav2vec [Baevski et al., 2020a] and HuBERT (2021). The following work [Lakhotia et al., 2021, Du et al., 2022] shows the codes from self-supervised models can also reconstruct content, and the inference speed is faster than WaveNet. However, the speaker identity has been discarded and the reconstruction quality is low AudioLM (2022). AudioLM (2022) trains speech-to-speech language models on both k-means tokens from a self-supervised model and acoustic tokens from a neural codec model, leading to high-quality speech-to-speech generation.</p> <p>In this paper, we follow AudioLM (2022) to leverage neural codec models to represent speech in discrete tokens. To compress audio for network transmission, codec models are able to encode waveform into discrete acoustic codes and reconstruct high-quality waveform even if the speaker is unseen in training. Compared to traditional audio codec approaches, the neural-based codec is significantly better at low bitrates, and we believe the quantized tokens contain sufficient information about the speaker and recording conditions. Compared to other quantization methods,the audio codec shows the following advantages:  1. It contains abundant speaker information and acoustic information, which could maintain speaker identity in reconstruction compared to HuBERT (2021) codes. 2. There is an off-the-shelf codec decoder to convert discrete tokens into a waveform, without the additional efforts on vocoder training like VQ-based methods that operated on spectrum [Du et al., 2022]. 3. It could reduce the length of time steps for efficiency to address the problem in $\\mu$-law transformation [van den Oord et al., 2016].</p> <p>We adopt a pre-trained neural audio codec model, EnCodec [D\u00e9fossez et al., 2022], as our tokenizer. EnCodec is a convolutional encoder-decoder model, whose input and output are both 24 kHz audio across variable bitrates. The encoder produces embeddings at 75 Hz for input waveforms at 24 kHz,which is a 320-fold reduction in the sampling rate. Each embedding is modeled by a residual vector quantization (RVQ), in which we choose eight hierarchy quantizers with 1024 entries each as shown in Fig.02.</p> <p></p> <p>This configuration corresponds to EnCodec at 6K bitrates for 24 kHz audio reconstruction. In this setting, given a 10-second waveform, the discrete representation is a matrix with750 \u00d7 8entries, where 750 =24,000\u00d710/320 is the downsampled time step and 8 is the number of quantizers. It is fine to choose other bitrate settings. A larger bitrate corresponds to more quantizers and better reconstruction quality. For example, if we choose EnCodecc at 12K bitrates, there are 16 quantizers are needed and the 10-second waveform corresponds to a matrix with 750\u00d716 entries. With the discrete codes from all quantizers, the convolutional decoder of EnCodec generates real-valued embeddings and reconstructs the waveform at 24 kHz.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#4vall-e","title":"4.VALL-E","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#41problem-formulation-regarding-tts-as-conditional-codec-language-modeling","title":"4.1.Problem Formulation: Regarding TTS as Conditional Codec Language Modeling","text":"<p>Given a dataset $\\mathcal{D}={\\mathbf{x}i, \\mathbf{y}_i}$, where $\\mathbf{y}$ is an audio sample and $\\mathbf{x} = {x_0,x_1, \\cdots x_L}$ is its corresponding phoneme transcription, we use a pre-trained neural codec model to encode each audio sample into discrete acoustic codes, denoted as $\\text{Encodec}(\\mathbf{y}) = C^{T\\times 8}$, where $C$ represents the two-dimensional acoustic code matrix, and $T$ is the downsampled utterance length. The row vector of each acoustic code matrix $c{t,:}$ represents the eight codes for frametand the column vector of each acoustic code matrix $c_{:,j}$ represents the code sequence from the $j$-th codebook, where $j \\in {1,\\cdots 8}$. After quantization, the neural codec decoder is able to reconstruct the waveform, denoted as $\\text{Decodec}(C)\\approx\\hat{\\mathbf{y}}$.</p> <p>Zero-shot TTS requires the model to synthesize high-quality speech for unseen speakers. In this work, we regard zero-shot TTS as a conditional codec language modeling task. We train a neural language model to generate an acoustic code matrix $C$ conditioned on a phoneme sequence $\\mathbf{x}$ and an acoustic prompt matri $\\tilde{C}^{T'\\times 8}$ with the optimization objective of $\\max p(C|\\mathbf{x},\\tilde{C})$. Here, $\\tilde{C}$ is obtained by the same neural codec with an enrolled recording as the input. We expect the neural language model learns to extract the content and speaker information from the phoneme sequence and the acoustic prompt, respectively. During inference, given a phoneme sequence and a 3-second enrolled recording of the unseen speaker, the acoustic code matrix with corresponding content and speaker\u2019s voice is firstly estimated by the trained language model. Then the neural codec decoder synthesizes the high-quality speech.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#42training-conditional-codec-language-modeling","title":"4.2.Training: Conditional Codec Language Modeling","text":"<p>The neural speech codec model allows us to operate on discrete audio representations. Due to residual quantization in the neural codec model, the tokens have a hierarchical structure: tokens from previous quantizers recover acoustic properties like speaker identity, while the consecutive quantizers learn fine acoustic details. Each quantizer is trained to model the residual from the previous quantizers. Motivated by this, we design two conditional language models in a hierarchical manner.</p> <p>For the discrete tokens from the first quantizer $c_{:,1}$, we train an autoregressive (AR) decoder-only language model. It is conditioned on the phoneme sequencexand the acoustic prompt $\\tilde{C}{:,1}$, formulated as  $$   p(c{:,1}|\\mathbf{x}, \\tilde{C}{:,1}; \\theta{AR}) =\\prod_{t=0}^T p(c_{t,1}|c_{&lt;t,1},\\tilde{c}{:,1}, \\mathbf{x}; \\theta{AR}) \\tag{1} $$</p> <p>Since VALL-E is a decoder-only LM, the concatenation of $\\tilde{c}{:,1}$ and $c{:,1}$ is a whole sequence, and we do not distinguish them or insert a specific token in training. Only $c_{:,1}$ is predicted while the prefix $\\tilde{c}_{:,1}$ is given during inference.</p> <p>For the discrete tokens from the second to the last quantizers, $c_{:,j}\\in[2,8]$, we train a non-autoregressive (NAR) language model. Since the tokens can not access each other in a NAR manner, to constrain the speaker identity, the acoustic prompt matrix $\\tilde{C}$ is used as an acoustic prompt. Thus, the model is conditioned on the phoneme sequencex, the acoustic prompt $\\tilde{C}$ and the predicted acoustic tokens belong to the previous codebooks $C_{:,&lt;j}$: $$   p(C_{:,2:8}|\\mathbf{x},\\tilde{C};\\theta_{NAR})=\\prod_{j=2}^{8}p(c_{:,j}|C_{:,&lt;j},\\mathbf{x},\\tilde{C};\\theta_{NAR}) \\tag{2} $$</p> <p>The combination of the AR model and the NAR model provides a good trade-off between speech quality and inference speed. On the one hand, the rate of the generated speech should be consistent with the enrolled recording, and it is hard to train a length predictor for different speakers since their speaking speed may be very diverse. In this case, the AR model is a more natural choice with its flexibility for acoustic sequence length prediction. On the other hand, for the consecutive stages, as the number of output slots follows the sequence length of the first stage, NAR can reduce the time complexity from $\\mathcal{O}(T)$ to $\\mathcal{O}(1)$. Overall, the prediction of C can be modeled as: $$   p(C|\\mathbf{x},\\tilde{C};\\theta)=p(c_{:,1}|\\tilde{C}{:,1}, \\mathbf{X}; \\theta{AR}) \\prod_{j=2}^{8}p(c_{:,j}|c_{:,&lt;j},\\mathbf{x},\\tilde{C};\\theta_{NAR}) \\tag{3} $$</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#421autoregressive-codec-language-modeling","title":"4.2.1.Autoregressive Codec Language Modeling","text":"<p>The autoregressive language model generates the tokens from the first quantizer. It comprises a phoneme embedding $W_x$, an acoustic embedding $W_a$, a transformer decoder, and a prediction layer. In order to generate speech with specific content, we use the phoneme sequence as the phoneme prompt of the language model. Thus, the model input is the concatenation of $\\mathbf{x}$ and $\\mathbf{c}{:,1}$, and two special <code>&lt;EOS&gt;</code> tokens are appended after each of them. We compute sinuous position embedding separately for prompt and input tokens. For the causal transformer model, each tokenct,1can attend to $(\\mathbf{x}, c{\\leq t,1})$ as illustrated in the left part of Fig.03.</p> <p></p> <p>The model is optimized to maximize the probability of the next token in the first codebook. We share the parameters of the output projection layer with the parameters of the acoustic embedding $W_a$.</p> <p>In the AR model, we do not explicitly extract an audio clip as the prompt in training. The training process is pure casual language model training. In this way, any prefix sequence $c_{&lt;t,1}$ is treated as a prompt for the latter part of the sequence $c_{\\geq t,1}$. During inference, given an enrolled recording, we should concatenate the phoneme sequence of the enrolled recording and the phoneme sequence for synthesis together. Meanwhile, the acoustic token sequence of the enrolled recording is used as the prefix in AR decoding, as formulated in Eq.01. We will study the superiority of this setting in the experiment.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#422non-autoregressive-codec-language-modeling","title":"4.2.2.Non-Autoregressive Codec Language Modeling","text":"<p>When we obtain the first quantizer codes by the AR model, we employ a non-autoregressive (NAR) model to generate codes of the other seven quantizers. The NAR model has a similar architecture to the AR model, except that it contains eight separate acoustic embedding layers. In each training step, we randomly sample a training stage $i\\in [2, 8]$. The model is trained to maximize the acoustic tokens from the $i$-th quantizer codebook. The acoustic tokens from stage $1$ to stage $i\u22121$ are embedded and summed up as model input: $$ \\begin{align}e_{c_{t,j}}&amp;=W_a^j\\odot c_{t,j}\\tag{4}\\\\mathbf{e_{c_t}}&amp;=\\sum_{j=1}^{i-1}e_{c_t,j}\\tag{5}\\end{align} $$</p> <p>where $\\odot$ indicates index selection.</p> <p>The phoneme sequence is also regarded as the prompt of the language model. Besides, to clone the unique voice of the given speaker, we also use the acoustic tokens from the enrolled speech as the acoustic prompt. Specifically, we first tokenize the enrolled speech with the neural codec model as $\\tilde{C}^{T\\times 8}$. The embedded representations from all of the eight codebooks are summed up as the acoustic prompt $e_{\\tilde{c}t}=\\sum{j=1}^8 e_{\\tilde{c}{t,j}}$. To predict the acoustic tokens from thei-th codebook, the transformer input is the concatenation of $(\\mathbf{e}{\\mathbf{x}}, \\mathbf{e}{\\tilde{c}}, \\mathbf{e}{c_{:,&lt;i}})$. The positional embeddings are also computed separately for prompts and the acoustic sequence. The current stage $i$ is injected into the network with Adaptive Layer Normalization [Xu et al., 2019] operator, i.e., $\\text{AdaLN}(h, i) = a_i\\text{LayerNorm}(h) + b_i$, where $h$ is the intermediate activations, $a_i$ and $b_i$ are obtained from a linear projection of the stage embedding. Unlike AR, the NAR model allows each token to attend to all the input tokens in the self-attention layer. We also share the parameters of the acoustic embedding layer and the output prediction layer, which means the weights of thej-th prediction layer are the same as the $(j + 1)$-th acoustic embedding layer.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#43inference-in-context-learning-via-prompting","title":"4.3.Inference: In-Context Learning via Prompting","text":"<p>In-context learning is a surprising ability of the text-based language model, which is able to predict labels for unseen inputs without additional parameter updates. For TTS, if the model can synthesize high-quality speech for unseen speakers without fine-tuning, the model is believed to have in-context learning capability. However, the in-context learning capability of existing TTS systems is not strong,because they either require additional fine-tuning or degrade dramatically for unseen speakers.</p> <p>For language models, prompting is necessary to enable in-context learning in the zero-shot scenario. We design prompts and inference as follows. We first convert the text into a phoneme sequence and encode the enrolled recording into an acoustic matrix, forming the phoneme prompt and acoustic prompt. Both prompts are used in the AR and NAR models. For the AR model, we use sampling-based decoding conditioned on the prompts since we observe that beam search may lead the LM into an infinity loop. Furthermore, the sampling-based method could significantly increase the diversity of the output. For the NAR model, we use greedy decoding to choose the token with the highest probability. Finally, we use the neural codec decoder to generate the waveform conditioned on the eight code sequences.</p> <p>The acoustic prompt may or may not semantically relate to the speech to be synthesized, resulting in two cases: - VALL-E:  Our main interest is to generate given content for unseen speakers. The model is given a text sentence, a segment of enrolled speech, and its corresponding transcription. We prepend the transcription phoneme of the enrolled speech to the phoneme sequence of the given sentence as the phoneme prompt, and use the first layer acoustic token of the enrolled speech\u02dcc:,1as an acoustic prefix. With the phoneme prompt and the acoustic prefix, VALL-E generates the acoustic tokens for the given text cloning the voice of this speaker. - VALL-E-continual:  In this setting, we use the whole transcription and the first 3 seconds of the utterance as the phoneme and acoustic prompts respectively, and ask the model to generate the continuations. The inference process is the same as setting VALL-E, except that the enrolled speech and the generated speech are semantically continuous.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#5experiment","title":"5.Experiment","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#51experiment-setup","title":"5.1.Experiment Setup","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#52librispeech-evaluation","title":"5.2.LibriSpeech Evaluation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#53vctk-evaluation","title":"5.3.VCTK Evaluation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#54qualitative-analysis","title":"5.4.Qualitative Analysis","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#6conclusion-limitations-future-work","title":"6.Conclusion, Limitations, Future Work","text":"<p>We introduced VALL-E, a language model approach for TTS with audio codec codes as intermediate representations.  We pre-train VALL-E with 60K hours of speech data, and show the in-context learning capability in zero-shot scenarios.  We achieve new state-of-the-art zero-shot TTS results on LibriSpeech and VCTK.  Furthermore, VALL-E could keep the acoustic environment and speaker\u2019s emotion in synthesis, and provide diverse outputs in different sampling-based decoding processes.</p> <p>Despite making significant progress, VALL-E still suffers from several issues.</p> <p>Synthesis robustness We observe that some words may be unclear, missed, or duplicated in speech synthesis.  It is mainly because the phoneme-to-acoustic language part is an autoregressive model, in which disordered attention alignments exist and no constraints to solving the issue.  The phenomenon is also observed in vanilla Transformer-based TTS, which was addressed by applying non-autoregressive models or modifying the attention mechanism in modeling.  In the future, we would like to leverage these techniques to solve the issue.</p> <p>Data coverage Even if we use 60K hours of data for training, it still cannot cover everyone\u2019s voice,especially accent speakers.  The worse result on VCTK than LibriSpeech also implies insufficient coverage of accent speakers. Moreover, the diversity of speaking styles is not enough, as LibriLight is an audiobook dataset, in which most utterances are in reading style.  In the future, we will further scale up the training data to improve the model performance across prosody, speaking style, and speaker similarity perspectives.  We believe the zero-shot TTS task could be almost solved through our approach with model and data scale-up.</p> <p>Model Structure Now, we use two models to predict codes of different quantizers.  A promising direction is to predict them with a large universal model.  Another interesting direction is using full NAR models to speed up model inference in the framework.</p> <p>Broader impacts Since VALL-E could synthesize speech that maintains speaker identity, it may carry potential risks in misuse of the model, such as spoofing voice identification or impersonating a specific speaker.  To mitigate such risks, it is possible to build a detection model to discriminate whether an audio clip was synthesized by VALL-E.  We will also put Microsoft AI Principles\u2217into practice when further developing the models.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.03_VALL-E_X/","title":"Speak Foreign Languages with Your Own Voice:Cross-Lingual Neural Codec Language Modeling","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec","\u8de8\u8bed\u79cd_Cross-Lingual"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.03_VALL-E_X/#abstract","title":"Abstract","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec","\u8de8\u8bed\u79cd_Cross-Lingual"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.03_VALL-E_X/#1introduction","title":"1.Introduction","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec","\u8de8\u8bed\u79cd_Cross-Lingual"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.03_VALL-E_X/#2related-works","title":"2.Related Works","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec","\u8de8\u8bed\u79cd_Cross-Lingual"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/","title":"SoundStorm: Efficient Parallel Audio Generation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#a","title":"A.\u6458\u8981","text":"<p>We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM (2022), and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM (2022), our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers\u2019 voices. Audio samples are available at https://google-research.github.io/seanet/soundstorm/examples/</p> <p>\u6211\u4eec\u63d0\u51fa\u4e86 SoundStorm, \u4e00\u4e2a\u7528\u4e8e\u9ad8\u6548\u975e\u81ea\u56de\u5f52\u97f3\u9891\u751f\u6210\u7684\u6a21\u578b. SoundStorm \u4ee5 AudioLM \u7684\u8bed\u4e49 token \u4f5c\u4e3a\u8f93\u5165, \u5e76\u4f9d\u8d56\u4e8e\u53cc\u5411\u6ce8\u610f\u529b\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5e76\u884c\u89e3\u7801\u6765\u751f\u6210\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684 token. \u4e0e AudioLM \u7684\u81ea\u56de\u5f52\u751f\u6210\u65b9\u6cd5\u76f8\u6bd4, SoundStorm \u80fd\u591f\u751f\u6210\u76f8\u540c\u8d28\u91cf\u7684\u97f3\u9891, \u4e14\u5728\u58f0\u97f3\u548c\u58f0\u5b66\u6761\u4ef6\u4e0b\u5177\u6709\u66f4\u9ad8\u7684\u4e00\u81f4\u6027\u7684\u540c\u65f6, \u901f\u5ea6\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7. \u6211\u4eec\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf, \u81ea\u7136\u5bf9\u8bdd\u7247\u6bb5\u6765\u5c55\u793a\u6a21\u578b\u6269\u5c55\u97f3\u9891\u751f\u6210\u66f4\u957f\u5e8f\u5217\u7684\u80fd\u529b. \u8fd9\u4e9b\u5bf9\u8bdd\u7247\u6bb5\u9700\u8981\u7ed9\u5b9a\u4e00\u4e2a\u5e26\u6709\u8bf4\u8bdd\u8005\u8f6c\u5f55\u811a\u672c\u548c\u4e00\u4e2a\u5e26\u6709\u8bf4\u8bdd\u4eba\u58f0\u97f3\u7684\u7b80\u77ed\u63d0\u793a.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#1","title":"1.\u5f15\u8a00","text":"<p>Modeling discrete representations of audio produced by neural codecs (SoundStream (2021); EnCodec (2022)) makes the task of audio generation amenable to the powerful Transformer-based sequence-to-sequence modeling approaches (Transformer (2017)). Casting unconditional and conditional audio generation as sequence-to-sequence modeling has unlocked rapid progress in speech continuation (AudioLM (2022)), text-to-speech (VALL-E (2023); SPEAR-TTS (2023)), and general audio and music generation (AudioGen (2022); MusicLM (2023)).</p> <p>\u5efa\u6a21\u7531\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u751f\u6210\u7684\u97f3\u9891\u79bb\u6563\u8868\u793a\u4f7f\u5f97\u97f3\u9891\u751f\u6210\u4efb\u52a1\u53ef\u4ee5\u91c7\u7528\u5f3a\u5927\u7684\u57fa\u4e8e Transformer \u5e8f\u5217\u5230\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5. \u5c06\u65e0\u6761\u4ef6\u548c\u6709\u6761\u4ef6\u97f3\u9891\u751f\u6210\u89c6\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u5efa\u6a21\u5df2\u7ecf\u89e3\u9501\u4e86\u5728\u8bed\u97f3\u5ef6\u7eed, \u6587\u672c\u8f6c\u8bed\u97f3, \u4e00\u822c\u97f3\u9891\u548c\u97f3\u4e50\u751f\u6210\u65b9\u9762\u7684\u5feb\u901f\u53d1\u5c55.</p> <p>For generating high-quality audio by modeling the tokens of a neural codec, the rate of the discrete representation must be increased, resulting in either an exponential growth in codebook size or in long token sequences. While the exponential growth of the codebook is prohibitive due to memory limitations, in turn, long token sequences also present computational challenges for autoregressive models. In particular, attention-based models, which are the main focus of this work, will incur quadratic runtime complexity with respect to the sequence length for calculating the self-attention. Thus, addressing the trade-off between perceptual quality and runtime is one of the core challenges for audio generation.</p> <p>\u4e3a\u4e86\u901a\u8fc7\u5efa\u6a21\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u7684 token \u6765\u751f\u6210\u9ad8\u8d28\u91cf\u97f3\u9891, \u79bb\u6563\u8868\u793a\u7684\u6bd4\u7387\u5fc5\u987b\u589e\u52a0, \u5bfc\u81f4\u4ee3\u7801\u672c\u5c3a\u5bf8\u6216\u957f token \u5e8f\u5217\u5448\u6307\u6570\u589e\u957f. \u867d\u7136\u7531\u4e8e\u5185\u5b58\u9650\u5236, codebook \u7684\u6307\u6570\u589e\u957f\u662f\u4e0d\u53ef\u884c\u7684, \u4f46\u53cd\u8fc7\u6765, \u957f token \u5e8f\u5217\u4e5f\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u5e26\u6765\u4e86\u8ba1\u7b97\u6311\u6218. \u7279\u522b\u662f\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u578b, \u548c\u7528\u4e8e\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\u7684\u5e8f\u5217\u957f\u5ea6\u5448\u4e8c\u6b21\u65f6\u95f4\u590d\u6742\u6027. \u56e0\u6b64, \u89e3\u51b3\u611f\u77e5\u8d28\u91cf\u548c\u8fd0\u884c\u65f6\u95f4\u7684\u6743\u8861\u662f\u97f3\u9891\u751f\u6210\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218.</p> <p>The problem of generating long audio token sequences can be addressed by at least three orthogonal approaches or a combination thereof: 1. Efficient attention mechanisms (Reformer (2020); Performers (2021); Nystr\u00f6mformer (2021); Perceiver AR (2022)), 2. Non-autoregressive, parallel decoding schemes (NAT (2017); Mask-Predict (2019); MaskGIT (2022)), 3. Custom architectures adapted to the special structure of the tokens produced by neural audio codecs (AudioGen (2022); VALL-E (2023); RQ-Transformer (2022)).</p> <p>However, in the context of modeling the token sequence of neural audio codecs, either unconditionally or based on weak conditioning such as text, the efficient generation of long, high-quality audio segments remains an open problem.</p> <p>\u751f\u6210\u957f\u97f3\u9891 token \u5e8f\u5217\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u81f3\u5c11\u4e09\u79cd\u6b63\u4ea4\u65b9\u6cd5\u6216\u7ec4\u5408\u6765\u89e3\u51b3: 1. \u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236; 2. \u975e\u81ea\u56de\u5f52\u5e76\u884c\u89e3\u7801\u65b9\u6848; 3. \u9002\u7528\u4e8e\u7531\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u751f\u6210\u7684 token \u7279\u6b8a\u7ed3\u6784\u7684\u81ea\u5b9a\u4e49\u7f51\u7edc\u7ed3\u6784.</p> <p>\u7136\u800c\u5728\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684 token \u5e8f\u5217\u5efa\u6a21\u7684\u80cc\u666f\u4e0b, \u65e0\u8bba\u662f\u5728\u65e0\u6761\u4ef6\u8fd8\u662f\u5f31\u6761\u4ef6 (\u5982\u6587\u672c) \u4e0b, \u9ad8\u6548\u751f\u6210\u957f\u4e14\u9ad8\u8d28\u91cf\u7684\u97f3\u9891\u7247\u6bb5\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898.</p> <p>We believe that it is the special structure of the audio token sequence that holds the most promise for future advances in long-sequence audio modeling. Concretely, both SoundStream (2021) and EnCodec (2022) rely on Residual Vector Quantization (RVQ), where each compressed audio frame is quantized by a series of quantizers, with each quantizer operating on the residual of the previous one, and the number of quantizers control-ling the overall bitrate. This induces a hierarchical token structure, where tokens from finer RVQ levels contribute less to the perceptual quality, allowing for efficient factorizations and approximations of the joint distribution of the token sequence. Hence, the models and decoding schemes should take this special structure of the input into account for efficient training and inference.</p> <p>\u6211\u4eec\u8ba4\u4e3a\uff0c\u97f3\u9891\u6807\u8bb0\u5e8f\u5217\u7684\u7279\u6b8a\u7ed3\u6784\u5bf9\u4e8e\u672a\u6765\u5728\u957f\u5e8f\u5217\u97f3\u9891\u5efa\u6a21\u65b9\u9762\u7684\u8fdb\u6b65\u6700\u6709\u5e0c\u671b\u3002 \u5177\u4f53\u6765\u8bf4\uff0cSoundStream\uff082021\uff09\u548cEnCodec\uff082022\uff09\u90fd\u4f9d\u8d56\u4e8eResidual Vector Quantization\uff08RVQ\uff09\uff0c\u5176\u4e2d\u6bcf\u4e2a\u538b\u7f29\u7684\u97f3\u9891\u5e27\u7531\u4e00\u7cfb\u5217\u91cf\u5316\u5668\u8fdb\u884c\u91cf\u5316\uff0c\u6bcf\u4e2a\u91cf\u5316\u5668\u5728\u4e4b\u524d\u7684\u91cf\u5316\u5668\u7684\u6b8b\u5dee\u4e0a\u64cd\u4f5c\uff0c\u91cf\u5316\u5668\u7684\u6570\u91cf\u63a7\u5236\u603b\u4f53\u6bd4\u7279\u7387\u3002 \u8fd9\u5f15\u5165\u4e86\u5206\u5c42\u7684\u6807\u8bb0\u7ed3\u6784\uff0c\u5176\u4e2d\u6765\u81ea\u66f4\u7cbe\u7ec6RVQ\u7ea7\u522b\u7684\u6807\u8bb0\u5bf9\u611f\u77e5\u8d28\u91cf\u7684\u8d21\u732e\u8f83\u5c0f\uff0c\u5141\u8bb8\u5bf9\u6807\u8bb0\u5e8f\u5217\u7684\u8054\u5408\u5206\u5e03\u8fdb\u884c\u6709\u6548\u7684\u56e0\u5f0f\u5206\u89e3\u548c\u8fd1\u4f3c\u3002 \u56e0\u6b64\uff0c\u6a21\u578b\u548c\u89e3\u7801\u65b9\u6848\u5e94\u8be5\u8003\u8651\u5230\u8f93\u5165\u7684\u8fd9\u79cd\u7279\u6b8a\u7ed3\u6784\uff0c\u4ee5\u4fbf\u8fdb\u884c\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002</p> <p>In this work, we present SoundStorm, a method for efficient and high-quality audio generation. SoundStorm addresses the problem of generating long audio token sequences by relying on: 1. an architecture adapted to the hierarchical structure of the audio tokens, 2. a parallel, non-autoregressive, confidence-based decoding scheme inspired by MaskGIT (2022) for residual vector-quantized token sequences.</p> <p>\u6211\u4eec\u63d0\u51fa\u4e86 SoundStorm, \u4e00\u79cd\u7528\u4e8e\u9ad8\u6548\u9ad8\u8d28\u91cf\u97f3\u9891\u751f\u6210\u7684\u65b9\u6cd5. SoundStorm \u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u89e3\u51b3\u751f\u6210\u957f\u97f3\u9891 token \u5e8f\u5217\u95ee\u9898: 1. \u9002\u7528\u4e8e\u97f3\u9891 token \u5c42\u6b21\u7ed3\u6784\u7684\u67b6\u6784; 2. \u7528\u4e8e\u6b8b\u5dee\u77e2\u91cf\u91cf\u5316 token \u5e8f\u5217\u7684\u5e76\u884c\u975e\u81ea\u56de\u5f52, \u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u89e3\u7801\u65b9\u6848.</p> <p>SoundStorm relies on a bidirectional attention-based Conformer (2020) that is trained to predict masked audio tokens produced by SoundStream given a condition-ing signal such as the semantic tokens of AudioLM (2022). On the input side, it sums up the embeddings of the tokens corresponding to the same SoundStream frame, such that the internal sequence length for the self-attention is identical to the number of SoundStream frames, and independent of the number of quantizers in the RVQ. The output embeddings are then processed by separate heads per RVQ level to predict the masked target tokens. At inference time, given the conditioning signal, SoundStorm starts with all audio tokens masked out, and fills in the masked tokens RVQ level-by-level over several iterations, predicting multiple tokens in parallel during a single iteration within a level. To support this inference scheme, we propose a masking scheme for training that mimics the inference procedure.</p> <p>SoundStorm \u4f9d\u8d56\u4e8e\u4e00\u4e2a\u57fa\u4e8e\u53cc\u5411\u6ce8\u610f\u529b\u7684Conformer\uff082020\uff09, \u8be5 Conformer \u7ecf\u8fc7\u8bad\u7ec3\u4ee5\u9884\u6d4b\u7531 SoundStream \u751f\u6210\u7684\u63a9\u7801\u97f3\u9891\u6807\u8bb0, \u7ed9\u5b9a\u4e00\u4e2a\u6761\u4ef6\u4fe1\u53f7, \u4f8b\u5982 AudioLM\uff082022\uff09\u7684\u8bed\u4e49\u6807\u8bb0. \u5728\u8f93\u5165\u65b9\u9762, \u5b83\u5c06\u5bf9\u5e94\u4e8e\u540c\u4e00 SoundStream \u5e27\u7684\u6807\u8bb0\u7684\u5d4c\u5165\u76f8\u52a0, \u4f7f\u5f97\u81ea\u6ce8\u610f\u529b\u7684\u5185\u90e8\u5e8f\u5217\u957f\u5ea6\u4e0e SoundStream \u5e27\u7684\u6570\u91cf\u76f8\u540c, \u5e76\u4e14\u4e0e RVQ \u4e2d\u7684\u91cf\u5316\u5668\u6570\u91cf\u65e0\u5173. \u7136\u540e\u8f93\u51fa\u5d4c\u5165\u7531\u6bcf\u4e2a RVQ \u7ea7\u522b\u7684\u5355\u72ec\u5934\u5904\u7406\u4ee5\u9884\u6d4b\u63a9\u7801\u76ee\u6807\u6807\u8bb0. \u5728\u63a8\u7406\u65f6\u95f4, \u7ed9\u5b9a\u6761\u4ef6\u4fe1\u53f7, SoundStorm \u4ece\u6240\u6709\u97f3\u9891\u6807\u8bb0\u90fd\u88ab\u63a9\u7801\u5f00\u59cb, \u7136\u540e\u5728\u51e0\u6b21\u8fed\u4ee3\u4e2d\u9010\u7ea7\u586b\u5145\u63a9\u7801\u6807\u8bb0, \u5728\u6bcf\u4e2a\u7ea7\u522b\u7684\u4e00\u6b21\u8fed\u4ee3\u4e2d\u5e76\u884c\u9884\u6d4b\u591a\u4e2a\u6807\u8bb0. \u4e3a\u4e86\u652f\u6301\u8fd9\u79cd\u63a8\u7406\u65b9\u6848, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u63a9\u7801\u65b9\u6848, \u8be5\u65b9\u6848\u6a21\u4eff\u63a8\u7406\u8fc7\u7a0b.</p> <p>We demonstrate that SoundStorm can serve as AudioLM (2022)\u2019s acoustic generator, replacing both AudioLM (2022)\u2019s stage two(coarse acoustic model) and stage three (fine acoustic model). SoundStorm produces audio two orders of magnitude faster than AudioLM (2022)\u2019s hierarchical autoregressive acoustic generator with matching quality and improved consistency in terms of speaker identity and acoustic conditions. Furthermore, we show that SoundStorm, coupled with the text-to-semantic modeling stage of SPEAR-TTS (2023), can synthesize high-quality, natural dialogues, allowing one to control the spoken content(via transcripts), speaker voices (via short voice prompts)and speaker turns (via transcript annotations). When synthesizing dialogues of 30 seconds, we measure a runtime of 2 seconds on a single TPU-v4 (2023).</p> <p>\u6211\u4eec\u5c55\u793a\u4e86 SoundStorm \u53ef\u4ee5\u4f5c\u4e3a AudioLM \u7684\u58f0\u5b66\u751f\u6210\u5668, \u53d6\u4ee3 AudioLM \u7684\u7b2c\u4e8c\u9636\u6bb5 (\u7c97\u58f0\u5b66\u6a21\u578b) \u548c\u7b2c\u4e09\u9636\u6bb5 (\u7ec6\u58f0\u5b66\u6a21\u578b) SoundStorm \u751f\u6210\u7684\u97f3\u9891\u901f\u5ea6\u6bd4 AudioLM \u7684\u5206\u5c42\u81ea\u56de\u5f52\u58f0\u5b66\u751f\u6210\u5668\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7, \u540c\u65f6\u4fdd\u6301\u76f8\u540c\u8d28\u91cf\u548c\u63d0\u9ad8\u7684\u8bed\u97f3\u548c\u58f0\u5b66\u6761\u4ef6\u4e00\u81f4\u6027. \u6b64\u5916, \u6211\u4eec\u5c55\u793a\u4e86 SoundStorm \u4e0e SPEAR-TTS (2023) \u7684\u6587\u672c\u5230\u8bed\u4e49\u5efa\u6a21\u9636\u6bb5\u76f8\u7ed3\u5408, \u53ef\u4ee5\u5408\u6210\u9ad8\u8d28\u91cf\u81ea\u7136\u7684\u5bf9\u8bdd, \u5141\u8bb8\u901a\u8fc7\u811a\u672c\u63a7\u5236\u8bf4\u8bdd\u5185\u5bb9, \u901a\u8fc7\u77ed\u8bed\u97f3\u63d0\u793a\u63a7\u5236\u8bf4\u8bdd\u8005\u58f0\u97f3\u548c\u901a\u8fc7\u811a\u672c\u6ce8\u91ca\u63a7\u5236\u8bf4\u8bdd\u8005\u8f6e\u6b21. \u5728\u5408\u6210 30 \u79d2\u7684\u5bf9\u8bdd\u65f6, \u6211\u4eec\u5728\u5355\u4e2a TPU-v4\uff082023\uff09\u4e0a\u6d4b\u91cf\u7684\u8fd0\u884c\u65f6\u95f4\u4e3a 2 \u79d2.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#2","title":"2.\u76f8\u5173\u5de5\u4f5c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#modeling-the-tokens-of-neural-audio-codecs","title":"Modeling the Tokens of Neural Audio Codecs","text":"<p>Unsupervised speech embeddings (Wav2Vec2.0; HuBERT; W2V-BERT) have provided a low-framerate representation of the underlying signal which remains rich enough after discretization for language models to generate intelligible speech from a specific speaker as a sequence of tokens (Generative Spoken Language Modeling (2021)). Neural audio codecs (SoundStream (2021); EnCodec (2022)), with their ability of reconstructing high-quality audio at very low bitrates, subsequently allowed for extending discrete modeling to audio signals as diverse as multi-speaker speech and piano (AutoLM (2022); SPEAR-TTS (2023)), music (MusicLM (2023)) or sound effects (AudioGen (2022)). In particular, AudioLM (2022) introduces a hierarchical sequence-to-sequence approach where high-level semantic tokens are generated as an intermediate representation, which is then used as a conditioning signal for predicting tokens of a SoundStream (2021) codec. While this hierarchical approach has demonstrated remarkable results for speech (SPEAR-TTS (2023)) and music modeling (MusicLM (2023); SingSong (2023)), the computational cost of modeling flattened SoundStream tokens with self-attention scales quadratically with the sequence length and thus the bitrate of the neural codec, preventing these models from generating long-form audio with high quality. SoundStorm alleviates this issue by modeling the multi-level tokens of the neural codec in parallel, inducing a two-order of magnitude speed-up over autoregressive modeling and unlocking the ability to scale audio generation abilities both in quality and in sequence length.</p> <p>\u65e0\u76d1\u7763\u8bed\u97f3\u5d4c\u5165\u63d0\u4f9b\u4e86\u57fa\u7840\u4fe1\u53f7\u7684\u4f4e\u5e27\u7387\u8868\u793a, \u5728\u79bb\u6563\u5316\u540e\u4ecd\u7136\u4fdd\u7559\u8db3\u591f\u4e30\u5bcc\u7684\u4fe1\u606f, \u4f7f\u5f97\u8bed\u8a00\u6a21\u578b\u4ece\u7279\u5b9a\u8bf4\u8bdd\u4eba\u5904\u4ee5\u4e00\u7cfb\u5217 token \u7684\u5f62\u5f0f\u751f\u6210\u53ef\u7406\u89e3\u7684\u8bed\u97f3. \u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668, \u7531\u4e8e\u5b83\u4eec\u80fd\u591f\u5728\u975e\u5e38\u4f4e\u6bd4\u7279\u7387\u4e0b\u91cd\u5efa\u9ad8\u8d28\u91cf\u97f3\u9891\u7684\u80fd\u529b, \u5141\u8bb8\u5c06\u79bb\u6563\u5efa\u6a21\u6269\u5c55\u5230\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3, \u94a2\u7434, \u97f3\u4e50\u6216\u58f0\u6548. \u7279\u522b\u662f AutoLM (2022) \u5f15\u5165\u4e86\u4e00\u79cd\u5c42\u6b21\u5e8f\u5217\u5230\u5e8f\u5217\u65b9\u6cd5, \u9ad8\u7ea7\u522b\u8bed\u4e49 token \u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a, \u4e4b\u540e\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u7528\u4e8e\u9884\u6d4b SoundStream \u7f16\u89e3\u7801\u5668\u7684 token. \u867d\u7136\u8fd9\u4e00\u5c42\u6b21\u65b9\u6cd5\u5df2\u7ecf\u5728\u8bed\u97f3\u548c\u97f3\u4e50\u5efa\u6a21\u65b9\u9762\u83b7\u5f97\u4e86\u663e\u8457\u6210\u679c, \u4f46\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u5bf9\u6241\u5e73\u5316 SoundStream token \u8fdb\u884c\u5efa\u6a21\u7684\u8ba1\u7b97\u6210\u672c\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u548c\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u7684\u6bd4\u7279\u7387\u5448\u4e8c\u6b21\u589e\u957f, \u4ece\u800c\u963b\u6b62\u4e86\u8fd9\u4e9b\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u957f\u5f62\u5f0f\u97f3\u9891. SoundStorm \u901a\u8fc7\u5e76\u884c\u5efa\u6a21\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u7684\u591a\u7ea7\u522b token \u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898, \u4e0e\u81ea\u56de\u5f52\u5efa\u6a21\u76f8\u6bd4\u901f\u5ea6\u63d0\u9ad8\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7, \u5e76\u5728\u8d28\u91cf\u548c\u5e8f\u5217\u957f\u5ea6\u65b9\u9762\u6269\u5c55\u97f3\u9891\u751f\u6210\u80fd\u529b.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#rvq-aware-architectures","title":"RVQ-Aware Architectures","text":"<p>A common design choice for modeling RVQ token sequences is to sum the embeddings corresponding to the same RVQ input embedding (frame) in order to reduce the sequence length. Operating on such sequences, AudioGen (2022) proposes a Transformer with $Q$ separate heads for the different RVQ levels, predicting the tokens for an RVQ frame in parallel. While providing a significant speedup for inference, the authors found that, for text-to-audio generation, this approach has an inferior performance compared to modeling the token sequence of a neural audio codec with similar bitrate and reconstruction quality, but with a single level of quantization.</p> <p>\u5bf9\u4e8e\u5efa\u6a21\u6b8b\u5dee\u77e2\u91cf\u91cf\u5316 token \u5e8f\u5217\u7684\u5e38\u89c1\u8bbe\u8ba1\u662f\u5c06\u5bf9\u5e94\u540c\u4e00\u4e2a RVQ \u8f93\u5165\u5d4c\u5165 (\u5e27) \u7684\u5d4c\u5165\u76f8\u52a0\u4ee5\u51cf\u5c11\u5e8f\u5217\u957f\u5ea6. \u5728\u5904\u7406\u8fd9\u4e00\u5e8f\u5217\u65f6, AudioGen (2022) \u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709 $Q$ \u4e2a\u5206\u522b\u7528\u4e8e\u4e0d\u540c RVQ \u7ea7\u522b\u7684\u72ec\u7acb\u5934\u7684 Transformer, \u5e76\u5e76\u884c\u9884\u6d4b RVQ \u5e27\u7684 tokens. \u867d\u7136\u4e3a\u63a8\u7406\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u52a0\u901f, \u4f46\u4f5c\u8005\u53d1\u73b0\u5bf9\u4e8e\u6587\u672c\u5230\u97f3\u9891\u751f\u6210, \u4e0e\u5177\u6709\u76f8\u4f3c\u6bd4\u7279\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u4f46\u53ea\u6709\u4e00\u7ea7\u91cf\u5316\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684 token \u5e8f\u5217\u5efa\u6a21\u76f8\u6bd4, \u8fd9\u79cd\u65b9\u6cd5\u6027\u80fd\u8f83\u5dee.</p> <p>VALL-E (2023) instead relies on a hybrid approach, where the tokens corresponding to the first RVQ level are predicted autoregressively, and the subsequent levels are produced non-autoregressively. The latter is achieved by a model that sums up the embeddings from the same RVQ input frame, and applies bidirectional self-attention to predict all tokens from RVQ level $q+1$ given all tokens from levels $1,\\cdots,q$, the acoustic prompt, and the phoneme sequence. During inference, tokens starting from the second level of the RVQ are produced iteratively, performing greedy decoding (choosing the most likely tokens) level-by-level. Level-wise greedy decoding represents the baseline for our method.</p> <p>VALL-E (2023) \u5219\u4f9d\u8d56\u4e8e\u6df7\u5408\u65b9\u6cd5, \u5176\u4e2d\u5bf9\u5e94\u7b2c\u4e00\u4e2a RVQ \u7ea7\u522b\u7684 token \u662f\u81ea\u56de\u5f52\u9884\u6d4b\u7684, \u540e\u7eed\u7ea7\u522b\u4ee5\u975e\u81ea\u56de\u5f52\u751f\u6210. \u540e\u8005\u901a\u8fc7\u4e00\u4e2a\u6a21\u578b\u5b9e\u73b0, \u5c06\u6765\u81ea\u540c\u4e00\u4e2a RVQ \u8f93\u5165\u5e27\u7684\u5d4c\u5165\u6c42\u548c, \u4e14\u5e94\u7528\u53cc\u5411\u81ea\u6ce8\u610f\u529b\u5728\u7ed9\u5b9a $1,\\cdots,q$ \u7ea7\u522b, \u58f0\u5b66\u63d0\u793a\u548c\u97f3\u7d20\u5e8f\u5217\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b RVQ $q+1$ \u7ea7\u522b\u7684 token. \u5728\u63a8\u7406\u65f6, \u4ece RVQ \u7684\u7b2c\u4e8c\u7ea7\u5f00\u59cb\u8fed\u4ee3\u5f0f\u751f\u6210 token, \u9010\u7ea7\u6267\u884c\u8d2a\u5fc3\u89e3\u7801 (\u9009\u62e9\u6700\u53ef\u80fd\u7684 token). \u9010\u7ea7\u8d2a\u5fc3\u89e3\u7801\u8868\u793a\u6211\u4eec\u65b9\u6cd5\u7684\u57fa\u7ebf.</p> <p>Modeling sequences produced by RVQ has been also investigated in domains other than audio. For example, the RQ-Transformer (2022) also adds up the embeddings corresponding to the same RVQ input frame, but factorizes the full joint distribution efficiently with a spatial and a depth Transformer, for modeling autoregressively the RVQ frames and tokens within the frames, respectively. While it has not been demonstrated yet, this approach, potentially coupled with parallel decoding schemes, is a promising future avenue for audio generation.</p> <p>\u9664\u4e86\u97f3\u9891\u9886\u57df, RVQ \u4ea7\u751f\u7684\u5e8f\u5217\u5efa\u6a21\u4e5f\u5728\u5176\u4ed6\u9886\u57df\u8fdb\u884c\u4e86\u7814\u7a76. \u4f8b\u5982 RQ-Transformer (2022) \u4e5f\u5c06\u5bf9\u5e94\u4e8e\u540c\u4e00 RVQ \u8f93\u5165\u5e27\u7684\u5d4c\u5165\u76f8\u52a0, \u4f46\u901a\u8fc7\u7a7a\u95f4\u548c\u6df1\u5ea6 Transformer \u6709\u6548\u5730\u5206\u89e3\u4e86\u8054\u5408\u5206\u5e03, \u5206\u522b\u7528\u4e8e\u81ea\u56de\u5f52\u5efa\u6a21 RVQ \u5e27\u548c\u5e27\u5185\u7684 token. \u867d\u7136\u8fd8\u6ca1\u6709\u5c55\u793a, \u4f46\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u4e0e\u5e76\u884c\u89e3\u7801\u65b9\u6848\u7ed3\u5408, \u662f\u97f3\u9891\u751f\u6210\u7684\u6709\u524d\u666f\u7684\u672a\u6765\u65b9\u5411.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#parallel-decoding","title":"Parallel Decoding","text":"<p>In order to improve the inference time and to allow bidirectional non-causal attention on the input sequence, parallel decoding schemes have been proposed for text (NAT (2017); Mask-Predict (2019)), image (MaskGIT (2022)) and video generation (Phenaki (2022)). Of particular relevance to our work is the parallel, iterative sampling scheme of MaskGIT (2022). During inference time, MaskGIT starts from masked tokens, and in each round, predicts a portion of the tokens based on confidence scores. The portion of the predicted tokens in each round is controlled by a schedule, and usually progressively increases over the iterations - once predicted, the tokens are treated as fixed. Our proposed decoding scheme can be seen as the extension of MaskGIT\u2019s decoding to token sequences produced by residual quantization.</p> <p>\u4e3a\u4e86\u63d0\u9ad8\u63a8\u7406\u65f6\u95f4\u4ee5\u53ca\u5141\u8bb8\u5728\u8f93\u5165\u5e8f\u5217\u4e0a\u8fdb\u884c\u53cc\u5411\u975e\u56e0\u679c\u6ce8\u610f\u529b, \u5df2\u7ecf\u6709\u4e86\u7528\u4e8e\u6587\u672c, \u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u7684\u5e76\u884c\u89e3\u7801\u65b9\u6848. \u548c\u672c\u9879\u5de5\u4f5c\u6700\u76f8\u5173\u7684\u662f MaskGIT (2022) \u7684\u5e76\u884c, \u8fed\u4ee3\u91c7\u6837\u65b9\u6848. \u5728\u63a8\u7406\u65f6, MaskGIT \u4ece\u63a9\u7801\u6807\u8bb0\u5f00\u59cb, \u5728\u6bcf\u4e00\u8f6e\u4e2d, \u6839\u636e\u7f6e\u4fe1\u5ea6\u5206\u6570\u9884\u6d4b\u4e00\u90e8\u5206 token. \u6bcf\u8f6e\u9884\u6d4b\u7684\u90e8\u5206 token \u901a\u8fc7\u4e00\u4e2a\u8ba1\u5212\u8868\u63a7\u5236, \u901a\u5e38\u5728\u8fed\u4ee3\u4e2d\u9010\u6e10\u589e\u52a0 - \u4e00\u65e6\u9884\u6d4b, token \u88ab\u89c6\u4e3a\u56fa\u5b9a. \u6211\u4eec\u63d0\u51fa\u7684\u89e3\u7801\u65b9\u6848\u53ef\u4ee5\u89c6\u4e3a MaskGIT \u7684\u89e3\u7801\u5728\u6b8b\u5dee\u91cf\u5316\u751f\u6210\u7684 token \u5e8f\u5217\u7684\u6269\u5c55.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#3","title":"3.\u65b9\u6cd5","text":"<p>SoundStorm receives as input a sequence of discrete tokens representing the conditioning signal and produces as output a sequence of SoundStream tokens, which can be decoded back to audio waveforms. We assume that the conditioning signal is time-aligned with the SoundStream frames or can be upsampled to the same rate. Such a conditioning signal is, for example, the semantic token sequence used in AudioLM (2022), SPEAR-TTS (2023), or MusicLM, which makes our method a drop-in replacement for the acoustic generators of these models.</p> <p>We leave the extension to other types of conditioning signals via cross-attention or to unconditional sampling for future work, and focus our presentation of SoundStorm as the acoustic generator within AudioLM (2022), replacing both AudioLM (2022)\u2019s coarse and fine acoustic modeling stages.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#31","title":"3.1.\u7f51\u7edc\u67b6\u6784","text":"<p>The architecture of the model is illustrated in Figure 1.</p> <p></p> <p>At the input side, we interleave the time-aligned conditioning tokens with the SoundStream tokens at the frame level, embed the resulting sequence, sum the embeddings corresponding to the same frame, including the embedding of the conditioning token, and pass the resulting continuous embeddings to a Conformer. Consequently, the sequence length for bidirectional self-attention in the Conformer is determined by the number of SoundStream frames (typically 50 per second), and thus is independent of the number of RVQ levels $Q$, allowing one to handle audio with length on the order of minutes. At the output side, we use $Q$ dense layers as heads to produce the target SoundStream tokens.</p> <p>\u5728\u8f93\u5165\u4fa7, \u6211\u4eec\u5c06\u65f6\u95f4\u5bf9\u9f50\u7684\u6761\u4ef6 token \u548c SoundStream token \u5728\u5e27\u7ea7\u522b\u4ea4\u9519, \u5d4c\u5165\u7ed3\u679c\u5e8f\u5217, \u5bf9\u5e94\u540c\u4e00\u5e27\u7684\u5d4c\u5165\u6c42\u548c, \u5305\u62ec\u6761\u4ef6 token \u5d4c\u5165, \u5e76\u5c06\u7ed3\u679c\u8fde\u7eed\u5d4c\u5165\u4f20\u9012\u7ed9 Conformer. \u56e0\u6b64, Conformer \u4e2d\u4f7f\u7528\u7684\u53cc\u5411\u81ea\u6ce8\u610f\u529b\u7684\u5e8f\u5217\u957f\u5ea6\u7531 SoundStream \u5e27\u6570\u51b3\u5b9a, \u548c RVG \u7ea7\u522b\u6570\u65e0\u5173, \u4ece\u800c\u53ef\u4ee5\u5904\u7406\u957f\u5ea6\u4e3a\u5206\u949f\u7ea7\u7684\u97f3\u9891. \u5728\u8f93\u51fa\u4fa7, \u6211\u4eec\u4f7f\u7528 $Q$ \u4e2a\u7a20\u5bc6\u5c42\u4f5c\u4e3a\u5934\u6765\u4ea7\u751f\u76ee\u6807 SoundStream token.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#32masking","title":"3.2.Masking","text":"<p>For designing our masking and decoding, we extend the masking and confidence-based parallel decoding scheme of MaskGIT (2022) to token sequences produced by RVQ. At a high level, our approach can be seen as following the strategy of MaskGIT (2022) per RVQ level in a coarse-to-fine order. The coarse-to-fine ordering is of particular importance, since it not only respects the conditional dependencies between levels of the RVQ hierarchy, but also exploits the conditional independence of tokens from finer levels given all tokens from coarser levels. The tokens of finer levels are responsible for local, fine acoustic details and can thus be sampled in parallel without a loss of audio quality.</p> <p>\u4e3a\u4e86\u8bbe\u8ba1\u6211\u4eec\u7684\u63a9\u7801\u548c\u89e3\u7801, \u6211\u4eec\u5c06 MaskGIT (2022) \u4e2d\u7684\u63a9\u7801\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5e76\u884c\u89e3\u7801\u65b9\u6848\u6269\u5c55\u5230\u7531 RVQ \u751f\u6210\u7684 token \u5e8f\u5217. \u5728\u9ad8\u7ea7\u522b, \u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u89c6\u4e3a\u6309\u7c97\u5230\u7ec6\u987a\u5e8f\u9010\u4e2a RVQ \u7ea7\u522b\u9075\u5faa MashGIT \u7b56\u7565. \u7c97\u5230\u7ec6\u7684\u987a\u5e8f\u7279\u522b\u91cd\u8981, \u56e0\u4e3a\u4e0d\u4ec5\u9075\u5faa\u4e86 RVQ \u5c42\u6b21\u7ed3\u6784\u4e2d\u5404\u4e2a\u7ea7\u522b\u4e4b\u95f4\u7684\u6761\u4ef6\u4f9d\u8d56\u6027, \u8fd8\u5229\u7528\u4e86\u7ed9\u5b9a\u6240\u6709\u66f4\u7c97\u7ea7\u522b token \u65f6\u6765\u81ea\u66f4\u7ec6\u7ea7\u522b token \u7684\u6761\u4ef6\u65e0\u5173\u6027. \u8f83\u7ec6\u7ea7\u522b\u7684 token \u8d1f\u8d23\u5c40\u90e8\u7684, \u7cbe\u7ec6\u7684\u58f0\u5b66\u7ec6\u8282, \u56e0\u6b64\u53ef\u4ee5\u5e76\u884c\u91c7\u6837\u800c\u4e0d\u635f\u5931\u97f3\u9891\u8d28\u91cf.</p> <p>We design our masking scheme for training accordingly. To enable voice prompting, we randomly sample a timestep $t\\in{1,\\cdots,T}$, where $T$ denotes the maximum sequence length, and we do not mask any tokens before this timestep. The conditioning tokens are never masked. Let $Y\\in{1,\\cdots, C}^{T\\times Q}$ denote the SoundStream tokens, where $C$ indicates the codebook size used in each RVQ level out of the $Q$ levels. Our masking scheme proceeds as follows: - Sample the prompt delimiter timestep $t\\sim\\mathcal{U}{0,T-1}$; - Sample the current RVQ level $q\\sim\\mathcal{U}{1,Q}$; - Sample the mask $M \\in {0, 1}^T$ according to a cosine schedule (MaskGIT (2022)) for level $q$, i.e., sample the masking ratio $p = \\cos(u)$ where $u \\sim \\mathcal{U}[0, \\pi/2]$, and sample iid $M_i\\sim \\text{Bernoulli}(p)$. - Mask the selected non-prompt tokens at the current RVQ level $q$ (mask $Y_{t',q}$ if $M_{t'}= 1$ and $t'&gt;t$) and all non-prompt tokens at finer RVQ levels ($Y_{&gt;t,&gt;q}$).</p> <p>\u6211\u4eec\u5bf9\u5e94\u5730\u8bbe\u8ba1\u4e86\u6211\u4eec\u7684\u63a9\u7801\u65b9\u6848. \u4e3a\u4e86\u80fd\u591f\u8bed\u97f3\u63d0\u793a, \u6211\u4eec\u968f\u673a\u91c7\u6837\u4e00\u4e2a\u65f6\u95f4\u6b65\u5e76\u4e0d\u5bf9\u8be5\u65f6\u95f4\u6b65\u4e4b\u524d\u7684 token \u8fdb\u884c\u63a9\u76d6. \u6761\u4ef6 token \u4e0d\u4f1a\u88ab\u63a9\u76d6. \u8bb0 SoundStream token \u4e3a $Y\\in{1,\\cdots, C}^{T\\times Q}$, $C$ \u4e3a\u5728\u6bcf\u4e2a RVQ \u7ea7\u522b\u4e2d\u4f7f\u7528\u7684\u7801\u672c\u5c3a\u5bf8, \u4e00\u5171\u6709 $Q$ \u4e2a\u7ea7\u522b. \u6211\u4eec\u7684\u63a9\u7801\u65b9\u6848\u5982\u4e0b: - \u91c7\u6837\u63d0\u793a\u7684\u5206\u9694\u7b26\u65f6\u95f4\u6b65 $t$; - \u91c7\u6837\u5f53\u524d RVQ \u7ea7\u522b $q$; - \u6839\u636e\u4f59\u5f26\u8c03\u5ea6\u5bf9\u7ea7\u522b $q$ \u91c7\u6837\u63a9\u7801 $M$, \u5373\u5148\u91c7\u6837\u63a9\u7801\u7387, \u518d\u6309\u7167\u4f2f\u52aa\u5229\u5206\u5e03\u91c7\u6837\u63a9\u7801; - \u5f53\u524d RVQ \u7ea7\u522b\u5bf9\u975e\u63d0\u793a token \u8fdb\u884c\u63a9\u7801.</p> <p>Given a masked token sequence, we train the model with cross-entropy loss with the ground-truth tokens as target, where the loss is only calculated on the masked tokens within the $q$-th RVQ level. An example of this masking scheme is illustrated in Figure01, with $T = 4$, $Q = 3$, $t = 0$ and $q = 2$.</p> <p>\u7ed9\u5b9a\u4e00\u4e2a\u63a9\u7801 token \u5e8f\u5217, \u6211\u4eec\u7528\u548c\u76ee\u6807 token \u4e4b\u95f4\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u6765\u8bad\u7ec3\u6a21\u578b, \u5176\u4e2d\u635f\u5931\u53ea\u5728\u7b2c $q$ \u4e2a RVQ \u7ea7\u522b\u7684\u63a9\u7801 token \u4e0a\u8ba1\u7b97. \u63a9\u7801\u65b9\u6848\u7684\u4f8b\u5b50\u5982\u56fe\u4e00\u6240\u793a.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#33iterative-parallel-decoding","title":"3.3.Iterative Parallel Decoding","text":"<p>Given a conditioning signal, our decoding scheme starts with all SoundStream tokens masked out except for the ones of the prompt (if provided). Then, it proceeds to sampling the tokens RVQ level-wise in a coarse-to-fine order, only proceeding to level $q + 1$ when all tokens for levels $1,\\cdots, q$ have been sampled. Within an RVQ level, we use the confidence-based sampling scheme of MaskGIT (2022). Namely, we perform multiple forward passes, and at each iteration $i$, we sample candidates for the masked positions, retaining $p_i$ of them based on confidence scores, where $p_i$ follows a cosine schedule. Compared to MaskGIT (2022), we use greedy decoding instead of confidence-based sampling for the last iteration within each RVQ level, which we found to improve the perceived audio quality.</p> <p>\u7ed9\u5b9a\u4e00\u4e2a\u6761\u4ef6\u4fe1\u53f7, \u6211\u4eec\u7684\u89e3\u7801\u65b9\u6848\u4ece\u9664\u4e86\u63d0\u793a\u4e4b\u5916\u7684\u6240\u6709 SoundStream token \u5f00\u59cb. \u7136\u540e, \u5b83\u9010\u7ea7\u5728\u7c97\u5230\u7ec6\u987a\u5e8f\u4e2d\u91c7\u6837 tokens. \u5728\u4e00\u4e2a\u7ea7\u522b\u5185, \u6211\u4eec\u4f7f\u7528\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u91c7\u6837\u65b9\u6848. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u6267\u884c\u591a\u6b21\u524d\u5411\u4f20\u9012, \u5728\u7b2c $i$ \u6b21\u8fed\u4ee3\u4e2d\u6211\u4eec\u91c7\u6837\u63a9\u7801\u4f4d\u7f6e\u7684\u5019\u9009, \u6839\u636e\u7f6e\u4fe1\u5ea6\u5206\u6570\u4fdd\u7559 $p_i$ \u4e2a, $p_i$ \u670d\u4ece\u4f59\u5f26\u8c03\u5ea6. \u548c MaskGIT \u76f8\u6bd4, \u6211\u4eec\u5728\u6bcf\u4e2a RVQ \u7ea7\u522b\u7684\u6700\u540e\u4e00\u6b21\u8fed\u4ee3\u4e2d\u4f7f\u7528\u8d2a\u5fc3\u89e3\u7801\u800c\u4e0d\u662f\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u91c7\u6837, \u6211\u4eec\u53d1\u73b0\u8fd9\u6837\u80fd\u591f\u63d0\u9ad8\u611f\u77e5\u5230\u7684\u97f3\u9891\u8d28\u91cf.</p> <p>Performing the decoding RVQ level-wise makes it possible to exploit the conditional independence assumption in finer levels, namely that multiple finer tokens can be sampled in parallel since they represent local, fine acoustic details. This implies that we can decrease the number of forward passes significantly as we progress to finer RVQ levels during decoding.</p> <p>\u9010\u7ea7\u8fdb\u884c\u89e3\u7801\u4f7f\u5f97\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u66f4\u7ec6\u7ea7\u522b\u4e2d\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u5047\u8bbe, \u5373\u591a\u4e2a\u66f4\u7ec6\u7684 token \u53ef\u4ee5\u5e76\u884c\u91c7\u6837, \u56e0\u4e3a\u5b83\u4eec\u8868\u793a\u5c40\u90e8\u4e14\u7cbe\u7ec6\u7684\u58f0\u97f3\u7ec6\u8282. \u8fd9\u610f\u5473\u7740\u968f\u7740\u6211\u4eec\u5728\u89e3\u7801\u4e2d\u8fdb\u884c\u5230\u66f4\u7ec6 RVQ \u7ea7\u522b\u65f6\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u524d\u5411\u4f20\u9012\u7684\u6b21\u6570.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#4","title":"4.\u5b9e\u9a8c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#41","title":"4.1.\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u8bbe\u7f6e","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#42","title":"4.2.\u8bed\u97f3\u6e05\u6670\u5ea6, \u97f3\u9891\u8d28\u91cf, \u58f0\u97f3\u8868\u793a\u548c\u58f0\u5b66\u4e00\u81f4\u6027","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#5","title":"5.\u5bf9\u8bdd\u5408\u6210","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#6","title":"6.\u7ed3\u8bba","text":"<p>In this paper we present SoundStorm, a model that can synthesize high-quality audio from discrete conditioning tokens efficiently. When compared to the acoustic generator of AudioLM (2022), SoundStorm is two orders of magnitude faster and achieves higher temporal consistency when generating long audio samples. By combining a text-to-semantic token model similar to SPEAR-TTS (2023) with SoundStorm, we can scale text-to-speech synthesis to longer contexts and generate natural dialogues with multiple speaker turns, controlling both the voices of the speakers and the generated content.</p> <p>\u672c\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86 SoundStorm, \u4e00\u4e2a\u53ef\u4ee5\u9ad8\u6548\u5730\u4ece\u79bb\u6563\u6761\u4ef6 token \u751f\u6210\u9ad8\u8d28\u91cf\u97f3\u9891\u7684\u6a21\u578b. \u548c AudioLM (2022) \u7684\u58f0\u5b66\u751f\u6210\u5668\u76f8\u6bd4, SoundStorm \u8fd0\u884c\u901f\u5ea6\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7, \u4e14\u5728\u751f\u6210\u957f\u97f3\u9891\u6837\u672c\u65f6\u83b7\u5f97\u66f4\u9ad8\u7684\u65f6\u5e8f\u4e00\u81f4\u6027. \u901a\u8fc7\u7ed3\u5408\u7c7b\u4f3c\u4e8e SPEAR-TTS (2023) \u7684\u6587\u672c\u5230\u8bed\u4e49 token \u6a21\u578b, \u80fd\u591f\u5c06\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u6269\u5c55\u5230\u66f4\u957f\u5185\u5bb9\u548c\u751f\u6210\u5177\u6709\u591a\u4e2a\u8bf4\u8bdd\u4eba\u8f6e\u6b21\u7684\u81ea\u7136\u5bf9\u8bdd, \u540c\u65f6\u63a7\u5236\u8bf4\u8bdd\u4eba\u7684\u58f0\u97f3\u548c\u751f\u6210\u7684\u5185\u5bb9.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2023.05_SoundStorm/2023.05_SoundStorm/#7","title":"7.\u66f4\u5e7f\u6cdb\u7684\u5f71\u54cd","text":"<p>SoundStorm is a model for high-quality, efficient generation of neural audio codec-derived representations of audio. In this work, we use it as a replacement for the acoustic generation pipeline of AudioLM (2022) and SPEAR-TTS (2023). We acknowledge that the audio samples produced by the model may be influenced by the biases present in the training data, for instance in terms of represented accents and voice characteristics. In our generated samples, we demonstrate that we can reliably control speaker characteristics via prompting. However, a more thorough analysis of any training data and its limitations would be an area of future work in line with our responsible AI principles.</p> <p>SoundStorm \u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u8d28\u91cf, \u9ad8\u6548\u751f\u6210\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5bfc\u51fa\u97f3\u9891\u8868\u793a\u7684\u6a21\u578b. \u6211\u4eec\u5c06\u5176\u4f5c\u4e3a AudioLM (2022) \u548c SPEAR-TTS (2023) \u7684\u58f0\u5b66\u751f\u6210\u7ba1\u9053\u7684\u66ff\u4ee3\u54c1. \u6211\u4eec\u627f\u8ba4\u6a21\u578b\u751f\u6210\u7684\u97f3\u9891\u6837\u672c\u53ef\u80fd\u4f1a\u53d7\u5230\u8bad\u7ec3\u6570\u636e\u4e2d\u504f\u5dee\u7684\u5f71\u54cd, \u4f8b\u5982\u5728\u4ee3\u8868\u7684\u53e3\u97f3\u548c\u58f0\u97f3\u7279\u5f81\u65b9\u9762. \u5728\u751f\u6210\u6837\u672c\u4e2d, \u6211\u4eec\u5c55\u793a\u4e86\u53ef\u4ee5\u901a\u8fc7\u63d0\u793a\u53ef\u9760\u5730\u63a7\u5236\u8bf4\u8bdd\u4eba\u7279\u5f81. \u7136\u800c\u5bf9\u4e8e\u8bad\u7ec3\u6570\u636e\u53ca\u5176\u5c40\u9650\u6027\u8fdb\u884c\u66f4\u5f7b\u5e95\u5730\u5206\u6790\u5c06\u662f\u672a\u6765\u5de5\u4f5c\u7684\u4e00\u4e2a\u9886\u57df, \u4e0e\u6211\u4eec\u7684\u8d1f\u8d23\u4efb AI \u5143\u7ec4\u4fdd\u6301\u4e00\u81f4.</p> <p>In turn, the ability to mimic a voice can have numerous malicious applications, including bypassing biometric identification and for the purpose of impersonation. Thus, it is crucial to put in place safeguards against potential misuse: to this end, we have verified that the audio generated by SoundStorm remains detectable by a dedicated classifier (98.5%\u3000using the same classifier as AudioLM (2022)). Hence, as a component of a larger system, we believe that SoundStorm would be unlikely to introduce additional risks to those discussed previously by AudioLM (2022) and SPEAR-TTS (2023). At the same time, relaxing the memory and computational requirements of AudioLM (2022) would make research in the domain of audio generation more accessible to a wider community. In the future, we plan to explore other approaches for detecting synthesized speech, e.g., audio watermarking, so that any potential product usage of this technology strictly follows our responsible AI principles.</p> <p>\u53cd\u8fc7\u6765, \u6a21\u4eff\u58f0\u97f3\u7684\u80fd\u529b\u53ef\u80fd\u6709\u8bb8\u591a\u6076\u610f\u5e94\u7528, \u5305\u62ec\u7ed5\u8fc7\u751f\u7269\u8bc6\u522b\u8ba4\u8bc1\u548c\u7528\u4e8e\u5192\u5145. \u56e0\u6b64, \u5fc5\u987b\u91c7\u53d6\u63aa\u65bd\u9632\u6b62\u6f5c\u5728\u7684\u6ee5\u7528: \u4e3a\u6b64\u6211\u4eec\u9a8c\u8bc1\u4e86 SoundStorm \u751f\u6210\u7684\u97f3\u9891\u4ecd\u7136\u53ef\u4ee5\u88ab\u4e13\u7528\u7684\u5206\u7c7b\u5668\u68c0\u6d4b\u5230 (AudioLM (2022) \u76f8\u540c \u7684\u5206\u7c7b\u5668, \u68c0\u6d4b\u7387\u4e3a 98.5%). \u56e0\u6b64\u4f5c\u4e3a\u66f4\u5927\u7cfb\u7edf\u7684\u4e00\u90e8\u5206, \u6211\u4eec\u8ba4\u4e3a SoundStorm \u4e0d\u592a\u53ef\u80fd\u5f15\u5165\u7531 AudioLM (2022) \u548c SPEAR-TTS (2023) \u8ba8\u8bba\u7684\u989d\u5916\u98ce\u9669. \u540c\u65f6, \u653e\u677e AudioLM (2022) \u7684\u5185\u5b58\u548c\u8ba1\u7b97\u8981\u6c42\u5c06\u4f7f\u5f97\u97f3\u9891\u751f\u6210\u9886\u57df\u7684\u7814\u7a76\u5bf9\u66f4\u5e7f\u6cdb\u793e\u533a\u66f4\u53ef\u8bbf\u95ee. \u672a\u6765\u6211\u4eec\u8ba1\u5212\u63a2\u7d22\u68c0\u6d4b\u5408\u6210\u8bed\u97f3\u7684\u5176\u4ed6\u65b9\u6cd5, \u4f8b\u5982\u97f3\u9891\u6c34\u5370, \u4ee5\u4fbf\u4efb\u4f55\u6f5c\u5728\u7684\u4ea7\u54c1\u4f7f\u7528\u8fd9\u79cd\u6280\u672f\u4e25\u683c\u9075\u5faa\u6211\u4eec\u7684\u8d1f\u8d23\u4efb AI \u539f\u5219.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/","title":"ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering","text":"\u4f5c\u8005 \u673a\u6784 Yakun Song \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 Zhuo Chen Microsoft \u738b\u6653\u98de Microsoft \u9a6c\u5b50\u9633 \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 \u9648\u8c10 \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V1, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups2. Audio samples are available at https://ereboas.github.io/ELLAV/. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Recently, deep generative AI has achieved remarkable results in various tasks, leading to the emergence of many transformative real-world applications (Brown et al., 2020; Ramesh et al., 2022; Ho et al., 2020; Rombach et al., 2022; Borsos et al., 2023; Kim et al., 2021; Chiang et al., 2019). With the advancement of generative models, there have been rapid developments in the field of speech synthesis as well. In particular, zero-shot TTS technology has gained increasing attention because it can synthesize high-quality target voices without the need of specified speaker\u2019s training data. As a state-of-the-art generative model family, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song and Ermon, 2020) progressively add noise to the training data and then learn the reverse process to generate samples. By leveraging diffusion models and their variants (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song and Ermon, 2020; Song et al., 2021; Lipman et al., 2023), many works have successfully applied them to the audio domain (Popov et al., 2021; Huang et al., 2022, 2023; Shen et al., 2023). Another major class of generative models is language modeling based on Transformer (Vaswani et al., 2017a). Devlin et al. (2019); Raffel et al. (2020); Lewis et al. (2020) utilize encoder-only or encoder-decoder architectures to build masked language models so that they selectively focus on relevant segments and effectively model relationships in long sequences. However, masked language model often requires fine-tuning to adapt to specific tasks, which can be inconvenient for practical usage and deployment. On the other hand, AR language models use a decoder-only architecture to predict the next token in a sequence as the training objective, which has demonstrated extremely powerful few-shot and zero-shot capabilities in many generative tasks (Brown et al., 2020; Thoppilan et al., 2022; Chowdhery et al., 2023). In light of this, VALL-E (Wang et al., 2023a) and subsequent works (Kharitonov et al., 2023; Rubenstein et al., 2023; Wang et al., 2023b) have successfully employed decoder-only language model for zero-shot TTS. These approaches first quantize the speech signal into a series of discrete acoustic tokens. Subsequently, they employ an AR language model to predict coarse-grained acoustic tokens, eliminating the necessity for explicit duration predictors or speaker encoders. Once trained on a large-scale corpus, such as LibriLight (Kahn et al., 2020), these approaches are capable of synthesizing speech with competitive fidelity and naturalness in a zero-shot manner.</p> <p>While VALL-E and its variants have achieved numerous impressive milestones, they still possess certain limitations that impact practical deployment. For instance, existing methods (Wang et al., 2023a; Kharitonov et al., 2023) directly concatenate phoneme tokens and acoustic tokens as a whole sequence to train language models. In this way, the alignment between audio and phoneme sequences is completely learned through the self-attention in the transformer, making it potentially unstable as self-attention does not explicitly capture the monotonic alignment between audio and phoneme. Additionally, the decoder-only language model architecture can lead to potential attention degradation issues (Fu et al., 2023), where the alignment quality between the target audio sequence and the source phoneme sequence deteriorates as the generated sequence increases, resulting in inaccurate or low-quality speech outputs. Another limitation stems from the nature of AR language modeling. Specifically, given a sequence x, the standard AR language model factorizes the likelihood p(x)over the dimensions of x via the chain rule p(x) = QTt=0p(xt|x&lt;t). AR models predict the current tokens solely based on the historical tokens without users\u2019 control in the inference process, and sometimes generate semantic repetitions or incoherence in the generated output (Yang et al., 2019; Brown et al., 2020). In the TTS task, correspondingly, VALL-E cannot directly determine which segment of the output audio corresponds to which prompt phoneme, thus there is no trivial way to promptly detect and prevent issues occurring in the generation process. These drawbacks can manifest as meaningless phoneme repetitions, transpositions, omissions, or even catastrophic infinite silence, i.e., during the process of generation, the model anomalously outputs silence or noise tokens for an extended period of time without stopping. Specifically, Tab.01 demonstrates the word error rate (WER) and the probability of the infinite silence in VALL-E samples at different threshold top-p for nuclear sampling (Holtzman et al., 2019). The detailed experimental setup is described in Section 4. Notably, a shift in the decoding strategy of VALL-E from fully sampling-based to fully greedy-based leads to a marked decline in sample quality. It should be emphasized that while sampling-based stochastic decoding strategies have advantages in terms of synthesis diversity, deterministic decoding strategies (e.g., beam search and its variants) are more suitable for cases where there is less tolerance for synthesis errors and more emphasis on fluency and coherence (Ippolito et al., 2019).</p> <p>Faced with the pros and cons of the existing methods, we introduce ELLA-V, a simple but effective language model approach for zero-shot TTS. ELLA-V proposes a generalized AR (GAR) language model to generate the first layer of residual vector quantizer (RVQ) codes of a neural codec model. Then as with VALL-E, ELLA-V employs a non-autoregressive (NAR) language model to obtain codes of the other RVQs. Our core innovation lies in 3 fold: - Firstly, ELLA-V inserts phone tokens into the corresponding positions of the acoustic sequence. Unlike existing methods, Connecting phoneme tokens with their corresponding acoustic tokens can help the language model capture the alignment between phoneme and acoustic modalities in local dependencies. - Secondly, instead of maximizing the expected log-likelihood of the hybrid sequence under a conventional casual mask or a prefix mask like VALL-E and UniLM (Bao et al., 2020), ELLA-V computes loss only on acoustic tokens and special tokensEndOfPhone( EOP ) andEndOfSentence(EOS). This training objective not only reduces the redundant computation of cross-modal alignment in the output based on experimental results, but also provides a natural way to have fine-grained control in inference: the model predicts EOP , and then the user provides the next phone token. Meanwhile, ELLA-V\u2019s GAR model always maintains awareness of the phoneme it is currently synthesizing, allowing it to promptly detect and truncate any abnormal phoneme to avoid any possible infinite silence issue. - Thirdly, we further propose an improvement to the input sequence. We introduce local advance, which involves shifting the EOP token and the next-word phoneme token a few frames ahead. Intuitively, the pronunciation of a phoneme, especially its ending, is not only influenced by the context in history but also by the upcoming phonemes. By advancing these special tokens, the GAR model can better utilize local dependencies to predict the pronunciation of the current phoneme.</p> <p>Experimental results, using comparable model configurations and 960 hours of speech data from LibriSpeech (Panayotov et al., 2015) as a training set, demonstrate the superiority of ELLA-V. Compared to the state-of-the-art zero-shot TTS system VALL-E, ELLA-V significantly improves the accuracy of synthesized speech, and demonstrates comparable or superior speaker similarity and speech naturalness on a series of subjective and objective experiments. ELLA-V achieves a WER of 2.28% on the test-clean set of LibriSpeech. Notably, ELLA-V works well on a wide spectrum of decoding strategies \u2013 even greedy decoding, and still has a substantially better speech accuracy than the best of VALL-E. We further conducted ablation experiments to investigate the effects of our proposed modifications. The results indicate that the global advance in ELLA-V significantly improves the model\u2019s performance, while the local advance enhances the stability of the generated output.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#2related-work","title":"2.Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#language-modeling","title":"Language Modeling\u00b7\u8bed\u8a00\u5efa\u6a21","text":"<p>Recently, language models have garnered increasing interest in both the academic and industrial communities. Compared to models that are confined to specific tasks, language models have been proven to possess the capability to solve a wide array of tasks, shining across various domains such as text (Brown et al., 2020; Chowdhery et al., 2023; Rae et al., 2021; Yu et al., 2022), images (Alayrac et al., 2022; Tsimpoukelli et al., 2021), and videos (Yang et al., 2022; Wang et al., 2022). In the audio domain, AudioLM (Borsos et al., 2023) trains language models on discretized audio tokens, achieving speech synthesis tasks through hierarchical prediction of these tokens. AudioGen (Kreuk et al., 2023) employs an auto-encoding approach to extract discrete encodings of raw audio, and trains a language model conditioned on textual features for controlled audio generation. LM-VC (Wang et al., 2023d) employs three language models\u2014a masked prefix language model, an external LM, and a prefix LM\u2014to achieve zero-shot voice conversion.Kakouros et al. (2023) investigates the role of word surprisal, extracted from language models, in influencing the prosody of speech synthesized by TTS systems. For zero-shot TTS, Wang et al. (2023a) approaches TTS as a conditional language modeling task rather than a continuous signal regression. By employing discrete audio codes obtained from pre-trained neural codec, it trains a discrete audio language model, achieving improved naturalness in speech and preservation of speaker characteristics. VALL-E-X (Zhang et al., 2023) extends VALL-E by utilizing source language speech and target language text as prompts when predicting the acoustic marker sequence of the target language speech. This approach supports high-quality zero-shot cross-lingual voice synthesis. These methods require only a single utterance of an unknown speaker as a prompt to generate high-quality, specified speech.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#speech-synthesis","title":"Speech Synthesis\u00b7\u8bed\u97f3\u5408\u6210","text":"<p>Speech synthesis has long been a significant topic in the fields of artificial intelligence, natural language processing, and speech processing. Early methods were based on Statistical Parametric Speech Synthesis (SPSS) (Zen et al., 2009), typically involving complex components such as text analysis models, acoustic models, and vocoders (e.g., hidden Markov model(HMM) (Yoshimura et al., 1999) based). While cost-effective in terms of data, the generated speech of SPSS still exhibited noticeable differences from natural human speech. With the advancement of modern neural networks, some work initially replaced HMMs with recurrent neural networks (RNNs) but still followed the SPSS paradigm (Fan et al., 2014; Zen and Sak, 2015; Valentini-Botinhao et al., 2016). Later, end-to-end neural TTS models were introduced, which synthesize Mel spectrograms and employ a vocoder (Oord et al., 2017; Prenger et al., 2019) for speech synthesis (Wang et al., 2017; Ar\u0131k et al., 2017; Ren et al., 2019). Some methods, utilizing techniques such as VAE (Hsu et al., 2019; Lee et al., 2022), flow (Miao et al., 2020; Kim et al., 2020), diffusion (Jeong et al., 2021; Kim et al., 2022; Popov et al., 2021), and others (Wu and Shi, 2022), have achieved promising performance in end-to-end speech synthesis.On the other hand, models like VALL-E (Wang et al., 2023a) and AudioLM (Borsos et al., 2023) utilize autoregressive Transformers to model discrete audio tokens, achieving great in-context learning performance.When it comes to zero-shot speech synthesis, autoregressive Transformer-based models can predict and generate audio without the need for an additional duration model, which strikes a favorable balance between efficiency and performance, and has been garnering increasing attention.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#3method","title":"3.Method\u00b7\u65b9\u6cd5","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#31overview","title":"3.1.Overview\u00b7\u6982\u89c8","text":"<p>Fig.01 demonstrates the overall architecture of ELLA-V. ELLA-V primarily follows a two-stage framework similar to VALL-E, considering zero-shot TTS as a conditional codec language modeling task. ELLA-V maps input text prompts and speech prompts into a unified vocabulary space with a text encoder and a neural codec, respectively. Different from VALL-E, an additional sequence order rearranging step is performed to the text-audio token sequence, after which, ELLA-V utilizes a decoder-only language model to learn to perform conditional generation on the hybrid sequences of phoneme and audio tokens. Detailed information about the language model will be presented in Section 3.2. To obtain discrete audio representations, we employ a pre-trained neural audio codec model, EnCodec (D\u00e9fossez et al., 2023), following VALL-E (Wang et al., 2023a). EnCodec transforms 24 kHz raw waveforms into 75 Hz discrete tokens usingLRVQ layers. The discrete acoustic tokens have a hierarchical structure, where the first layer quantizer contains semantic information and coarse-grained acoustic contours, while subsequent L \u2212 1quantizers learn fine-grained acoustic details. In our experiments, we use the same settings as VALL-E, withL = 8. For each quantizer, we set the codebook size to 1024. In this setting, each second of the waveform is represented by75 \u00d7 8 discrete tokens from RVQ.</p> <p>To obtain phoneme sequences, we apply the Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) to the input audio and text transcriptions. Notably, MFA not only serves as a text tokenizer but also extracts alignment relationships between phonemes and the corresponding speech. The forced alignment information is essential for ELLA-V to change sequence order.In Section 3.2, we will provide a detailed explanation of how this information is used to construct the target sequence.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#32training-codec-language-model","title":"3.2.Training: Codec Language Model\u00b7\u8bad\u7ec3\uff1a\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b","text":"<p>ELLA-V employs a Generalized Autoregressive Codec language model for the prediction of the first quantization layer in the EnCodec, which corresponds to capturing semantic information and coarse-grained acoustic profiles. Subsequently, a non-autoregressive language model is utilized to generate codes for the subsequent quantization layers, aimed at reconstructing fine-grained acoustic details. Specifically, given a speech corpus D = {xi, yi}, wherexrepresents an audio sample, andyis its text transcription. We utilize the EnCodec to extract the discrete representation ofx, formulated as  whereCrepresents the two-dimensional acoustic code matrix, andTis the downsampled utterance length. We employ MFA to obtain the phoneme sequenceP1:ncorresponding to the transcriptiony, while also extracting forced alignment information between the audio x and the transcription y:  wherenis the number of phonemes of the audio samplex, andlidenotes the length of thei-th phoneme of the discrete audio sequence. MFA treats silence also as a kind of phoneme, so that the original audio sequence is partitioned intonconsecutive intervals corresponding tonphonemes. Specifically, let\u27e8Ci\u27e9li\u00d78represent the audio sequence corresponding to the i-th phoneme:</p> <p>After quantization, we utilize the EnCodec decoder to reconstruct the audio waveform from the discrete acoustic sequenceC, formulated as \u02c6x \u2248 DeCodec(C). For the zero-shot TTS task, the optimization objective is max p(C|P, \u02c6C), where\u02c6Cis the acoustic prompt of the unseen speaker. We use language modeling to generate acoustic tokens for the unseen speaker, by learning on the mixed sequence composed of phonemes and codec codes, consistent with previous works (Wang et al., 2023a; Rubenstein et al., 2023). Unlike existing approaches, ELLA-V does not concatenate phoneme tokens and acoustic tokens directly to form the target sequence for training the language model. Instead, ELLA-V interleaves phoneme and acoustic tokens in order to make it easier for language models to learn the alignment between audio and text. Specifically, we insert each phoneme tokenPi(except the silence phoneme) into the corresponding position of the audio sequence, so that each phoneme\u2019s audio\u27e8Ci\u27e9 is sandwiched between Pi and EOP tokens. We also prepend the phoneme sequence to the beginning of the mixed sequence, which is referred to as global advance. In Section 3.4, we further propose a variant sequence order with higher generation stability, named local advance, which moves the non-acoustic tokens of the sequence several frames forward.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#321","title":"3.2.1.","text":"<p>Generalized Autoregressive Codec (GAR) Codec Language Model\u00b7\u901a\u7528\u81ea\u56de\u5f52\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b</p> <p>As shown in Fig.02, ELLA-V first constructs a hybrid sequenceH:,1of acoustic and phoneme tokens, structured as:</p> <p>It is worth noting that the MFA (Montreal Forced Aligner) treats silence as a distinct phoneme, whereas our phoneme sequencePexclusively comprises phonemes other than silence. To clarify, we retain the acoustic component associated with silence but do not sandwich it with an EOP and a specific silence phoneme, nor do we use a silence phoneme in the global advance part. We design a GAR language model to learn the continuation task on the aforementioned hybrid sequence, to generate the discrete acoustic code sequenceC:,1. The GAR model consists of multiple Transformer decoder layers (Vaswani et al., 2017b). After training, it can generate discrete audio codes for a specified text prompt and acoustic prompt. GAR is also responsible for predicting EOP and EOSto indicate the conclusion of a phoneme and the entire sentence, respectively. The optimization of GAR is achieved by maximizing the likelihood of the acoustic partC:,1of the hybrid sequenceH:,1, as well as the special EOP andEOStokens. Under forward factorization, this process is formulated as:</p> <p>where H has a size ofTH\u00d7 8,{P}denotes the phoneme set,\ufffd\u02dcCi\ufffdis the concatenation of \u27e8Ci\u27e9 along with its broadcast trailing EOP and/or EOStokens,\u02dcCis then the concatenation of\u27e8Ci\u27e9, and \u03b8 GAR represents neural network parameters of GAR model.The factorization of the training objective naturally encapsulates the core intuition of the GAR model: GAR generates the audio sequence phoneme-by-phoneme. GAR produces maximum likelihood predictions for each phoneme token successively, indicating the end of generating a specified phoneme by predicting EOP . Through global advancement, GAR can directly infer the next phoneme to be generated without relying on network predictions. After the prediction for the last phoneme is completed, GAR stops the generation process by predictingEOS. The generated sequence by GAR is self-aligned, as it can instantly know the corresponding position of any generated acoustic token in relation to the phoneme prompt. During training, we apply a bidirectional mask to the phoneme sequence before the BOS in the hybrid sequence, while a unidirectional mask is used for the part after BOS . We frame the training as a next-token-prediction language modeling task on the hybrid sequence. However, it\u2019s important to note that the model does not predict phonemes (or BOS). In other words, as shown in Fig.02, we only compute loss when the token to be predicted is not a phoneme (or BOS). During inference, whenever the model predicts an EOP for a phoneme, the next phoneme token is directly appended to the end of the sequence, which will be further discussed in Section 4.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#322non-autoregressive-nar-codec-language-model","title":"3.2.2.Non-Autoregressive (NAR) Codec Language Model\u00b7\u975e\u81ea\u56de\u5f52\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b","text":"<p>In the second stage, the NAR language model is employed to predict the codes from the second to the last quantization layers in parallel. The input-output sequence construction of the NAR model follows the same pattern as used in the GAR model discussed in Section 3.2.1. Specifically, the i-th columnH:,iof the hybrid sequence matrixHis structured as:</p> <p>And in practice if Pi represents the silence,C:,i will not be sandwiched by Pi and EOP . The NAR model takes the previously generated hybrid sequence of the previous j \u2212 1layers as input and predicts the codes of the j-th layer in parallel, formulated as:</p> <p>where {C:,j} denotes the acoustic token set of the j-th quantizer. In this formulation, The embeddings of tokens from the previous j \u2212 1quantizers are summed up to feed the NAR model to predict the j-th layer. Intuitively, both the GAR and NAR model of ELLA-V compute the loss on the acoustic tokens of the target sequence, and GAR additionally computes loss for EOP and EOS.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#33inference","title":"3.3.Inference\u00b7\u63a8\u7406","text":"<p>ELLA-V can use a short clip of speech from an unseen speaker as an acoustic prompt to synthesize speech for a specified text prompt. Fig.03 illustrates the inference process of the GAR model. While VALL-E may get stuck in an infinite loop during inference, resulting in the synthesis of either infinite silence or repetitive pronunciation (Wang et al., 2023a), ELLA-V is capable of generating EOP and promptly truncating abnormally long phonemes. Following an EOP , we can directly append the next phoneme token to the end of the generated sequence, ensuring the proper generation of speech without abnormal pauses or repetitions. For the GAR model, we employ a sampling-based decoding strategy, whereas for the NAR model, we use a greedy decoding approach to strike a balance between efficiency and performance.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#34local-advance","title":"3.4.Local Advance\u00b7\u5c40\u90e8\u8fdb\u6b65","text":"<p>One intuition is that the pronunciation of a phoneme is strongly related to the pronunciation of the phonemes just before and after it. However, due to the autoregressive nature of the GAR model, an acoustic token cannot attend to the following phoneme tokens, even though we can leverage the transformer\u2019s ability to model long-term dependencies through global advance to provide complete context for the acoustic token generation. To further harness the powerful capability of the transformer in modeling local dependencies, ELLA-V introduces an additional change in the sequence order based on Section 3.2. Specifically, we move the phoneme token and the EOP token ahead by a few frames, referred to as local advance.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#4experiment","title":"4.Experiment\u00b7\u5b9e\u9a8c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#41experi-mental-setup","title":"4.1.Experi-mental Setup\u00b7\u5b9e\u9a8c\u8bbe\u7f6e","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#data-tasks","title":"Data &amp; Tasks\u00b7\u6570\u636e\u4e0e\u4efb\u52a1","text":"<p>We trained ELLA-V using the Librispeech (Panayotov et al., 2015) 960h training dataset. We utilized Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) to obtain forced alignment information for the audio-transcription pairs. Sentences with unrecognized or unknown phones by MFA were excluded. The open-source 24kHz checkpoint3of EnCodec(D\u00e9fossez et al., 2023) was used as the codec to generate discrete acoustic tokens. The LibriSpeech training data was upsampled to 24 kHz before feeding it into EnCodec. In evaluating the model, two zero-shot TTS tasks were considered. For the zero-shot TTS continuation task, we adhered to methodologies established by previous works (Wang et al., 2023a; Le et al., 2023; Wang et al., 2023c), selecting examples ranging from 4 seconds to 10 seconds from the LibriSpeech test-clean dataset as our test set. In this task, we used the complete phoneme transcription as the text prompt and the first 3 seconds of the test audio sample as the acoustic prompt. The model was required to generate continuations. For the zero-shot TTS cross-speaker task, we designed a hard case set comprising 100 hard sentences, as outlined in the demo page . These sentences included challenging phonetic patterns, alliteration, and unusual (abnormal) combinations of words that might pose difficulties for a TTS system to generate natural-sounding speech. In this case, we randomly picked 3-second sentences from the LibriSpeech test-clean subset as the acoustic prompt. We then concatenated the transcription of this segment and the target phoneme sequence in the hard case set to form the text prompt. The model was tasked with cloning the voice of the speaker to say the specified target text in the hard case set.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#training-configuration","title":"Training Configuration\u00b7\u8bad\u7ec3\u914d\u7f6e","text":"<p>For both GAR and NAR models, we stacked 12 Transformer decoder layers with an embedding dimension of 1024, a hidden state dimension of 1024, and a feed-forward layer dimension of 4096. All models were trained in parallel using 8 NVIDIA Tesla V100 GPUs with a batch size of 16384 tokens for GAR and 12288 tokens for NAR per GPU, respectively, learning a total of 320k steps. We used the AdamW optimizer with \u03b21= 0.9,\u03b22= 0.999,\u03f5 = 10\u22129. We employed an inverse-sqrt learning rate scheduler with warm-up. For the first32000updates, we linearly increased the learning rate from10\u22127to a peak of 5 \u00d7 10\u22124. The weight decay was 0.01.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#baseline","title":"Baseline\u00b7\u57fa\u7ebf","text":"<p>In our research, we benchmarked the performance of zero-shot speech synthesis against VALL-E (Wang et al., 2023a). This system was originally trained on a substantial 60k hours of audio from the Librilight dataset (Kahn et al., 2020). To ensure a rigorous evaluation, we reproduced the VALL-E model and adapted it to train on the LibriSpeech 960h dataset. We also adjusted the model dimensions and the number of layers to match the parameter settings of ELLA-V and VALL-E. Both GAR (or AR) and NAR models of VALL-E and ELLA-V have 154.3M parameters. Moreover, to mitigate any potential bias introduced by the audio codec, we pre-processed the authentic speech samples using EnCodec\u2019s encoder and decoder. We also include the result for Encodec reconstructed speech for reference, denoted as Ground-Truth Encodec.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#evaluation-metrics","title":"Evaluation Metrics\u00b7\u8bc4\u4f30\u6307\u6807","text":"<p>We evaluated our system with several objective metrics. Speaker similarity (SPK) and WER served as our primary measures. SPK was assessed using the fine-tuned WavLMTDNN model4(Chen et al., 2022), scoring similarity on a scale of -1 to 1, with values above 0.86 indicate the same speaker identity (This value comes from the release model card page). The WER was determined by comparing the synthesized speech to the original text using the Conformer-Transducer model5(Gulati et al., 2020). In addition to these standard metrics, we introduced two novel measures: INF% and CUT%. INF% quantified the frequency of generating infinitely long audio, indicative of a failure in synthesis. It is used to measure the likelihood of the model falling into abnormal repetition (such as infinite silence). A higher INF% indicates poorer stability in the generated output of the model. In the practical implementation, INF% referred to the proportion of sentences for which generation was not stopped when the length of the generated audio reached twice the original, serving as a proxy for infinite generation. On the other hand, as discussed in the previous session, the design of ELLA-V enables the control of the duration for each phoneme during inference, thus avoiding the synthesis failure. In our experiments, we forcibly truncate the synthesis of phonemes with a length greater than 0.4 seconds. CUT% is used to measure the frequency of forced cuts of phonemes in synthesis by ELLA-V. For each objective metric, we reported average values over three experimental runs with different random seeds. For subjective analysis, we relied on the mean opinion score (MOS). 30 test samples were chosen for this purpose, with each sample being evaluated by at least 15 listeners for aspects like naturalness and speaker similarity. The comparative mean option score (CMOS) and the similarity mean option score (SMOS) were the key subjective metrics used. SMOS was rated on a 1 to 5 scale, in 0.5point increments, to gauge speaker similarity, while CMOS, ranging from -1 to 1, assessed the overall naturalness and quality of the synthesized speech against the baseline.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#42results","title":"4.2.Results\u00b7\u7ed3\u679c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#zero-shot-tts-continuation-tasktts","title":"Zero-Shot TTS Continuation Task\u00b7\u96f6\u6837\u672cTTS\u7eed\u5199\u4efb\u52a1","text":"<p>We present the evaluation results in Tab.02, where a comparison between ELLA-V and VALL-E is shown. First, regarding speaker similarity, both subjective (SMOS) and objective (SPK) results indicate that ELLA-V and VALL-E performed similarly, which can be attributed to their shared backbone approach, combining (G)AR and NAR. Meanwhile, CMOS testing shows that ELLA-V achieved a +0.10 score, demonstrating a higher generation quality (i.e., naturalness) compared to VALL-E. Additionally, WERs calculated between the recognized text of synthesized audio and the ground-truth text show that ELLA-V is significantly better than VALL-E (2.28 versus 5.00). This underscores ELLA-V\u2019s enhanced capability in synthesizing higher-quality and more robust speech. Overall, ELLA-V substantially improved the synthesis accuracy and robustness of the language model-based TTS framework without affecting the naturalness and speaker similarity. This conclusion is not only corroborated by this easy continuation task, but also validated via the challenging synthesis sets in the subsequent section.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#zero-shot-tts-cross-speaker-task-on-hard-casestts","title":"Zero-Shot TTS Cross-Speaker Task on hard cases\u00b7\u96f6\u6837\u672cTTS\u8de8\u8bf4\u8bdd\u4efb\u52a1(\u56f0\u96be\u6848\u4f8b)","text":"<p>VALL-E utilized a traditional AR model that frequently resulted in alignment errors, including repetitions, transpositions, and omissions, particularly in more challenging synthesis cases (see Section 4.1 for details of the challenging synthesis set). Tab.03 presents the WER comparison of VALL-E and ELLA-V on the 100 particularly hard synthesis sentences. In contrast to VALL-E, ELLA-V demonstrates markedly lower WER, signifying its enhanced robustness. This substantial reduction in errors translates to more accurate and reliable voice synthesis applications, significantly improving user experience in real-world scenarios. Regarding VALL-E\u2019s tendency to fall into infinite silence, an intuitive explanation is that the silence patterns in the training data are relatively simple and many of them are repetitive. In this case, a traditional language model is prone to overfitting to these patterns. During testing, when the model encounters silence, it assigns a high probability to silence. This leads to issues such as beam search, which is based on maximum likelihood, getting stuck in a loop. However, ELLA-V does not face this problem.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#analysis-of-decoding-strategies","title":"Analysis of Decoding Strategies\u00b7\u89e3\u7801\u7b56\u7565\u5206\u6790","text":"<p>To demonstrate the stability of ELLA-V under different decoding strategies, we conducted an ablation study, testing the decoding performance with different top-p values for nuclear sampling, by varyingp \u2208 {1, 0.99, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0(greedy)}.The results are shown in Fig.05. We can observe that as top_p decreases, the accuracy of VALL-E\u2019s synthesized speech significantly decreases. At this point, VALL-E is more prone to generating a large number of overfit silence tokens, leading to a significant increase in INF%. And compared to VALL-E, the audio synthesized by ELLA-V is less sensitive to rate changes in the top_p sampling strategy, whose WER consistently outperforms VALL-E. When the local advance is set to 5 or 10 tokens, the generated audio exhibits significant stronger robustness. On the other hand, as shown in Fig.05 (right), as top_p decreases, VALL-E tends to get stuck in infinite loops of failed generation, while the generation of ELLA-V remains significantly stable. Moreover, ELLA-V can promptly handle (truncate) the synthesis of exceptional phonemes, resulting in significantly higher robustness.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#ablation-study","title":"Ablation Study\u00b7\u6d88\u878d\u5b9e\u9a8c","text":"<p>In this paragraph, we conduct ablation experiments. (1) To investigate the impact of global phoneme information on synthesized speech, we removed the global phoneme sequence at the beginning of the trained sequence (abbr. ELLA-V-noglobal). (2) To investigate whether it is necessary to provide the specific phoneme token before its corresponding acoustic tokens during both training and inference, rather than just using the EOP separator, we removed all phoneme tokens following BOS in the mixed sequence (abbr. ELLA-V-nophn). The experimental results are shown in Tab.04. It is observed that the accuracy of synthesized speech significantly deteriorated either when global phoneme tokens were not used or when local phoneme tokens were disabled within the hybrid sequence. It is also notable that even in the absence of global advance (i.e., in the ELLA-V-no global configuration), the SPK and WER of the synthesized audio were comparable to those of VALL-E. These findings indicate the importance of both local and global information in achieving more accurate synthesized audios, meanwhile, combining both of them potentially leads to further enhancements in accuracy.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#5conclusion","title":"5.Conclusion\u00b7\u7ed3\u8bba","text":"<p>In this paper, we introduce ELLA-V, a simple and efficient two-stage zero-shot TTS framework based on language modeling. By learning interleaved sequences of acoustic and text tokens, our proposed GAR model can provide fine-grained control over synthesized audio at the phoneme level and can better leverage local dependencies to predict the pronunciation of the current phoneme. Experimental results demonstrate that ELLA-V achieves higher accuracy and more stable results under different threshold top-p for nuclear sampling. We aspire for this work to advance research in enhancing the robustness of speech generation.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/","title":"VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech","text":"\u4f5c\u8005 \u673a\u6784 \u4f5c\u8005 \u673a\u6784 Chenpeng Du \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 Yiwei Guo \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 Hankun Wang \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 Yifan Yang \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 Zhikang Niu \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 Shuai Wang Shenzhen Research Institute of Big Data Hui Zhang AISpeech Ltd \u9648\u8c10 \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 \u4fde\u51ef \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>Recent TTS models with decoder-only Transformer architecture, such as SPEAR-TTS (2023) and VALL-E (2023), achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt. However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and repeating. To address this limitation, we propose ==VALL-T==, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer. Consequently, ==VALL-T== retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3% in the word error rate. Furthermore, the controllability of alignment in ==VALL-T== during decoding facilitates the use of untranscribed speech prompts, even in unknown languages. It also enables the synthesis of lengthy speech by utilizing an aligned context window. The audio samples are available at http://cpdu.github.io/vallt.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Text-to-speech (TTS) synthesis is a monotonic sequence-to-sequence task, maintaining a strict order between the input phoneme sequence and the output speech sequence. Moreover, the output speech sequence is at frame-level and one phoneme may correspond to multiple frames of speech, so the output sequence is significantly longer than its corresponding input phoneme sequence. Mainstream neural text-to-speech models, such as FastSpeech2, GradTTS (Popov et al., 2021) and VoiceFlow (Guo et al., 2024), integrate a duration prediction module. Prior to training, the target duration is conventionally derived using the Viterbi forced alignment algorithm. During training, this module is optimized by minimizing the mean square error (MSE) between predicted and target durations. In the inference phase, the duration predictor module predicts the duration for each input phoneme, establishing the alignment between the input and output sequences accordingly. The encoded input phoneme sequence is then expanded to the frame level based on the predicted duration and is subsequently passed to the speech decoder. This mechanism enforces monotonic alignment constraints on the sequence-to-sequence process, ensuring robustness in the synthesis of speech.</p> <p>Over the past two years, utilizing discrete speech tokens for speech generation is proposed in GSLM (Lakhotia et al., 2021) and VQTTS (Du et al., 2022), paving the way for integrating cutting-edge language modeling techniques into TTS systems. Inspired by exceptional strides in natural language processing driven by decoder-only large Transformer models like GPT 3 (Brown et al., 2020) and the LLaMA2 (2023), Tortoise-TTS (2023), SPEAR-TTS (2023), VALL-E (2023) and LauraGPT (Wang et al., 2023b) adopted the decoder-only architecture for TTS, achieving remarkable naturalness. SPEAR-TTS (2023) and VALL-E (2023) also have the ability to perform zero-shot speaker adaptation through Auto-Regressive (AR) continuation from a given speech prompt. Furthermore, these decoder-only TTS models, unlike traditional neural TTS model, circumvent explicit duration modeling and the requirement for phoneme durations obtained prior to training. This characteristic offers convenience and simplifies training process, especially when training on large scale datasets. However, the implicit duration modeling within these systems lacks the monotonic alignment constraints, often leading to hallucination issues like mispronunciation, word skipping and repeating.</p> <p>In fact, we do have a training scheme named Transducer (Graves, 2012) designed specifically for monotonic sequence-to-sequence task and has demonstrated success in automatic speech recognition (ASR) (He et al., 2019). It adopts a modularized architecture, composed of an encoder, a prediction network and a joint network. However, such modularized architecture of Transducer is specifically designed for ASR as a classification task, making it less suited for TTS as a generation task. Further insights into this matter will be discussed in Chapter 3.</p> <p>To achieve the best of both worlds, we propose ==VALL-T==, a generative Transducer model that utilizes the decoder-only Transformer architecture. Specifically, alongside the conventional absolute position embedding, we incorporate additional relative position embeddings into the input phoneme sequence. Here, a relative position of 0 specifies the current phoneme under synthesis, allowing us to explicitly guide the monotonic generation process through shifting the relative positions from left to right. To the best of our knowledge, this is the first work that implements Transducer with a decoder-only Transformer architecture. ==VALL-T== presents several advantages compared to previous TTS models: - ==VALL-T== introduces monotonic alignment constraints without altering the decoder-only architecture, leading to a better robustness against hallucination. - ==VALL-T== utilizes implicit duration modeling, removing the necessity for acquiring phoneme durations before training. - The alignment controllability of ==VALL-T== during decoding enables the utilization of untranscribed speech prompts, even in unknown languages.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#2related-work","title":"2.Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#21decoder-only-zero-shot-tts-with-speech-promptstts","title":"2.1.Decoder-Only Zero-Shot TTS with Speech Prompts\u00b7\u4f7f\u7528\u8bed\u97f3\u63d0\u793a\u7684\u4ec5\u89e3\u7801\u5668\u7684\u96f6\u6837\u672cTTS","text":"<p>Zero-shot TTS refers to the ability to generate speech in the voice of an unseen speaker given only a short sample of that speaker\u2019s speech. Decoder-only TTS models, such as VALL-E (2023), are able to perform zero-shot speaker adaptation through auto-regressive continuation from the target speaker\u2019s sample. Therefore, the speech sample of the target speaker is also named speech prompt.</p> <p>Specifically, in the training process, illustrated in Fig.01(a), the phoneme and speech sequences are concatenated along the time axis and fed into a decoder-only Transformer model. It is assumed that the speaker\u2019s voice remains constant within each training utterance. In the inference phase, as shown in Fig.01(b), a speech prompt yp is required to determine the voice of the generated speech. The phoneme transcription of the speech prompt xp and the speech prompt itself yp are positioned at the beginning of the input and output sequences respectively, followed by the input phonemes to be generated and their corresponding output speech tokens. The process of auto-regressive continuation from the speech prompt is believed to preserve the speaker\u2019s voice in the generated output.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#22transducer","title":"2.2.Transducer\u00b7\u8f6c\u5f55\u5668","text":"<p>The Transducer model (Graves, 2012), also known as RNN- T, is designed for monotonic sequence-to-sequence tasks and comprises three components: an encoder, a prediction network, and a joint network. Here, the prediction network is an auto-regressive network, such as RNN and LSTM. Transducer model also introduces a special output token called blank, denoted as \\varnothing , which signifies the alignment boundary between output and input sequence. We define Yas the vocabulary of output tokens and \\bar{Y} = Y \u222a { \\varnothing }as the extended vocabulary. Also, we denote the lengths of the input sequence x and output sequence y as T and U and the size of the extended vocabulary \\bar{Y} as \u00afV .</p> <p>In the training phase, as shown in Fig.02(a), the encoder and prediction network encode the two sequences x and y respectively, yielding encoded hidden sequences f and g. Subsequently, we slice the hidden vectors ft and gu at position st and u respectively, then send them to the joint network to calculate the probability pt,u= Pr(\\bar{y}t+u|ft, gu) for the next token prediction, where \\bar{y}t+u\u2208 \\bar{Y}. We iterate over all possible sliced hidden vectors of the two sequences, from f0tofT \u22121and from g0togU, generating a matrix p of shapeT \u00d7(U +1)whose entry at(t, u)is pt,u. Each path \\bar{y} from the bottom left corner to the top right corner represents an alignment between x and y, with a length ofT +U. Fig.02(b) demonstrates an example of the alignment path where \\bar{y} = [y1, y2,  \\varnothing , y3,  \\varnothing , y4, y5,  \\varnothing , y6,  \\varnothing ]. The training criterion of Transducer model is to maximize the probability ofPr(y|x), which is the summation of the probabilities of all possible alignment paths \\bar{y}, that is where fti and gui are sliced hidden vectors at corresponding positions specified by the alignment path $\\bar{y}$. In practice, this probability can be effectively calculated with dynamic programming. In the inference phase, the prediction network auto-regressively predicts the next token, conditioning on the sliced input hidden vectors that slide from f0tofT \u22121whenever the blank token $\\varnothing$ emerges. The Transducer model has demonstrated remarkable success in ASR. However, its modularized architecture is not suitable enough for generation tasks. Recently, some literatures have explored the application of Transducer to TTS (Chen et al., 2021; Kim et al., 2023), but they still rely on the typical modularized architecture and consequently result in limited performance. Different from the previous works, we propose for the first time to implement Transducer with a decoder-only architecture that achieves better performance.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#3vall-t-decoder-only-generative-transducervall-t","title":"3.VALL-T: Decoder-Only Generative Transducer\u00b7VALL-T: \u4ec5\u89e3\u7801\u5668\u7684\u751f\u6210\u8f6c\u5f55\u5668","text":"<p>Current modularized Transducer model has demonstrated significant success in ASR. Nevertheless, its suitability for generation tasks is limited. Typically, the joint network is a small network, comprising only one or a few linear projection layers, and the prediction network is LSTM or Transformer blocks. This architecture introduces a limitation wherein the input condition x is not incorporated into the generation process until it reaches the joint network. Worse still, the joint network is too small to effectively integrate input conditions into the generation process. Moreover, the modularized Transducer model utilizes slicing to denote specific positions. Consequently, the joint network is unable to explicitly perceive the input context, further making difficulties in achieving satisfactory performance for conditional generation tasks.</p> <p>To address the above issues, we propose ==VALL-T== that integrates the encoder, the prediction network and the joint network into one single decoder-only Transformer architecture and leverages relative position embedding to denote the corresponding positions. We discuss the training and inference details below.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#31training","title":"3.1.Training\u00b7\u8bad\u7ec3","text":"<p>We use a decoder-only architecture for ==VALL-T==. Similar to the approach in the previous work VALL-E (2023), we concatenate the input phoneme and output speech tokens along the time axis and present them to the model as a unified sequence. Unlike traditional RNN and LSTM architectures, the Transformer lacks a specific time order for input tokens, relying instead on position embeddings to indicate their positions. The position indices for the input sequence range from0to T \u2212 1and are converted into position embeddings through a sinusoidal function (Vaswani et al., 2017). Similarly, the output sequence adopts position indices from0toU, including an additional <code>&lt;sos&gt;</code> token at the beginning. Following VALL-E (2023), we utilize a triangular attention mask for the output sequence, facilitating auto-regressive generation. This mask ensures that each speech token attends to only previously generated tokens, maintaining a proper sequential order in the output.</p> <p>Beyond the typical absolute position indices starting from 0, we introduce additional relative position indices in ==VALL-T== for input tokens. The relative position index0specifies the current phoneme under synthesis. The phonemes to its left are assigned negative position indices starting from \u22121, while those to its right are assigned positive position indices starting from1. These relative position indices are converted to relative position embeddings with a same sinusoidal function as the absolute position indices. The resulting absolute and relative position embeddings are added to the input phoneme embeddings and subsequently presented to the decoder-only Transformer. In adopting this approach, the model gains awareness of the phoneme presently undergoing synthesis, specifically the one assigned a relative position of0, and the phonemes serving as its preceding and subsequent contexts.</p> <p>To eliminate the need for explicit duration modeling, we introduce a special output token called blank, which serves as a marker denoting the end of each phoneme\u2019s generation. Consequently, the output projection following the decoder-only Transformer projects the hidden sequence into a size of\u00afV. The projected hidden sequence, with a length of U + 1, undergoes a Softmax function to yield a sequence representing the output distribution. Illustrated in Fig.03, we iteratively assign relative position0to each of theT phonemes and subsequently stack every output sequence, each of lengthU + 1. This stacking process results in a matrix p of shapeT \u00d7 (U + 1). The optimization of ==VALL-T== utilizes the Transducer loss, calculated using this matrix and the ground-truth speech tokens, to maximize the probability of p(y|x) following Equation (1).</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#32monotonic-auto-regressive-inference","title":"3.2.Monotonic Auto-Regressive Inference\u00b7\u5355\u8c03\u81ea\u56de\u5f52\u63a8\u7406","text":"<p>Let us first consider the auto-regressive inference process without a speech prompt. Initially, the relative position 0is designated to the first phoneme, starting the speech generation from the <code>&lt;sos&gt;</code> token. The model then auto-regressively produces speech tokens based on the input phoneme tokens and previously generated speech tokens until the blank token \\varnothing emerges. The emergence of \\varnothing denotes the completion of the first phoneme\u2019s generation and triggers a shift in relative positions. We iteratively conduct the above process until the appearance of \\varnothing for the last phoneme, indicating the conclusion of the entire generation process for the input phoneme sequence. Since the model is encouraged to generate speech tokens for the phoneme assigned relative position0by Transducer loss during training, the step-by-step shifting operation during decoding facilitates the monotonic generation process and consequently enhance the robustness against hallucination.</p> <p>Next, we consider the integration of the speech prompt for zero-shot speaker adaptation. Following the approach used in VALL-E (2023), the phoneme transcription of the speech prompt is placed at the start of the input sequence, while the speech prompt itself is positioned at the beginning of the output sequence. The two sequences are followed by the input phonemes to be generated and their corresponding output speech tokens respectively. Given that the speech prompt are provided, we assign the relative position 0 to the first phoneme right after the prompt transcription, as shown in Fig.03, and perform speech continuation. Likewise, the relative positions undergo a shift each time \\varnothing emerges, repeating until the generation for the final phoneme is completed.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#33pseudo-prompt-transcription-for-untranscribed-speech-prompt","title":"3.3.Pseudo Prompt Transcription for Untranscribed Speech Prompt\u00b7\u672a\u8f6c\u5f55\u8bed\u97f3\u63d0\u793a\u7684\u4f2a\u63d0\u793a\u8f6c\u5f55","text":"<p>In previous decoder-only TTS models, the alignment is learned implicitly with self-attentions. These models have to discern which phoneme is currently being synthesized at each time step solely based on the self-attentions between the input tokens and the preceding output tokens. Therefore, they rely on correct transcription of the speech prompt to get correct alignment and start the generation accordingly. However, in practice, it is inconvenient to obtain transcribed speech prompt, so we hope to leverage speech prompt directly and eliminate the need of its transcription.</p> <p>In ==VALL-T==, it is evident that the alignment is controllable during inference, allowing us to manipulate the generation process by assigning position0to the phoneme we intend to synthesize without relying on a paired speech prompt and its transcription. Accordingly, we can perform zero-shot adaptation with untranscribed speech prompts. Specifically, given an untranscribed speech prompt, we use the phoneme sequence of a random utterance, referred to as pseudo prompt transcription, as its transcription and place it at the beginning of the input sequence. Then the generation can start correctly by leveraging exactly the same algorithm as described in section 3.2. The reason for using a pseudo prompt transcription rather than no prompt transcription lies in the presence of absolute position embeddings in the input sequence. We need to avoid unseen alignment pattern in the view of absolute position embeddings. Moreover, since there is no necessity for transcribing the speech prompt, the utilization of untranscribed speech prompts can be expanded to include prompts in unknown languages. This enables cross-lingual zero-shot adaptive speech synthesis.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#34aligned-context-window-for-lengthy-speech-synthesis","title":"3.4.Aligned Context Window for Lengthy Speech Synthesis\u00b7\u957f\u8bed\u97f3\u5408\u6210\u7684\u4e0a\u4e0b\u6587\u5bf9\u9f50\u7a97\u53e3","text":"<p>Decoder-only Transformer models have very limited ability to generalize to unseen position embeddings. That means if we are synthesizing lengthy speech that exceeds the maximum length encountered during training, the performance would be degraded.</p> <p>Fortunately, in ==VALL-T==, the alignment is available during inference, allowing us to employ aligned context window that constrains both the input and output sequence length simultaneously. Specifically, at each decoding step, we retain only n phonemes that precede the current phoneme and m phonemes that follow it, creating a constrained sliding context window on input phonemes. Also, we preserve only the speech tokens corresponding to the n preceding phonemes given the alignment and discard more distant history, forming a context window on the output sequence as well. Hence, by leveraging aligned context window, ==VALL-T== consistently maintains a limited context on both input and output sequence, allowing it to generate speech of any lengths.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#4experiments-and-results","title":"4.Experiments and Results\u00b7\u5b9e\u9a8c\u4e0e\u7ed3\u679c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#41setup","title":"4.1.Setup\u00b7\u8bbe\u7f6e","text":"<p>In our experiments, we leverage our Encodec (2022) (Defossez et al., 2022) speech tokenizer whose frame shift is 20ms and the sampling rate of output waveforms is 16k. It comprises 8 residual vector quantization (RVQ) indices for each frame. To ensure a fair comparison between VALL-E (2023) and our proposed model ==VALL-T==, we follow the approach introduced in VALL-E (2023) that predicts the sequence of the first RVQ index with the auto-regressive models and then predicts the remaining 7 RVQ indices conditioned on the first RVQ index with a separate non-auto-regressive (NAR) model. Both the input and output sequences are encoded with BPE (Sennrich et al., 2016) algorithm to shorten sequence lengths and diminish GPU memory consumption. ==VALL-T== adopts an identical architecture to VALL-E (2023), containing 12 layers of Transformer blocks. Each block comprises 12 attention heads and has a hidden dimension of 1024.</p> <p>We use LibriTTS (Zen et al., 2019) dataset in our experiments, which is a multi-speaker transcribed English speech dataset. Its training set consists of approximately 580 hours of speech data from 2,306 speakers. We train our model for 40 epochs using a ScaledAdam (Yao et al., 2023) optimizer. The learning rate scheduler is Eden (Yao et al., 2023) with a base learning rate of0.05, an epoch scheduling factor of 4 and a step scheduling factor of 5000.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#42alignment-analysis","title":"4.2.Alignment Analysis\u00b7\u5bf9\u9f50\u5206\u6790","text":"<p>We first do alignment analysis to check if relative position embedding in ==VALL-T== indicates the alignment as expected. Given the speech y and its transcription x, we iterate over all relative positions and calculate the matrix p of output distributions in the shape ofT \u00d7(U +1). Then we calculate the forward variables, backward variables and posterior probabilities accordingly. The concepts of forward variable, backward variables, and posterior probabilities were initially introduced in Hidden Markov Models (Young et al., 2002) and were also introduced in Transducer (Graves, 2012). The definitions and calculation for these values are elaborated in Appendix A.</p> <p>In Fig.04, we illustrate an example of the forward variable, backward variable, and posterior probability for ==VALL-T==, with darker colors indicating lower values. The values are plotted on a logarithmic scale. In Fig.04(a) and 4(b), we can see a faint bright line on the diagonal of the two graphs.</p> <p>Pixel-wise summing the values from Fig.04(a) and Fig.04(b) produces Fig.04(c), which represents the posterior probability. The diagonal line becomes much clearer in this composite figure, indicating that ==VALL-T== correctly models the alignment between the input and output sequences with relative position embeddings. Accordingly, ==VALL-T== is capable of forced alignment, where the most probable path from the bottom-left corner to the top-right corner in the posterior probability map serves as the alignment path. The alignment path for this example is depicted in Fig.04(d). Since ground-truth labels for alignment are unavailable, our alignment analysis here only focuses on qualitative aspects.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#43evaluation-on-zero-shot-ttstts","title":"4.3.Evaluation on Zero-Shot TTS\u00b7\u96f6\u6837\u672cTTS\u7684\u8bc4\u4f30","text":"<p>In this section, we conduct an evaluation of our models on zero-shot TTS task. The task refers to synthesizing speech in the voices of unseen speakers given speech prompts and their corresponding transcriptions. Our test set uses a same test set as in (Du et al., 2024), containing 500 utterances and involving 37 speakers from the LibriTTS test set. Each speaker is assigned a specific speech prompt. Before assessing the performance of our models, we conduct speech resynthesis using our Encodec (2022) to evaluate the speech tokenizer. We also do an experiment named \u201cNAR resynthesis\u201d. In this experiment, we send the ground-truth first RVQ index to the NAR model for predicting the remaining 7 RVQ indices. Then, we convert all the 8 RVQ indices to waveform using the Encodec (2022) decoder. The purpose of the NAR resynthesis experiment is to demonstrate the performance degradation introduced by the NAR model, so we can better analyze the results of the entire pipelines, where the AR models are the primary focus of our paper.</p> <p>The baselines of this experiment include two models. One is the popular decoder-only TTS model VALL-E (2023) and another is the recently proposed TTS model with a modularized Transducer architecture called \u201cTransduce and Speak\u201d (Kim et al., 2023). The main evaluation metric in this paper is the word error rate (WER). In our evaluation process, we first synthesize speech for the test set, and then perform speech recognition using a well-known ASR model, Whisper1(Radford et al., 2023). The transcriptions obtained from the ASR model are then compared to the ground-truth input text to calculate the word error rate. Tab.01 shows that ==VALL-T== attains significant lower WER than baselines, which is a 28.3% relative reduction when compared to VALL-E (2023) and is only 0.41 higher than NAR resynthesis, suggesting the robustness of ==VALL-T==.</p> <p>Additionally, we present the mel-cepstral distortion (MCD) in the table, serving as a metric for quantifying the distance between the generated speech and the corresponding ground-truth recordings. ==VALL-T== also achieves the lowest MCD across all models. Further evaluations extend to Mean Opinion Score (MOS) listening tests for naturalness and speaker similarity. 15 listeners were tasked with rating each utterance on a scale from 1 to 5, with higher scores indicating better naturalness and similarity. Note that the speaker similarity is evaluated between the generated speech and the provided speech prompt, not the corresponding ground-truth speech. This distinction arises from the variability in a speaker\u2019s timbre across different utterances, and the goal is to emulate solely the timbre of the given prompt. In the listening tests, ==VALL-T== achieves a naturalness score comparable to VALL-E (2023), with a slightly better speaker similarity. Finally, the evaluation extends to the calculation of Speaker Embedding Cosine Similarity (SECS), measured using a pretrained speaker verification model2. This metric measures the speaker similarity by assessing the cosine similarity between the speaker embeddings of the generated speech and the provided speech prompt. While ==VALL-T== exhibits a marginally lower SECS value than VALL-E (2023), it still surpasses other models and does not detrimentally affect human perception according to the results of subjective listening tests on similarity.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#44leveraging-untranscribed-speech-prompts","title":"4.4.Leveraging Untranscribed Speech Prompts\u00b7\u5229\u7528\u672a\u8f6c\u5f55\u8bed\u97f3\u63d0\u793a","text":"<p>The alignment controllability of ==VALL-T== allow us to leverage untranscribed speech prompts for zero-shot TTS. In this experiment, we still use a same test set as in the previous section, excluding the transcription of the speech prompts to simulate a scenario where prompt transcriptions are unavailable. One utterance is randomly chosen from the LibriTTS test set, and its phoneme transcription serves as the pseudo prompt transcription for generating all utterances in the test set. We compare the proposed approach with three baselines. The first baseline is generating with ==VALL-T== but do not use any prompt transcription. The remaining two baselines use VALL-E (2023), one utilizing pseudo prompt transcriptions and the other using no prompt transcription.</p> <p>The results are presented in Tab.02. We find VALL-E (2023) consistently fails to perform continuation in the absence of the correct prompt transcription, regardless of whether pseudo prompt transcriptions are provided or not. Although VALL- T exhibits improved robustness, it still fails in continuation tasks when no prompt transcription is used. This failure is caused by the unseen alignment pattern in the view of absolute position embeddings. When provided with pseudo prompt transcriptions, ==VALL-T== successfully accomplishes the continuation from the speech prompt. The WER is significantly lower than the three baselines and even lower than both the results obtained using real prompt transcription and using NAR resynthesis in Tab.01. This improvement may be attributed to the reduced noise in the fixed pseudo prompt transcription compared to the diverse real prompt transcriptions. This result further demonstrate the robustness of ==VALL-T==.</p> <p>Similarly, we observe a lower MCD compared with other baselines with the proposed approach. We do not conduct listening tests on the three baselines since it makes no sense to assess the naturalness and similarity for entirely incorrect generated audio samples. The naturalness of the proposed approach is almost the same as that observed when using real prompt transcriptions while its speaker similarity is slightly lower. We can also observe that in SECS evaluation.</p> <p>Next, we extend the utilization of untranscribed speech prompts to those spoken in unknown languages. Specifically, we continue to use the same test set as in the previous experiments, but leverage speech prompts from 10 German and 10 Spanish speakers randomly selected from the Multilingual Librispeech dataset (Pratap et al., 2020), simulating the speech prompt in unknown languages. Employing the same English pseudo prompt transcription as in the previous experiment for both ==VALL-T== and the baseline VALL-E (2023), we generate continuations from the speech prompts in German and Spanish. The results are posted in Tab.03. VALL-E (2023) continues to fail in the generation due to the unknown prompt transcription. On the contrary, ==VALL-T== still successfully performs the zero-shot TTS from the speech prompts in German and Spanish, achieving a WER of 4.22. Note that the similarity MOS and SECS in this experiment cannot be directly compared with the corresponding results in Tab.01 and 2 since the speakers of the speech prompts differ. We do not have corresponding ground-truth speech that speaks the utterances in the test set in the voice of German and Spanish speakers, so we also do not calculate the MCD in this experiment.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#45evaluation-on-lengthy-speech-generation","title":"4.5.Evaluation on Lengthy Speech Generation\u00b7\u957f\u8bed\u97f3\u751f\u6210\u7684\u8bc4\u4f30","text":"<p>We also evaluate our model on lengthy speech synthesis that exceeds the maximum length encountered during training. Due to the limitation of GPU memory, the maximum duration of training utterances is approximately 15 seconds. The test set for this experiment consists of 85 utterances, each formed by concatenating five utterances from the previous test set to simulate lengthy utterance. The generated speech in this test set exceeds 20 seconds. We use n = 50and m = 15 as the context window size.</p> <p>Examining the results in Tab.04, we observe that ==VALL-T== exhibits superior generalization to long speech compared to VALL-E (2023), attributed to its utilization of relative position embedding, even in the absence of an aligned context window. In contrast, VALL-E (2023) often starts mumbling after generating approximately 20 seconds of speech and frequently terminates prematurely without completing the generation. Upon applying the aligned context window, the WER of ==VALL-T== further decreases and approaches the result of generating normal utterances. Additionally, the gap in MOS scores for naturalness and speaker similarity between generated speech and ground-truth is also comparable to the result of synthesizing normal utterances.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.01_VALL-T/2024.01_VALL-T/#5conclusion","title":"5.Conclusion\u00b7\u7ed3\u8bba","text":"<p>In this research, we present ==VALL-T==, a decoder-only generative Transducer model designed to improve the robustness and controllability of TTS models. ==VALL-T== incorporates monotonic alignment constraints into the decoder-only TTS framework, enabling implicit modeling of phoneme durations. Therefore, this model eliminates the need for acquiring phoneme durations before training. ==VALL-T== supports forced alignment given input phonemes and the corresponding output speech by searching the best path on the posterior probability map. This alignment is controllable during inference, facilitating zero-shot synthesis with untranscribed speech prompts even in unknown languages. Additionally, ==VALL-T== exhibits the capability of streaming generation, coupled with an aligned context window for synthesizing lengthy speech. These features make ==VALL-T== a powerful model for TTS applications.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/","title":"PAVITS: Exploring Prosody-Aware VITS for End-to-End Emotional Voice Conversion","text":"","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#abstract","title":"Abstract","text":"<p>In this paper, we propose Prosody-Aware VITS (PAVITS) for emotional voice conversion (EVC), aiming to achieve two major objectives of EVC: high content naturalness and high emotional naturalness, which are crucial for meeting the demands of human perception. To improve the content naturalness of converted audio, we have developed an end-to-end EVC architecture inspired by the high audio quality of VITS. By seamlessly integrating an acoustic converter and vocoder, we effectively address the common issue of mismatch between emotional prosody training and run-time conversion that is prevalent in existing EVC models. To further enhance the emotional naturalness, we introduce an emotion descriptor to model the subtle prosody variations of different speech emotions. Additionally, we propose a prosody predictor, which predicts prosody features from text based on the provided emotion label. Notably, we introduce a prosody alignment loss to establish a connection between latent prosody features from two distinct modalities, ensuring effective training. Experimental results show that the performance of PAVITS is superior to the state-of-the-art EVC methods. Speech Samples are available at https://jeremychee4.github.io/pavits4EVC/.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#1introduction","title":"1.Introduction","text":"<p>Emotional voice conversion (EVC) endeavors to transform the state of a spoken utterance from one emotion to another, while preserving the linguistic content and speaker identity [1]. It brings the capability to facilitate emotional communication between individuals [2], enhancing the user experience in human-computer interaction [3], and even achieving a seamless integration of human presence within the virtual world [4].</p> <p>There are two distinct challenges in EVC: one is low content naturalness, and the other is that the converted audio lacks the richness of emotion compared to human voice [1]. Previous studies were focused on frame-based solutions, such as CycleGAN [5] and StarGAN [6,7]. However, due to the fixed-length nature and poor training stability, the naturalness of converted audio is quite low to apply in practice. To address this challenge, autoencoder-based [8,9] especially for sequence-to-sequence (seq2seq) [10,11] frameworks raise much interests for its variable-length speech generation. It achieves an acceptable naturalness through the joint training with Text-to-speech (TTS) [12], which is used to capture linguistic information and avoid mispronunciation as well as skipping-words. Since speech emotion is inherently supra-segmental [13], it is difficult to learn emotional representation from the spectrogram. To tackle this, various pretraining methods, such as leveraging speech emotion recognition (SER) model [14] and 2-stage training strategy [15], are introduced to extract emotional feature for EVC system.</p> <p>Despite these works have achieved great success in EVC, the converted audio still falls short in meeting human\u2019s perceptual needs, which implies that these two challenges still remain to be effectively addressed. Remarkably, current EVC models generally operate in a cascade manner, i.e., the acoustic converter and the vocoder [1, 5, 7, 8], resulting in a mismatch between emotional prosody training and run-time conversion, ultimately leading to a degradation in audio quality, which is vital to evaluate content naturalness and impacts the perceptual experience of emotional utterance. However, there is no EVC model that attempt to bridge this gap, let alone models that aim to capture prosody variations at a finer granularity. To handle the similar issue, multiple solutions have been explored in TTS, including FastSpeech2s (2020), EATS [17], VITS (2021) [19], etc., seeking to alleviate the mismatch between acoustic feature generation and waveform reconstruction by integrating these two stages together.</p> <p>In this paper, inspired by the high audio quality of VITS (2021), we propose Prosody-Aware VITS (PAVITS) for EVC, a novel end-to-end system with implicit prosody modeling to enhance content naturalness and emotional naturalness. To our best knowledge, PAVITS is the first EVC method in solving the mismatch between acoustic feature conversion and waveform reconstruction. Compared to original VITS (2021), our approach involves several key innovations. In order to improve content naturalness with speech quality, we build upon VITS (2021) to solve the two-stage mismatch in EVC, and apply multi-task learning since TTS can significantly reduce the mispronunciation. To enhance emotional naturalness, we introduce an emotion descriptor to capture prosody differences associated with different emotional states in speech. By utilizing Valence-Arousal-Dominance values as condition, emotional representation at utterance-level is learned. Latent code is further refined by a prosody integrator, which incorporates with speaker identity and linguistic content to model finer-grained prosody variations. Then frame-level prosody features are obtained from normalizing flow. We also introduce a prosody predictor that leverages emotion labels and phoneme-level text embedding to predict frame-level emotional prosody features. Finally, we devise a prosody alignment loss to connect two modalities, aligning prosody features obtained from audio and text, respectively.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#2proposed-method","title":"2.Proposed Method","text":"<p>As shown in Fig.01, inspired by VITS (2021), the proposed model is constructed based on conditional variational autoencoder (CVAE), consisting of four parts: a textual prosody prediction module, an acoustic prosody modeling module, an information alignment module, and an emotional speech synthesis module.</p> <p>The textual prosody prediction (TPP) module predicts the prior distribution $p(z_1|c_1)$ as:</p> <p>$$     z_1= TPP (c_1) \\sim p (z_1| c_1)\\tag{1}  $$</p> <p>where $c_1$ including text $t$ and emotion label $e$.</p> <p>The acoustic prosody modeling (APM) module disentangles emotional features with intricate prosody variation, speaker identity, and linguistic content from the source audio given emotion label, forming the posterior distribution $q(z_2|c_2)$ as: </p> <p>$$     z_2= APM (c_2) \\sim q (z_2|c_2)\\tag{2}  $$</p> <p>where $c_2$ including audio $y$ and emotion label $e$.</p> <p>The information alignment module facilitates the alignment of text and speech, as well as the alignment of textual and acoustic prosody representations. In emotional speech synthesis (ESS) module, the decoder reconstructs waveform $\\hat{y}$ according to latent representation $z$.</p> <p>$$     \\hat{y} = Decoder (z) \\sim p (y | z)\\tag{3} $$</p> <p>where $z$ comes from $z_1$ or $z_2$.</p> <p>While the proposed model can perform both EVC and emotional TTS after training, EVC will be the main focus of this paper. In the following, we will introduce the details of the four modules.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#21textual-prosody-prediction-module","title":"2.1.Textual Prosody Prediction Module","text":"<p>Given condition $c_1$ including text $t$ and emotion label $e$, the textual prosody prediction module provides the prior distribution $p(z_1|c_1)$ of CVAE. The text encoder takes phonemes as input and extracts linguistic information $h_{text}$ at first. Considering the extensive prosody variation associated with each phoneme, we employ a prosody predictor to extend the representation to frame-level and predict the prosody variation (a fine-grained prior normal distribution with mean $\\mu_{\\theta}$ and variance $\\sigma_{\\theta}$ generated by a normalizing flow $f_{\\theta}$) based on emotion label. </p> <p>$$     p(z_1|c_1) = \\mathcal{N}(f_{\\theta}(z_1); \\mu_{\\theta}(c_1);\\sigma_{\\theta}(c_1))\\left|\\det\\dfrac{\\partial f_{\\theta}(z_1)}{\\partial z}\\right|\\tag{4} $$</p> <p>Text Encoder: Since the training process is constrained by the volume of textual content within parallel datasets, we initially convert text or characters into a phoneme sequence as a preprocessing step to maximize the utility of the available data, resulting in improved compatibility with the acoustic prosody modeling module. Similar to VITS (2021), text encoder comprises multiple Feed-Forward Transformer (FFT) blocks with a linear projection layer for representing linguistic information.</p> <p>Prosody Predictor: Prosody predictor leverages phoneme-level linguistic information extracted by the text encoder to anticipate frame-level prosody variation given discrete emotion label. It has been observed that simply increasing the depth of stacked flow does not yield satisfactory emotional prosody variations, unlike the prosody predictor. Therefore, the inclusion of the prosody predictor guarantees a continuous enhancement in prosody modeling for both the TPP and APM modules. The prosody predictor comprises multiple one-dimensional convolution layers and a linear projection layer. Furthermore, we integrate predicted emotional prosody information with linguistic information as input for the duration predictor, which significantly benefits the modeling of emotional speech duration.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#22acoustic-prosody-modeling-module","title":"2.2.Acoustic Prosody Modeling Module","text":"<p>The acoustic prosody modeling module provides emotional features with fine-grained prosody variation based on dimensional emotion representation, i.e., Valence-Arousal-Dominance values.Speaker identity and speech content information are also disentangled from the source audio and then complete feature fusion through the prosody integrator as the posterior distribution $q (z_2|c_2)$. </p> <p>$$     q(z_2|c_2) = \\mathcal{N}(f_{\\theta}(z_2); \\mu_{\\theta}(c_2);\\sigma_{\\theta}(c_2))\\tag{5} $$</p> <p>Speaker encoder: Considering the APM module\u2019s increased focus on understanding emotional prosody more thoroughly compared to previous models, it\u2019s apparent that speaker characteristics could unintentionally be overlooked during conversion.Recognizing the critical role of fundamental frequency (F0) in speaker modeling [20], we augment the F0 predictor of VISinger (2021) by adding multiple one-dimensional convolutional layers and a linear layer to construct the speaker encoder, which tackles the issue effectively.</p> <p>Emotion descriptor: To enhance PAVITS\u2019s emotional naturalness, we employ a specific SER system rooted in Russell\u2019s circumplex theory [22] to predict dimensional emotion representation, encompassing Valence-Arousal-Dominance values as a conditional input. This input guides the capture of nuanced prosody variations, which ensures that while satisfying human perception of emotions at utterance-level, natural prosody variations are retained from segment-level down to frame-level, preserving intricate details. It consists of a SER module [23] and a linear projection layer.</p> <p>Prosody Integrator: The prosody integrator incorporates a combination of speaker identity attributes, emotional prosody characteristics, and intrinsic content properties extracted from the linear spectrogram. It is constructed using multiple convolution layers, WaveNet residual blocks, and a linear projection layer.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#23information-alignment-module","title":"2.3.Information Alignment Module","text":"<p>In VITS (2021), the existing alignment mechanism, which is called Monotonic Alignment Search (MAS), solely relies on textual and acoustic features from parallel datasets. Thus, it is insufficient in capturing emotional prosody nuances, hindering effective linkage between the TPP and APM modules. To overcome this limitation, we propose an additional prosody alignment loss function based on Kullback-Leibler divergence, to facilitate joint training for frame-level prosody modeling across the TPP and APM modules, with the goal of enhancing prosody information integration and synchronization within our model.</p> <p>$$     L_{psd} = D_{KL}(q(z_2|c+2)| p(z_1|c_1))\\tag{6} $$</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#24emotional-speech-synthesis-module","title":"2.4.Emotional Speech Synthesis Module","text":"<p>In the emotional speech synthesis module, the decoder generates a waveform based on latent $z$, employing adversarial learning to continuously enhance naturalness in both content and emotion. To improve the naturalness of content, $L_{recon-cls}$ minimizes the $L_1$ distance between predicted and target spectrograms, $L_{recon-fm}$ minimizes the $L_1$ distance between feature maps extracted from intermediate layers in each discriminator, aimed at enhancing training stability. Since the former predominantly influences the early-to-mid stage, while the latter assumes a more prominent role in mid-to-late stage, we introduce two coefficients to balance their contributions as follows.</p> <p>$$     L_{recon}= \\gamma L_{recon-cls}+ \\beta L_{recon-fm}(G)\\tag{7} $$</p> <p>To enhance the perception of emotions, $L_{emo-cls}$ represents the loss function for emotional classification, while $L_{emo-fm}$ denotes the loss associated with feature mapping for emotion discrimination.</p> <p>$$     L_{emo}= L_{emo-cls}+ \\beta L_{emo-fm}(G)\\tag{8} $$</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#25final-loss","title":"2.5.Final Loss","text":"<p>By combining CVAE with adversarial training, we formulate the overall loss function as follows:</p> <p>$$ \\begin{align}     L &amp;= L_{recon}+ L_{adv}(G) + L_{emo}+ L_{psd}+ L_{F0}+ L_{dur}\\tag{9}\\     L(D) &amp;= L_{adv}(D)\\tag{10} \\end{align} $$</p> <p>where $L_{adv}(G)$ and $L_{adv}(D)$ represent the adversarial loss for the Generator and Discriminator respectively, $L_{F0}$ minimizes the $L_2$ distance between the predicted F0 and corresponding ground truth, $L_{dur}$ minimizes the $L_2$ distance between the predicted duration and ground truth which is obtained through estimated alignment.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#26run-time-conversion","title":"2.6.Run-Time Conversion","text":"<p>At runtime, there are two converting methods: a fixed-length approach (Audio-$z_2$-Audio, named PAVITS-FL) and a variable-length approach (Audio-Text-$z_1$-Audio, named PAVITS-VL). The former uses APM module for latent $z$ prediction from audio, ensuring robustness as it remains unaffected by text encoding, but is constrained by a fixed spectrum length due to Dynamic Time Warping (DTW) limitations. The latter employs TPP module to predict latent $z$ from corresponding text obtained through automatic speech recognition (ASR) technique, which is not bound by duration modeling and offers greater naturalness. Finally, the ESS module\u2019s decoder takes latent $z$ (either $z_1$ or $z_2$) as input and synthesizes the converted waveform without a separate vocoder.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#3experiments","title":"3.Experiments","text":"","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#31datasets","title":"3.1.Datasets","text":"<p>We perform emotional conversion on a Mandarin corpus belonged to Emotional Speech Dataset (ESD) [24] from neutral to angry, happy, sad, and surprise, denoted as Neu-Ang, Neu-Hap, Neu-Sad, Neu-Sur respectively. For each emotion pair, we use 300 utterances for training, 30 utterances for evaluation, and 20 utterances for test. The total duration of training data is around 80 minutes (16 minutes per emotion category), which is absolutely small compared to others.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#32experimental-setup","title":"3.2.Experimental Setup","text":"<p>We train the following models for comparison. - CycleGAN [25] (baseline): CycleGAN-based EVC model with WORLD vocoder. - StarGAN [26] (baseline): StarGAN-based EVC model with WORLD vocoder. - Seq2seq-WA2 [15] (baseline): Seq2seq-based EVC model employing 2-stage training strategy with WaveRNN vocoder. - VITS (2021) (baseline):EVC model constructed by original VITS, operating independently in both fixed-length and variable-length, take the average as the result. - PAVITS-FL (proposed): the proposed model based on VITS, incorporates all the contributions outlined in the paper, but operate within a fixed-length framework. - PAVITS-VL (proposed): the proposed model based on VITS, incorporates all the contributions outlined in the paper, but operate within a variable-length framework leveraging ASR to obtain text from source audio.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#33results-discussion","title":"3.3.Results &amp; Discussion","text":"<p>Mel-cepstral distortion (MCD) was calculated for objective evaluation, as depicted in Tab.01.In terms of subjective evaluation, Mean Opinion Score (MOS) tests were conducted to appraise both the quality and naturalness of speech as shown in Tab.02. The naturalness score was derived by averaging the scores for content naturalness and emotional prosody naturalness, as rated by 24 participants, each of whom assessed a total of 148 utterances. We further report emotional similarity results between converted audio and human voice to gauge emotional naturalness as illustrated in Fig.02.</p> <p>Through the above-mentioned metrics, it is obvious that the proposed PAVITS achieves competitive performance on both objective and subjective evaluation. From the perspective of objective MCD and subjective MOS, both original VITS and our proposed PAVITS models always outperform other models with traditional vocoder or neural vocoder, which proves that the integration of neural acoustic converter and vocoder is suitable for EVC task to enhance speech quality and naturalness. It is worth noting that even in the case of the fixed-length PAVITS-FL model, there is a reduction of over 0.4 in MCD when compared to the variable-length seq2seq model and the original VITS model. Furthermore, there has been an enhancement of 0.6 and 0.2 in MOS, respectively. To some extent, it reflects how human tend to be influenced by audio quality when assessing model naturalness, especially when there are significant differences in quality being compared.</p> <p>As depicted in Fig.02, our proposed PAVITS-VL (variable-length) model aligns more closely with human perception in the converted audio, which attributed to the model\u2019s capacity for fine-grained granularity in modeling speech emotion, incorporating implicit prosody cues.To further show the effectiveness of our method, we visualize the spectrogram of testing clips, as exemplified in Fig.03. It is readily apparent that the spectrogram converted by PAVITS exhibits finer details in prosody variations within the pertinent frequency bands, while simultaneously preserving descriptive information for other frequency bands. Consequently, the audio generated by PAVITS possesses a prosody naturalness and emotional accuracy that closely approximates the ground truth spectrogram.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#34ablation-study","title":"3.4.Ablation Study","text":"<p>We further conduct an ablation study to validate different contributions.We remove prosody predictor, prosody alignment, and prosody integrator in turn and let the subjects evaluate quality and naturalness of converted audio. From Tab.03, we can see that all scores are degraded with the removal of different components. When remove prosody predictor, the speech quality does not undergo significant changes, as the original VITS primarily relies on textual features as input. However, a significant decrease in naturalness is observed, attributed to the loss of explicit emotion label for TPP module as a conditioning factor. This highlights the importance of aligning with APM module on the basis of information asymmetry, which reflects the ingenious design of prosody modeling structure. Note that the performance of PAVITS is worse than VITS after deleting prosody alignment, it might be attributed the fact that latent prosody representations are not constrained during training, which damages the original MAS mechanism present in VITS. To further show the contribution from the prosody integrator, we replace it with a simple concatenation. Both speech quality and naturalness show a slight decrease, indicating that utilizing prosody integrator for information fusion is quite effective for APM module.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#4conclusion","title":"4.Conclusion","text":"<p>In this paper, we propose Prosody-Aware VITS (PAVITS) for emotional voice conversion (EVC). By integrating acoustic prosody modeling (APM) module with textual prosody prediction (TPP) module through prosody alignment, the fine-grained emotional prosody features across various scales of emotional speech can be learned effectively. Experimental results on ESD corpus demonstrate the superiority of our proposed PAVITS for content naturalness and emotional naturalness, even when dealing with limited data scenarios. In the future, we will explore the controllable emotional prosody modeling to allow better interpretability of EVC.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/","title":"MM-TTS: A Unified Framework for Multimodal, Prompt-Induced Emotional Text-to-Speech Synthesis","text":""},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>Emotional Text-to-Speech (E-TTS) synthesis has gained significant attention in recent years due to its potential to enhance human-computer interaction. However, current E-TTS approaches often struggle to capture the complexity of human emotions, primarily relying on oversimplified emotional labels or single-modality inputs. To address these limitations, we propose the Multimodal Emotional Text-to-Speech System (MM-TTS), a unified framework that leverages emotional cues from multiple modalities to generate highly expressive and emotionally resonant speech. MM-TTS consists of two key components:  (1) the Emotion Prompt Alignment Module (EP-Align), which employs contrastive learning to align emotional features across text, audio, and visual modalities, ensuring a coherent fusion of multimodal information;  (2) the Emotion Embedding-Induced TTS (EMI-TTS), which integrates the aligned emotional embeddings with state-of-the-art TTS models to synthesize speech that accurately reflects the intended emotions.</p> <p>Extensive evaluations across diverse datasets demonstrate the superior performance of MM-TTS compared to traditional E-TTS models. Objective metrics, including Word Error Rate (WER) and Character Error Rate (CER), show significant improvements on ESD dataset, with MM-TTS achieving scores of 7.35% and 3.07%, respectively. Subjective assessments further validate that MM-TTS generates speech with emotional fidelity and naturalness comparable to human speech. Our code and pre-trained models are publicly available at https://anonymous.4open.science/r/MMTTS-D214.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Emotional text-to-speech (E-TTS) technology has emerged as a transformative force in artificial intelligence and multimedia, significantly enhancing the richness of human-computer interaction [40]. By imbuing synthetic voices with emotional depth, E-TTS transcends mere speech replication, bringing virtual agents and digital characters to life. The incorporation of emotional expressiveness captivates users and fosters deeper emotional connections, catering to the fundamental human need for empathetic communication [5,49]. This heightened level of engagement has the potential to revolutionize industries from entertainment and education to healthcare and customer service.</p> <p>Despite substantial advancements in text-to-speech (TTS) technologies resulting in naturalistic, high-quality speech, the primary focus has been on linguistic accuracy rather than capturing the inherent emotional nuances [21,31,36\u201338,45]. Contemporary E-TTS approaches have sought to address this gap by integrating emotion labels or utilizing emotional reference speech [5,6,11,23,26,46,49]. However, these methods often struggle to fully capture the intricacies of human emotions due to their reliance on oversimplified representations or single-modality inputs. To truly unlock the potential of E-TTS and create emotionally resonant synthetic speech, a more comprehensive approach leveraging rich emotional information across multiple modalities is crucial.</p> <p>Consider a voiceover scenario where one aims to generate emotionally consistent speech for a given dialogue, potentially with access to visual cues such as facial expressions, silent video clips, and reference audio samples, as shown in Fig.01. Traditional E-TTS approaches relying on unimodal information have inherent limitations. Facial expressions are dynamic, and their emotional interpretation can vary significantly depending on context. Similarly, using reference emotional audio alone fails to capture the nuanced interplay of emotions across scenarios. Moreover, in real-world applications, the availability of different modalities may vary. Leveraging multimodal cues enables a more comprehensive understanding of the underlying emotional state, leading to the generation of speech with more intricate and nuanced emotional expressions tailored to the specific context. Such multimodal integration enables a more flexible approach to emotional speech synthesis, catering to the intricate demands of human-computer interaction and content creation (e.g.Fig.01).</p> <p>In response, we present the Multimodal Emotional Text-to-Speech System (MM-TTS), a groundbreaking framework designed to elevate the expressiveness of synthesized speech by incorporating multimodal cues encompassing text, audio, and visual information (Sec.3). MM-TTS comprises two critical components: the Emotion Prompt Alignment Module (EP-Align) and the Emotion Embedding-Induced TTS (EMI-TTS). EP-Align enables the seamless fusion of multimodal information by aligning emotional data across modalities through a cue anchoring mechanism (Sec. 3.1). EMI-TTS leverages these aligned emotional embeddings to generate expressive and emotionally resonant speech suitable for a wide range of applications (Sec.3.2). The integration of EP-Align and EMI-TTS within MM-TTS leads to emotionally intelligent speech synthesis.</p> <p>To validate the effectiveness of MM-TTS, we conduct extensive testing across diverse datasets, benchmarking its performance against traditional models in crucial speech quality metrics. By investigating the impact of MM-TTS on speech content accuracy with varied TTS framework, we find that it consistently improves performance on ESD dataset with Word Error Rate (WER) and Character Error Rate (CER) scores of 7.35% and 3.07%, respectively (Sec.4.3.1). Subjective evaluations further corroborate these objective measures, confirming that MM-TTS matches the emotional fidelity and naturalness of human speech. The combination of strong objective performance and subjective approval positions MM-TTS at the forefront of Emotional TTS technologies, highlighting its potential to revolutionize human-computer interactions through emotional speech synthesis. The key contributions are threefold:</p> <ul> <li>We introduce the Emotion Prompt Alignment Module (EP-Align), a novel approach employing contrastive learning to synchronize emotional features across modalities. By effectively aligning emotional features and filtering out complex noise in multimodal content, EP-Align addresses the challenge of distribution discrepancies, serving as a critical component in enabling high-quality emotional speech generation.</li> <li>Building upon EP-Align, we develop the Emotion EmbeddingInduced TTS (EMI-TTS) framework, which seamlessly integrates advanced TTS models with the aligned emotional embeddings to synthesize speech that authentically reflects intended emotions. The incorporation of these embeddings enhances the naturalness and credibility of the generated audio, resulting in a more engaging and immersive user experience.</li> <li>We conduct extensive evaluations across a wide spectrum of emotional categories and scenarios to validate the adaptability and superior processing capabilities of MM-TTS. The results confirm that MM-TTS consistently maintains emotional consistency while preserving individual speaker characteristics, highlighting its transformative potential in revolutionizing human-computer interactions by providing a more engaging experience.</li> </ul>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#2related-work","title":"2.Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#21text-to-speech-synthesis","title":"2.1.Text-to-Speech Synthesis","text":"<p>Text-to-speech (TTS) technology, known as speech synthesis, aims to generate human-like speech from text. Recent neural network-based end-to-end TTS models, such as those developed in [21,31,36\u201338,45], have markedly improved speech quality over traditional methods like concatenation synthesis [17] and statistical parametric synthesis [41,48]. Typically, these models involve converting text or phonemes into linguistic features using embeddings, expanding these features to meet the acoustic model\u2019s requirements via a length regulator, and then converting the acoustic features to waveforms using a vocoder. Different models employ various approaches for the acoustic model. For example, the Tacotron series uses RNN-based models [38,39,45], VITS utilizes flow-based models [8,21], and FastSpeech features Transformer-based models [36,37,44]. While these state-of-the-art TTS models have made some progress in generating natural and high-quality speech, they have primarily focused on linguistic accuracy and clarity, often overlooking the emotional dimensions conveyed through speech.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#22emotional-text-to-speech-synthesis","title":"2.2.Emotional Text-to-Speech Synthesis","text":"<p>Emotional Text-to-Speech (ETTS) focuses on enhancing synthesized speech with emotional expressiveness [40]. Researchers have developed methods to integrate emotions into TTS systems.  (1) One technique uses emotion labels as additional conditioning data in TTS models. For example, EmoSpeech [6] modifies the FastSpeech2 framework by adding speaker and emotion labels via Conditional Layer Normalization. Similar strategies in works like [5,23,49] employ Tacotron-based models where emotion labels are embedded into hidden features as conditions. EmoDiff [11] uses a diffusion model [16] with soft labels for emotions, replacing the traditional one-hot vectors.  (2) Another approach leverages referenced speech capturing the desired emotional state. Li et al. [26] introduce a method incorporating dual emotion classifiers and a style loss to align the generated speech\u2019s emotional content with the reference mel-spectrum. Moreover, [46] explores techniques for transferring the emotional style from reference speech to synthesized speech.  (3) Some studies utilize textual descriptions of emotions as conditioning data, with [47] and [12] demonstrating effective conveyance of target emotions through guided textual descriptions in speech synthesis. Despite advancements, previous ETTS approaches often face limitations like reliance on a single modality or simplistic emotional representations. Tab.01 outlines these differences across existing models, focusing on supported emotion categories, conditioning data types, and zero-shot learning capabilities. Addressing these limitations, our proposed framework, MM-TTS, employs multimodal representation learning to integrate diverse emotion-related data, enabling the generation of emotionally rich speech.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#23multi-modal-representation-learning","title":"2.3.Multi-Modal Representation Learning","text":"<p>Multi-modal representation learning, a pivotal area in modern research, aims to find a shared latent space that effectively integrates diverse modalities such as text, images, videos, and audio [10,13,22,27,28,34].  This integration facilitates a deeper understanding of the emotional content underlying these modalities. Multi-modal encoders extract contextually relevant features from each modality, capturing their unique characteristics. These features are then aligned using supervised or unsupervised methods, creating connections that aid in cross-modal tasks [3,4,7]. Particularly, key advancements include the Contrastive Language-Image Pretraining (CLIP) model [34], which establishes a joint embedding for images and text, and the AudioCLIP model [13], which extends this to include audio. Furthermore, Koepke et al. [22] have explored combining audio and visual data to enable applications like sound localization and audio-visual event detection. Our framework, MMTTS, builds on these developments by using multi-modal representation learning to improve the emotional expressiveness of synthesized speech. By extracting and aligning feature representations from text, images, videos, and audio with emotion tags through contrastive learning, MM-TTS captures comprehensive emotion-related information. This enriched understanding allows MM-TTS to align explicit and implicit emotional cues, generating nuanced and emotionally rich synthesized speech.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#3mm-tts-framework","title":"3.MM-TTS Framework","text":"<p>The primary goal of MM-TTS is to leverage emotions extracted from multiple modalities to generate emotional speech for different individuals in a data-efficient manner. Fig.02 illustrates the framework comprising two main components: (1) Emotion Prompt Alignment Module (EP-Align) and (2) Emotion Embedding-induced TTS (EMI-TTS).</p> <p>(1) Emotion Prompt Alignment Module (EP-Align) plays a crucial role in aligning the emotional representations derived from various modalities, such as visual data, audio segments, and textual descriptions. It takes a multimodal emotional data tuple $Tup^{emo}=$ as input, where $v$ represents visual data (image or video), $a$ is an audio segment, $s$ denotes an emotional text description, and $p$ is an emotional prompt label. EP-Align processes these inputs using a set of multimodal encoders $\\mathcal{E} = {\\mathcal{E}^{vis}, \\mathcal{E}^{audio}, \\mathcal{E}^{tex}, \\mathcal{E}^{prop}}$, which extract emotion features and generate a unified emotion embedding. This emotion embedding is then passed to the EMI-TTS component. <p>(2) Emotion Embedding-induced TTS (EMI-TTS) component takes the aligned emotion embedding, along with the input text Tex and a pre-trained TTS model from the model library $\\mathcal{M}$, to generate emotional speech. The emotion embedding provides the necessary emotional context, while the input text and the TTS model determine the content and the overall style of the generated speech.</p> <p>Formally, the audio with emotion can be generated by MM-TTS $\\Phi$ as follows:</p> <p>$$ \\tag{1} $$</p> <p>The MM-TTS framework aims to address the challenges associated with generating emotional speech by leveraging the complementary information provided by multiple modalities. By aligning the emotional representations from different sources and integrating them into the TTS process, MM-TTS enables the synthesis of speech that accurately conveys the desired emotions while maintaining the speaker\u2019s characteristics. In the following, we will delve into the details of the Emotion Prompt Alignment Module (EP-Align) and the Emotion Embedding-induced TTS (EMI-TTS) component.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#31emotion-prompt-alignment-module","title":"3.1.Emotion Prompt Alignment Module","text":"<p>Inspired by Contrastive Language-Image Pre-training (CLIP) [34], EP-Align employs a set of multimodal encoders to extract emotion features from various modalities. These encoders are denoted as $\\mathcal{E} = {\\mathcal{E}^{vis}, \\mathcal{E}^{audio}, \\mathcal{E}^{tex}, \\mathcal{E}^{prop}}$, corresponding to vision, audio, text, and emotion prompts, respectively. The extracted features are then aligned into a unified emotion embedding space.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#311multimodal-encoders","title":"3.1.1.Multimodal Encoders.","text":"<p>Each modality is processed by a dedicated encoder to extract relevant emotion features. Given a multimodal emotional data tuple $Tup^{emo}=$, the encoders generate feature representations as follows: <p>$$ \\tag{2} $$</p> <p>where $f^{vis}, f^{audio}, f^{tex}, f^{prop}$ represent the feature representations for visual, audio, textual, and emotional prompts, respectively. Each feature representation has a dimensionality of\ud835\udc3e, which is set to 512 in this work.</p> <p>The architectures of the text encoder $\\mathcal{E}^{tex}$, prompt encoder $\\mathcal{E}^{prop}$, and visual encoder $\\mathcal{E}^{vis}$ are similar to those in CLIP [34]. The audio encoder $\\mathcal{E}^{audio}$ follows the design from [1] for effective audio encoding. During training, the text and image encoders are initialized with pre-trained CLIP weights, while the audio encoder is initialized with a pre-trained model from [14]. To handle video inputs, the visual encoder $\\mathcal{E}^{vis}$ processes a sequence of frames, extracts a feature bank, and performs average pooling in the temporal dimension to obtain the final representation. For audio inputs, an additional linear projection layer is employed to map the audio features to the same dimension as the text and image features.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#312multimodal-emotional-alignment","title":"3.1.2.Multimodal Emotional Alignment.","text":"<p>Aligning the emotion representations from different modalities is a critical step in MM-TTS. We propose a prompt-anchoring scheme that leverages prompt-based bridging to facilitate the alignment of multimodal representations. First, we construct vision-prompt (vis-pro) and audio-prompt (aud-pro) embedding spaces by projecting the vision, audio, and text features using learned projection matrices:</p> <p>$$ \\tag{3} $$</p> <p>where $W^{vis\u2212pro}$, $W^{aud\u2212pro}$, $W^{tex\u2212pro}\\in\\mathbb{R}^{K\\times K}$ are the projection matrices, and $u^{vis}$, $u^{audio}$, and $u^{tex}$ }are the projected embeddings in the vis-pro, aud-pro, and tex-pro spaces, respectively.</p> <p>The prompt embedding $u^{prop}$ is obtained by multiplying the corresponding projection matrix based on the alignment modality:</p> <p>$$ \\tag{4} $$</p> <p>We use $u^{exp}$ to denote the explicit emotion embedding obtained from the emotion prompt and $u^{imp}$ to represent the implicit emotion embeddings from vision, audio, or text. The cosine similarity between the explicit and implicit embeddings is computed as:</p> <p>$$ \\tag{5} $$</p> <p>where $t$ is a learned temperature parameter, and $\\sigma$ denotes the normalization operation.</p> <p>The multimodal alignment loss $L_{align}$ is defined as the symmetric cross-entropy loss between the explicit and implicit embeddings:</p> <p>$$ \\tag{6} $$</p> <p>During inference, EP-Align selects the emotion prompt embedding $u^{prop}$ with the highest similarity score to the implicit embeddings as the aligned emotion representation $u^{emo}$. This aligned emotion representation captures the emotional content conveyed by the multimodal inputs and serves as a unified representation for the subsequent TTS process. By leveraging the prompt-anchoring scheme and the multimodal encoders, EP-Align effectively aligns the emotion representations from different modalities into a shared embedding space. This alignment enables the TTS model to generate emotional speech that is consistent with the input emotional context, regardless of the source modality.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#32emotion-embedding-induced-tts","title":"3.2.Emotion Embedding-induced TTS","text":"<p>With the aligned emotion embedding $u^{emo}$ obtained from the previous EP-Align module, the Emotion Embedding-induced TTS (EM-TTS) component generates emotional speech by integrating the emotion representation into the TTS process.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#321prompt-anchoring-multimodal-fusion","title":"3.2.1.Prompt-Anchoring Multimodal Fusion.","text":"<p>To mitigate the bias among the multimodal emotion embedding spaces, we employ prompt-anchoring to fuse the aligned emotion embedding $u^{emo}$ with the input text representation. Specifically, we use the implicit emotion embedding $u^{imp}$ to retrieve the most similar prompt embedding $u^{prop}$ based on cosine similarity. The retrieved prompt embedding serves as an anchor to bridge the implicit and explicit emotion representations, enabling a more coherent integration of the emotion embedding into the TTS model.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#322unified-emotional-tts-framework","title":"3.2.2Unified Emotional-TTS Framework.","text":"<p>EMI-TTS provides a unified framework that integrates various TTS models, such as Tacotron2[38], VITS [21], and FastSpeech2 [36]. These models typically consist of a text encoder, a length regulator, an acoustic model, and a vocoder. The text encoder converts the input text Tex into a sequence of linguistic features $h_{lg}$ . The length regulator adjusts the duration of the linguistic features to match the desired speech length. The acoustic model generates acoustic features $h_{ac}$ conditioned on the linguistic features and the emotion and speaker embeddings. Finally, the vocoder converts the acoustic features into a speech waveform. In EMI-TTS, the emotion embedding $u^{emo}$ and speaker embedding $u_{spk}$ are concatenated and integrated into the TTS model as additional conditioning inputs. The emotion embedding provides the necessary emotional context, while the speaker embedding captures the speaker\u2019s characteristics. This allows the TTS model to generate speech that reflects both the desired emotion and the target speaker\u2019s style.</p> <p>Formally, the EMI-TTS process can be represented as follows:</p> <p>$$ \\tag{7} $$</p> <p>where $h_{lg}^{emo}$ represents the linguistic features conditioned on the emotion and speaker embeddings, and $Audio^{emo}$ is the generated emotional speech. During training, EMI-TTS optimizes the same loss functions as the base TTS models without requiring additional loss terms. The model is trained on paired data consisting of input text, emotion features, and speaker information. The vocoder used in EMI-TTS depends on the specific TTS model: VITS uses its original decoder, while Tacotron2 and FastSpeech2 employ a WaveNet[42] vocoder.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#323model-library","title":"3.2.3.Model Library.","text":"<p>EMI-TTS incorporates several state-of-the-art TTS models, including Tacotron2, VITS, and FastSpeech2. By providing a diverse set of TTS models, EMI-TTS offers flexibility in generating emotional speech across different scenarios and requirements.</p> <p>(1) Tacotron2: Building upon the traditional Tacotron model[38], this version integrates WaveNet technology[42] for improved sound generation. The Character Encoder transforms text into hidden states $h_{lg}$ , enhanced with emotion embeddings $u^{emo}$ to create emotionally enriched states $h_{lg}^{emo}$:</p> <p>$$ \\tag{8} $$</p> <p>This enhancement adjusts the Location Sensitive Attention to a more nuanced Location &amp; Emotion Sensitive Attention, allowing the model to better capture emotional nuances in speech synthesis.</p> <p>(2) VITS: This model innovatively combines variational inference with normalizing flows and adversarial training, as outlined in VITS[21]. It includes a unique setup comprising a Text Encoder, Length Regulator, and an Emotion-condition Flow, among other components. It utilizes an Emotion-condition Flow to adjust phoneme sequences encoded by the Text Encoder into emotion-laden hidden states $h_{lg}^{emo}$</p> <p>$$ \\tag{9} $$</p> <p>where $d$ is the duration predicted for each unit. This architecture allows for a more dynamic and emotionally responsive speech output during inference.</p> <p>(3) FastSpeech2: Leveraging a Transformer architecture, this model significantly enhances the synthesis speed and quality of speech as outlined in FastSpeech2[36]. It incorporates a Mel-spectrogram Decoder that uses Conditional Cross-Attention to embed emotional nuances effectively into the synthesized speech:</p> <p>$$ \\tag{10} $$</p> <p>where $\\tilde{h}{lg} = h{lg} + u^{emo}$ represents the integration of the text input hidden states and emotion embeddings. This single-step computation efficiently handles the extension of hidden states, their emotional modulation, and the application of cross-attention, resulting in seamless and expressive speech synthesis.</p> <p>By employing prompt-anchoring multimodal fusion and a unified emotional-TTS framework, EMI-TTS generates emotional speech that accurately reflects the desired emotions while preserving the target speaker\u2019s characteristics. The choice of the TTS model can be based on the specific requirements of the application, such as synthesis speed, voice quality, and emotional expressiveness. More details are in the supplementary material.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#4experiments","title":"4.Experiments","text":""},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#41experimental-setting","title":"4.1.Experimental Setting","text":""},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#411dataset","title":"4.1.1.Dataset","text":"<p>Our MM-TTS framework has been evaluated on three different datasets: - Multimodal EmotionLines Dataset (MELD) [32] was utilized to evaluate the Emo-Alignment module\u2019s performance in extracting and aligning emotions from both explicit and implicit multimodal cues. The MELD dataset contains 13,708 utterances from the TV series Friends, covering emotions such as Anger, Disgust, Sadness, Joy, Neutral, Surprise, and Fear. It is divided into training (9,989 utterances), validation (1,109 utterances), and test sets (2,610 utterances) to ensure a comprehensive evaluation. - Emotion Speech Dataset (ESD) [50] was used to test the model\u2019s capability in vocal emotion expression. The ESD includes 17,500 utterances by 10 speakers, categorized into five emotions: neutral, happy, angry, sad, and surprised. The dataset was split into training (14,000 utterances), validation (1,750 utterances), and test subsets (1,750 utterances) for structured evaluation. - Real-world Expression Database (RAF-DB) [25] was employed to evaluate the model\u2019s ability to recognize complex compound emotions from visual data. The RAF-DB is divided into a basic emotion dataset with 15,339 images in seven categories and a compound emotion dataset of 3,954 images in 11 classes.</p> <p>Training and test splits for the basic dataset are 12,271 and 3,068 images, respectively, while the compound dataset is divided into 3,162 images for training and 792 for testing.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#412evaluation-metric","title":"4.1.2.Evaluation Metric","text":"<p>To evaluate MM-TTS framework, we use both objective metrics and subjective human evaluations. - Objective Metrics focus on the model\u2019s ability to accurately extract and classify emotion features from multimodal inputs and to assess the quality of synthesized speech.  (1) Multi-Modal Emotion Alignment Accuracy: We evaluate the ability of MM-TTS to extract emotion features by comparing implicit emotion embeddings from each modality with explicit emotion descriptions. The accuracy is calculated based on how often the predicted emotion class matches the ground truth label, determined by the highest similarity score among potential emotion prompts.  (2) Speech Quality Metrics (WER and CER): The quality of generated speech is objectively measured using the Whisper [35] ASR model. We calculate the Word Error Rate (WER) and Character Error Rate (CER) on ESD dataset by comparing the transcribed synthesized speech to the actual ground truth text, aiming to investigate the impact of our MM-TTS on speech content accuracy. - Subjective Human Evaluations are conducted using the Mean Opinion Score (MOS) to evaluate emotional expressiveness, naturalness of speech, and speaker similarity of the synthesized audio. (1) Emotion Similarity MOS: Raters evaluate how well the synthesized speech conveys the intended emotions. Each sample\u2019s emotion alignment is scored on a scale from 1 to 5, with higher scores indicating a closer match to the target emotion.  (2) Speech Naturalness MOS: This metric assesses the naturalness of the speech output. Raters listen to the synthesized utterances and score them based on how naturally they mimic human speech. (3) Speaker Similarity MOS: Raters assess how accurately the synthesized speech matches the voice characteristics of the target speaker. Samples are scored based on their similarity to reference utterances, focusing on voice timbre, pitch, and style.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#42implementation-details","title":"4.2.Implementation Details","text":""},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#421data-preprocessing","title":"4.2.1.Data Preprocessing","text":"<p>We convert emotion labels into short description sentences to serve as emotion prompts for the models. For text, we employ two strategies: for the flow-based model, we use the International Phonetic Alphabet (IPA) sequences similar to Glow-TTS [19], utilizing phonemizer software [2] for conversion. For Transformer-based models, text sequences are transformed into phoneme sequences with duration information using the Montreal Forced Aligner (MFA) [30]. Audio inputs undergo the Short-time Fourier transform (STFT) to extract linear spectrograms [21], and are also converted into mel-spectrograms as per the methods in Tacotron2 [38]. Images are resized and normalized to a tensor format $V\\in\\mathbb{R}^{h\\times w\\times c}$, with the dimensions $h$ and $w$ both set to 244, and $c$ representing the three color channels of RGB. Video preprocessing involves segmenting videos into frames, uniformly sampling 8 frames across the sequence, and converting these frames into tensors similar to image processing.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#422model-training","title":"4.2.2.Model Training","text":"<p>The training process is segmented into two main phases to manage resource utilization effectively. Initially, the Emotion Prompt Alignment Module (EP-Align) is trained using multi-modal pairs from all datasets. Subsequently, the prompts generated by the trained EP-Align are utilized with audio-text pairs to train the Emotion Embedding-induced TTS (EMI-TTS). For the EPAlign module, we employ contrastive learning to fine-tune encoders from various modalities. We use RoBERTa [29] and InstructERC[24] for text, VGGish [15] and wav2vec2 [1] for audio, and ResNet[18] and ViT [9,34] for images and videos. The training of EP-Align was conducted over 100 epochs on 4 NVIDIA A100 GPUs. For EMI-TTS, the flow-based models underwent 200,000 training steps until convergence, while Transformer-based models were trained for 40,000 steps and Recurrent-based models completed 250,000 steps.</p> <p>More details are in the supplementary material.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#43comparison-with-the-state-of-the-art","title":"4.3.Comparison with the State-of-the-Art","text":"<p>We compare our MM-TTS model to existing state-of-the-art TTS models across several metrics to demonstrate its effectiveness in synthesizing emotional and accurate speech.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#431word-error-rate-and-character-error-rate-wer-and-cer","title":"4.3.1.Word Error Rate and Character Error Rate (WER and CER).","text":"<p>Tab.02 presents a comparison of WER and CER for different TTS models on ESD to illustrate the performance improvement achieved by the MM-TTS framework. The MM-TTS implementation with the FastSpeech structure achieves the lowest WER at 7.35% and CER at 3.07%, indicating an effective approximation to the ground truth audio quality. This performance surpasses other implementations such as VITS, Tacotron, and the EmoSpeech [6] models. This improvement is due to the multimodal emotion alignment within MM-TTS, which improves the extraction and representation of emotion features, allowing for better model convergence and superior generalization capabilities in speech synthesis. This methodological refinement results in more natural and accurate emotional speech output, establishing MM-TTS as a significant advance over traditional label-based emotion encoding methods.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#432emotion-similarity-mean-opinion-score-mos","title":"4.3.2.Emotion Similarity Mean Opinion Score (MOS).","text":"<p>The subjective evaluations presented in Tab.03 illustrate the effectiveness of the MM-TTS models in generating emotionally congruent speech. The MM-TTS (FastSpeech) variant notably scored an average MOS of 4.37, closely matching the ground truth (MOS of 4.57) and surpassing other models. This performance highlights the MM-TTS framework\u2019s capability to effectively capture and render nuanced emotional expressions, with scores across various emotions showing consistent improvement over other TTS models. This indicates a robust alignment of emotional tones in speech synthesis, positioning MM-TTS (FastSpeech) as a leading solution in emotional TTS technologies.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#433speech-naturalness-and-speaker-similarity-mos","title":"4.3.3.Speech Naturalness and Speaker Similarity MOS.","text":"<p>The MMTTS models exhibit strong performance in speech naturalness and speaker similarity, as demonstrated in Tables 4 and 5. These evaluations confirm the MM-TTS\u2019s capability to maintain high standards of voice quality and speaker-specific traits across different emotional expressions. Specifically, MM-TTS (FastSpeech) consistently scores near or above ground truth levels, underscoring its effectiveness in producing natural-sounding and speaker-consistent emotional speech, crucial for enhancing user engagement in personalized TTS applications. These findings not only demonstrate the technical excellence of MM-TTS but also highlight its practical effectiveness in real-world scenarios where emotional variance and speaker identity are crucial for user interaction and satisfaction.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#44ablation-study","title":"4.4.Ablation Study","text":"<p>We perform ablation studies to quantify the impact of EP-Align on emotion classification accuracy and synthesized speech quality across different modalities. Particularly, we assess the impact of EPAlign using the MELD dataset for multi-modal emotion recognition and the RAF-DB dataset for compound emotion classification. The effectiveness of EP-Align is measured in terms of emotion recognition accuracy (weighted F1 scores) and speech synthesis quality (WER, CER, and MOS), respectively.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#441effectiveness-of-multimodal-fusion","title":"4.4.1.Effectiveness of Multimodal Fusion.","text":"<p>To evaluate the impact of the Emotion Prompt Alignment Module (EP-Align) on improving emotion recognition, we conducted a series of tests using the MELD dataset, which is known for its rich multimodal input. Tab.06 clearly demonstrates that EP-Align significantly improves emotion recognition accuracy. Notably, the configuration using combined modalities, which synthesizes text, audio, and video inputs, shows the most marked improvement, with a F1 score increase from 0.68 to 0.75. This highlights EP-Align\u2019s effectiveness in multimodal fusion, proving crucial in environments where emotional cues are inherently dispersed across different channels. By seamlessly blending these inputs, EP-Align enhances the coherence and precision of emotion classification systems. Such advancements are particularly beneficial for developing more sophisticated, context-aware applications in sectors like interactive voice response systems and affective computing.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#442confusion-matrices-analysis","title":"4.4.2.Confusion Matrices Analysis.","text":"<p>Confusion matrices are instrumental in evaluating classification models by illustrating the accuracy across different classes and highlighting potential areas of misclassification. Figures 3 and 4 present the confusion matrices for the MELD and RAF-DB datasets, respectively, following the integration of the Emotion Prompt Alignment Module (EP-Align). These matrices provide insights into EP-Align\u2019s performance in aligning multimodal inputs to predict complex emotional states accurately. Specifically, Fig.03 depicts the classification results for the MELD dataset, which includes a range of basic and complex emotions. This visualization helps identify which emotions EP-Align accurately recognizes and where it tends to confuse one emotion for another. For instance, a common misclassification might be the confusion between \u2019joy\u2019 and \u2019surprise\u2019, which often share similar expressive features. By examining these overlaps, we can better understand the nuances of EP-Align\u2019s performance, including its strengths in distinguishing closely related emotional states and its limitations.</p> <p>Similarly, the confusion matrix for the RAF-DB dataset, shown in Fig.04, illustrates the classification results for compound emotions, which are inherently more complex due to the blending of multiple emotional states. This matrix is particularly useful for assessing EP-Align\u2019s effectiveness in resolving the intricacies of compound emotions. For example, the misclassifications between \u2019happiness\u2019 combined with \u2019surprise\u2019 versus \u2019happiness\u2019 combined with \u2019sadness\u2019 indicate the challenges in distinguishing between subtle emotional blends. Insights from this analysis guide further improvements in EP-Align\u2019s algorithm to enhance its sensitivity to subtle emotional distinctions.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#443impact-on-synthesized-speech-quality","title":"4.4.3.Impact on Synthesized Speech Quality.","text":"<p>To empirically verify the impact of EP-Align on synthesized speech quality, we conducted a detailed analysis comparing the performance of speech synthesis systems with and without EP-Align. As Tab.07 shows, integrating EP-Align results in notable improvements across all evaluated metrics. Specifically, the reduction in WER from 8.50% to 7.35% and in CER from 3.90% to 3.07% suggest that EP-Align significantly enhances the phonetic precision of the synthesized speech. Furthermore, the increase in MOS from 4.12 to 4.37 highlights an improvement in the overall subjective listening experience of the synthesized speech. These improvements suggest that EP-Align effectively adjusts the alignment of phonetic elements, which is crucial for producing clear and accurate speech. This capability not only enhances the intelligibility of the speech output but also its naturalness, thus contributing positively to user experience.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#5conclusions","title":"5.Conclusions","text":"<p>In this work, we introduce MM-TTS, a multimodal framework that revolutionizes emotional speech synthesis by harnessing textual, auditory, and visual information. The EP-Align module ensures seamless emotional feature alignment across modalities through contrastive learning, while EMI-TTS elegantly incorporates these features into cutting-edge TTS models. The resulting emotionally rich speech closely mirrors human emotional expression, as demonstrated by our rigorous evaluations. MM-TTS surpasses traditional E-TTS models in both objective and subjective metrics, showcasing its ability to generate natural and emotionally resonant speech. By open-sourcing our code and models, we aim to drive further innovation and contribute to the progress of emotionally intelligent speech synthesis. MM-TTS sets a new standard for emotional speech synthesis, paving the way for more empathetic and engaging human-computer interactions across diverse applications. In future work, we plan to integrate additional modalities like gesture and facial expression to further enhance emotional expressiveness.</p> <p>Furthermore, we aim to apply our framework to low-resource languages and accented speech, expanding the reach of emotionally intelligent speech synthesis.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#6supplementary-materials","title":"6.Supplementary Materials","text":"<p>This section complements the comprehensive discussions presented in our paper on the Multi-Modal Text-to-Speech (MM-TTS) framework. The purpose of this content is to provide additional technical details, empirical evidence, and demonstrative examples that support the innovative approaches we have developed for emotion embedding-induced speech synthesis. Within this supplementary material, readers will find extensive evaluations, interactive demonstrations, and comparative analyses that illustrate the effectiveness and versatility of the MM-TTS framework across various application scenarios. The additional demonstrations are organized into three primary demos:</p> <p>(1) Voiceover Scenario Demonstrations: This demo presents visual and auditory examples that showcase the MM-TTS framework\u2019s ability to synthesize emotionally resonant voice from multimodal inputs. The demonstrations highlight the framework\u2019s capacity to effectively integrate and interpret emotional cues from visual (facial expressions), auditory (tone of voice), and textual (script context) data sources to generate context-appropriate speech outputs.</p> <p>(2) Emotional Text-to-Speech Synthesis Comparisons: In this demo, we provide a series of comparative demonstrations that evaluate the emotional speech outputs generated by our EMI-TTS system against those produced by traditional emotional TTS approaches. These comparisons aim to showcase the enhanced expressiveness and naturalness of speech synthesized through our framework, emphasizing the improvements achieved over existing technologies.</p> <p>(3) Zero-Shot Emotional Speech Synthesis: This demo focuses on demonstrating the zero-shot capabilities of our framework by presenting synthesized speech outputs that reflect complex and compound emotions not explicitly encountered during the training process. These examples underscore the adaptability and creative potential of the MM-TTS framework, highlighting its ability to enable personalized and dynamic speech generation applications.</p> <p>Note that each demo is supplemented with relevant figures, audio samples, and detailed annotations to facilitate a comprehensive understanding of the methodologies employed and the results obtained. The provided materials are intended to assist researchers and practitioners in exploring the full capabilities of the MM-TTS framework and its potential applications in real-world scenarios.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#61implementation-details-of-emotion-embedding-induced-tts","title":"6.1.Implementation Details of Emotion Embedding-Induced TTS","text":"<p>This section presents implementation details of the emotion embedding-induced text-to-speech (EMI-TTS) component, mentioned in Section 3.2 of the original paper, as shown in Fig.02, which aims to introduce emotional expression into text-to-speech (TTS) synthesis.</p> <p>The EMI-TTS component is implemented across three TTS architectures: Variant VITS, Variant FastSpeech2, and Variant Tacotron2.</p> <p>The following subsections describe the integration of emotion embedding into each architecture, outlining key enhancements to support expressive speech synthesis.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#611variants-vits","title":"6.1.1.Variants VITS.","text":"<p>Variant VITS models incorporate text-to-speech (TTS) architecture and emotion-aware processing technology. These elements are integrated to achieve speech synthesis with emotional content. The model includes an effectively conditioned flow, a structure designed to incorporate affective context into the speech synthesis process. The model is inspired by the architecture of WaveGlow [33] and Glow-TTS [20], using affine coupling layers containing Emotional WaveNet residual blocks. These blocks capture and express emotional nuances in synthesized speech.</p> <p>The Variant VITS model, as shown on the left of the Emotion embedding-induced TTS (EMI-TTS) part of Fig.02, uses global conditioning techniques [43] to integrate emotional information into the speech synthesis pipeline. These techniques insert emotion embeddings, represented as $u^{emo}$, into components such as the Spectrogram Encoder and Emotional WaveNet (EWN) in Emotion-condition FLow. The integration process is controlled by the following operations:</p> <p>$$ $$</p> <p>where $AX()$ represents the affine Xform operator in the emotional conditional flow, $h$ represents the initial hidden feature, and $h'$ is the updated hidden feature after single-flow layer transformation.</p> <p>This approach propagates emotional information throughout the network, allowing the synthesized speech to reflect the intended emotional state.</p> <p>The Variant VITS model also includes a linear layer that converts emotion embeddings before adding them to the length modifier $h_{lg}$ and the emotion vocoder $h_{ac}$ . This transformation infuses emotion into these components, as shown in the following equation:</p> <p>$$ $$</p> <p>where this process modifies the pitch, prosodic aspects, duration, and sound texture of the synthesized speech to match the desired emotional characteristics. The full implementation of the Variant VITS model is publicly available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/VITS/model/models.py.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#612variant-fastspeech2","title":"6.1.2.Variant FastSpeech2.","text":"<p>The FastSpeech2 architecture is improved by the Variant FastSpeech2 model, as shown in the middle of the Emotion embedding-induced TTS (EMI-TTS) part of Fig.02, resulting in advancements in expressive speech synthesis. This model incorporates a Conditional Cross-Attention mechanism, inspired by the EmoSpeech approach [6], to manage the interplay between speaker identity and emotional tone. The integration of these elements aims to enhance emotional modulation in the synthesized speech.</p> <p>The architecture employs a technique that combines the aligned emotion embedding $u^{emo}$ with the speaker embedding. These embeddings are concatenated to form a unified conditioning feature, denoted as $c$. The dimension of the conditioning feature matches the dimension $d$ of the model\u2019s hidden states, with $h_{lg}$ used for textual processing and $h_{ac}$ for acoustic features. The embeddings are processed using learnable matricesW\ud835\udc5e,W\ud835\udc58, andW $v$ , which compute queries, keys, and values, respectively, as shown in the following equations:</p> <p>$$ $$</p> <p>where the attention mechanism applies a softmax function to the scaled dot product of $Q$ and $K^{\\mathsf{T}}$. The scaling is performed by dividing the dot product by the square root of the dimension $d$, as shown in the following equation:</p> <p>$$ $$</p> <p>where this operation reweights the contributions of the emotion and speaker embeddings within each attention layer, allowing their integration into the hidden states and features. The impact of this integration can be observed in components such as the Duration Predictor and the Mel-spectrogram Decoder.</p> <p>Particularly, the attentional mechanism in the Variant FastSpeech2 model aims to modulate speech outputs to reflect the intended emotional states. The model strives to balance naturalness and clarity in the synthesized speech, which is relevant for applications in voice-assisted technologies and interactive systems where emotional resonance is important. The implementation details of the Variant FastSpeech2 model, including configuration files and source code, are made publicly available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/FastSpeech2/.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#613variation-tacotron2","title":"6.1.3.Variation Tacotron2.","text":"<p>The Tacotron2 framework has evolved into the Variant Tacotron2 model, as shown in the right of the Emotion embedding-induced TTS (EMI-TTS) part of Fig.02, which incorporates emotional expressions into its speech synthesis process. The model integrates aligned emotion embeddings $u^{emo}$ by concatenating them with character-encoded hidden states $h_{lg}$ and speaker embeddings $u_{spk}$ . This approach allows emotional context to be taken into account throughout the synthesis process, thereby affecting the prosody and intonation of the speech:</p> <p>$$ $$</p> <p>where this adjustment enhances the traditional position-sensitive attention mechanism and transforms it into a position &amp; emotion-sensitive attention system. New attention mechanisms address the spatial and emotional components of speech synthesis, producing output that maintains natural intonation and is consistent with the desired emotional tone.</p> <p>The process involves the following steps: (1) Concatenation of embeddings: Aligned emotion embeddings $u^{emo}$ concatenated with hidden states derived from character encoders and speaker embeddings. This composite feature vector is used as input to the attention mechanism. (2) Enhanced attention mechanism: The attention mechanism has been updated to incorporate emotional cues and adjust how it processes text and sound information. This modification enables the model to capture a wider range of emotional nuances. (3) Integrated into synthesis: Adjustments to the attention mechanism directly impact the speech synthesis pipeline, affecting elements such as phoneme duration and intonation patterns to align with the intended emotional context.</p> <p>These enhancements are designed to improve emotional speech synthesis, enabling TTS systems to generate output that reflects a wider range of emotions. Integrating emotional cues into the synthesis process can facilitate applications where emotional expression is crucial. Implementation details, including source code and configuration files, are available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/Tacotron2/.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#62more-demonstrations","title":"6.2.More Demonstrations","text":"<p>To further demonstrate the capabilities and potential applications of the proposed MM-TTS framework, we have developed an interactive demo available at https://anonymous.4open.science/api/repo/MMTTS-D214/file/demo/index.html?v. These demos showcase various use cases, emotional speech samples, comparisons with other models, and the framework\u2019s zero-shot abilities.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#621voiceover-scenario","title":"6.2.1.Voiceover Scenario.","text":"<p>In the Voiceover Scenario module, as shown in Fig.05, we illustrate how the Emotion Prompt Alignment Module (EPAlign) can extract emotional cues from multimodal scenarios, enabling the Emotion Embedding-Induced TTS (EMI-TTS) to generate contextually appropriate emotional speech. This capability is particularly valuable in applications that require voiceovers or narrations to seamlessly align with the emotional tone and context of multimedia content.</p> <p>This Fig.05 presents a dialogue scene from a movie clip, with the left four columns representing multimodal references for emotion extraction: the origin video without sound, the origin face image, the origin speech audio, and the origin text transcript. The fifth column displays the inferred aligned emotion representation $u^{emo}$\u2019s class, where $u^{emo}$ is obtained through the EPAlign module by aligning the emotional cues from these multimodal inputs into a shared embedding space. The final two columns showcase the emotional speech generated by EMI-TTS for two different speakers, effectively conveying the identified emotion while preserving the respective speaker characteristics.</p> <p>By leveraging the complementary emotional information present across various modalities, such as visual cues (character expressions and scene visuals), auditory cues (speech waveforms), and textual cues (dialogue transcripts), EPAlign can disentangle and align the intricate emotional nuances exhibited in the multimedia content.</p> <p>This aligned emotion representation is then seamlessly integrated into the EMI-TTS component, enabling the generation of emotional speech that accurately captures and reflects the intended affective tone within the given context.</p> <p>The Voiceover Scenario module exemplifies the power of the MM-TTS framework in generating emotionally resonant voiceovers and narrations for multimedia applications. By effectively aligning and fusing emotional cues from multiple modalities, MM-TTS can produce voiceovers that not only convey the desired emotional expressions but also maintain consistency with the overall emotional context of the multimedia content, thereby enhancing the immersive experience for end-users.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#622emotional-text-to-speech-synthesis","title":"6.2.2.Emotional Text-to-Speech Synthesis.","text":"<p>The Emotional Text-toSpeech Synthesis demo, as shown in Fig.06, presents randomly selected emotional speech samples generated by EMI-TTS, along with comparative samples from other models [6,23]. This comparison highlights EMI-TTS\u2019s ability to provide a unified framework that integrates various TTS architectures, such as Tacotron2 [38], VITS[21], and FastSpeech2 [36]. By leveraging the EPAlign and EMI-TTS components, MM-TTS demonstrates enhanced emotional speech generation capabilities, outperforming traditional approaches.</p> <p>For this demo, we randomly selected 20 text samples with associated emotion labels, comprising two sentences from each of 10 different speakers. These text samples were then used to generate emotional speech using three variants of MM-TTS (corresponding to the integrated TTS architectures), as well as VITS (label), EmotionalTTS [23], and EmoSpeech [6] for a comprehensive evaluation.</p> <p>By presenting this diverse set of emotional speech samples, we aim to demonstrate the effectiveness of the proposed MM-TTS framework in capturing and conveying a wide range of emotional expressions across various TTS architectures. The comparison not only showcases the naturalness and expressiveness of the generated speech but also highlights the ability of MM-TTS to outperform traditional approaches in terms of emotional speech generation capabilities.</p> <p>Moreover, this demo underscores the versatility and modularity of the MM-TTS framework, as it seamlessly integrates multiple TTS architectures while leveraging the EPAlign and EMI-TTS components to enhance emotion representation and synthesis. This flexibility enables researchers and practitioners to leverage the strengths of different TTS architectures while benefiting from the improved emotional speech generation capabilities offered by the MM-TTS framework.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04.29_MM-TTS/#623zero-shot-emotional-speech","title":"6.2.3.Zero-Shot Emotional Speech.","text":"<p>The zero-shot emotional speech demo, as shown in Fig.07, showcases the zero-shot generalization capabilities of MM-TTS by generating emotional speech guided by complex, compound emotions. By aligning emotional prompts within the shared emotion space using EPAlign, EMI-TTS can synthesize emotional speech for emotion categories unseen during training. This ability opens up new avenues for creative expression and personalization in emotional speech synthesis, enabling users to craft tailored emotional expressions beyond those encountered in the training data.</p> <p>Through this interactive demo, we aim to provide researchers and practitioners with a comprehensive understanding of MM-TTS\u2019s potential applications, showcasing its ability to generate contextually appropriate, high-quality emotional speech across various scenarios. By addressing the challenges of multimodal emotion disentanglement and alignment, MM-TTS represents a significant step forward in emotional speech synthesis, with far-reaching implications for human-computer interaction, multimedia content creation, and beyond.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/","title":"2024.04 MM TTS","text":"<p>\u6807\u9898: \"MM-TTS: A Unified Framework for Multimodal, Prompt-Induced Emotional Text-to-Speech Synthesis\" \u4f5c\u8005:   - Xiang Li   - Zhi-Qi Cheng   - Jun-Yan He   - Xiaojiang Peng   - Alexander G. Hauptmann \u673a\u6784:  \u4ee3\u7801: https://anonymous.4open.science/r/MMTTS-D214 ArXiv: https://arxiv.org/abs/2404.18398 \u63d0\u51fa\u65f6\u95f4: 2024-04-29 \u51fa\u7248\u793e:  \u53d1\u8868\u671f\u520a:  \u53d1\u8868\u65f6\u95f4:  \u5f15\u6587\u6570\u91cf: 50 \u88ab\u5f15\u6b21\u6570: 0  tags: DOI: aliases:   - MM-TTS  ArXiv\u6700\u65b0\u7248\u672c: \"1\"  ArXiv\u6700\u65b0\u65f6\u95f4: 2024-04-29  PageNum:  13 Demo:</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#mm-tts-a-unified-framework-for-multimodal-prompt-induced-emotional-text-to-speech-synthesis","title":"MM-TTS: A Unified Framework for Multimodal, Prompt-Induced Emotional Text-to-Speech Synthesis","text":""},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>Emotional Text-to-Speech (E-TTS) synthesis has gained significant attention in recent years due to its potential to enhance human-computer interaction. However, current E-TTS approaches often struggle to capture the complexity of human emotions, primarily relying on oversimplified emotional labels or single-modality inputs. To address these limitations, we propose the Multimodal Emotional Text-to-Speech System (MM-TTS), a unified framework that leverages emotional cues from multiple modalities to generate highly expressive and emotionally resonant speech. MM-TTS consists of two key components:  (1) the Emotion Prompt Alignment Module (EP-Align), which employs contrastive learning to align emotional features across text, audio, and visual modalities, ensuring a coherent fusion of multimodal information;  (2) the Emotion Embedding-Induced TTS (EMI-TTS), which integrates the aligned emotional embeddings with state-of-the-art TTS models to synthesize speech that accurately reflects the intended emotions.</p> <p>Extensive evaluations across diverse datasets demonstrate the superior performance of MM-TTS compared to traditional E-TTS models. Objective metrics, including Word Error Rate (WER) and Character Error Rate (CER), show significant improvements on ESD dataset, with MM-TTS achieving scores of 7.35% and 3.07%, respectively. Subjective assessments further validate that MM-TTS generates speech with emotional fidelity and naturalness comparable to human speech. Our code and pre-trained models are publicly available at https://anonymous.4open.science/r/MMTTS-D214.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Emotional text-to-speech (E-TTS) technology has emerged as a transformative force in artificial intelligence and multimedia, significantly enhancing the richness of human-computer interaction [40]. By imbuing synthetic voices with emotional depth, E-TTS transcends mere speech replication, bringing virtual agents and digital characters to life. The incorporation of emotional expressiveness captivates users and fosters deeper emotional connections, catering to the fundamental human need for empathetic communication [5,49]. This heightened level of engagement has the potential to revolutionize industries from entertainment and education to healthcare and customer service.</p> <p>Despite substantial advancements in text-to-speech (TTS) technologies resulting in naturalistic, high-quality speech, the primary focus has been on linguistic accuracy rather than capturing the inherent emotional nuances [21,31,36\u201338,45]. Contemporary E-TTS approaches have sought to address this gap by integrating emotion labels or utilizing emotional reference speech [5,6,11,23,26,46,49]. However, these methods often struggle to fully capture the intricacies of human emotions due to their reliance on oversimplified representations or single-modality inputs. To truly unlock the potential of E-TTS and create emotionally resonant synthetic speech, a more comprehensive approach leveraging rich emotional information across multiple modalities is crucial.</p> <p>Consider a voiceover scenario where one aims to generate emotionally consistent speech for a given dialogue, potentially with access to visual cues such as facial expressions, silent video clips, and reference audio samples, as shown in Figure 1. Traditional E-TTS approaches relying on unimodal information have inherent limitations. Facial expressions are dynamic, and their emotional interpretation can vary significantly depending on context. Similarly, using reference emotional audio alone fails to capture the nuanced interplay of emotions across scenarios. Moreover, in real-world applications, the availability of different modalities may vary. Leveraging multimodal cues enables a more comprehensive understanding of the underlying emotional state, leading to the generation of speech with more intricate and nuanced emotional expressions tailored to the specific context. Such multimodal integration enables a more flexible approach to emotional speech synthesis, catering to the intricate demands of human-computer interaction and content creation (e.g.Figure 1).</p> <p>In response, we present the Multimodal Emotional Text-to-Speech System (MM-TTS), a groundbreaking framework designed to elevate the expressiveness of synthesized speech by incorporating multimodal cues encompassing text, audio, and visual information (Sec.3). MM-TTS comprises two critical components: the Emotion Prompt Alignment Module (EP-Align) and the Emotion Embedding-Induced TTS (EMI-TTS). EP-Align enables the seamless fusion of multimodal information by aligning emotional data across modalities through a cue anchoring mechanism (Sec. 3.1). EMI-TTS leverages these aligned emotional embeddings to generate expressive and emotionally resonant speech suitable for a wide range of applications (Sec.3.2). The integration of EP-Align and EMI-TTS within MM-TTS leads to emotionally intelligent speech synthesis.</p> <p>To validate the effectiveness of MM-TTS, we conduct extensive testing across diverse datasets, benchmarking its performance against traditional models in crucial speech quality metrics. By investigating the impact of MM-TTS on speech content accuracy with varied TTS framework, we find that it consistently improves performance on ESD dataset with Word Error Rate (WER) and Character Error Rate (CER) scores of 7.35% and 3.07%, respectively (Sec.4.3.1). Subjective evaluations further corroborate these objective measures, confirming that MM-TTS matches the emotional fidelity and naturalness of human speech. The combination of strong objective performance and subjective approval positions MM-TTS at the forefront of Emotional TTS technologies, highlighting its potential to revolutionize human-computer interactions through emotional speech synthesis. The key contributions are threefold:</p> <ul> <li>We introduce the Emotion Prompt Alignment Module (EP-Align), a novel approach employing contrastive learning to synchronize emotional features across modalities. By effectively aligning emotional features and filtering out complex noise in multimodal content, EP-Align addresses the challenge of distribution discrepancies, serving as a critical component in enabling high-quality emotional speech generation.</li> <li>Building upon EP-Align, we develop the Emotion EmbeddingInduced TTS (EMI-TTS) framework, which seamlessly integrates advanced TTS models with the aligned emotional embeddings to synthesize speech that authentically reflects intended emotions. The incorporation of these embeddings enhances the naturalness and credibility of the generated audio, resulting in a more engaging and immersive user experience.</li> <li>We conduct extensive evaluations across a wide spectrum of emotional categories and scenarios to validate the adaptability and superior processing capabilities of MM-TTS. The results confirm that MM-TTS consistently maintains emotional consistency while preserving individual speaker characteristics, highlighting its transformative potential in revolutionizing human-computer interactions by providing a more engaging experience.</li> </ul>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#2related-work","title":"2.Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":""},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#21text-to-speech-synthesis","title":"2.1.Text-to-Speech Synthesis","text":"<p>Text-to-speech (TTS) technology, known as speech synthesis, aims to generate human-like speech from text. Recent neural network-based end-to-end TTS models, such as those developed in [21,31,36\u201338,45], have markedly improved speech quality over traditional methods like concatenation synthesis [17] and statistical parametric synthesis [41,48]. Typically, these models involve converting text or phonemes into linguistic features using embeddings, expanding these features to meet the acoustic model\u2019s requirements via a length regulator, and then converting the acoustic features to waveforms using a vocoder. Different models employ various approaches for the acoustic model. For example, the Tacotron series uses RNN-based models [38,39,45], VITS utilizes flow-based models [8,21], and FastSpeech features Transformer-based models [36,37,44]. While these state-of-the-art TTS models have made some progress in generating natural and high-quality speech, they have primarily focused on linguistic accuracy and clarity, often overlooking the emotional dimensions conveyed through speech.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#22emotional-text-to-speech-synthesis","title":"2.2.Emotional Text-to-Speech Synthesis","text":"<p>Emotional Text-to-Speech (ETTS) focuses on enhancing synthesized speech with emotional expressiveness [40]. Researchers have developed methods to integrate emotions into TTS systems.  (1) One technique uses emotion labels as additional conditioning data in TTS models. For example, EmoSpeech [6] modifies the FastSpeech2 framework by adding speaker and emotion labels via Conditional Layer Normalization. Similar strategies in works like [5,23,49] employ Tacotron-based models where emotion labels are embedded into hidden features as conditions. EmoDiff [11] uses a diffusion model [16] with soft labels for emotions, replacing the traditional one-hot vectors.  (2) Another approach leverages referenced speech capturing the desired emotional state. Li et al. [26] introduce a method incorporating dual emotion classifiers and a style loss to align the generated speech\u2019s emotional content with the reference mel-spectrum. Moreover, [46] explores techniques for transferring the emotional style from reference speech to synthesized speech.  (3) Some studies utilize textual descriptions of emotions as conditioning data, with [47] and [12] demonstrating effective conveyance of target emotions through guided textual descriptions in speech synthesis. Despite advancements, previous ETTS approaches often face limitations like reliance on a single modality or simplistic emotional representations. Table 1 outlines these differences across existing models, focusing on supported emotion categories, conditioning data types, and zero-shot learning capabilities. Addressing these limitations, our proposed framework, MM-TTS, employs multimodal representation learning to integrate diverse emotion-related data, enabling the generation of emotionally rich speech.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#23multi-modal-representation-learning","title":"2.3.Multi-Modal Representation Learning","text":"<p>Multi-modal representation learning, a pivotal area in modern research, aims to find a shared latent space that effectively integrates diverse modalities such as text, images, videos, and audio [10,13,22,27,28,34].  This integration facilitates a deeper understanding of the emotional content underlying these modalities. Multi-modal encoders extract contextually relevant features from each modality, capturing their unique characteristics. These features are then aligned using supervised or unsupervised methods, creating connections that aid in cross-modal tasks [3,4,7]. Particularly, key advancements include the Contrastive Language-Image Pretraining (CLIP) model [34], which establishes a joint embedding for images and text, and the AudioCLIP model [13], which extends this to include audio. Furthermore, Koepke et al. [22] have explored combining audio and visual data to enable applications like sound localization and audio-visual event detection. Our framework, MMTTS, builds on these developments by using multi-modal representation learning to improve the emotional expressiveness of synthesized speech. By extracting and aligning feature representations from text, images, videos, and audio with emotion tags through contrastive learning, MM-TTS captures comprehensive emotion-related information. This enriched understanding allows MM-TTS to align explicit and implicit emotional cues, generating nuanced and emotionally rich synthesized speech.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#3mm-tts-framework","title":"3.MM-TTS Framework","text":"<p>The primary goal of MM-TTS is to leverage emotions extracted from multiple modalities to generate emotional speech for different individuals in a data-efficient manner. Figure 2 illustrates the framework comprising two main components: (1) Emotion Prompt Alignment Module (EP-Align) and (2) Emotion Embedding-induced TTS (EMI-TTS).</p> <p>(1) Emotion Prompt Alignment Module (EP-Align) plays a crucial role in aligning the emotional representations derived from various modalities, such as visual data, audio segments, and textual descriptions. It takes a multimodal emotional data tuple $Tup^{emo}=$ as input, where $v$ represents visual data (image or video), $a$ is an audio segment, $s$ denotes an emotional text description, and $p$ is an emotional prompt label. EP-Align processes these inputs using a set of multimodal encoders $\\mathcal{E} = {\\mathcal{E}^{vis}, \\mathcal{E}^{audio}, \\mathcal{E}^{tex}, \\mathcal{E}^{prop}}$, which extract emotion features and generate a unified emotion embedding. This emotion embedding is then passed to the EMI-TTS component. <p>(2) Emotion Embedding-induced TTS (EMI-TTS) component takes the aligned emotion embedding, along with the input text Tex and a pre-trained TTS model from the model library $\\mathcal{M}$, to generate emotional speech. The emotion embedding provides the necessary emotional context, while the input text and the TTS model determine the content and the overall style of the generated speech.</p> <p>Formally, the audio with emotion can be generated by MM-TTS $\\Phi$ as follows:</p> <p>$$ \\tag{1} $$</p> <p>The MM-TTS framework aims to address the challenges associated with generating emotional speech by leveraging the complementary information provided by multiple modalities. By aligning the emotional representations from different sources and integrating them into the TTS process, MM-TTS enables the synthesis of speech that accurately conveys the desired emotions while maintaining the speaker\u2019s characteristics. In the following, we will delve into the details of the Emotion Prompt Alignment Module (EP-Align) and the Emotion Embedding-induced TTS (EMI-TTS) component.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#31emotion-prompt-alignment-module","title":"3.1.Emotion Prompt Alignment Module","text":"<p>Inspired by Contrastive Language-Image Pre-training (CLIP) [34], EP-Align employs a set of multimodal encoders to extract emotion features from various modalities. These encoders are denoted as $\\mathcal{E} = {\\mathcal{E}^{vis}, \\mathcal{E}^{audio}, \\mathcal{E}^{tex}, \\mathcal{E}^{prop}}$, corresponding to vision, audio, text, and emotion prompts, respectively. The extracted features are then aligned into a unified emotion embedding space.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#311multimodal-encoders","title":"3.1.1.Multimodal Encoders.","text":"<p>Each modality is processed by a dedicated encoder to extract relevant emotion features. Given a multimodal emotional data tuple $Tup^{emo}=$, the encoders generate feature representations as follows: <p>$$ \\tag{2} $$</p> <p>where $f^{vis}, f^{audio}, f^{tex}, f^{prop}$ represent the feature representations for visual, audio, textual, and emotional prompts, respectively. Each feature representation has a dimensionality of\ud835\udc3e, which is set to 512 in this work.</p> <p>The architectures of the text encoder $\\mathcal{E}^{tex}$, prompt encoder $\\mathcal{E}^{prop}$, and visual encoder $\\mathcal{E}^{vis}$ are similar to those in CLIP [34]. The audio encoder $\\mathcal{E}^{audio}$ follows the design from [1] for effective audio encoding. During training, the text and image encoders are initialized with pre-trained CLIP weights, while the audio encoder is initialized with a pre-trained model from [14]. To handle video inputs, the visual encoder $\\mathcal{E}^{vis}$ processes a sequence of frames, extracts a feature bank, and performs average pooling in the temporal dimension to obtain the final representation. For audio inputs, an additional linear projection layer is employed to map the audio features to the same dimension as the text and image features.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#312multimodal-emotional-alignment","title":"3.1.2.Multimodal Emotional Alignment.","text":"<p>Aligning the emotion representations from different modalities is a critical step in MM-TTS. We propose a prompt-anchoring scheme that leverages prompt-based bridging to facilitate the alignment of multimodal representations. First, we construct vision-prompt (vis-pro) and audio-prompt (aud-pro) embedding spaces by projecting the vision, audio, and text features using learned projection matrices:</p> <p>$$ \\tag{3} $$</p> <p>where $W^{vis\u2212pro}$, $W^{aud\u2212pro}$, $W^{tex\u2212pro}\\in\\mathbb{R}^{K\\times K}$ are the projection matrices, and $u^{vis}$, $u^{audio}$, and $u^{tex}$ }are the projected embeddings in the vis-pro, aud-pro, and tex-pro spaces, respectively.</p> <p>The prompt embedding $u^{prop}$ is obtained by multiplying the corresponding projection matrix based on the alignment modality:</p> <p>$$ \\tag{4} $$</p> <p>We use $u^{exp}$ to denote the explicit emotion embedding obtained from the emotion prompt and $u^{imp}$ to represent the implicit emotion embeddings from vision, audio, or text. The cosine similarity between the explicit and implicit embeddings is computed as:</p> <p>$$ \\tag{5} $$</p> <p>where $t$ is a learned temperature parameter, and $\\sigma$ denotes the normalization operation.</p> <p>The multimodal alignment loss $L_{align}$ is defined as the symmetric cross-entropy loss between the explicit and implicit embeddings:</p> <p>$$ \\tag{6} $$</p> <p>During inference, EP-Align selects the emotion prompt embedding $u^{prop}$ with the highest similarity score to the implicit embeddings as the aligned emotion representation $u^{emo}$. This aligned emotion representation captures the emotional content conveyed by the multimodal inputs and serves as a unified representation for the subsequent TTS process. By leveraging the prompt-anchoring scheme and the multimodal encoders, EP-Align effectively aligns the emotion representations from different modalities into a shared embedding space. This alignment enables the TTS model to generate emotional speech that is consistent with the input emotional context, regardless of the source modality.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#32emotion-embedding-induced-tts","title":"3.2.Emotion Embedding-induced TTS","text":"<p>With the aligned emotion embedding $u^{emo}$ obtained from the previous EP-Align module, the Emotion Embedding-induced TTS (EM-TTS) component generates emotional speech by integrating the emotion representation into the TTS process.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#321prompt-anchoring-multimodal-fusion","title":"3.2.1.Prompt-Anchoring Multimodal Fusion.","text":"<p>To mitigate the bias among the multimodal emotion embedding spaces, we employ prompt-anchoring to fuse the aligned emotion embedding $u^{emo}$ with the input text representation. Specifically, we use the implicit emotion embedding $u^{imp}$ to retrieve the most similar prompt embedding $u^{prop}$ based on cosine similarity. The retrieved prompt embedding serves as an anchor to bridge the implicit and explicit emotion representations, enabling a more coherent integration of the emotion embedding into the TTS model.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#322unified-emotional-tts-framework","title":"3.2.2Unified Emotional-TTS Framework.","text":"<p>EMI-TTS provides a unified framework that integrates various TTS models, such as Tacotron2[38], VITS [21], and FastSpeech2 [36]. These models typically consist of a text encoder, a length regulator, an acoustic model, and a vocoder. The text encoder converts the input text Tex into a sequence of linguistic features $h_{lg}$ . The length regulator adjusts the duration of the linguistic features to match the desired speech length. The acoustic model generates acoustic features $h_{ac}$ conditioned on the linguistic features and the emotion and speaker embeddings. Finally, the vocoder converts the acoustic features into a speech waveform. In EMI-TTS, the emotion embedding $u^{emo}$ and speaker embedding $u_{spk}$ are concatenated and integrated into the TTS model as additional conditioning inputs. The emotion embedding provides the necessary emotional context, while the speaker embedding captures the speaker\u2019s characteristics. This allows the TTS model to generate speech that reflects both the desired emotion and the target speaker\u2019s style.</p> <p>Formally, the EMI-TTS process can be represented as follows:</p> <p>$$ \\tag{7} $$</p> <p>where $h_{lg}^{emo}$ represents the linguistic features conditioned on the emotion and speaker embeddings, and $Audio^{emo}$ is the generated emotional speech. During training, EMI-TTS optimizes the same loss functions as the base TTS models without requiring additional loss terms. The model is trained on paired data consisting of input text, emotion features, and speaker information. The vocoder used in EMI-TTS depends on the specific TTS model: VITS uses its original decoder, while Tacotron2 and FastSpeech2 employ a WaveNet[42] vocoder.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#323model-library","title":"3.2.3.Model Library.","text":"<p>EMI-TTS incorporates several state-of-the-art TTS models, including Tacotron2, VITS, and FastSpeech2. By providing a diverse set of TTS models, EMI-TTS offers flexibility in generating emotional speech across different scenarios and requirements.</p> <p>(1) Tacotron2: Building upon the traditional Tacotron model[38], this version integrates WaveNet technology[42] for improved sound generation. The Character Encoder transforms text into hidden states $h_{lg}$ , enhanced with emotion embeddings $u^{emo}$ to create emotionally enriched states $h_{lg}^{emo}$:</p> <p>$$ \\tag{8} $$</p> <p>This enhancement adjusts the Location Sensitive Attention to a more nuanced Location &amp; Emotion Sensitive Attention, allowing the model to better capture emotional nuances in speech synthesis.</p> <p>(2) VITS: This model innovatively combines variational inference with normalizing flows and adversarial training, as outlined in VITS[21]. It includes a unique setup comprising a Text Encoder, Length Regulator, and an Emotion-condition Flow, among other components. It utilizes an Emotion-condition Flow to adjust phoneme sequences encoded by the Text Encoder into emotion-laden hidden states $h_{lg}^{emo}$</p> <p>$$ \\tag{9} $$</p> <p>where $d$ is the duration predicted for each unit. This architecture allows for a more dynamic and emotionally responsive speech output during inference.</p> <p>(3) FastSpeech2: Leveraging a Transformer architecture, this model significantly enhances the synthesis speed and quality of speech as outlined in FastSpeech2[36]. It incorporates a Mel-spectrogram Decoder that uses Conditional Cross-Attention to embed emotional nuances effectively into the synthesized speech:</p> <p>$$ \\tag{10} $$</p> <p>where $\\tilde{h}{lg} = h{lg} + u^{emo}$ represents the integration of the text input hidden states and emotion embeddings. This single-step computation efficiently handles the extension of hidden states, their emotional modulation, and the application of cross-attention, resulting in seamless and expressive speech synthesis.</p> <p>By employing prompt-anchoring multimodal fusion and a unified emotional-TTS framework, EMI-TTS generates emotional speech that accurately reflects the desired emotions while preserving the target speaker\u2019s characteristics. The choice of the TTS model can be based on the specific requirements of the application, such as synthesis speed, voice quality, and emotional expressiveness. More details are in the supplementary material.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#4experiments","title":"4.Experiments","text":""},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#41experimental-setting","title":"4.1.Experimental Setting","text":""},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#411dataset","title":"4.1.1.Dataset","text":"<p>Our MM-TTS framework has been evaluated on three different datasets: - Multimodal EmotionLines Dataset (MELD) [32] was utilized to evaluate the Emo-Alignment module\u2019s performance in extracting and aligning emotions from both explicit and implicit multimodal cues. The MELD dataset contains 13,708 utterances from the TV series Friends, covering emotions such as Anger, Disgust, Sadness, Joy, Neutral, Surprise, and Fear. It is divided into training (9,989 utterances), validation (1,109 utterances), and test sets (2,610 utterances) to ensure a comprehensive evaluation. - Emotion Speech Dataset (ESD) [50] was used to test the model\u2019s capability in vocal emotion expression. The ESD includes 17,500 utterances by 10 speakers, categorized into five emotions: neutral, happy, angry, sad, and surprised. The dataset was split into training (14,000 utterances), validation (1,750 utterances), and test subsets (1,750 utterances) for structured evaluation. - Real-world Expression Database (RAF-DB) [25] was employed to evaluate the model\u2019s ability to recognize complex compound emotions from visual data. The RAF-DB is divided into a basic emotion dataset with 15,339 images in seven categories and a compound emotion dataset of 3,954 images in 11 classes.</p> <p>Training and test splits for the basic dataset are 12,271 and 3,068 images, respectively, while the compound dataset is divided into 3,162 images for training and 792 for testing.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#412evaluation-metric","title":"4.1.2.Evaluation Metric","text":"<p>To evaluate MM-TTS framework, we use both objective metrics and subjective human evaluations. - Objective Metrics focus on the model\u2019s ability to accurately extract and classify emotion features from multimodal inputs and to assess the quality of synthesized speech.  (1) Multi-Modal Emotion Alignment Accuracy: We evaluate the ability of MM-TTS to extract emotion features by comparing implicit emotion embeddings from each modality with explicit emotion descriptions. The accuracy is calculated based on how often the predicted emotion class matches the ground truth label, determined by the highest similarity score among potential emotion prompts.  (2) Speech Quality Metrics (WER and CER): The quality of generated speech is objectively measured using the Whisper [35] ASR model. We calculate the Word Error Rate (WER) and Character Error Rate (CER) on ESD dataset by comparing the transcribed synthesized speech to the actual ground truth text, aiming to investigate the impact of our MM-TTS on speech content accuracy. - Subjective Human Evaluations are conducted using the Mean Opinion Score (MOS) to evaluate emotional expressiveness, naturalness of speech, and speaker similarity of the synthesized audio. (1) Emotion Similarity MOS: Raters evaluate how well the synthesized speech conveys the intended emotions. Each sample\u2019s emotion alignment is scored on a scale from 1 to 5, with higher scores indicating a closer match to the target emotion.  (2) Speech Naturalness MOS: This metric assesses the naturalness of the speech output. Raters listen to the synthesized utterances and score them based on how naturally they mimic human speech. (3) Speaker Similarity MOS: Raters assess how accurately the synthesized speech matches the voice characteristics of the target speaker. Samples are scored based on their similarity to reference utterances, focusing on voice timbre, pitch, and style.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#42implementation-details","title":"4.2.Implementation Details","text":""},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#421data-preprocessing","title":"4.2.1.Data Preprocessing","text":"<p>We convert emotion labels into short description sentences to serve as emotion prompts for the models. For text, we employ two strategies: for the flow-based model, we use the International Phonetic Alphabet (IPA) sequences similar to Glow-TTS [19], utilizing phonemizer software [2] for conversion. For Transformer-based models, text sequences are transformed into phoneme sequences with duration information using the Montreal Forced Aligner (MFA) [30]. Audio inputs undergo the Short-time Fourier transform (STFT) to extract linear spectrograms [21], and are also converted into mel-spectrograms as per the methods in Tacotron2 [38]. Images are resized and normalized to a tensor format $V\\in\\mathbb{R}^{h\\times w\\times c}$, with the dimensions $h$ and $w$ both set to 244, and $c$ representing the three color channels of RGB. Video preprocessing involves segmenting videos into frames, uniformly sampling 8 frames across the sequence, and converting these frames into tensors similar to image processing.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#422model-training","title":"4.2.2.Model Training","text":"<p>The training process is segmented into two main phases to manage resource utilization effectively. Initially, the Emotion Prompt Alignment Module (EP-Align) is trained using multi-modal pairs from all datasets. Subsequently, the prompts generated by the trained EP-Align are utilized with audio-text pairs to train the Emotion Embedding-induced TTS (EMI-TTS). For the EPAlign module, we employ contrastive learning to fine-tune encoders from various modalities. We use RoBERTa [29] and InstructERC[24] for text, VGGish [15] and wav2vec2 [1] for audio, and ResNet[18] and ViT [9,34] for images and videos. The training of EP-Align was conducted over 100 epochs on 4 NVIDIA A100 GPUs. For EMI-TTS, the flow-based models underwent 200,000 training steps until convergence, while Transformer-based models were trained for 40,000 steps and Recurrent-based models completed 250,000 steps.</p> <p>More details are in the supplementary material.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#43comparison-with-the-state-of-the-art","title":"4.3.Comparison with the State-of-the-Art","text":"<p>We compare our MM-TTS model to existing state-of-the-art TTS models across several metrics to demonstrate its effectiveness in synthesizing emotional and accurate speech.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#431word-error-rate-and-character-error-rate-wer-and-cer","title":"4.3.1.Word Error Rate and Character Error Rate (WER and CER).","text":"<p>Table 2 presents a comparison of WER and CER for different TTS models on ESD to illustrate the performance improvement achieved by the MM-TTS framework. The MM-TTS implementation with the FastSpeech structure achieves the lowest WER at 7.35% and CER at 3.07%, indicating an effective approximation to the ground truth audio quality. This performance surpasses other implementations such as VITS, Tacotron, and the EmoSpeech [6] models. This improvement is due to the multimodal emotion alignment within MM-TTS, which improves the extraction and representation of emotion features, allowing for better model convergence and superior generalization capabilities in speech synthesis. This methodological refinement results in more natural and accurate emotional speech output, establishing MM-TTS as a significant advance over traditional label-based emotion encoding methods.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#432emotion-similarity-mean-opinion-score-mos","title":"4.3.2.Emotion Similarity Mean Opinion Score (MOS).","text":"<p>The subjective evaluations presented in Table 3 illustrate the effectiveness of the MM-TTS models in generating emotionally congruent speech. The MM-TTS (FastSpeech) variant notably scored an average MOS of 4.37, closely matching the ground truth (MOS of 4.57) and surpassing other models. This performance highlights the MM-TTS framework\u2019s capability to effectively capture and render nuanced emotional expressions, with scores across various emotions showing consistent improvement over other TTS models. This indicates a robust alignment of emotional tones in speech synthesis, positioning MM-TTS (FastSpeech) as a leading solution in emotional TTS technologies.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#433speech-naturalness-and-speaker-similarity-mos","title":"4.3.3.Speech Naturalness and Speaker Similarity MOS.","text":"<p>The MMTTS models exhibit strong performance in speech naturalness and speaker similarity, as demonstrated in Tables 4 and 5. These evaluations confirm the MM-TTS\u2019s capability to maintain high standards of voice quality and speaker-specific traits across different emotional expressions. Specifically, MM-TTS (FastSpeech) consistently scores near or above ground truth levels, underscoring its effectiveness in producing natural-sounding and speaker-consistent emotional speech, crucial for enhancing user engagement in personalized TTS applications. These findings not only demonstrate the technical excellence of MM-TTS but also highlight its practical effectiveness in real-world scenarios where emotional variance and speaker identity are crucial for user interaction and satisfaction.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#44ablation-study","title":"4.4.Ablation Study","text":"<p>We perform ablation studies to quantify the impact of EP-Align on emotion classification accuracy and synthesized speech quality across different modalities. Particularly, we assess the impact of EPAlign using the MELD dataset for multi-modal emotion recognition and the RAF-DB dataset for compound emotion classification. The effectiveness of EP-Align is measured in terms of emotion recognition accuracy (weighted F1 scores) and speech synthesis quality (WER, CER, and MOS), respectively.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#441effectiveness-of-multimodal-fusion","title":"4.4.1.Effectiveness of Multimodal Fusion.","text":"<p>To evaluate the impact of the Emotion Prompt Alignment Module (EP-Align) on improving emotion recognition, we conducted a series of tests using the MELD dataset, which is known for its rich multimodal input. Table 6 clearly demonstrates that EP-Align significantly improves emotion recognition accuracy. Notably, the configuration using combined modalities, which synthesizes text, audio, and video inputs, shows the most marked improvement, with a F1 score increase from 0.68 to 0.75. This highlights EP-Align\u2019s effectiveness in multimodal fusion, proving crucial in environments where emotional cues are inherently dispersed across different channels. By seamlessly blending these inputs, EP-Align enhances the coherence and precision of emotion classification systems. Such advancements are particularly beneficial for developing more sophisticated, context-aware applications in sectors like interactive voice response systems and affective computing.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#442confusion-matrices-analysis","title":"4.4.2.Confusion Matrices Analysis.","text":"<p>Confusion matrices are instrumental in evaluating classification models by illustrating the accuracy across different classes and highlighting potential areas of misclassification. Figures 3 and 4 present the confusion matrices for the MELD and RAF-DB datasets, respectively, following the integration of the Emotion Prompt Alignment Module (EP-Align). These matrices provide insights into EP-Align\u2019s performance in aligning multimodal inputs to predict complex emotional states accurately. Specifically, Figure 3 depicts the classification results for the MELD dataset, which includes a range of basic and complex emotions. This visualization helps identify which emotions EP-Align accurately recognizes and where it tends to confuse one emotion for another. For instance, a common misclassification might be the confusion between \u2019joy\u2019 and \u2019surprise\u2019, which often share similar expressive features. By examining these overlaps, we can better understand the nuances of EP-Align\u2019s performance, including its strengths in distinguishing closely related emotional states and its limitations.</p> <p>Similarly, the confusion matrix for the RAF-DB dataset, shown in Figure 4, illustrates the classification results for compound emotions, which are inherently more complex due to the blending of multiple emotional states. This matrix is particularly useful for assessing EP-Align\u2019s effectiveness in resolving the intricacies of compound emotions. For example, the misclassifications between \u2019happiness\u2019 combined with \u2019surprise\u2019 versus \u2019happiness\u2019 combined with \u2019sadness\u2019 indicate the challenges in distinguishing between subtle emotional blends. Insights from this analysis guide further improvements in EP-Align\u2019s algorithm to enhance its sensitivity to subtle emotional distinctions.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#443impact-on-synthesized-speech-quality","title":"4.4.3.Impact on Synthesized Speech Quality.","text":"<p>To empirically verify the impact of EP-Align on synthesized speech quality, we conducted a detailed analysis comparing the performance of speech synthesis systems with and without EP-Align. As Table 7 shows, integrating EP-Align results in notable improvements across all evaluated metrics. Specifically, the reduction in WER from 8.50% to 7.35% and in CER from 3.90% to 3.07% suggest that EP-Align significantly enhances the phonetic precision of the synthesized speech. Furthermore, the increase in MOS from 4.12 to 4.37 highlights an improvement in the overall subjective listening experience of the synthesized speech. These improvements suggest that EP-Align effectively adjusts the alignment of phonetic elements, which is crucial for producing clear and accurate speech. This capability not only enhances the intelligibility of the speech output but also its naturalness, thus contributing positively to user experience.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#5conclusions","title":"5.Conclusions","text":"<p>In this work, we introduce MM-TTS, a multimodal framework that revolutionizes emotional speech synthesis by harnessing textual, auditory, and visual information. The EP-Align module ensures seamless emotional feature alignment across modalities through contrastive learning, while EMI-TTS elegantly incorporates these features into cutting-edge TTS models. The resulting emotionally rich speech closely mirrors human emotional expression, as demonstrated by our rigorous evaluations. MM-TTS surpasses traditional E-TTS models in both objective and subjective metrics, showcasing its ability to generate natural and emotionally resonant speech. By open-sourcing our code and models, we aim to drive further innovation and contribute to the progress of emotionally intelligent speech synthesis. MM-TTS sets a new standard for emotional speech synthesis, paving the way for more empathetic and engaging human-computer interactions across diverse applications. In future work, we plan to integrate additional modalities like gesture and facial expression to further enhance emotional expressiveness.</p> <p>Furthermore, we aim to apply our framework to low-resource languages and accented speech, expanding the reach of emotionally intelligent speech synthesis.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#6supplementary-materials","title":"6.Supplementary Materials","text":"<p>This section complements the comprehensive discussions presented in our paper on the Multi-Modal Text-to-Speech (MM-TTS) framework. The purpose of this content is to provide additional technical details, empirical evidence, and demonstrative examples that support the innovative approaches we have developed for emotion embedding-induced speech synthesis. Within this supplementary material, readers will find extensive evaluations, interactive demonstrations, and comparative analyses that illustrate the effectiveness and versatility of the MM-TTS framework across various application scenarios. The additional demonstrations are organized into three primary demos:</p> <p>(1) Voiceover Scenario Demonstrations: This demo presents visual and auditory examples that showcase the MM-TTS framework\u2019s ability to synthesize emotionally resonant voice from multimodal inputs. The demonstrations highlight the framework\u2019s capacity to effectively integrate and interpret emotional cues from visual (facial expressions), auditory (tone of voice), and textual (script context) data sources to generate context-appropriate speech outputs.</p> <p>(2) Emotional Text-to-Speech Synthesis Comparisons: In this demo, we provide a series of comparative demonstrations that evaluate the emotional speech outputs generated by our EMI-TTS system against those produced by traditional emotional TTS approaches. These comparisons aim to showcase the enhanced expressiveness and naturalness of speech synthesized through our framework, emphasizing the improvements achieved over existing technologies.</p> <p>(3) Zero-Shot Emotional Speech Synthesis: This demo focuses on demonstrating the zero-shot capabilities of our framework by presenting synthesized speech outputs that reflect complex and compound emotions not explicitly encountered during the training process. These examples underscore the adaptability and creative potential of the MM-TTS framework, highlighting its ability to enable personalized and dynamic speech generation applications.</p> <p>Note that each demo is supplemented with relevant figures, audio samples, and detailed annotations to facilitate a comprehensive understanding of the methodologies employed and the results obtained. The provided materials are intended to assist researchers and practitioners in exploring the full capabilities of the MM-TTS framework and its potential applications in real-world scenarios.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#61implementation-details-of-emotion-embedding-induced-tts","title":"6.1.Implementation Details of Emotion Embedding-Induced TTS","text":"<p>This section presents implementation details of the emotion embedding-induced text-to-speech (EMI-TTS) component, mentioned in Section 3.2 of the original paper, as shown in Figure 2, which aims to introduce emotional expression into text-to-speech (TTS) synthesis.</p> <p>The EMI-TTS component is implemented across three TTS architectures: Variant VITS, Variant FastSpeech2, and Variant Tacotron2.</p> <p>The following subsections describe the integration of emotion embedding into each architecture, outlining key enhancements to support expressive speech synthesis.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#611variants-vits","title":"6.1.1.Variants VITS.","text":"<p>Variant VITS models incorporate text-to-speech (TTS) architecture and emotion-aware processing technology. These elements are integrated to achieve speech synthesis with emotional content. The model includes an effectively conditioned flow, a structure designed to incorporate affective context into the speech synthesis process. The model is inspired by the architecture of WaveGlow [33] and Glow-TTS [20], using affine coupling layers containing Emotional WaveNet residual blocks. These blocks capture and express emotional nuances in synthesized speech.</p> <p>The Variant VITS model, as shown on the left of the Emotion embedding-induced TTS (EMI-TTS) part of Figure 2, uses global conditioning techniques [43] to integrate emotional information into the speech synthesis pipeline. These techniques insert emotion embeddings, represented as$u^{emo}$, into components such as the Spectrogram Encoder and Emotional WaveNet (EWN) in Emotion-condition FLow. The integration process is controlled by the following operations:</p> <p>$$ $$</p> <p>where $AX()$ represents the affine Xform operator in the emotional conditional flow, $h$ represents the initial hidden feature, and $h'$ is the updated hidden feature after single-flow layer transformation.</p> <p>This approach propagates emotional information throughout the network, allowing the synthesized speech to reflect the intended emotional state.</p> <p>The Variant VITS model also includes a linear layer that converts emotion embeddings before adding them to the length modifier $h_{lg}$ and the emotion vocoder $h_{ac}$ . This transformation infuses emotion into these components, as shown in the following equation:</p> <p>$$ $$</p> <p>where this process modifies the pitch, prosodic aspects, duration, and sound texture of the synthesized speech to match the desired emotional characteristics. The full implementation of the Variant VITS model is publicly available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/VITS/model/models.py.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#612variant-fastspeech2","title":"6.1.2.Variant FastSpeech2.","text":"<p>The FastSpeech2 architecture is improved by the Variant FastSpeech2 model, as shown in the middle of the Emotion embedding-induced TTS (EMI-TTS) part of Figure 2, resulting in advancements in expressive speech synthesis. This model incorporates a Conditional Cross-Attention mechanism, inspired by the EmoSpeech approach [6], to manage the interplay between speaker identity and emotional tone. The integration of these elements aims to enhance emotional modulation in the synthesized speech.</p> <p>The architecture employs a technique that combines the aligned emotion embedding $u^{emo}$ with the speaker embedding. These embeddings are concatenated to form a unified conditioning feature, denoted as $c$. The dimension of the conditioning feature matches the dimension $d$ of the model\u2019s hidden states, with $h_{lg}$ used for textual processing and $h_{ac}$ for acoustic features. The embeddings are processed using learnable matricesW\ud835\udc5e,W\ud835\udc58, andW $v$ , which compute queries, keys, and values, respectively, as shown in the following equations:</p> <p>$$ $$</p> <p>where the attention mechanism applies a softmax function to the scaled dot product of $Q$ and $K^{\\mathsf{T}}$. The scaling is performed by dividing the dot product by the square root of the dimension $d$, as shown in the following equation:</p> <p>$$ $$</p> <p>where this operation reweights the contributions of the emotion and speaker embeddings within each attention layer, allowing their integration into the hidden states and features. The impact of this integration can be observed in components such as the Duration Predictor and the Mel-spectrogram Decoder.</p> <p>Particularly, the attentional mechanism in the Variant FastSpeech2 model aims to modulate speech outputs to reflect the intended emotional states. The model strives to balance naturalness and clarity in the synthesized speech, which is relevant for applications in voice-assisted technologies and interactive systems where emotional resonance is important. The implementation details of the Variant FastSpeech2 model, including configuration files and source code, are made publicly available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/FastSpeech2/.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#613variation-tacotron2","title":"6.1.3.Variation Tacotron2.","text":"<p>The Tacotron2 framework has evolved into the Variant Tacotron2 model, as shown in the right of the Emotion embedding-induced TTS (EMI-TTS) part of Figure 2, which incorporates emotional expressions into its speech synthesis process. The model integrates aligned emotion embeddings$u^{emo}$by concatenating them with character-encoded hidden states $h_{lg}$ and speaker embeddings $u_{spk}$ . This approach allows emotional context to be taken into account throughout the synthesis process, thereby affecting the prosody and intonation of the speech:</p> <p>$$ $$</p> <p>where this adjustment enhances the traditional position-sensitive attention mechanism and transforms it into a position &amp; emotion-sensitive attention system. New attention mechanisms address the spatial and emotional components of speech synthesis, producing output that maintains natural intonation and is consistent with the desired emotional tone.</p> <p>The process involves the following steps: (1) Concatenation of embeddings: Aligned emotion embeddings$u^{emo}$concatenated with hidden states derived from character encoders and speaker embeddings. This composite feature vector is used as input to the attention mechanism. (2) Enhanced attention mechanism: The attention mechanism has been updated to incorporate emotional cues and adjust how it processes text and sound information. This modification enables the model to capture a wider range of emotional nuances. (3) Integrated into synthesis: Adjustments to the attention mechanism directly impact the speech synthesis pipeline, affecting elements such as phoneme duration and intonation patterns to align with the intended emotional context.</p> <p>These enhancements are designed to improve emotional speech synthesis, enabling TTS systems to generate output that reflects a wider range of emotions. Integrating emotional cues into the synthesis process can facilitate applications where emotional expression is crucial. Implementation details, including source code and configuration files, are available at: https://anonymous.4open.science/r/MMTTS-D214/EMITTS/Tacotron2/.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#62more-demonstrations","title":"6.2.More Demonstrations","text":"<p>To further demonstrate the capabilities and potential applications of the proposed MM-TTS framework, we have developed an interactive demo available at https://anonymous.4open.science/api/repo/MMTTS-D214/file/demo/index.html?v. These demos showcase various use cases, emotional speech samples, comparisons with other models, and the framework\u2019s zero-shot abilities.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#621voiceover-scenario","title":"6.2.1.Voiceover Scenario.","text":"<p>In the Voiceover Scenario module, as shown in Figure 5, we illustrate how the Emotion Prompt Alignment Module (EPAlign) can extract emotional cues from multimodal scenarios, enabling the Emotion Embedding-Induced TTS (EMI-TTS) to generate contextually appropriate emotional speech. This capability is particularly valuable in applications that require voiceovers or narrations to seamlessly align with the emotional tone and context of multimedia content.</p> <p>This figure presents a dialogue scene from a movie clip, with the left four columns representing multimodal references for emotion extraction: the origin video without sound, the origin face image, the origin speech audio, and the origin text transcript. The fifth column displays the inferred aligned emotion representation $u^{emo}$\u2019s class, where$u^{emo}$is obtained through the EPAlign module by aligning the emotional cues from these multimodal inputs into a shared embedding space. The final two columns showcase the emotional speech generated by EMI-TTS for two different speakers, effectively conveying the identified emotion while preserving the respective speaker characteristics.</p> <p>By leveraging the complementary emotional information present across various modalities, such as visual cues (character expressions and scene visuals), auditory cues (speech waveforms), and textual cues (dialogue transcripts), EPAlign can disentangle and align the intricate emotional nuances exhibited in the multimedia content.</p> <p>This aligned emotion representation is then seamlessly integrated into the EMI-TTS component, enabling the generation of emotional speech that accurately captures and reflects the intended affective tone within the given context.</p> <p>The Voiceover Scenario module exemplifies the power of the MM-TTS framework in generating emotionally resonant voiceovers and narrations for multimedia applications. By effectively aligning and fusing emotional cues from multiple modalities, MM-TTS can produce voiceovers that not only convey the desired emotional expressions but also maintain consistency with the overall emotional context of the multimedia content, thereby enhancing the immersive experience for end-users.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#622emotional-text-to-speech-synthesis","title":"6.2.2.Emotional Text-to-Speech Synthesis.","text":"<p>The Emotional Text-toSpeech Synthesis demo, as shown in Figure 6, presents randomly selected emotional speech samples generated by EMI-TTS, along with comparative samples from other models [6,23]. This comparison highlights EMI-TTS\u2019s ability to provide a unified framework that integrates various TTS architectures, such as Tacotron2 [38], VITS[21], and FastSpeech2 [36]. By leveraging the EPAlign and EMI-TTS components, MM-TTS demonstrates enhanced emotional speech generation capabilities, outperforming traditional approaches.</p> <p>For this demo, we randomly selected 20 text samples with associated emotion labels, comprising two sentences from each of 10 different speakers. These text samples were then used to generate emotional speech using three variants of MM-TTS (corresponding to the integrated TTS architectures), as well as VITS (label), EmotionalTTS [23], and EmoSpeech [6] for a comprehensive evaluation.</p> <p>By presenting this diverse set of emotional speech samples, we aim to demonstrate the effectiveness of the proposed MM-TTS framework in capturing and conveying a wide range of emotional expressions across various TTS architectures. The comparison not only showcases the naturalness and expressiveness of the generated speech but also highlights the ability of MM-TTS to outperform traditional approaches in terms of emotional speech generation capabilities.</p> <p>Moreover, this demo underscores the versatility and modularity of the MM-TTS framework, as it seamlessly integrates multiple TTS architectures while leveraging the EPAlign and EMI-TTS components to enhance emotion representation and synthesis. This flexibility enables researchers and practitioners to leverage the strengths of different TTS architectures while benefiting from the improved emotional speech generation capabilities offered by the MM-TTS framework.</p>"},{"location":"TTS/Papers/2024.04.29_MM-TTS/2024.04_MM-TTS/#623zero-shot-emotional-speech","title":"6.2.3.Zero-Shot Emotional Speech.","text":"<p>The zero-shot emotional speech demo, as shown in Figure 7, showcases the zero-shot generalization capabilities of MM-TTS by generating emotional speech guided by complex, compound emotions. By aligning emotional prompts within the shared emotion space using EPAlign, EMI-TTS can synthesize emotional speech for emotion categories unseen during training. This ability opens up new avenues for creative expression and personalization in emotional speech synthesis, enabling users to craft tailored emotional expressions beyond those encountered in the training data.</p> <p>Through this interactive demo, we aim to provide researchers and practitioners with a comprehensive understanding of MM-TTS\u2019s potential applications, showcasing its ability to generate contextually appropriate, high-quality emotional speech across various scenarios. By addressing the challenges of multimodal emotion disentanglement and alignment, MM-TTS represents a significant step forward in emotional speech synthesis, with far-reaching implications for human-computer interaction, multimedia content creation, and beyond.</p>"},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/","title":"CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models","text":"<p>Xiang Li, Fan Bu, Ambuj Mehrish, Yingting Li, Jiale Han, Bo Cheng, Soujanya Poria</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#abstract","title":"Abstract","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#1introduction","title":"1.Introduction","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#2related-work","title":"2.Related Work","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#3background-consistency-models","title":"3.Background: Consistency Models","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#4cm-tts","title":"4.CM-TTS","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#5experiments","title":"5.Experiments","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#6results-and-discussion","title":"6.Results and Discussion","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/","title":"HyperTTS: Parameter Efficient Adaptation in Text-to-Speech Using Hypernetworks","text":"<p>Yingting Li, Rishabh Bhardwaj, Ambuj Mehrish, Bo Cheng, Soujanya Poria</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#abstract","title":"Abstract","text":"<p>Neural speech synthesis, or text-to-speech (TTS), aims to transform a signal from the text domain to the speech domain. While developing TTS architectures that train and test on the same set of speakers has seen significant improvements, out-of-domain speaker performance still faces enormous limitations. Domain adaptation on a new set of speakers can be achieved by fine-tuning the whole model for each new domain, thus making it parameter-inefficient. This problem can be solved by Adapters that provide a parameter-efficient alternative to domain adaptation. Although famous in NLP, speech synthesis has not seen much improvement from Adapters. In this work, we present HyperTTS, which comprises a small learnable network, \"hypernetwork\", that generates parameters of the Adapter blocks, allowing us to condition Adapters on speaker representations and making them dynamic. Extensive evaluations of two domain adaptation settings demonstrate its effectiveness in achieving state-of-the-art performance in the parameter-efficient regime. We also compare different variants of HyperTTS, comparing them with baselines in different studies.Promising results on the dynamic adaptation of adapter parameters using hypernetworks open up new avenues for domain-generic multi-speaker TTS systems. The audio samples and code are available at https://github.com/declare-lab/HyperTTS.</p> <p>\u795e\u7ecf\u8bed\u97f3\u5408\u6210\u6216\u6587\u672c\u8f6c\u8bed\u97f3\u7684\u76ee\u7684\u662f\u5c06\u6587\u672c\u57df\u7684\u4fe1\u53f7\u8f6c\u6362\u4e3a\u8bed\u97f3\u57df. \u867d\u7136\u5f00\u53d1\u5728\u76f8\u540c\u7684\u8bf4\u8bdd\u4eba\u96c6\u5408\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7684\u8bed\u97f3\u5408\u6210\u67b6\u6784\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb, \u4f46\u5bf9\u4e8e\u57df\u5916\u8bf4\u8bdd\u4eba\u7684\u6027\u80fd\u4ecd\u7136\u9762\u4e34\u7740\u5de8\u5927\u7684\u9650\u5236. \u8fd9\u4e00\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7 Adapter, \u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u9886\u57df\u81ea\u9002\u5e94\u6765\u89e3\u51b3. \u5c3d\u7ba1\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5f88\u6709\u540d, \u4f46\u8bed\u97f3\u5408\u6210\u8fd8\u6ca1\u6709\u4ece Adapter \u4e2d\u83b7\u5f97\u592a\u591a\u7684\u6539\u8fdb. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86 HyperTTS, \u5b83\u7531\u4e00\u4e2a\u5c0f\u578b\u53ef\u5b66\u4e60\u7684\u7f51\u7edc, \"\u8d85\u7f51\u7edc\", \u751f\u6210 Adapter \u5757\u7684\u53c2\u6570, \u5141\u8bb8\u6211\u4eec\u6839\u636e\u8bf4\u8bdd\u4eba\u8868\u793a\u6765\u6761\u4ef6\u5316\u9002\u914d\u5668, \u5e76\u4f7f\u5176\u52a8\u6001. \u5728\u4e24\u4e2a\u9886\u57df\u81ea\u9002\u5e94\u8bbe\u7f6e\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d, \u6211\u4eec\u8bc1\u660e\u4e86 HyperTTS \u5728\u53c2\u6570\u9ad8\u6548\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd. \u6211\u4eec\u8fd8\u6bd4\u8f83\u4e86 HyperTTS \u7684\u4e0d\u540c\u53d8\u4f53, \u4e0e\u4e0d\u540c\u7814\u7a76\u4e2d\u7684\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83. \u901a\u8fc7\u4f7f\u7528\u8d85\u7f51\u7edc\u6765\u52a8\u6001\u9002\u914d\u5668\u53c2\u6570\u7684\u53ef\u884c\u6027, \u6211\u4eec\u5f00\u8f9f\u4e86\u65b0\u7684\u591a\u8bf4\u8bdd\u4eba TTS \u7cfb\u7edf\u7684\u9886\u57df\u901a\u7528\u9053\u8def.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#1introduction","title":"1.Introduction","text":"<p>Neural text-to-speech (TTS) synthesis has trans formed our interactions with digital content by converting text into natural-sounding speech.Cur rent TTS systems are often limited to predefined speaker styles or specific sets of speaker IDs (Ren et al., 2019a), reducing their utility in multi-speaker environments with unseen speakers. To make TTS scalable and economical, parameter-efficient adaptation of such systems to new speakers is an important, but highly challenging problem (Li et al., 2023b). Zero-shot and few-shot speaker adaptation techniques (Shen et al., 2023; Li et al., 2023a; Casanova et al., 2021; Cooper et al., 2020; Casanova et al., 2022; Shen et al., 2023) have gained prominence in the domain of TTS, aiming at accommodating new speakers and styles with limited speaker-specific data. While these methods excel in scenarios with constrained data, it\u2019s important to note that when sufficient data is available, fine-tuning the model offers distinct advantages. Fine-tuning allows for highly personalized and tailored speech synthesis, precise control over the alignment of synthesized speech with the speaker\u2019s characteristics, and the production of higher-quality, more natural-sounding speech. In this paper, we assume sufficient availability of data from the adaptation domain.When adapting a multi-speaker TTS model (backbone model) to a target domain, the traditional approach involves complete fine-tuning of the entire back bone (Figure 1-Fine-tuning).However, this approach is resource-intensive, requiring separate copies of model parameters for each new target domain. To make the adaptation scalable, recent research has introduced parameter-efficient do main adaptation methods using Adapters, as seen in NLP (Houlsby et al., 2019) and speech (Li et al., 2023b). Adapters incorporate small blocks of learn able dense layers into each block of the backbone model, with the aim of learning additional parameters while keeping the main model parameters fixed (Figure 1-AdapterTTS). Despite the advantages demonstrated by adapters in various NLP tasks, their direct application in adapting a TTS backbone to a target domain has shown limited improvements (Li et al., 2023b) Since learning a generic TTS system that works well across different speaker styles is a more difficult problem than learning one network per speaker (Ren et al., 2019a, 2021), we hypothesize the same is the case with adapters. Forcing a static set of adapter parameters to perform well across multiple speakers of the adaptation domain can be challenging and potentially infeasible due to under parameterization (Mehrish et al., 2023a; Biadsy et al., 2022). In this paper, we present HyperTTS, a pioneer ing approach for the parameter-efficient adaptation of TTS models to new speakers.This method conditions adapters on speaker embeddings, expanding the learnable parameter space through a \"hypernetwork\". The main highlights of HyperTTS are:. 1. Dynamic Adapters: Instead of keeping the adapters static, for each speaker in the adaptation domain, HyperTTS learns speaker adaptive adapters.Adapter conditioning on speaker representations is observed to unlock adapter capabilities and make them performant which was a challenge with static adapters (Li et al., 2023b). 2. Parameter Sampling: A large set of speak ers makes it infeasible to keep the space of adapter parameters discrete. To facilitate this, we employ parameter sampling from a continuous distribution defined by a learnable hyper network. 3. ParameterEfficiency: Compared to parameter-expensive fine-tuning, it achieves competitive results with less than1% of the backbone parameters, making it highly practical and resource-friendly for scalable applications.</p> <p>We perform a comprehensive set of experiments to showcase HyperTTS\u2019s effectiveness (see Figure 1) compared to traditional methods like static bottleneck adapters (AdapterTTS) and full model fine-tuning (TTS-FT). Our experiments cover datasets from diverse environmental conditions, such as LibriTTS and VCTK, representing various accents from different regions. Results highlight HyperTTS\u2019s parameter-efficient performance advantages over the baselines across both objective and subjective metrics.Notably, HyperTTS can even surpass fine-tuning in performance with only a 20% increase in parameters (Table 6-HyperTTS<sub>e/v/d</sub>). A key strength of HyperTTS lies in its remarkable parameter efficiency: it achieves results within 1 point of fine-tuning while using less than 1% of the parameter count in the backbone. This practical and resource-friendly approach enables real-world applications.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#2related-work","title":"2.Related Work","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#text-to-speech-models","title":"Text-to-Speech Models","text":"<p>The rise of deep learning has transformed TTS technology, with neural network-based architectures like Tacotron (2017); Tacotron2 (2017), FastSpeech2 (2020), and Transformer-TTS (2018) leading the way. These models represent significant progress in TTS, leveraging deep learning techniques.</p> <p>Autoregressive TTS models (Tacotron (2017); Flowtron (2020); FastSpeech; FastSpeech2 (2020); Glow-TTS (2020); FastPitch (2020)), while effective, face limitations in maintaining alignment in long utterances and exhibit slower training and inference speeds with longer sequences.</p> <p>In contrast, non-autoregressive (parallel) models separate phoneme duration estimation from decoding, reducing latency and enhancing training efficiency. These models typically rely on external aligners or pre-trained autoregressive models for phoneme duration. To achieve training efficiency and support end-to-end TTS, this paper focuses on a non-autoregressive TTS model with an alignment framework based on the RAD-TTS (2022) alignment learning objective.</p> <p>Recently, several speech models have been compared to GPT in natural language processing, with a focus on in-context learning for speech. Notably, VALL-E (2023) and SPEAR-TTS (2023) leverage emerging codecs to learn discrete speech tokens and employ a vocoder-like decodec to convert these tokens into waveforms. Meanwhile, Voicebox (2023), inspired by flow-matching and aligned with the Fastspeech (2019) framework, utilizes continuous features like Mel spectrogram and HiFi-GAN (2020).</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#speaker-adaptation-in-tts","title":"Speaker Adaptation in TTS","text":"<p>Speaker adaptation is a crucial aspect of TTS systems, aiming to personalize the synthesized speech by modify ing the voice characteristics to match those of a specific target speaker. Over the years, various techniques and approaches have been proposed to address the challenges associated with speaker adaptation in TTS (Jia et al., 2018; Chen et al.; Min et al., 2021; Hsieh et al., 2022; Gabry\u00b4s et al.2022). Furthermore, several studies have focused on exploring parameter-efficient methods for adapt ing TTS to new sets of speakers, addressing the need for effective adaptation in diverse speaker scenarios. These approaches aim to accommodate a wide range of linguistic variations (Pamisetty et al., 2023; Do et al., 2022), including diverse ac cents (Yang et al., 2023), speakers (Luo et al., 2021; Miao et al., 2021; Mehrish et al., 2023a), and low-resource scenarios introduced by the tar get domain (Azizah and Jatmiko, 2022; Mehrish et al., 2023a; Lux and Vu, 2022), while maintain ing the number of trainable parameters. HYPER TTS primarily focuses on contributing in the line of parameter-efficient domain adaptation of the back bone TTS model to a target set of speakers.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#dynamic-parameters","title":"Dynamic Parameters","text":"<p>Parameter generation, although not popular in speech, has been used in various forms in other domains, such as Klein et al. (2015); Riegler et al. (2015) in NLP and Ha et al. (2017) in computer vision. Specific to adapters, Bhardwaj et al. (2022); Chen et al. (2020) make prompt tokens dynamic by conditioning their val ues on input text using a parameter prompt generator network, (\u00dcst\u00fcn et al., 2022; Mahabadi et al., 2021) used hypernetworks for generating adapter down and up-projection weights. Shared hypernetworks obviate the need to maintain a separate set of parameters for each task (or new setting) and generate weights for each block of the backbone network (Mahabadi et al., 2021). To the best of our knowledge, this is the first work that studies the utility of a parameter generator in the domain of speech (Mehrish et al., 2023b).</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#3methodology","title":"3.Methodology","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#31encoder","title":"3.1.Encoder","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#32variance-adaptor","title":"3.2.Variance Adaptor","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#duration-predictor","title":"Duration Predictor","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#pitch-predictor","title":"Pitch Predictor","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#energy-predictor","title":"Energy Predictor","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#33mel-decoder-and-postnet","title":"3.3.Mel-Decoder and Postnet","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#34hypernetwork","title":"3.4.Hypernetwork","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#implementation","title":"Implementation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#4experiments","title":"4.Experiments","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#41baseline-models","title":"4.1.Baseline Models","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#tts-0","title":"TTS-0","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#reference-and-reference-voc","title":"Reference and Reference (Voc.)","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#tts-ft-full-fine-tuning","title":"TTS-FT (Full Fine-Tuning)","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#adaptertts","title":"AdapterTTS","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#hypertts","title":"HyperTTS","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#42datasets","title":"4.2.Datasets","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#43model-configuration","title":"4.3.Model Configuration","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#backbone-model-pre-training","title":"Backbone Model Pre-training","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#44evaluation-metrics","title":"4.4.Evaluation Metrics","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#objective-metrics","title":"Objective Metrics","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#subjective-metrics","title":"Subjective Metrics","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#5results-discussions","title":"5.Results &amp; Discussions","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#51subjective-evaluation","title":"5.1.Subjective Evaluation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#52impact-of-parameter-efficiency","title":"5.2.Impact of Parameter Efficiency","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#53output-of-hypernetwork","title":"5.3.Output of Hypernetwork","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#54other-discussions","title":"5.4.Other Discussions","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#layernorms-standard-conditional","title":"Layernorms (Standard &amp; Conditional)","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#low-rank-adaptation","title":"Low-Rank Adaptation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#6conclusion","title":"6.Conclusion","text":"<p>In this paper, we present HyperTTS, an approach that enhances the effectiveness of adapters by conditioning them on speaker embeddings. Utilizing a \"hypernetwork\" to customize adapter block weights for the TTS backbone network, we significantly expand the adapter parameter space. This dynamic method replaces the conventional static adapter parameter set, enabling input-conditioned parameter sampling. Additionally, the hypernetwork\u2019s continuous parameter space theoretically allows the generation of adapter parameters for numerous speakers without increasing hypernetwork parameters. This makes HyperTTS an excellent choice for multi-speaker TTS adaptation, surpassing traditional adapter limitations.</p> <p>Limitations  While hypernetworks exhibit promising enhancements in both adaptation domains, there are training challenges to address. Time and resource constraints may have led to potential underfitting, negatively impacting performance. Additionally, hypernetworks tend to overfit the backbone model on the adaptation domain, warranting further research to enhance their generalizability. Notably, the relatively higher number of parameters in hypernetworks poses potential inefficiency for low-resource training.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/","title":"LLaMA-VITS: Enhancing TTS Synthesis with Semantic Awareness","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#abstract","title":"Abstract","text":"<p>Recent advancements in Natural Language Processing (NLP) have seen Large-scale Language Models (LLMs) excel at producing high-quality text for various purposes. Notably, in Text-To-Speech (TTS) systems, the integration of BERT for semantic token generation has underscored the importance of semantic content in producing coherent speech outputs. Despite this, the specific utility of LLMs in enhancing TTS synthesis remains considerably limited. This research introduces an innovative approach, ==LLaMA-VITS==, which enhances TTS synthesis by enriching the semantic content of text using LLM. ==LLaMA-VITS== integrates semantic embeddings from LLaMA2 (2023) with the VITS model, a leading end-to-end TTS framework. By leveraging LLaMA2 (2023) for the primary speech synthesis process, our experiments demonstrate that ==LLaMA-VITS== matches the naturalness of the original VITS (ORI-VITS) and those incorporate BERT (BERT-VITS), on the LJSpeech dataset, a substantial collection of neutral, clear speech. Moreover, our method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem dataset, a curated selection of emotionally consistent speech from the EmoV_DB dataset, highlighting its potential to generate emotive speech.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#1introduction","title":"1.Introduction","text":"<p>Text-to-Speech (TTS) synthesis is a technology that transforms written text into its spoken equivalent, thereby enhancing content accessibility. This technology finds application in the production of audiobooks (Chen et al., 2022) and virtual assistants (Wu et al., 2023). However, traditional TTS models, which primarily focus on the acoustic features, often fall short in comprehending the semantic and emotional information embedded within the text. With the significant advancements in Natural Language Processing (NLP) technologies, particularly through Language Models (LMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018; Brown et al., 2020), which have demonstrated formidable capabilities in understanding and generating natural language, researchers have proposed various BERT-based TTS models (Mukherjee et al., 2022; Abbas et al., 2022; Li et al., 2023; Guo et al., 2022) to improve the expressiveness of synthesized speech. Nonetheless, the effectiveness and flexibility of BERT-based TTS models in diverse applications are limited due to the smaller parameter size of BERT models and the necessity for designing specific fine-tuning tasks to enhance their capabilities. On the other hand, Large-scale Language Models (LLMs), such as LLaMA2 (2023), not only require decreasing computational re sources and achieve higher levels of text generation but also possess excellent zero-shot learning capabilities. Moreover, they can achieve improvements comparable to fine-tuning by adjusting only a minimal number of parameters through prompt tuning (Liu et al., 2022; Tu et al., 2022). However, the potential of these LLMs for TTS tasks has not been fully explored. In light of this context, we introduce ==LLaMA-VITS==, a model that leverages semantic representations extracted from LLaMA2 (2023) on top of a state-of-the-art TTS model, VITS (2021), enabling the generated speech to retain acoustic information while understanding and expressing semantics and emotions. Through comprehensive objective and subjective evaluations, ==LLaMA-VITS== has been verified to surpass TTS baselines without semantic input or those integrated with BERT. The main contributions encapsulate:  - We propose LLaMA-VITS model that utilizes the semantic understanding and expression capabilities of LLaMA2 (2023), offering equal or superior acoustic performance compared to baseline models, along with a significantly enhanced ability to understand and express semantics and emotions. - Through empirical analysis, we demonstrate that global tokens in ==LLaMA-VITS== provide more significant improvements than sequential to kens, contrasting with observations in BERT-based TTS models. - We quantitatively verified our findings using both subjective and objective metrics. Our code, models, audio demos, and the filtered single female speaker emotional dataset EmoV_DB_bea_sem are available at https://github.com/xincanfeng/vitsgpt.git.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#2related-works","title":"2.Related Works","text":"<p>TTS technology has significantly advanced in learning acoustic features through structural evolution. However, comprehending and conveying semantics remain challenging. Since BERT-like LMs have demonstrated profound capabilities in understanding semantics through extensive pre-training on vast text corpora, some studies have integrated BERT-like LMs with TTS technology to enhance synthesized speech. Nonetheless, research on incorporating GPT-like LMs within TTS technology is notably scarce.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#21text-to-speech-models","title":"2.1.Text-to-Speech Models","text":"<p>TTS task aims to generate natural, fluent, and easily comprehensible speech. Traditional TTS systems, e.g., a Statistical Parametric Speech Synthesis (SPSS) system (Taylor, 2009), usually comprise multiple distinct components. These include a frontend module that converts text into linguistic features (such as duration and pitch), an acoustic model that maps these linguistic features to acoustic features, and a vocoder responsible for generating speech waveforms from the acoustic features. Over the past decades, the complexity of traditional models has been notable, attributed to their reliance on manually engineered features and the intricate communication between modules.</p> <p>Transitioning from Hidden Markov Models (HMM) based models (Black et al., 2007), through Deep Neural Networks (DNN) models (Zen et al., 2013), to Generative Adversarial Networks (GAN) based models (Saito et al., 2017), there has been a no table enhancement in voice quality, yet the architectural complexity remains significant.</p> <p>The advent of end-to-end TTS models marks a significant milestone, increasingly reducing the distinction between synthesized speech and human voice. End-to-end models are capable of trans forming raw text directly into final speech output, which not only streamlines the structural complexity of TTS systems and facilitates easier deployment but also significantly reduces the dependency on manual feature engineering, simplifying the training process. Moreover, they notably enhance the naturalness and intelligibility of the speech, thereby be coming the predominant architecture in TTS models. For instance, Char2Wav (2017) introduces an attentive encoder-decoder frame work for direct speech synthesis from text input. Tacotron (2017) undertakes training from the ground up and directly predicts linear spectrograms. Furthermore, the speech produced by Tacotron2 (2017) closely mirrors the natural human voice.</p> <p>In the realm of end-to-end TTS models, many have adopted a non-autoregressive architecture. This architecture enables parallel data processing, where the model\u2019s output generation does not depend on the output of the previous time step, thereby enhancing processing speed. It also circumvents the error accumulation issue inherent in traditional autoregressive models, which significantly boosts TTS performance. FastSpeech (2019) and its variants exemplify this trend. FastSpeech (2019) employs a transformer-based architecture to generate mel-spectrograms in parallel. Building on FastSpeech (2019), FastPitch (2020) predicts pitch contours during inference, enabling the production of more expressive and high quality speech. FastSpeech2 (2020) further incorporates explicit duration prediction and introduces pitch and energy as conditional inputs.</p> <p>Previous non-autoregressive approaches typically involve distinct training phases for acoustic models and vocoders. VITS (2021) introduces a more natural-sounding output compared to these two-stage systems through its one-stage parallel end-to-end architecture. Innovatively, VITS (2021) incorporates variational inference combined with normalizing flows and employs an adversarial training methodology. Due to VITS (2021)\u2019s exemplary performance across multiple benchmarks, we select it as the foundational TTS model for our system.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#22-fine-tuning-bert-like-lms-for-tts","title":"2.2. Fine-tuning BERT-like LMs for TTS","text":"<p>While TTS models have increasingly advanced in replicating acoustic features, insufficient training data can hinder the model\u2019s ability to learn the semantic nuances of the same input across different contexts, thus limiting its expressiveness. Consequently, researchers have turned to leveraging the transfer learning capabilities of BERT-like LMs. Ultimately, TTS systems that incorporate pre-trained and fine-tuned BERT-like LMs have achieved better understandings of semantics and enhanced generated speech, marking a significant advancement.</p> <p>Hayashi et al. (2019) utilized a pre-trained BERT model as an auxiliary input to enhance a Tacotron2 based TTS system, resulting in improved speech naturalness. Similarly, Yang et al. (2019) applied a pre-trained BERT model to achieve enhanced front end accuracy. Kenter et al. (2020) demonstrated that integrating a BERT model, pre-trained on extensive unlabeled data and fine-tuned for speech, into an RNN-based TTS system enhances prosody. Kenter et al. (2020) specifically suggest updating the BERT\u2019s parameters during the training of their RNN-based speech synthesis model, emphasizing the critical role of fine-tuning the BERT component for optimal outcomes. As prompt tuning draws wide attention in guiding text or image generation, PromptTTS (2022) takes a prompt representation with both style and content descriptions from a BERT model as input to generate speech with precise style control and high speech quality.</p> <p>In particular, Mukherjee et al. (2022) utilized a pre-trained BERT model to develop a text emotion classification model, employing the final hidden states of the initial <code>[CLS]</code> token as a comprehensive representation of the text. Researchers such as Kenter et al. (2020); Li et al. (2021); Abbas et al. (2022) have applied word-level BERT to capture the semantic and syntactic structure of sentences, thereby aiding TTS synthesis. Li et al. (2023) introduced a phoneme-level BERT, designed with a preliminary task of predicting corresponding graphemes in addition to regular masked phoneme predictions, to enhance the naturalness of speech synthesized from out-of-distribution (OOD) texts.</p> <p>However, despite BERT\u2019s acknowledged capacity to provide detailed word importance, syn tactic and semantic insights, and general knowledge (Hayashi et al., 2019; Kenter et al., 2020), its effectiveness is constrained by the particularities of fine-tuning approaches. Furthermore, BERT\u2019s inherent non-generative nature might limit its ability to account for information outside the immediate sentence context.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#23integrating-gpt-like-lms-for-tts","title":"2.3.Integrating GPT-like LMs for TTS","text":"<p>Considering semantic understanding and expression capabilities, BERT is primarily utilized for com prehension tasks. In comparison, GPT excels not only in understanding text but also in generating natural and coherent text. Moreover, with the larger model parameters, GPT is particularly adept at zero-shot or few-shot learning, enabling its direct application to various tasks with little to no need for fine-tuning or structural modifications.</p> <p>However, research on leveraging GPT-like models to aid TTS systems is very limited. Stephenson et al. (2021) explores the potential of improving speech synthesis naturalness by text input lookahead with GPT prediction. Such an approach potentially restricts TTS applications, as altering the input is often undesirable. Furthermore, the findings were not verified by human subjective evaluation. Saito et al. (2023) suggest employing ChatGPT to aid in empathetic dialogue speech synthesis by extracting the context of conversations. They particularly instruct ChatGPT to produce three key words that encapsulate the intention, emotion, and speaking Style of speech observed in the dialogue history. These keywords are subsequently utilized to train a speech synthesis model. However, due to the inaccessibility of ChatGPT to the public, the re searchers resort to processing ChatGPT\u2019s outputs with BERT to extract embeddings. This approach essentially positions ChatGPT as an alternative to manual annotation, yet it does not delve into investigating ChatGPT\u2019s internal representations and their potential impact on speech-related tasks.</p> <p>In our study, we selected LLaMA2 (2023), a GPT-like LM, for integration into our TTS system, motivated by its technological advancements and potential for di verse applications. LLaMA2 (2023) stands out as one of the largest publicly accessible LMs, rivaling proprietary models such as GPT3.5 (OpenAI et al., 2024) and PaLM (540B) (Chowdhery et al., 2022), and sur passes other open-source alternatives like MPT and Falcon (Almazrouei et al., 2023) in benchmark evaluations. Additionally, the novel architecture of LLaMA2 (2023) not only ensures enhanced security but also facilitates the extension of various down stream tasks (Touvron et al., 2023).</p> <p>Related research that employs LLaMA2 (2023) in speech and other multimodal tasks (Radhakrishnan et al., 2023; Zhang et al., 2023), coupled with the ongoing efforts to reduce computing costs associated with LLaMA2, underscores the model\u2019s significant research interest and its promising prospects in multimodal applications.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#3methodology","title":"3.Methodology","text":"<p>We propose leveraging semantic embeddings de rived from a GPT-like LM to improve TTS synthesis. In our work, LLaMA2 (2023) is employed as the GPT-like model, as elaborated in Sec.2.3, and VITS (2021) is utilized as the TTS model for generating audio from phoneme embeddings, as detailed in Sec.2.1. In essence, we extract semantic embeddings $E_{s}$ from the final hidden layer of LLaMA2 (2023) and integrate them with the original acoustic text embeddings $E_{a}$ of VITS (2021), forming enhanced text embeddings $E_{as}$ for speech synthesis. Specifically, either a global token or a sequence of tokens is used to encapsulate the semantic attributes of an input sentence for varying objectives. The distinctions between these two token types are further explicated in Sec.3.1.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#31semantic-embeddings-derived-from-llama2","title":"3.1.Semantic Embeddings Derived from LLaMA2","text":"<p>For each input sentence $s$, we extract information from the final hidden layer before the output of LLaMA2 (2023). Different strategies are employed to cre ate various tokens that serve as the semantic em bedding for the sentence.</p> <p>Let $E_{s}$ denote the semantic embedding of sentence $s$, and $H_{LLaMA}^F(s)$ represent the output of the LLaMA2 (2023) model for sentence $s$ at the final hidden layer $F$. Therefore, $E_{s}$ can be expressed as: </p> <p>$$     E_{s}=  H_{LLaMA}^F(s)\\tag{1} $$ </p> <p>Here, $H_{LLaMA}^F(s)$ is a vector that encapsulates the semantic representation of sentence $s$ after pro cessing through all layers of the LLaMA2 (2023), culminating in the final layer.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#formulation-for-global-tokens","title":"Formulation for Global Tokens","text":"<p>We explored five types of global tokens to represent the over arching semantic features of an input sentence, namely <code>[AVE]</code>, <code>[PCA]</code>, <code>[LAST]</code>, <code>[EIS_Word]</code>, and <code>[EIS_Sentence]</code>, with each strategy employing a single token.</p> <p>In the <code>[AVE]</code> strategy, the semantic token is de rived by calculating the average of all tokens\u2019 out put vectors for sentence $s$, formulated as: </p> <p>$$     E_{s}^{AVE}= \\dfrac{1}{n}\\sum_{i=1}^n H_{LLaMA}^F(s,i)\\tag{2}  $$</p> <p>Here, $E_{s}^{AVE}$ denotes the semantic token obtained using the <code>[AVE]</code> strategy, and $H_{LLaMA}^F(s,i)$ represents the output of the $i$ th token of sentence $s$ at the final hidden layer $F$ of LLaMA2, with $s$ comprising $n$ tokens.</p> <p>For the <code>[PCA]</code> strategy, we apply Principal Component Analysis to the output vectors of sentence sto extract principal components and rescale the mean of the PCA results according to the original data\u2019s value range. This rescaling ensures that the PCA-processed data maintains a scale consistent with the original data, preserving the relative importance of semantic information numerically. Formulated as: </p> <p>$$     E_{s}^{PCA}= \\text{PCArescale}(H_{LLaMA}^F(s)) \\tag{3} $$</p> <p>In the <code>[LAST]</code> strategy, the semantic token is obtained by selecting the last token from the output vector of sentence s, as shown in the formula: </p> <p>$$     E_{s}^{LAST}= H_{LLaMA}^F(s, n)\\tag{4}  $$</p> <p>where $H_{LLaMA}^F(s, n)$ refers to the representation of the last token of sentence $s$ after processing through all layers of LLaMA2 (2023) at the final layer.</p> <p>In the <code>[EIS_Word]</code> and <code>[EIS_Sentence]</code> strategies, unlike the above approaches that utilize the sentence itself for representation, we derive the semantic representation of sentence $s$ based on LLaMA2 (2023)\u2019s comprehension $u$. Adapted from Saito et al. (2023)\u2019s practice, we employ prompts as illustrated in 2a and 2b, respectively, to obtain LLaMA2 (2023)\u2019s understanding of sentence $s$ in terms of Emotion, Intention, and speaking Style, denoted as $u$, and calculate the average of this understanding\u2019s representation to serve as the semantic embedding.</p> <p>In the <code>[EIS_Word]</code> strategy, LLaMA2 (2023) is prompted to describe Emotion, Intention, and speaking Style with three separate words, resulting in the following formula for the final semantic token: </p> <p>$$     E_{s}^{\\text{EISWord}} = \\dfrac{1}{m} [\\sum_{i} H_{LLaMA}^{F}(u_E, i) + \\sum_j H_{LLaMA}^{F}(u_I, j) +\\sum_k H_{LLaMA}^{F}(u_S, k)] \\tag{5}  $$</p> <p>where $u_E$, $u_I$, $u_S$ are the representations of LLaMA2 (2023)\u2019s output expressing the sentence\u2019s Emotion, Intention, and speaking Style at the final hid den layer, respectively, with $i$, $j$, $k$ indicating the tokens of each output word, and m being the total number of these tokens.</p> <p>In the <code>[EIS_Sentence]</code> strategy, LLaMA2 (2023) is guided to describe its understanding of the input sentence\u2019s Emotion, Intention, and speaking Style with an easy-to-understand sentence, leading to the fol lowing formula for the final semantic token: </p> <p>$$     E_s^{\\text{EISSentence}} = \\dfrac{1}{m}\\sum_{i=1}^m H_{LLaMA}^{F}(u_{EIS}, i)\\tag{6}  $$</p> <p>where $u_{EIS}$ is the representation of LLaMA2 (2023)\u2019s output expressing the understanding of the original sentence at the final hidden layer, and $m$ is the total number of tokens in this sentence representation.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#formulation-for-sequential-tokens","title":"Formulation for Sequential Tokens","text":"<p>In the implementation of sequential tokens strategies, we focus on utilizing the complete set of tokens from the input sentence to represent its semantic in formation. Unlike the global token approaches, sequential tokens strategies encompass representations based on either text or phonemes, aiming to better align with the TTS model\u2019s potential em phasis on acoustic features. The mathematical representations for these two strategies are as follows:  Under the <code>[TEX]</code> strategy, we directly employ all tokens from the textual form of sentence $s$ to represent its semantic information. If the output of sentence $s$ at the final hidden layer $F$ of LLaMA2 (2023) consists of $n$ tokens, then the semantic token $T_{s}^{TEX}$ is represented as a sequence: </p> <p>$$     E_s^{TEX}= {H_{LLaMA}^F(s,1), H_{LLaMA}^F(s, 2),\\cdots, H_{LLaMA}^F(s, n)} \\tag{7}  $$</p> <p>In the <code>[PHO]</code> strategy, we consider the complete set of tokens from the phonemic form. Here, $s_{pho}$ denotes the phonemic representation of sentence $s$. If the output of $s_{pho}$ at the final hidden layerF of LLaMA2 (2023) comprises $m$ tokens, then the semantic token $T_{s}^{PHO}$ is represented as a sequence: </p> <p>$$     E_{s}^{PHO}={H_{LLaMA}^F(s_{pho}, 1),H_{LLaMA}^F(s_{pho}, 2),\\cdots,H_{LLaMA}^F(s_{pho}, m)}\\tag{8}  $$</p> <p>In both strategies, $H_{LLaMA}^F(s, i)$ and $H_{LLaMA}^F(s_{pho}, i)$ respectively represent the outputs of the $i$ th token of sentence $s$ in its textual and phonemic forms at the final hidden layer $F$ of LLaMA2 (2023). This representation allows the TTS model to leverage the complete semantic information of a sentence, whether based on text or phonemes.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#32fusing-semantic-embeddings-with-acoustic-embeddings","title":"3.2.Fusing Semantic Embeddings with Acoustic Embeddings","text":"<p>To align the dimensions of semantic embedding extracted from LLaMA2 (2023), denoted as $E_{s}$, with the acoustic embeddings from VITS (2021), denoted as $E_{a}$, we employ a linear projection. The original dimension of $E_{s}$, $d_{LLaMA}$, is projected to match the dimension of VITS (2021) acoustic embedding, $d_{VITS}$, using a linear transformation matrix $W$ of dimensions $d_{VITS}\\times d_{LLaMA}$. The projected semantic embedding, $E_s'$, is calculated as follows: </p> <p>$$     E_s'= W \\cdot E_{s} \\tag{9}  $$ </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#fusing-global-embedding-with-acoustic-embedding","title":"Fusing Global Embedding with Acoustic Embedding","text":"<p>To obtain an embedding $E_{as}$ that integrates both semantic and acoustic information, for global tokens, we simply add the dimensionally unified global embedding to VITS (2021)\u2019s acoustic em bedding, as shown in the equation: </p> <p>$$     E_{as} = E_{a} + E_s\u2032\\tag{10} $$ </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#fusing-sequential-embeddings-to-enhance-text-embeddings","title":"Fusing Sequential Embeddings to Enhance Text Embeddings","text":"<p>We utilize the Scaled Dot Product Attention mechanism to merge sequential embeddings with VITS (2021)\u2019s original acoustic embedding to gain enhanced embedding $E_{as}$ , which can be described by the following mathematical formulas:  First, calculate the attention scores $A$:</p> <p>$$     A = \\dfrac{q\\cdot k^{\\mathsf{T}}}{\\gamma} \\tag{11} $$</p> <p>where $q$ is the acoustic embedding $E_{a}$ in VITS (2021) with dimensions $[b, t, d]$;  $k$ and $v$ denotes the semantic embedding $E_s'$ from LLaMA2 (2023), also with dimensions $[b, t, d]$;  $b$ is the batch size,tis the sequence length, and $d$ is the embedding dimension; $\\gamma$ is temperature for scaling. $k^{\\mathsf{T}}$ denotes the transpose of $k$, transforming $k$ from $[b, t, d]$ to $[b, d, t]$ for matrix multiplication. The resulting $A$ has dimensions $[b, t, t]$.</p> <p>If a source mask or target mask is present, a masking operation is applied, setting the attention scores at masked positions to a very low value (e.g.,\u22126e4) to nearly eliminate their weight contribution in the subsequent softmax step.</p> <p>Next, apply the softmax function and dropout to the attention scores, obtaining the final attention weights $W_{attn}$: </p> <p>$$     W_{attn}= \\text{Dropout}(\\text{Softmax}(A))\\tag{12}  $$</p> <p>Finally, the output $E_{as}$ is calculated by weighting $v$ with the attention weights: </p> <p>$$     E_{as} = W_{attn}\\cdot v $$ </p> <p>The output $E_{as}$ , viewed as text embedding fused with semantic information, has dimensions $[b, t, d]$ that match those of $q$.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#4experiments","title":"4.Experiments","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#41experimental-settings","title":"4.1.Experimental Settings","text":"<p>We propose ==LLaMA-VITS== which uses semantic to kens derived from LLaMA2 (2023) to enhance acoustic embedding in VITS (2021) for better TTS performance. To show the effectiveness of our method, we experimented with two baseline models. In the ORI-VITS baseline, we use the original VITS (2021) without external semantic information. In the BERT-VITS baseline, we extract various semantic tokens according to for mer research introduced in Section \u00a7 2.2. Specifically, we use the <code>[CLS]</code> token of BERT as the global token. To form the baseline of the sequential token in BERT, we use all the tokens in the sentence trained by text or phoneme, named <code>[BERT_TEX]</code> and <code>[BERT_PHO]</code>, respectively. In our proposed ==LLaMA-VITS==, we derive global token <code>[AVE]</code>, <code>[LAST]</code>, <code>[PCA]</code>, <code>[EIS_Word]</code>, and <code>[EIS_Sentence]</code>, and sequential tokens <code>[TEX]</code> and <code>[PHO]</code> from LLaMA2 (2023), cor responding to those in BERT-VITS. We use LLaMA2 (2023) (13b) to generate semantic embeddings of dimension 5120. <code>[CLS]</code> and <code>[BERT_TEX]</code> tokens are extracted from BERT-base-uncased model which has a parameter size of 110M that generates token embedding of 768 dimensions. <code>[BERT_PHO]</code> token is extracted from BERT-x-phone-base model whose parameter size is 88M to generate token embedding of 768 dimensions.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#global-token-extraction","title":"Global Token Extraction","text":"<p>In our proposed ==LLaMA-VITS==, global strategy <code>[LAST]</code> only uses the last to ken in the final hidden layer of LLaMA2 (2023) for each sentence. <code>[AVE]</code> uses the average of all tokens for each sentence. <code>[PCA]</code> uses the concatenation of all tokens whose dimension was reduced by Principal Component Analysis (PCA). <code>[EIS_Word]</code> and <code>[EIS_Sentence]</code> use the average of tokens for an answer, which is formed in three words or a sentence by prompts shown in Fig.02, to describe the Emotion, Intention, and speaking Style of the transcript. In BERT-VITS baseline, global strategy <code>[CLS]</code> only uses the first token from the BERT-base-uncased model for each input sentence.</p> <p></p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#sequential-token-extraction","title":"Sequential Token Extraction","text":"<p>In our proposed ==LLaMA-VITS==, sequential strategy <code>[TEX]</code> concatenates the sequence of tokens in a sentence generated by LLaMA2 (2023) using text input. <code>[PHO]</code> concatenates the sequence of tokens of a sentence generated by LLaMA2 (2023) using phonemic input. In the baseline BERT-VITS, sequential strategy <code>[BERT_TEX]</code> concatenates all the tokens in a sentence extracted from BERT-base-uncased model. <code>[BERT_PHO]</code> concatenates all the tokens in a sentence extracted from BERT-x-phone-base model.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#datasets","title":"Datasets","text":"<p>We utilized full LJSpeech, 1-hour LJSpeech, and EmoV_DB_bea_sem dataset for experimental verification. LJSpeech4comprises 24 hours recorded of English speech by sin gle female speaker, where we evaluate how the embeddings extracted from LLaMA2 (2023) can help improve the speech naturalness.Besides full LJSpeech dataset, we also randomly filtered 1 hour LJSpeech which contains only 1-hour records as an ablation study to show how dataset size influences. EmoV_DB5(Adigwe et al., 2018) is a database of emotional speech that contains data for male and female actors in English and French. EmoV_DB covers 5 emotion classes, amused, an gry, disgusted, neutral, and sleepy. To factor out the effect of different speakers, we filtered the original EmoV_DB dataset into the speech of a specific female English speaker, bea. Then we use LLaMA2 (2023) to predict the emotion label of the transcript cho sen from the above 5 emotion classes, and select the audio samples which has the same predicted emotion. The filtered dataset contains 22.8-min records for training. We named the filtered dataset EmoV_DB_bea_sem and investigated how the semantic embeddings from LLaMA2 (2023) behave in naturalness and expressiveness on it. Please refer to Appendix A 12 for more dataset statistics.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#implementation-hyper-parameters-training","title":"Implementation, Hyper-parameters, Training","text":"<p>Our ==LLaMA-VITS== system was built on the VITS (2021) framework using its original implementation, augmented with semantic embeddings de rived from LLaMA2 (2023) (Touvron et al., 2023) using its original implementation7. For training LJSpeech, we use the public configs in the original implementation of VITS (2021). For EmoV_DB_bea_sem, we use the same config as LJSpeech but changed batch size from 64 to 16 since this dataset is much smaller. Besides implementing our proposed ==LLaMA-VITS==, we extracted corresponding semantic tokens <code>[CLS]</code>, <code>[BERT_TEX]</code> from BERT-uncased-base model and <code>[BERT_PHO]</code> from BERT pre-trained on phoneme for comparison. In comparing the experimental results, we choose 100k-step results on both full LJSpeech and 1-hour LJSpeech datasets since they are rather large. On EmoV_DB_bea_sem, we used the pre-trained checkpoint of LJSpeech on 100k step and compare the fine-tuning results on EmoV_DB_bea_sem at 150k-step since it is rather small.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Both subjective and objective metrics are implemented for a comprehensive evaluation. In subjective evaluation, we con duct Emotion Similarity Mean Opinion Score (ES MOS) (Zhu et al., 2023) experiments to evaluate emotion similarity for EmoV_DB_bea_sem.In the subjective evaluation, we compared <code>[AVE]</code>, <code>[TEX]</code> and <code>[PHO]</code> strategies in our ==LLaMA-VITS== with the corresponding token <code>[CLS]</code>, <code>[BERT_TEX]</code> and <code>[BERT_PHO]</code> extracted from different BERT models and the baseline ORI-VITS who does not con tain semantic tokens, with the ground truth samples GT.</p> <p>In evaluating ESMOS, we randomly chose 5 samples from the total 51 test samples proportionally divided by us and received 100 test results from different speakers on Amazon Mechanical Turk. The result significance level is thus 500. Each participant is asked to give a score on emotion similarity compared with ground truth in a 5-scale: Excellent Match 5, Good Match 4, Fair Match 3, Poor Match 2, Bad Match 1.</p> <p>In objective evaluation,we utilize UTokyo-SaruLab Mean Opinion Score (UTMOS) (Saeki et al., 2022), Mel-Cepstral Distortion (MCD), and speech recognition performance measured by Character Error Rate (CER) and Word Error Rate (WER). UTMOS is a MOS prediction network using speech samples from previous Blizzard Challenges and Voice Conversion Challenges, which has reached the best performance in VoiceMOS Challenge 2022. We evaluate objective intelligibility by using Whisper-large (Radford et al., 2022). For calculating UTMOS, we use the implementation in SpeechMOS. For calculating MCD and ASR, we use the evaluation implementation of ESPnet (Hayashi et al., 2020, 2021).</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#5experiment-results","title":"5.Experiment Results","text":"<p>We evaluated our proposed ==LLaMA-VITS== along with baselines ORI-VITS and BERT-VITS models on three distinct datasets: the full LJSpeech, the 1 hour LJSpeech, and EmoV_DB_bea_sem. The experimental outcomes provide a comprehensive understanding of the model performance and the impact of semantic tokens selection. A summary of these results is articulated below and can be referenced in Table 1.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#51results-on-full-ljspeech","title":"5.1.Results on full LJSpeech","text":"<p>The ORI-VITS baseline, achieving a UTMOS of 4.19 \u00b1 0.05, an MCD of7.32 \u00b1 0.61, a CER of6.2, and a WER of 16.5. Enhancements were observed with the BERT-VITS baseline.Specifically, BERT-VITS with <code>[BERT_TEX]</code> semantic tokens demonstrated superior performance in UTMOS (4.22\u00b10.05) and MCD (7.27 \u00b1 0.61), indicating improved speech quality and reduced mel-cepstral distortion. Additionally, a reduced CER of5.9and WER of15.9were noted, highlighting enhanced automatic speech recognition accuracy. Our proposed ==LLaMA-VITS==, integrating various global and sequential semantic tokens, displayed competitive performance.The <code>[PCA]</code> strategy stood out, achieving an MCD of7.23 \u00b1 0.61, indicating optimal mel-cepstral distortion.The <code>[EIS_Sentence]</code>, <code>[AVE]</code>, and <code>[LAST]</code> tokens yielded a top-tier UTMOS of4.21\u00b10.04/0.05, underscoring their effectiveness in enhancing perceived speech quality.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#52results-on-1-hour-ljspeech","title":"5.2.Results on 1-hour LJSpeech","text":"<p>In the more challenging 1-hour LJSpeech dataset, all models experienced a slight performance de crease, an expected outcome given the reduced training data size. BERT-VITS baseline with <code>[CLS]</code> tokens exhibited notable MCD performance (7.39 \u00b1 0.62), while the <code>[BERT_PHO]</code> excelled in UTMOS (4.05 \u00b1 0.07), reflecting enhanced speech naturalness and reduced mel-cepstral distortion. ==LLaMA-VITS== with <code>[AVE]</code> tokens achieved the high est UTMOS (4.10 \u00b1 0.07), while <code>[EIS_Sentence]</code> tokens resulted in the most favorable MCD (7.36 \u00b1 0.59), illustrating the model\u2019s versatility and efficacy in different token configurations.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#53results-on-emov_db_bea_sem","title":"5.3.Results on EmoV_DB_bea_sem","text":"<p>On this even more challenging dataset, a small improvement observed in BERT-VITS only exists in the <code>[BERT_TEX]</code> with a CER of 4.4. While our proposed ==LLaMA-VITS== displayed no table enhancements. The <code>[TEX]</code> strategy achieves an ESMOS of3.22 \u00b1 0.07, indicating much more emotiveness. The <code>[LAST]</code> yielded the best performance on CER of4.3and WER of17.4, other strategies also perform better than or comparable to BERT-VITS, underscoring its effectiveness in enhancing perceived speech expressiveness.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#54analysis","title":"5.4.Analysis","text":"<p>Speaking of the strengths of different tokens, BERT based tokens generally contribute to improving MCD and ASR scores, indicating the enriched semantic understanding translated to speech qual ity. Tokens of ==LLaMA-VITS== exhibited a balanced performance across all metrics, with specific to ken configurations excelling in particular aspects. For instance, <code>[PCA]</code> token emerged as a strong contender in reducing MCD, <code>[AVE]</code> enhanced the UTMOS scores, <code>[TEX]</code> had superior performance to improve ESMOS score. In individual comparisons, ==LLaMA-VITS==\u2019s five global tokens generally outperformed BERT-VITS on the UTMOS metric for naturalness. In the ESMOS metric for emotional expression, ==LLaMA-VITS==\u2019s two sequential tokens also generally sur passed BERT-VITS, particularly the <code>[TEX]</code> token. Therefore, we can infer that GPT-like LMs may have greater potential for TTS tasks than BERT like models. Further, our results reflect different patterns of gains from GPT-like and BERT-like models in TTS tasks. For instance, in the UTMOS naturalness metric, ==LLaMA-VITS==\u2019s global tokens often outperformed sequential tokens, which is the opposite for BERT-VITS; in the ESMOS emotion metric, ==LLaMA-VITS==\u2019s sequential token <code>[TEX]</code> significantly outperformed other tokens, while for BERT-VITS, global tokens performed better. Overall, ==LLaMA-VITS== showed a different pattern in UTMOS compared to BERT-VITS, and superior performance in ESMOS. These results highlight the potential for further exploration of semantic to ken types and fusion methods to achieve more significant enhancements in speech synthesis, particularly in scenarios constrained by limited and complex training data.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#6discussion","title":"6.Discussion","text":"<p>In this section, we discuss factors influencing current outcomes.Based on this discussion, we also point out the directions for future work in Appendix 13.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#61gpt-like-vs-bert-like","title":"6.1.GPT-like vs BERT-like","text":"<p>Initial observations from our experiments indicate that, even without any fine-tuning of LLaMA2 (2023), ==LLaMA-VITS== significantly outperforms both BERT-VITS and ORI-VITS in terms of emotional expressive ness. This finding opens up avenues for future research into emotive TTS tasks. Furthermore, a comparison between BERT-VITS and ==LLaMA-VITS== highlights their distinct performance traits. BERT-VITS, leveraging deep con textual embeddings, provides profound semantic insights yet encounters challenges in customization and adaptability across a range of TTS tasks. Conversely, ==LLaMA-VITS== can provide a more versa tile and adaptable approach, with its array of token types demonstrating particular advantages across various evaluation metrics.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#62semantic-token-strategy","title":"6.2.Semantic Token Strategy","text":"<p>The varying effectiveness of distinct semantic to kens underscores the importance of careful selection and integration tailored to the particular goals of TTS systems. Optimizing the type of token and method of fusion can be instrumental in enhancing aspects such as speech naturalness, emotional expressiveness, Mel Cepstral Distortion (MCD), or Automatic Speech Recognition (ASR) performance.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#7conclusion","title":"7.Conclusion","text":"<p>In summary, this study exemplifies a significant stride towards optimized TTS synthesis by integrating semantic tokens, leveraging the strengths of ==LLaMA-VITS==. Our findings, validated by comprehensive experiments on the LJSpeech and EmoV_DB_bea_sem datasets, underscore the pivotal role of semantic embeddings in enhancing speech quality, naturalness, and emotiveness. The adaptability and efficacy of ==LLaMA-VITS==, especially, open new vistas for customized and context sensitive TTS applications.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#8limitations","title":"8.Limitations","text":"<p>Compared with our baseline which uses different BERT models, we only tested our method using LLaMA2 (2023). As Kenter et al. (2020) indicate for their BERT-based TTS model, small BERT models work better than big ones, but the parameter size of our proposed GPT-based TTS influence is yet stud ied by our research. Although BERT-based TTS models are normally finetuned on speech tasks to provide more explicit acoustic information for TTS, we didn\u2019t try designing prompts to generate acoustic features and only studied how general semantic information can help. Our experiments were conducted only on clean datasets with limited size, and the effect on more complex datasets is to be further explored. The integration of LLaMA2 (2023)\u2019s embeddings introduces additional computational costs, potentially limiting real-time applications.</p> <p>\u76f8\u6bd4\u4e8e\u57fa\u7ebf\u6a21\u578b\u4f7f\u7528\u4e86\u4e0d\u540c\u7684 BERT \u6a21\u578b, \u6211\u4eec\u7684\u65b9\u6cd5\u4ec5\u4f7f\u7528\u4e86 LLaMA2.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/","title":"2024.04 RALL E","text":"<p>@import \"../../style.less\"</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#rall-e-robust-codec-language-modeling-with-chain-of-thought-prompting-for-text-to-speech-synthesis","title":"RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis","text":"\u4f5c\u8005 \u673a\u6784 \u4f5c\u8005 \u673a\u6784 \u8f9b\u5fb7\u6cf0 Microsoft\u4e1c\u4eac\u5927\u5b66 \u8c2d\u65ed Microsoft \u6c88\u9534 Microsoft\u6d59\u6c5f\u5927\u5b66 \u741a\u6cfd\u8c26 Microsoft\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66 \u6768\u4e1c\u8d85 \u9999\u6e2f\u4e2d\u6587\u5927\u5b66 \u738b\u8fdc\u7a0b \u9999\u6e2f\u4e2d\u6587\u5927\u5b66(\u6df1\u5733) \u9ad8\u9053 \u614e\u4e4b\u4ecb \u4e1c\u4eac\u5927\u5b66 \u733f\u6e21 \u6d0b \u4e1c\u4eac\u5927\u5b66 \u5218\u6811\u6770 Microsoft \u674e\u52b2\u5b87 Microsoft \u8d75\u80dc Microsoft","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#abstract","title":"Abstract","text":"<p>We present ==RALL-E==, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind ==RALL-E== is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, ==RALL-E== first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, ==RALL-E== utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E (2023), ==RALL-E== significantly improves the WER of zero-shot TTS from 6.3% (without reranking) and 2.1% (with reranking) to 2.8% and 1.0%, respectively. Furthermore, we demonstrate that ==RALL-E== correctly synthesizes sentences that are hard for VALL-E (2023) and reduces the error rate from 68% to 4%.</p> <p>\u6211\u4eec\u63d0\u51fa\u4e86 ==RALL-E==, \u4e00\u79cd\u9c81\u68d2\u7684\u7528\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u7684\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5. \u4e4b\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b (Large Language Models, LLMs) \u7684\u5de5\u4f5c\u5728\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272, \u4f46\u540c\u65f6\u4e5f\u5b58\u5728\u4e00\u4e9b\u4e0d\u7a33\u5b9a\u6027, \u5982\u4e0d\u7a33\u5b9a\u97f5\u5f8b (\u5947\u602a\u7684\u97f3\u9ad8\u548c\u8282\u594f/\u65f6\u957f) \u548c\u9ad8\u8bcd\u9519\u8bef\u7387 (Word Error Error, WER), \u8fd9\u4e9b\u90fd\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u56de\u5f52\u9884\u6d4b\u65b9\u5f0f\u6709\u5173. ==RALL-E== \u7684\u6838\u5fc3\u601d\u60f3\u662f\u601d\u7ef4\u94fe (Chain-of-Thought, CoT) \u63d0\u793a, \u5b83\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u7684\u6b65\u9aa4, \u4ee5\u63d0\u5347\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u7684\u9c81\u68d2\u6027. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u601d\u60f3, ==RALL-E== \u9996\u5148\u9884\u6d4b\u8f93\u5165\u6587\u672c\u7684\u97f5\u5f8b\u7279\u5f81 (\u97f3\u9ad8\u548c\u65f6\u957f), \u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a\u4e2d\u95f4\u6761\u4ef6\u7528\u4e8e\u9884\u6d4b\u601d\u7ef4\u94fe\u5f62\u5f0f\u7684\u8bed\u97f3 Tokens. \u5176\u6b21, ==RALL-E== \u5229\u7528\u9884\u6d4b\u7684\u65f6\u957f\u63d0\u793a\u6765\u5f15\u5bfc Transformer \u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u6743\u91cd\u8ba1\u7b97, \u4ee5\u5f3a\u5236\u6a21\u578b\u5728\u9884\u6d4b\u8bed\u97f3 Token \u65f6\u4e13\u6ce8\u4e8e\u76f8\u5e94\u7684\u97f3\u7d20\u548c\u97f5\u5f8b\u7279\u5f81. \u7efc\u5408\u7684\u5ba2\u89c2\u8bc4\u4f30\u548c\u4e3b\u89c2\u8bc4\u4f30\u8bf4\u660e, \u76f8\u6bd4 VALL-E (2023), ==RALL-E== \u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u7684\u8bcd\u9519\u8bef\u7387\u8868\u73b0, \u4ece 6.3% (\u4e0d\u4f7f\u7528 reranking) \u548c 2.1% (\u4f7f\u7528 reranking) \u5206\u522b\u4e0b\u964d\u5230 2.8% \u548c 1.0%. \u6b64\u5916, \u6211\u4eec\u8fd8\u5c55\u793a\u4e86 ==RALL-E== \u80fd\u591f\u6b63\u786e\u5408\u6210 VALL-E (2023) \u96be\u4ee5\u5408\u6210\u7684\u53e5\u5b50, \u5e76\u5c06\u9519\u8bef\u7387\u4ece 68% \u4e0b\u964d\u5230 4%.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#1introduction","title":"1.Introduction","text":"<p>Large language models (LLMs) have demonstrated great progress in natural language generation [1, 20]. With a sufficient model size LLMs emerge powerful in-context learning abilities that can handle unseen tasks with a text instruction (usually called prompt) in a zero-shot or few-shot manner [31]. Moreover, the simple yet effective next-token prediction task of LLMs makes it easy to apply LLMs on other fields, such as vision [5] and speech synthesis [29], as long as the data can be converted to discrete speech tokens. This work focuses on the language modeling of text-to-speech (TTS) synthesis. Recent work [14,29] have shown that TTS can be modeled by a decoder-only language model by using a neural codec [4,34] to convert continuous waveforms into discrete tokens. These methods, typically leverage tens of thousands of hours of speech data, emerge in-context learning ability that can clone a speaker\u2019s voice by providing a short audio prompt to the language model, and thus have impressive performance on zero-shot TTS. However, due to the sequential generation property of language models, such codec language models suffer from poor robustness. Although the AutoRegressive (AR) prediction style of language models enables the model to generate speech with diverse prosody patterns, they can also cause bad cases with unnatural prosody. Moreover, since there is no strict alignment between text and speech, the models can omit or repeat words in the input text. This is quite different from TTS methods based on Non-AutoRegressive (NAR) generative models [12,16,25], which predicts all tokens at the same time, thus have high robustness but relatively low diversity. As suggested by previous work [12,33], LLM-based TTS have a higher Word Error Rate (WER) than NAR TTS even if they have similar performance on other metrics. To alleviate this problem, a simple but effective method is to sample the same input text multiple times and select the best one [14,33]. However, such a reranking method further increases the inference time. In this paper, we present ==RALL-E== (the abbreviation of Robust VALL-E), a method to improve the robustness of LLM-based TTS. The core idea of ==RALL-E== is inspired from the Chain-of-Thought (CoT) prompting [32]. In CoT prompting, the LLM is instructed to generate an intermediate result that is used as a condition for the prediction of the final result. The CoT prompting breaks a complex task into several simpler steps, so that can improve the robustness of LLMs, especially on hard tasks like arithmetic [32]. To adapt CoT prompting to LLM-based TTS, ==RALL-E== predicts prosody tokens (pitch and duration) before predicting speech tokens to stabilize the prosody. Given an input sentence, ==RALL-E== first predicts phoneme-level pitch and duration of the input, then predicts speech tokens conditioning on both the input phonemes and the predicted prosody tokens. Furthermore, ==RALL-E== utilizes the predicted duration to mask irrelevant phonemes and prosody tokens when computing self-attention weights, so that the codec language model is enforced to concentrate on tokens around the phoneme and prosody token the speech token corresponds to. We use VALL-E (2023) [29], a recent powerful LLM-based TTS method, as the baseline, and conduct experiments on a large dataset with 44K hours speech data. Results of comprehensive objective and subjective evaluations demonstrate that RALL significantly improves the robustness of LLM-based TTS by reducing the WER on the LibriSpeech [18] test-clean set from6.3%(w/o reranking) and2.1%(with reranking) to2.8%and 1.0%, respectively. Furthermore, we evaluate the performance of ==RALL-E== on 50 particularly hard sentences. As demonstrated in Tab.01, compared to VALL-E (2023), ==RALL-E== significantly reduces WER from68%to4%by eliminating almost all types of error, which demonstrates the superior robustness of ==RALL-E== (see Section 4.4 for more details). The contributions of this work are summarized as follows:  - We present ==RALL-E==, a robust codec language modeling method with chain-of-thought prompting for TTS. ==RALL-E== improves the robustness of LLM-based TTS by (1) incorporating prosody tokens as chain-of-thought prompting to stabilize the generation of speech tokens and (2) using duration-guided masking to enhance the alignment between phoneme and speech tokens. - We conduct comprehensive objective and subjective evaluations. Experimental results demonstrate that ==RALL-E== obtains significantly better robustness than the baseline VALL-E (2023) and two previous works. - We further evaluate ==RALL-E== on sentences that are particularly hard to synthesize for LLM-based TTS. The results demonstrate that ==RALL-E== correctly synthesizes hard sentences and reduces the error rate from68%to4%compared to VALL-E (2023), which closely approaches the performance of non-autoregressive TTS. Audio samples can be found at https://ralle-demo.github.io/RALL-E.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#2related-work","title":"2.Related Work","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#llm-based-tts","title":"LLM-Based TTS","text":"<p>Inspired by the success of LLMs [1,20], several recent works adopt language models to model TTS (SPEAR-TTS (2023), VALL-E (2023), UniAudio (2023)) and begin to use decoder-only architecture based on Transformer (2017). In such models, text and speech tokens are concatenated together and fed to a single transformer. The whole model is trained on a next-token prediction task like a language model. The LLM-based TTS systems are typically trained on tens of thousands of hours of speech data and have hundreds of millions of parameters, hence can leverage the emergent abilities of LLMs like in-context learning [31] to enable zero-shot TTS (VALL-E (2023)). Besides, recent works (AudioPalM (2023); VioLA (2023); UniAudio (2023)) have shown the decoder-only architecture can be used to learn multiple tasks, as the input and output are processed jointly by a language model, and the model can be signaled to generate results for different tasks by inputting pre-defined special tokens. ==RALL-E== focuses on the robustness problem of LLM-based TTS.</p> <p>\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u83b7\u5f97\u7075\u611f, \u8fd1\u671f\u7684\u4e00\u4e9b\u5de5\u4f5c\u5c06\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u5efa\u6a21\u8bed\u97f3\u5408\u6210\u5e76\u5f00\u59cb\u4f7f\u7528\u4ec5\u6709 Transformer \u89e3\u7801\u5668\u7684\u67b6\u6784. \u8fd9\u4e9b\u6a21\u578b\u5c06\u6587\u672c\u548c\u8bed\u97f3\u6807\u8bb0\u4e32\u8054\u8d77\u6765, \u8f93\u5165\u5230\u5355\u4e2a Transformer \u4e2d. \u6574\u4e2a\u6a21\u578b\u5728\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3, \u5982\u8bed\u8a00\u6a21\u578b\u4e00\u6837. \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u901a\u5e38\u662f\u5728\u6570\u5343\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\u4e0a\u8bad\u7ec3\u7684, \u5177\u6709\u6570\u767e\u4e07\u4e2a\u53c2\u6570, \u56e0\u6b64\u53ef\u4ee5\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u80fd\u529b, \u5982\u4e0a\u4e0b\u6587\u5b66\u4e60, \u5b9e\u73b0\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210. \u9664\u6b64\u4e4b\u5916, \u8fd1\u671f\u7684\u5de5\u4f5c [22,30,33] \u8868\u660e, \u4ec5\u6709\u89e3\u7801\u5668\u67b6\u6784\u7684\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u591a\u4e2a\u4efb\u52a1, \u56e0\u4e3a\u8f93\u5165\u548c\u8f93\u51fa\u662f\u7531\u8bed\u8a00\u6a21\u578b\u4e00\u8d77\u5904\u7406\u7684, \u5e76\u4e14\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u9884\u5b9a\u4e49\u7684\u7279\u6b8a\u6807\u8bb0\u4fe1\u53f7\u751f\u6210\u4e0d\u540c\u4efb\u52a1\u7684\u7ed3\u679c. ==RALL-E== \u7740\u91cd\u4e8e\u8bed\u8a00\u6a21\u578b\u5efa\u6a21\u8bed\u97f3\u5408\u6210\u7684\u5065\u58ee\u6027\u95ee\u9898.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#robust-autoregressive-tts","title":"Robust Autoregressive TTS","text":"<p>The robustness of AR TTS is a popular topic in the literature. For encoder-decoder AR TTS, several previous works enforce the attention weights to be monotonic [2,9, 36] that can effectively improve the robustness. In addition, Shen et al.[24] proposed a non-attentive Tacotron, in which the attention module was replaced by a duration predictor to determine the alignment path before decoding. For decoder-only TTS, a key difference is that the attention weights are computed on text and context at the same time, hence the whole attention weights should not be monotonic. Song et al. proposed ELLA-V (2024) that interleaves the speech tokens with phonemes by inserting a phoneme token and a special End Of Phone(EOP) token at the beginning and end of the speech tokens corresponding to the phoneme, respectively. While the inserted phoneme and the EOP token indicate the duration of each phoneme, such an implicit way entangles the prediction of speech tokens and duration together. ==RALL-E== disentangles the predictions of duration and speech tokens by predicting the duration of all phonemes before the speech tokens, hence has higher controllability over the generation process. Du et al. proposed VALL-T (2024) that uses an unsupervised transducer loss [7] to implicitly model the duration of phonemes. Compared to RALL-E, although VALL-T doesn\u2019t rely on external alignment tools during training, its training time is considerably decelerated since the transducer loss requires the model to perform a forward process for every phoneme. Besides, like ELLA-V (2024), VALL-T (2024) also entangles the predictions of duration and speech tokens, thus has weaker controllability than RALL-E.</p> <p>\u81ea\u56de\u5f52\u8bed\u97f3\u5408\u6210\u7684\u5065\u58ee\u6027\u662f\u4e00\u4e2a\u70ed\u95e8\u8bdd\u9898. \u5bf9\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u81ea\u56de\u5f52\u8bed\u97f3\u5408\u6210, \u4e4b\u524d\u7684\u5de5\u4f5c\u5df2\u7ecf\u63d0\u51fa\u4e86\u5f3a\u5316\u6ce8\u610f\u529b\u6743\u91cd\u4e3a\u5355\u8c03\u7684\u7b56\u7565 [2,9, 36], \u80fd\u591f\u6709\u6548\u63d0\u5347\u8bed\u97f3\u5408\u6210\u7684\u5065\u58ee\u6027. \u6b64\u5916, Shen \u7b49\u4eba\u63d0\u51fa\u4e86\u4e00\u4e2a\u975e\u6ce8\u610f\u529b\u7684 Tacotron, \u5176\u4e2d\u66ff\u6362\u4e86\u6ce8\u610f\u529b\u6a21\u5757, \u7528\u4e00\u4e2a\u65f6\u957f\u9884\u6d4b\u5668\u5728\u89e3\u7801\u524d\u786e\u5b9a\u5bf9\u9f50\u8def\u5f84.</p> <p>\u5bf9\u4e8e\u4ec5\u6709\u89e3\u7801\u5668\u7684\u8bed\u97f3\u5408\u6210, \u5173\u952e\u533a\u522b\u662f\u6ce8\u610f\u529b\u6743\u91cd\u662f\u540c\u65f6\u8ba1\u7b97\u6587\u672c\u548c\u4e0a\u4e0b\u6587\u7684, \u56e0\u6b64\u6ce8\u610f\u529b\u6743\u91cd\u4e0d\u5e94\u8be5\u662f\u5355\u8c03\u7684. Song \u7b49\u4eba\u63d0\u51fa ELLA-V (2024), \u5176\u4e2d\u63d2\u5165\u4e86\u4e00\u4e2a\u97f3\u7d20\u6807\u8bb0\u548c\u4e00\u4e2a\u7279\u6b8a\u7684\u7ed3\u675f\u97f3\u7d20\u6807\u8bb0, \u524d\u8005\u5bf9\u5e94\u4e8e\u97f3\u7d20\u7684\u65f6\u957f, \u540e\u8005\u5bf9\u5e94\u4e8e\u97f3\u7d20\u7684\u7ed3\u675f. \u8fd9\u79cd\u9690\u5f0f\u7684\u65b9\u5f0f\u5c06\u8bed\u97f3\u6807\u8bb0\u548c\u65f6\u957f\u9884\u6d4b\u8054\u7cfb\u5728\u4e00\u8d77, \u5bfc\u81f4\u6a21\u578b\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u63a7\u5236\u529b\u8f83\u5f31. ==RALL-E== \u901a\u8fc7\u9884\u6d4b\u6240\u6709\u97f3\u7d20\u7684\u65f6\u957f, \u89e3\u8026\u4e86\u65f6\u957f\u548c\u8bed\u97f3\u6807\u8bb0\u7684\u9884\u6d4b, \u56e0\u6b64\u53ef\u4ee5\u66f4\u597d\u5730\u63a7\u5236\u751f\u6210\u8fc7\u7a0b. Du \u7b49\u4eba\u63d0\u51fa VALL-T (2024), \u4f7f\u7528\u65e0\u76d1\u7763\u7684\u8f6c\u6362\u5668\u635f\u5931 [7] \u6a21\u578b\u97f3\u7d20\u7684\u65f6\u957f. \u4e0e ==RALL-E== \u76f8\u6bd4, VALL-T \u867d\u7136\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u4f9d\u8d56\u5916\u90e8\u5bf9\u9f50\u5de5\u5177, \u4f46\u8bad\u7ec3\u65f6\u95f4\u663e\u8457\u51cf\u5c11, \u56e0\u4e3a\u8f6c\u6362\u5668\u635f\u5931\u9700\u8981\u6a21\u578b\u5bf9\u6bcf\u4e2a\u97f3\u7d20\u8fdb\u884c\u4e00\u6b21\u524d\u5411\u8ba1\u7b97. \u6b64\u5916, \u4e0e ELLA-V (2024) \u4e00\u6837, VALL-T (2024) \u4e5f\u5c06\u65f6\u957f\u548c\u8bed\u97f3\u6807\u8bb0\u8054\u7cfb\u5728\u4e00\u8d77, \u56e0\u6b64\u63a7\u5236\u529b\u8f83\u5f31.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#3rall-e","title":"3.RALL-E","text":"<p>The overview of ==RALL-E== is illustrated in Fig.01.</p> <p></p> <p>The core idea of ==RALL-E== is CoT prompting that generates intermediate results to assist and stabilize the generation of speech tokens and improve the robustness of LLM-based TTS. To accomplish this idea, we first propose to predict two kinds of phoneme-level prosody tokens: pitch and duration before predicting the speech tokens. The distributions of the prosody tokens are modeled together with speech tokens by a single Transformer so that they can influence the duration and pitch of the predicted speech tokens. To further utilize the predicted duration to guide the generation and improve the robustness, we propose duration-guided masking to enhance the alignment between speech tokens, phonemes, and prosody tokens learned by the language model. At each decoding step of the speech tokens, ==RALL-E== masks phonemes and prosody tokens that are irrelevant to the synthesis of the current speech token based on the duration information. In the following sections, we first briefly introduce VALL-E (2023) since we apply the proposed method to it in the experiments. We then formulate and introduce ==RALL-E== in detail. It should be stressed that, though we use VALL-E (2023) to implement ==RALL-E==, the proposed method can be applied in any decoder-only AR TTS model.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#31preliminary-vall-e","title":"3.1.Preliminary: VALL-E","text":"<p>We inherit most symbols from the original paper of VALL-E (2023) for ease of reading. Readers are recommended to refer to the original paper for more details.</p> <p>Generally, VALL-E (2023) is a decoder-only LLM-based TTS system that uses two Transformers [28] to predict speech tokens from the text. The speech tokens here are extracted from EnCodec (2022), a neural audio codec based on residual vector quantization (RVQ) [34] that can convert continuous speech signal into discrete tokens. After predicting the discrete tokens, the waveforms can be reconstructed by feeding the tokens into the decoder of EnCodec. An RVQ typically contains $N$ quantization layers ($N = 8$ in VALL-E (2023)), hence at each time step the encoded speech has $N$ tokens.  Formally, given speech $\\mathbf{y}$ and its transcription $\\mathbf{x}$, the discrete speech token matrix $\\mathbf{C}$ encoded by the codec has a shape of $T \\times N$, where $T$ is the total time step. In addition tox, to clone a speaker\u2019s voice and utilize the in-context learning ability of LLMs, VALL-E (2023) receives a short prompt $\\tilde{\\mathbf{C}}^{T'\\times N}$ as input before predicting $\\mathbf{C}$. Hence, VALL-E (2023) models and maximizes the following distribution:</p> <p>$$   \\mathbb{P}(\\mathbf{C}|\\mathbf{x},\\tilde{\\mathbf{C}}).\\tag{1} $$</p> <p>VALL-E (2023) predicts speech tokens hierarchically where the speech tokens of the 1st layer of RVQ are first predicted by an AR Transformer, and the tokens of the rest layers are predicted by a NAR Transformer. This is because RVQ uses a residual quantization method, i.e. higher layers encode the information that is not encoded by the lower layers, hence tokens of the1st layer contain most information of the waveforms, and the information encoded by the rest layers gradually decreases. The AR Transformer takes the phoneme sequence $\\mathbf{x}$, and speech tokens of the1st layer of the prompt $\\tilde{c}{:,1}$ as input to predict the target speech tokens of the1st layer $\\mathbf{c}{:,1}$ sequentially, i.e. maximizes the following distribution:</p> <p>$$   \\mathbb{P}(\\mathbf{c}{:,1}|\\mathbf{x},\\tilde{\\mathbf{c}}{:,1};\\theta_{AR})=\\prod_{t=1}^T\\mathbb{P}(\\mathbf{c}{t,1}|\\mathbf{x},\\mathbf{c}{&lt;t,1},\\tilde{\\mathbf{c}}{:,1};\\theta{AR}),\\tag{2} $$</p> <p>where $\\theta_{\\text{AR}}$ is the trainable parameters of the AR Transformer. The NAR Transformer predicts all target speech tokens $\\mathbf{c}{:,j}$ of the $j$-th layer at the same time with the phoneme sequence $\\mathbf{x}$, the prompt $\\tilde{\\mathbf{C}}$, and target speech tokens $\\mathbf{c}{:,&lt;j}$ of all layers less than $j$ as the conditions, i.e. maximizes the following distribution:</p> <p>$$   \\mathbb{P}(\\mathbf{c}{:,2:N}|\\mathbf{x},\\tilde{\\mathbf{C}};\\theta{NAR})=\\prod_{j=2}^N\\mathbb{P}(\\mathbf{c}{:,j}|\\mathbf{x},\\mathbf{c}{:,&lt;j},\\tilde{\\mathbf{C}};\\theta_{NAR}),\\tag{3} $$</p> <p>where $\\theta_{\\text{NAR}}$ is the trainable parameters of the NAR Transformer. By combining Eq.2 and Eq.3, VALL-E (2023) breaks Eq.1 into the following form:</p> <p>$$   \\mathbb{P}(\\mathbf{C}|\\mathbf{x},\\tilde{\\mathbf{C}})=\\mathbb{P}(\\mathbf{c}{:,1}|\\mathbf{x},\\tilde{\\mathbf{c}}{:,1};\\theta_{AR})\\mathbb{P}(\\mathbf{c}{:,2:N}|\\mathbf{x},\\tilde{\\mathbf{C}};\\theta{NAR}).\\tag{4} $$</p> <p>It is noteworthy that in practice the two Transformers have the same architecture but have different attention masks during computation. Specifically, both the two Transformers use a bidirectional mask for the phoneme sequence $\\mathbf{x}$, which means every phoneme $x_i$ can attend to all other phonemes $x_{\\neq i}$. However, for the speech tokens, the AR Transformers uses a unidirectional mask so that $\\mathbf{c}{t,1}$ can only attend to previous tokens $\\mathbf{c}{&lt;t,1}$, while the NAR Transformer still uses a bidirectional mask.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#32prosody-tokens-as-chain-of-thought-prompts","title":"3.2.Prosody Tokens as Chain-of-Thought Prompts","text":"<p>One of the problems of LLM-based TTS is that it directly generates speech from phonemes with no restriction on the prosody, e.g. pitch, duration, etc, which usually results in speech with unstable prosody. A similar problem is also observed in Wei et al.[32] where the authors find LLMs cannot directly answer a complex question like arithmetic and propose CoT prompting to solve this problem. The idea of CoT prompting is breaking a complex task into several simpler tasks so that LLMs can utilize the intermediate results to reach the final answers. As shown in Wei et al.[32], by CoT prompting the correct rate of LLMs on complex tasks can be significantly improved. This motivates us to adapt CoT prompting to LLM-based TTS by generating intermediate prosody tokens before generating speech tokens to alleviate the robustness problem of LLM-based TTS.</p> <p>\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7684\u95ee\u9898\u4e4b\u4e00\u662f\u5b83\u76f4\u63a5\u4ece\u97f3\u7d20\u751f\u6210\u8bed\u97f3, \u800c\u5bf9\u97f5\u5f8b\u6ca1\u6709\u9650\u5236, \u5982\u97f3\u9ad8, \u65f6\u957f\u7b49, \u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u751f\u6210\u97f5\u5f8b\u4e0d\u7a33\u5b9a\u7684\u8bed\u97f3. Wei \u7b49\u4eba\u7684\u7814\u7a76\u4e2d\u4e5f\u80fd\u89c2\u5bdf\u5230\u76f8\u4f3c\u95ee\u9898, \u4ed6\u4eec\u53d1\u73b0\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u4e0d\u80fd\u76f4\u63a5\u56de\u7b54\u590d\u6742\u7684\u95ee\u9898, \u5982\u7b97\u672f\u7b49, \u4ece\u800c\u63d0\u51fa\u4e86\u601d\u7ef4\u94fe\u63d0\u793a\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898. \u601d\u7ef4\u94fe\u63d0\u793a\u7684\u601d\u60f3\u662f\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u82e5\u5e72\u4e2a\u7b80\u5355\u4efb\u52a1\u4f7f\u5f97\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5229\u7528\u4e2d\u95f4\u7ed3\u679c\u6765\u83b7\u5f97\u6700\u7ec8\u7b54\u6848. \u5982 Wei \u7b49\u4eba\u6240\u5c55\u793a\u7684, \u901a\u8fc7\u601d\u7ef4\u94fe\u63d0\u793a, \u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6b63\u786e\u7387. \u8fd9\u6fc0\u52b1\u6211\u4eec\u5c06\u601d\u7ef4\u94fe\u63d0\u793a\u5e94\u7528\u4e8e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210, \u901a\u8fc7\u5728\u751f\u6210\u8bed\u97f3\u4e4b\u524d\u751f\u6210\u4e2d\u95f4\u97f5\u5f8b\u6807\u8bb0\u6765\u7f13\u89e3\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u7684\u5065\u58ee\u6027\u95ee\u9898.</p> <p>To incorporate pitch and duration in the AR Transformer of VALL-E (2023), we first get the alignment between phonemes and speech tokens and extract the pitch value for each speech token. We then compute phoneme-level pitch value based on the duration and linearly quantize it to $M_p$ buckets. We define a maximal duration value $M_d$, and all duration values that exceed $M_d$ will be truncated to the maximum. ==RALL-E== predicts the two prosody tokens before the speech tokens in a CoT style.</p> <p>\u4e3a\u4e86\u5728 VALL-E (2023) \u7684\u81ea\u56de\u5f52 Transformer \u4e2d\u6574\u5408\u97f3\u9ad8\u548c\u65f6\u957f, \u6211\u4eec\u9996\u5148\u83b7\u5f97\u97f3\u7d20\u548c\u8bed\u97f3 token \u4e4b\u95f4\u7684\u5bf9\u9f50, \u5e76\u63d0\u53d6\u6bcf\u4e2a\u8bed\u97f3 token \u7684\u97f3\u9ad8\u503c. \u7136\u540e\u6211\u4eec\u57fa\u4e8e\u65f6\u957f\u8ba1\u7b97\u97f3\u7d20\u7ea7\u522b\u7684\u97f3\u9ad8\u503c, \u5e76\u5c06\u5176\u7ebf\u6027\u91cf\u5316\u5230 $M_p$ \u4e2a\u6876\u4e2d. \u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u6700\u5927\u65f6\u957f\u503c $M_d$, \u8d85\u51fa\u8be5\u503c\u7684\u65f6\u957f\u503c\u5c06\u88ab\u622a\u65ad. ==RALL-E== \u4ee5 CoT \u98ce\u683c\u5728\u8bed\u97f3 tokens \u524d\u9884\u6d4b\u4e24\u4e2a\u97f5\u5f8b tokens.</p> <p>Formally, assume $p$, $d$ are the discrete pitch and duration sequences of the target speech tokens $\\mathbf{C}$, $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$ are the ones of the prompt $\\tilde{\\mathbf{C}}$, we model and maximize the following distribution:</p> <p>\u5f62\u5f0f\u4e0a, \u5047\u8bbe $p$, $d$ \u662f\u76ee\u6807\u8bed\u97f3 tokens $\\mathbf{C}$ \u7684\u79bb\u6563\u97f3\u9ad8\u548c\u65f6\u957f\u5e8f\u5217, $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$ \u662f\u63d0\u793a $\\tilde{\\mathbf{C}}$ \u7684\u97f3\u9ad8\u548c\u65f6\u957f\u5e8f\u5217, \u6211\u4eec\u53ef\u4ee5\u5efa\u6a21\u5e76\u6700\u5927\u5316\u4e0b\u5217\u5206\u5e03:</p> <p>$$   \\mathbb{P}(\\mathbf{p},\\mathbf{d}|\\mathbf{x},\\tilde{\\mathbf{p}},\\tilde{\\mathbf{d}};\\theta_{AR})=\\prod_{t=1}^L\\mathbb{P}(p_t,d_t|\\mathbf{x},\\mathbf{p}{&lt;t},\\mathbf{d}{&lt;t},\\tilde{\\mathbf{p}},\\tilde{\\mathbf{d}};\\theta_{AR}),\\tag{5} $$ </p> <p>where $L$ is the length of $\\mathbf{x}$. </p> <p>\u5176\u4e2d $L$ \u4e3a $\\mathbf{x}$ \u7684\u957f\u5ea6.</p> <p>In practice, the model predicts $p_t$ and $d_t$ with two separate heads, and their embeddings are summed up and fed to the model for the prediction of the next step. </p> <p>\u5b9e\u9645\u4e0a, \u6a21\u578b\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acb\u7684\u5934\u9884\u6d4b $p_t$ \u548c $d_t$, \u4ed6\u4eec\u7684\u5d4c\u5165\u5c06\u88ab\u6c42\u548c\u540e\u8f93\u5165\u6a21\u578b\u4ee5\u8fdb\u884c\u4e0b\u4e00\u6b65\u7684\u9884\u6d4b.</p> <p>==RALL-E== then predicts the speech tokens with $p$ and $d$ as a new condition, which makes Eq.2 becomes:</p> <p>==RALL-E== \u7136\u540e\u4ee5 $p$ \u548c $d$ \u4f5c\u4e3a\u65b0\u7684\u6761\u4ef6, \u4f7f\u5f97\u65b9\u7a0b 2 \u53d8\u4e3a:</p> <p>$$ \\mathbb{P}(\\mathbf{c}{:,1}|\\mathbf{x},\\tilde{\\mathbf{c}}{:,1},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta_{AR})=\\prod_{t=1}^T\\mathbb{P}(\\mathbf{c}{t,1}|\\mathbf{x},\\mathbf{c}{&lt;t,1},\\tilde{\\mathbf{c}}{:,1},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta{AR}).\\tag{6} $$</p> <p>The above two equations can be jointly optimized by the AR Transformer.  Although the proposed method adds additional $L$ decoding steps, since $L&lt;&lt;T$, it intuitively has little influence on the efficiency.</p> <p>\u4e0a\u9762\u7684\u4e24\u4e2a\u65b9\u7a0b\u53ef\u4ee5\u901a\u8fc7 AR Transformer \u8054\u5408\u4f18\u5316.  \u5c3d\u7ba1\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u589e\u52a0\u4e86 $L$ \u4e2a\u89e3\u7801\u6b65\u9aa4, \u4f46\u7531\u4e8e $L&lt;&lt;T$, \u5176\u5b9e\u9645\u5f71\u54cd\u5f88\u5c0f.</p> <p>For the NAR Transformer, we simply sum the embeddings of the phoneme, pitch, and duration together as the input. This makes Eq.3 becomes:</p> <p>\u5bf9\u4e8e\u975e\u81ea\u56de\u5f52 Transformer, \u6211\u4eec\u53ea\u9700\u5c06\u97f3\u7d20, \u97f3\u9ad8, \u65f6\u957f\u7684\u5d4c\u5165\u5411\u91cf\u76f8\u52a0\u4f5c\u4e3a\u8f93\u5165. \u8fd9\u4f7f\u5f97\u65b9\u7a0b 3 \u53d8\u4e3a: $$ \\begin{aligned}\\mathbb{P}(\\mathbf{c}{:,2:N}|\\mathbf{x},\\tilde{\\mathbf{C}},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta{NAR})&amp;=\\prod_{j=2}^N\\mathbb{P}(\\mathbf{c}{:,j}|\\mathbf{x},\\mathbf{c}{:,&lt;j},\\tilde{\\mathbf{C}},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta_{NAR}).\\end{aligned}\\tag{7} $$ </p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#33enhancing-alignment-with-duration-guided-masking","title":"3.3.Enhancing Alignment with Duration-Guided Masking","text":"<p>As the left side of Fig.02 illustrates, since the speech token attends to all phonemes in the AR Transformer of VALL-E (2023), the alignment between the phonemes and the speech tokens is implicitly modeled by the self-attention of VALL-E (2023). This can be imprecise and causes errors like word omissions or hallucinations. Though ==RALL-E== introduces prosody CoT prompting to guide and stabilize the generation, we still find the model can fail to align in the experiments. We thus propose duration guided masking to fully utilize the intermediate duration results and boost the robustness.</p> <p>\u5982\u56fe 2 \u5de6\u4fa7\u6240\u793a, \u7531\u4e8e VALL-E (2023) \u7684\u81ea\u56de\u5f52 Transformer \u4e2d\u7684\u8bed\u97f3 token \u80fd\u591f\u5173\u6ce8\u5230\u6240\u6709\u97f3\u7d20, \u56e0\u6b64\u97f3\u7d20\u548c\u8bed\u97f3 token \u7684\u5bf9\u9f50\u662f\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u9690\u5f0f\u5efa\u6a21\u7684. \u7136\u800c, \u8fd9\u79cd\u5bf9\u9f50\u53ef\u80fd\u4e0d\u7cbe\u786e, \u5bfc\u81f4\u8bf8\u5982\u8bcd\u6c47\u9057\u6f0f\u6216\u5e7b\u89c9\u7b49\u9519\u8bef. \u5c3d\u7ba1 ==RALL-E== \u63d0\u51fa\u4e86\u97f5\u5f8b CoT \u63d0\u793a\u6765\u5f15\u5bfc\u548c\u7a33\u5b9a\u751f\u6210, \u4f46\u6211\u4eec\u4ecd\u7136\u53d1\u73b0\u6a21\u578b\u5728\u5b9e\u9a8c\u4e2d\u53ef\u80fd\u65e0\u6cd5\u5bf9\u9f50. \u56e0\u6b64, \u6211\u4eec\u63d0\u51fa\u4e86\u65f6\u957f\u5bfc\u5411\u63a9\u7801 (duration-guided masking) \u6765\u5145\u5206\u5229\u7528\u4e2d\u95f4\u65f6\u957f\u7ed3\u679c\u5e76\u63d0\u5347\u9c81\u68d2\u6027.</p> <p>As the right side of Fig.02 illustrates, in the proposed duration-guided masking, the speech token is restricted to only attend on a phoneme (prosody token) window centered at the phoneme (prosody token) it corresponds to. We define the window size ask, thus each speech token can attend on $2k + 1$ phonemes and $2k + 1$ prosody tokens. All phonemes and prosody tokens at other positions will be masked out, hence their attention weights are always zero. When $k = 0$ the speech token strictly attends to the phoneme it corresponds to. If the alignment is perfect this should be enough. However, in the experiments, we found that the alignment results obtained by our alignment tool usually have errors. We thus loosen the restriction by also allowing the speech token to attend at the near phonemes of the corresponding phoneme. Another reason for this design is that the pronunciation of a phoneme is usually dependent on near phonemes. As one will see in Section.4.3 and Appendix.A, the experimental results verify the effectiveness of this design.</p> <p>\u5982\u56fe 2 \u53f3\u4fa7\u6240\u793a, \u5728\u6240\u63d0\u51fa\u7684\u65f6\u957f\u5bfc\u5411\u63a9\u7801\u4e2d, \u8bed\u97f3 token \u53ea\u80fd\u5173\u6ce8\u5230\u4e0e\u5176\u5bf9\u5e94\u7684\u97f3\u7d20 (\u97f5\u5f8b token) \u5904\u4e8e\u4e2d\u5fc3\u4f4d\u7f6e\u7684\u7a97\u53e3\u5185\u7684\u97f3\u7d20 (\u97f5\u5f8b token). \u6211\u4eec\u5b9a\u4e49\u7a97\u53e3\u5927\u5c0f\u4e3a $k$, \u56e0\u6b64\u6bcf\u4e2a\u8bed\u97f3 token \u53ef\u4ee5\u5173\u6ce8\u5230 $2k + 1$ \u4e2a\u97f3\u7d20\u548c $2k + 1$ \u4e2a\u97f5\u5f8b token. \u6240\u6709\u5176\u4ed6\u4f4d\u7f6e\u7684\u97f3\u7d20\u548c\u97f5\u5f8b token \u90fd\u5c06\u88ab\u63a9\u7801\u6389, \u56e0\u6b64\u5b83\u4eec\u7684\u6ce8\u610f\u529b\u6743\u91cd\u603b\u662f\u4e3a\u96f6. \u5f53 $k = 0$ \u65f6, \u8bed\u97f3 token \u4e25\u683c\u5173\u6ce8\u5230\u5b83\u5bf9\u5e94\u7684\u97f3\u7d20. \u7136\u800c, \u5728\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u53d1\u73b0\u6240\u83b7\u5f97\u7684\u5bf9\u9f50\u7ed3\u679c\u7ecf\u5e38\u5b58\u5728\u9519\u8bef. \u56e0\u6b64, \u6211\u4eec\u901a\u8fc7\u5141\u8bb8\u8bed\u97f3 token \u5173\u6ce8\u5230\u4e0e\u5176\u5bf9\u5e94\u7684\u97f3\u7d20\u76f8\u90bb\u7684\u97f3\u7d20\u6765\u653e\u677e\u9650\u5236. \u53e6\u4e00\u4e2a\u539f\u56e0\u662f, \u4e00\u4e2a\u97f3\u7d20\u7684\u53d1\u97f3\u901a\u5e38\u4f9d\u8d56\u4e8e\u9644\u8fd1\u7684\u97f3\u7d20. \u5982 \u7b2c 4.3 \u8282 \u548c \u9644\u5f55 A \u6240\u793a, \u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8fd9\u79cd\u8bbe\u8ba1\u7684\u6709\u6548\u6027.</p> <p>For the NAR Transformer, we obtained almost no gain when applying the proposed masking strategy to it in our preliminary experiments. Thus we only apply the masking strategy on the AR Transformer.</p> <p>\u5bf9\u4e8e\u975e\u81ea\u56de\u5f52 Transformer, \u6211\u4eec\u5728\u521d\u6b65\u5b9e\u9a8c\u4e2d\u5e76\u672a\u83b7\u5f97\u6240\u63d0\u51fa\u7684\u63a9\u7801\u7b56\u7565\u7684\u660e\u663e\u6536\u76ca. \u56e0\u6b64, \u6211\u4eec\u53ea\u5728\u81ea\u56de\u5f52 Transformer \u4e0a\u5e94\u7528\u63a9\u7801\u7b56\u7565.</p> <p>The general inference procedure follows VALL-E (2023) with two differences. First, before sampling the speech tokens $\\mathbf{c}{:,1}$ the prosody tokens $\\mathbf{p}$ and $\\mathbf{d}$ are sampled conditioning on the phoneme sequence $\\mathbf{x}$ and acoustic prompt $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$. Second, although normal language models depend on a special token <code>&lt;eos&gt;</code> as the stop condition, since we know the total duration $D = \\sum^L{t=1}d_t$, we propose a duration guided inference method that forces the inference to stop at the $D$-th step. This method ensures no phoneme is omitted or repeated as it continues the inference if the <code>&lt;eos&gt;</code> token is predicted before the $D$-th step and stops at the right step as guided by the predicted duration..</p> <p>\u6574\u4f53\u63a8\u7406\u8fc7\u7a0b\u9075\u5faa VALL-E (2023) \u7684\u4e00\u822c\u8fc7\u7a0b, \u4f46\u6709\u4e24\u4e2a\u5dee\u522b. \u9996\u5148, \u5728\u91c7\u6837\u8bed\u97f3 tokens $\\mathbf{c}{:,1}$ \u4e4b\u524d, \u5148\u91c7\u6837\u97f5\u5f8b tokens $\\mathbf{p}$ \u548c $\\mathbf{d}$ \u4f5c\u4e3a\u6761\u4ef6, \u6761\u4ef6\u662f\u97f3\u7d20\u5e8f\u5217 $\\mathbf{x}$ \u548c\u58f0\u5b66\u63d0\u793a $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$. \u5176\u6b21, \u867d\u7136\u666e\u901a\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u4e8e\u7279\u6b8a\u7b26\u53f7 <code>&lt;eos&gt;</code> \u4f5c\u4e3a\u505c\u6b62\u6761\u4ef6, \u4f46\u7531\u4e8e\u6211\u4eec\u77e5\u9053\u603b\u65f6\u957f $D = \\sum^L{t=1}d_t$, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65f6\u957f\u5bfc\u5411\u63a8\u7406\u65b9\u6cd5, \u5f3a\u5236\u63a8\u7406\u505c\u6b62\u4e8e $D$-th \u6b65. \u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u786e\u4fdd\u4e0d\u4f1a\u9057\u6f0f\u6216\u91cd\u590d\u4efb\u4f55\u97f3\u7d20, \u56e0\u4e3a\u5982\u679c\u5728 $D$-th \u6b65\u9884\u6d4b\u5230 <code>&lt;eos&gt;</code> \u7b26\u53f7, \u63a8\u7406\u5c31\u4f1a\u7ee7\u7eed, \u76f4\u5230\u9884\u6d4b\u7684\u65f6\u957f\u8fbe\u5230\u8981\u6c42\u7684\u65f6\u957f.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#4experiments","title":"4.Experiments","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#41setup","title":"4.1.Setup","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#data","title":"Data","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#model-configuration","title":"Model Configuration","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#training-and-inference","title":"Training and Inference","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#baseline-methods","title":"Baseline Methods","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#objective-metrics","title":"Objective Metrics","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#word-error-rate-wer","title":"Word Error Rate (WER)","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#reranked-wer-wer-r","title":"Reranked WER (WER-R)","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#substitution-sub-deletion-del-insertion-ins","title":"Substitution (Sub), Deletion (Del), Insertion (Ins)","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#utmos","title":"UTMOS","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#speaker-similarity-sim","title":"Speaker Similarity (SIM)","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#subjective-metrics","title":"Subjective Metrics","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#42main-results","title":"4.2.Main Results","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#43ablation-study","title":"4.3.Ablation Study","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#44evaluation-on-hard-sentences","title":"4.4.Evaluation on Hard Sentences","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#5conclusion","title":"5.Conclusion","text":"<p>This paper presents ==RALL-E==, a robust codec language modeling method with CoT prompting for TTS. To address the robustness problem of LLM-based TTS, ==RALL-E== (1) incorporates prosody features (pitch and duration) in the LLM as a CoT prompting to assist and stabilize the generation of speech tokens, and (2) proposes duration-guided masking that enforces the model to attend on relevant phonemes (prosody features) corresponding to each speech token. We conduct comprehensive objective and subjective evaluations and demonstrate that ==RALL-E== can significantly improve the robustness of LLM-based TTS compared to the baseline VALL-E (2023) and two previous works. Furthermore, we show that ==RALL-E== can correctly synthesize sentences that are particularly hard to synthesize for VALL-E (2023) with a 4% error rate that even approaches the performance of non-autoregressive TTS.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#awindow-size-study","title":"A.Window Size Study","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#btranscript-of-the-50-hard-sentences","title":"B.Transcript of the 50 hard sentences","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/Authors/Jungil_Kong/","title":"Jungil Kong","text":""},{"location":"TTS/Papers/Authors/%E5%BC%A0%E8%87%AA%E5%BC%BA_%28Ziqiang_Zhang%29/","title":"\u5f20\u81ea\u5f3a (Ziqiang Zhang)","text":"<p>2020 \u4e2d\u56fd\u79d1\u6280\u5927\u5b66\u535a\u58eb\u5165\u5b66</p>"},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/","title":"StoryTTS: A Highly Expressive Text-to-Speech Dataset with Rich Textual Expressiveness Annotations","text":"\u4f5c\u8005 \u673a\u6784","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>While acoustic expressiveness has long been studied in expressive text-to-speech (ETTS), the inherent expressiveness in text lacks sufficient attention, especially for ETTS of artistic works. In this paper, we introduce StoryTTS, a highly ETTS dataset that contains rich expressiveness both in acoustic and textual perspective, from the recording of a Mandarin storytelling show. A systematic and comprehensive labeling framework is proposed for textual expressiveness. We analyze and define speech-related textual expressiveness in StoryTTS to include five distinct dimensions through linguistics, rhetoric, etc. Then we employ large language models and prompt them with a few manual annotation examples for batch annotation. The resulting corpus contains 61 hours of consecutive and highly prosodic speech equipped with accurate text transcriptions and rich textual expressiveness annotations. Therefore, StoryTTS can aid future ETTS research to fully mine the abundant intrinsic textual and acoustic features. Experiments are conducted to validate that TTS models can generate speech with improved expressiveness when integrating with the annotated textual labels in StoryTTS.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>The advancement of deep learning has significantly enhanced the quality of text-to-speech (TTS), enabling TTS models [1, 2, 3, 4, 5, 6] to produce speech that closely resembles human speech. However, these models tend to excel in synthesizing speech with relatively simple emotional characteristics. When it comes to expressive performance genres such as novels, poems, talk shows, and others, these methods still fall short of delivering the desired level of expressiveness. Such performances are usually rich in expressive text. This form of text influences the cadence and rhythm of the speaker\u2019s delivery and refers to a form of written language that can bring to life the meaning and feeling of the subject matter to be conveyed. A common approach is mine semantic and syntactic features from the expressive text. [7] introduced a TTS model that explicitly incorporates text-context semantic information extracted from a pre-trained BERT [8] model. This approach effectively enhances the expressiveness of the synthesized speech. In [9], a syntax graph is constructed for each input sentence based on its dependency tree. Following this, they employed a graph encoder to extract syntactic features, resulting in improved duration and pitch prediction.</p> <p>However, these studies often rely on the coarse-grained semantic representations of pre-trained language models or basic syntactic structures, and they have not conducted a comprehensive and thorough exploration of textual expressiveness, especially speech-related expressiveness. To achieve natural and expressive TTS synthesis, it\u2019s essential to convey the emotional stance of the text, which requires identifying speech-related textual expressive features in the text. This is also supported by literature [10], where a thorough investigation was conducted into the relationship between prosody and linguistics. This study underscored that elements like the emotional content of the text, sentence patterns, and syntax have a direct impact on reading expressiveness. Hence, understanding how to generalize, summarize, and characterize speech-related expressive features from expressive text might be crucial for expressive TTS.</p> <p>In this paper, we introduce StoryTTS, a highly expressive TTS dataset with rich expressiveness from both acoustic and textual perspectives. We initially construct the dataset from the recording of a Mandarin storytelling show with careful revision of transcripts and punctuations. Then we establish a systematic and comprehensive labeling framework for textual expressiveness. Specifically, we analyze and define speech-related textual expressiveness in StoryTTS to include five distinct dimensions through linguistics, rhetoric, and literary studies. These dimensions include rhetorical devices, sentence patterns, scenes, imitated characters, and emotional colors. Then we employ large language models (LLMs) and prompt them with a few manual annotation examples for batch annotation. The resulting corpus contains 61 hours of consecutive and highly prosodic speech equipped with accurate text transcriptions and rich textual expressiveness annotations. We further conduct experiments to validate that TTS models can produce speech with enhanced expressiveness when integrating the annotations from StoryTTS. Our contributions can be summarized as follows:</p> <ul> <li>We construct StoryTTS, the first TTS dataset that contains rich expressiveness in both speech and texts and is also equipped with comprehensive annotations for speech-related textual expressiveness. This dataset is also of high acoustic quality, organized by consecutive chapters, and of sufficient size. We release the StoryTTS dataset online. </li> <li>We establish a framework powered by LLMs to annotate textual expressiveness in five distinct dimensions. </li> <li>We conduct experiments to validate that TTS models can produce speech with enhanced expressiveness when integrating the annotated textual expressiveness labels.</li> </ul>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#2storytts","title":"2.StoryTTS","text":"<p>The construction of StoryTTS is introduced in detail in this section, including data selection and retrieval, audio quality analysis, speech segmentation and automatic recognition, manual correction of recognition errors, and the enhancement of punctuation. The detailed statistics of StoryTTS are shown in Table 1.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#21data-selecting-retrieval","title":"2.1.Data Selecting &amp; Retrieval","text":"<p>Storytelling, also known as \u201cPingshu\u201d (\u8bc4\u4e66), is a traditional Chinese oral art form where performers narrate stories, imitate various voices, and portray characters to enthrall audiences. This form of oral art is usually based on historical novels, which makes storytelling not only rich in speech prosody but also diverse in textual expressiveness, such as linguistic structures, figures of speech, role-playing, etc. Hence, storytelling shows satisfy our goal to a great extent. We selected a storytelling show titled \u201cZhi Sheng Dong fang Shuo\u201d (\u667a\u5723\u4e1c\u65b9\u6714), recounting the legend of Dongfang Shuo (\u4e1c\u65b9\u6714), a key figure in the development of the Han Dynasty in ancient China. This performance is skillfully delivered by a female artist, Lian Liru (\u8fde\u4e3d\u5982). To construct the dataset, we retrieved the recorded speech data from a public website, which is organized into 160 consecutive chapters. These chapters have an approximate duration of 24 minutes each, hence amounting to 64 hours in total, including interval breaks.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#22audio-quality-analysis","title":"2.2.Audio Quality Analysis","text":"<p>We also estimated the signal-to-noise ratio (SNR) of the speech data, where the noise power was computed using the silence segments predicted by a voice activity detection (VAD) tool. As can be seen in Table 1, the SNR is estimated to be 32dB, indicating the high audio quality of the waveforms. Subsequently, we conducted statistical analysis on multiple common Mandarin (ZH) and English (EN) datasets, as shown in Table 2.The results reveal that StoryTTS exhibits a significantly higher pitch standard deviation than other datasets, providing compelling evidence of its substantial acoustic expressiveness. Furthermore, StoryTTS includes expressiveness annotations, which will be further explained in the next section.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#23speech-segmentation-automatic-recognition","title":"2.3.Speech Segmentation &amp; Automatic Recognition","text":"<p>To process the originally coarsely segmented speech data, we implemented a three-step approach. We first employed a VAD tool to segment the chapter-level speech into utterances based on the duration of silence segments. Long silences were also removed in this step, resulting in 60.9 hours of speech. Subsequently, given the absence of matching text transcripts, we obtained text transcripts using Whisper [17], a popular speech recognition model. We observed that there still exist speech segments that remained excessively long after the VAD process. To address this issue, we identified these speech segments and their corresponding texts. We manually divided the prolonged text into smaller sentences and then utilized the Aeneas2 tool for synchronizing text fragments with speech. This alignment allowed us to accurately cut the speech, producing a final dataset of 33108 pairs of speech and text.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#24manual-correction-of-recognition-errors","title":"2.4.Manual correction of recognition errors","text":"<p>Given the extremely variable pitch and speaking rate in the storytelling performances, the speech recognition results exhibited a higher error rate compared to standard speech. In response to this challenge, we meticulously reviewed every speech segment line by line and rectified the recognition errors. Furthermore, we have made diligent efforts to replace onomatopoeic elements in the speech with appropriate words from the corresponding text.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#25punctuation-enhancement","title":"2.5.Punctuation enhancement","text":"<p>Punctuation plays a vital role in text presentation, conveying emotions like surprise or shock through exclamation points and indicating character dialogue or inner thoughts through double quotes. Although Whisper can identify some punctuation marks, it still falls far short of expectations. Thus, during our text review process, we made careful punctuation corrections and additions to ensure precise punctuation usage whenever possible. This attention to punctuation accuracy also significantly benefited our subsequent work on textual sentiment analysis.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#3llm-driven-expressiveness-annotation","title":"3.LLM-Driven Expressiveness Annotation","text":"<p>The expressive texts within StoryTTS exhibit a high degree of colloquialism and are rich in role-playing, psychological, action, and environmental descriptions. Inspired and guided by [10], we introduced a systematic and comprehensive labeling framework that harnesses the power of LLMs.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#31exploring-textual-expressiveness","title":"3.1.Exploring Textual Expressiveness","text":"<p>In our investigation of speech-related textual expressiveness, we classify it into five dimensions drawn from the fields of literary studies, linguistics, and rhetoric. These dimensions include rhetorical devices, sentence patterns, scenes, imitated characters, and emotional colors.</p> <p>Rhetorical devices, like hyperbole, and sentence patterns, such as declarative sentences, are commonly employed textual expressive devices. For instance, using an exclamatory sentence or incorporating rhetorical devices like hyperbole can evoke emotions of excitement or surprise. We also employed scenes such as roleplaying, taking into account the characteristics of StoryTTS. For example, a role-playing scene often carries strong emotional content, while an aside typically lacks emotional elements. The specific categorization regarding sentence patterns, scenes, and rhetorical devices is shown in Table 3.</p> <p>Emotional colors often have a direct impact on the performer\u2019s expression. Instead of categorizing them into various polarities or predefined categories, we have chosen a more precise approach: summarizing the emotional color of a sentence using several words. This method can provide a more accurate description of the text\u2019s emotion compared to traditional categorization. In the case of imitated characters in StoryTTS, the performer often mimics the speech patterns of characters when delivering their lines. For example, she deliberately lowers the pitch and slows down when portraying an old man while raising the pitch and speeding up when mimicking a villain. We categorized the characters into 19 role types based on characteristics like age, gender, and status, which also served as the basis for the performer\u2019s mimicry. The six most frequent role types are shown in Figure 2.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#32batch-annotation-via-llms","title":"3.2.Batch annotation via LLMs","text":"<p>LLMs have found applications in a wide range of studies.[18] demonstrated that GPT3 [19] performs well in data annotation tasks at a relatively low cost, making it a viable choice for individuals and organizations with limited budgets. To expedite the labeling process and reduce costs, we utilized GPT4 [20] and Claude2 [21], both of which are more powerful than GPT3, for batch annotation. During our annotation process, Claude2 was used for annotating sentence patterns, rhetorical devices, scenes, and imitated characters. However, when it came to summarizing emotional colors in the text, we found Claude2\u2019s performance to be suboptimal. Consequently, we turned to GPT4, which proved to be more proficient in this aspect. In our prompt, we initiated the persona of a linguist for the LLMs. We then proceeded to guide the model, informing it that the input texts were consecutive and required labeling with contextual information. For instance, two consecutive sentences might belong to the same role-play scenario. Subsequently, we provided details about the sources and features of the text, emphasizing their richness in elements such as onomatopoeia, inner monologue, and role-plays. Finally, we instructed the model to annotate each sentence in a prescribed format, adhering to the prompts and requirements outlined. These annotations align with the guidelines detailed in Section 3, where sentence patterns, scenes, rhetorical devices and imitated characters must be assigned specific categories, and each emotional color should be summarized in several words. Initially, we attempted labeling in the zero-shot setting, but the results exhibited low accuracy. Consequently, we transitioned to the few-shot setting, where we provided the model with comprehensive and diverse labeled text and explained the rationale behind each labeling decision. In this setting, the model demonstrates improved accuracy and meets our labeling requirements. An illustration of annotation using LLMs is shown in Figure 1.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#4experiments-results","title":"4.Experiments &amp; Results","text":"<p>In this section, we build TTS models to analyze the impact of annotated textual expressiveness labels on synthetic speech.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#41model-architecture","title":"4.1.Model Architecture","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#411baseline-model","title":"4.1.1.Baseline Model","text":"<p>Our baseline model is implemented based on VQTTS [22], which utilizes self-supervised vector-quantized (VQ) [23, 24, 25, 26], acoustic features rather than traditional mel-spectrogram. Specifically, it consists of an acoustic model, t2v, and a vocoder, v2w. T2v accepts the phoneme sequence and then outputs the VQ Acoustic feature and Auxiliary feature, which consist of pitch, energy, and probability of voice [27], and v2w receives them and thus synthesizes the waveform. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#412expressiveness-encoder","title":"4.1.2.Expressiveness Encoder","text":"<p>To fully leverage our expressiveness annotations, we developed an expressiveness encoder. We employed four separate learnable embedding tables to supply information to the model for the four labels: sentence pattern, scene, rhetorical method, and imitated character. For each sentence, we assigned four category numbers based on these four expressive labels. We then inputted these numbers into the corresponding embedding tables, with vector dimensions of 32, 32, 64, and 256, respectively.</p> <p>Regarding the modeling of emotional color, we employed distinct model structures. Given that emotion descriptions typically condense into several words, representing the overall sentiment of a sentence, while emotions may change within a single sentence. For instance, in an exclamatory sentence, emotion often intensifies towards the end. We initially extracted word-level embeddings for the entire sentence using a pre-trained BERT. Subsequently, we extracted the embedding of emotional color using a Sentence BERT. Through cross-attention [28] between these embeddings, we aimed to capture the distribution of emotions at different locations within the text, enhancing their expressive accuracy. Following this, we up-sampled the results to the phoneme level based on the word-to-phoneme correspondence and added them to the encoder output, along with the previous four embeddings. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#42experimental-setup","title":"4.2.Experimental setup","text":"<p>We conducted experiments to evaluate the impact of each of the five textual expressiveness labels on the expressiveness of the synthesized speech. Additionally, we assessed the cumulative effect of utilizing all these labels together. For these experiments, we trained an acoustic model separately for 300 epochs using a batch size equal to 8. The vocoder was shared, and we trained 100 epochs on StoryTTS using a batch size of 8. The remaining model configurations and parameters remained consistent with those in [29]. Each experiment was performed on a single 2080 Ti GPU. To preprocess the text data, we utilized our internal Grapheme-to-Phoneme (G2P) tool for text-to-phoneme conversion. We also set aside 5% of the text for test and validation sets, where the test set consists of 3 consecutive chapters. To obtain ground truth phoneme duration, we employed the Montreal Forced Aligner [30], which conducts forced alignment using Kaldi [31].</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#43speech-synthesis-evaluation","title":"4.3.Speech Synthesis Evaluation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#431metrics","title":"4.3.1.Metrics","text":"<p>We performed a mean opinion score (MOS) listening test involving 20 native listeners who were asked to rate each sample. MOS ratings were based on a 1-5 scale with 0.5-point increments and 95% confidence intervals. During our tests, we instructed listeners to specifically assess the level of expressiveness in the synthesized speech, all while evaluating speech quality. For objective evaluations, we computed Mel-cepstral distortion (MCD) using dynamic time warping (DTW). Additionally, we analyzed the log F0 root mean square error (log-F0 RMSE), also computed with DTW. MCD measures general speech quality, while log-F0 RMSE assesses performance in terms of speech prosody. Lower values for both of these metrics indicate better sound quality and rhythm in speech performance.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#432results","title":"4.3.2.Results","text":"<p>Table 4 presents the evaluation results. It can be seen that when incorporating the expressiveness labels into the model, both the subjective and objective scores outperform the baseline model. Specifically, sentence patterns and scene boosts were relatively minimal. This might be attributed to the prevalence of declarative sentences in the dataset, resulting in limited information acquired by the model. Additionally, while scene types are distributed fairly evenly, their diversity is insufficient to furnish the model with adequate information, owing to the different character imitations found in role-playing and inner monologue scenes. Rhetorical devices and emotional colors bring more obvious enhancements. Among the individual expressiveness labels, the imitated characters stand out as the most effective, as they directly provide information about the characters currently being mimicked, enabling the model to efficiently learn how the mimicked characters speak and thus synthesize speech close to that of the original data. </p> <p>Finally, the fusion of all expressive labels provides the most significant enhancement. It outperforms other setups significantly in both objective and subjective metrics, supplying the model with increasingly accurate information about imitated characters and scenes. This fusion also benefits from the complementary nature of sentence patterns, rhetorical devices, and emotional colors.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/Datasets/2024.04_StoryTTS/2024.04_StoryTTS/#5conclusions","title":"5.Conclusions","text":"<p>This paper presented StoryTTS, the first TTS dataset that encompasses rich expressiveness from both acoustic and textual perspectives. Derived from a high-quality recording of a Mandarin storytelling show, this dataset serves as a valuable resource for researchers aiming to investigate acoustic expressiveness on the one hand. Additionally, we conducted a comprehensive analysis of expressive text and categorized speech-related textual expressiveness into five distinct dimensions. Then we employed LLMs and provided them with a few manual annotation examples for batch annotation. The effective labeling of LLMs also offers insights for similar data labeling endeavors. The dataset is thus equipped with abundant textual expressiveness annotations.Experimental results demonstrated that TTS models can generate speech with significantly improved expressiveness when incorporated with the annotated textual expressiveness labels. Future work may focus on integrating these expressiveness annotations with acoustic expressiveness to further enhance expressive speech synthesis.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u60c5\u611f_Emotional"]},{"location":"TTS/Papers/LLM/2023.02_LLaMA/2023.02_LLaMA/","title":"LLaMA: Open and Efficient Foundation Language Models","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2023.02_LLaMA/2023.02_LLaMA/#abstract","title":"Abstract","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2023.02_LLaMA/2023.02_LLaMA/#1introduction","title":"1.Introduction","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2023.02_LLaMA/2023.02_LLaMA/#2related-works","title":"2.Related Works","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2023.02_LLaMA/2023.07_LLaMA2/","title":"Llama 2: Open Foundation and Fine-Tuned Chat Models","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2023.02_LLaMA/2023.07_LLaMA2/#abstract","title":"Abstract","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2023.02_LLaMA/2023.07_LLaMA2/#1introduction","title":"1.Introduction","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2023.02_LLaMA/2023.07_LLaMA2/#2related-works","title":"2.Related Works","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/","title":"OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework","text":"\u4f5c\u8005 \u673a\u6784 Sachin Mehta Apple Mohammad Hossein Sekhavat Apple Qingqing Cao Apple Maxwell Horton Apple Yanzi Jin Apple Chenfan Sun Apple Iman Mirzadeh Apple Mahyar Najibi Apple Dmitry Belenko Apple Peter Zatloukal Apple Mohammad Rastegari Apple","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release ==OpenELM==, a state-of-the-art open language model. ==OpenELM== uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the Transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, ==OpenELM== exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2\u00d7 fewer pre-training tokens.</p> <p>\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u590d\u73b0\u6027\u548c\u900f\u660e\u5ea6\u5bf9\u4e8e\u5148\u8fdb\u5f00\u653e\u7814\u7a76\u81f3\u5173\u91cd\u8981, \u786e\u4fdd\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6, \u80fd\u591f\u6df1\u5165\u7814\u7a76\u6570\u636e\u548c\u6a21\u578b\u504f\u89c1, \u4ee5\u53ca\u6f5c\u5728\u98ce\u9669. \u4e3a\u6b64, \u6211\u4eec\u53d1\u5e03\u4e86 OpenELM, \u4e00\u4e2a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u8bed\u8a00\u6a21\u578b. OpenELM \u4f7f\u7528\u9010\u5c42\u7f29\u653e\u7b56\u7565\u4ee5\u6709\u6548\u5730\u5728 Transformer \u6a21\u578b\u7684\u6bcf\u4e2a\u5c42\u4e2d\u5206\u914d\u53c2\u6570, \u4ece\u800c\u589e\u5f3a\u7cbe\u5ea6. \u4f8b\u5982, \u5bf9\u4e8e\u4e00\u4e2a\u53c2\u6570\u89c4\u6a21\u7ea6\u4e3a 1 \u4ebf\u7684\u6a21\u578b, OpenELM \u76f8\u8f83\u4e8e OLMo \u63d0\u9ad8\u4e86 2.36% \u7684\u51c6\u786e\u7387, \u800c\u53ea\u8981\u6c42 2 \u500d\u5c11\u4e8e OLMo \u7684\u9884\u8bad\u7ec3\u6570\u636e.</p> <p>Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.</p> <p>\u548c\u4e4b\u524d\u7684\u7814\u7a76\u53ea\u63d0\u4f9b\u6a21\u578b\u6743\u91cd\u548c\u63a8\u7406\u4ee3\u7801, \u4ee5\u53ca\u5728\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u4e0d\u540c, \u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u5b8c\u6574\u6846\u67b6, \u5305\u62ec\u8bad\u7ec3\u65e5\u5fd7, \u591a\u4e2a\u68c0\u67e5\u70b9, \u4ee5\u53ca\u9884\u8bad\u7ec3\u914d\u7f6e. \u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u7528\u4e8e\u5c06\u6a21\u578b\u8f6c\u6362\u4e3a MLX \u5e93\u7528\u4e8e\u5728 Apple \u8bbe\u5907\u4e0a\u8fdb\u884c\u63a8\u7406\u548c\u5fae\u8c03\u7684\u4ee3\u7801. \u6b64\u5b8c\u6574\u7684\u53d1\u5e03\u65e8\u5728\u6fc0\u53d1\u548c\u52a0\u5f3a\u5f00\u653e\u7814\u7a76\u793e\u533a, \u4e3a\u672a\u6765\u5f00\u653e\u7814\u7a76\u5960\u5b9a\u57fa\u7840.</p> <p>Our source code along with pre-trained model weights and training recipes is available at https://github.com/apple/corenet. Additionally, ==OpenELM== models can be found on HuggingFace at: https://huggingface.co/apple/OpenELM.</p> <p>\u6211\u4eec\u7684\u6e90\u4ee3\u7801, \u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u548c\u8bad\u7ec3\u65b9\u6cd5\u90fd\u5728 Github \u4e0a\u53d1\u5e03, \u5730\u5740\u4e3a https://github.com/apple/corenet. \u53e6\u5916, OpenELM \u6a21\u578b\u53ef\u4ee5\u5728 HuggingFace \u4e0a\u627e\u5230, \u5730\u5740\u4e3a https://huggingface.co/apple/OpenELM.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Transformer-based [48] large language models (LLM) are revolutionizing the field of natural language processing [7,46]. These models are isotropic, meaning that they have the same configuration (e.g., number of heads and feed-forward network dimensions) for each Transformer layer. Though such isotropic models are simple, they may not allocate parameters efficiently inside the model.</p> <p>\u57fa\u4e8e Transformer \u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6b63\u5728\u53d8\u9769\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df. \u8fd9\u4e9b\u6a21\u578b\u662f\u5404\u5411\u540c\u6027\u7684, \u8fd9\u610f\u5473\u7740\u5b83\u4eec\u7684\u6bcf\u4e2a Transformer \u5c42\u90fd\u5177\u6709\u76f8\u540c\u7684\u914d\u7f6e (\u4f8b\u5982, \u6ce8\u610f\u529b\u5934\u6570\u548c\u524d\u9988\u7f51\u7edc\u7ef4\u5ea6). \u867d\u7136\u8fd9\u6837\u7684\u5404\u5411\u540c\u6027\u6a21\u578b\u5f88\u7b80\u5355, \u4f46\u5b83\u4eec\u53ef\u80fd\u5728\u6a21\u578b\u5185\u90e8\u4e0d\u5145\u5206\u5730\u5206\u914d\u53c2\u6570.</p> <p>In this work, we develop and release ==OpenELM==, a family of pre-trained and fine-tuned models on publicly available datasets. At the core of ==OpenELM== lies layer-wise scaling [30], enabling more efficient parameter allocation across layers. This method utilizes smaller latent dimensions in the attention and feed-forward modules of the Transformer layers closer to the input, and gradually widening the layers as they approach the output.</p> <p>\u672c\u9879\u5de5\u4f5c\u5f00\u53d1\u5e76\u53d1\u5e03\u4e86 OpenELM, \u4e00\u7c7b\u5728\u516c\u5f00\u53ef\u7528\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u6a21\u578b. OpenELM \u7684\u6838\u5fc3\u662f\u9010\u5c42\u7f29\u653e, \u5b83\u4f7f\u5f97\u53c2\u6570\u5206\u914d\u66f4\u52a0\u6709\u6548. \u8fd9\u79cd\u65b9\u6cd5\u5728 Transformer \u5c42\u7684\u6ce8\u610f\u529b\u6a21\u5757\u548c\u524d\u9988\u6a21\u5757\u4e2d\u4f7f\u7528\u8f83\u5c0f\u7684\u9690\u85cf\u5c42\u7ef4\u5ea6, \u5e76\u968f\u7740\u63a5\u8fd1\u8f93\u51fa\u4e0d\u65ad\u52a0\u5bbd.</p> <p>We release the complete framework, encompassing data preparation, training, fine-tuning, and evaluation procedures, alongside multiple pre-trained checkpoints and training logs, to facilitate open research. Importantly, ==OpenELM== outperforms existing open LLMs that are pre-trained using publicly available datasets (Tab.01).  For example, ==OpenELM== with 1.1 billion parameters outperforms OLMo [17], which has 1.2 billion parameters, by 2.36% while requiring 2\u00d7 fewer pre-training tokens.</p> <p>\u6211\u4eec\u53d1\u5e03\u4e86\u5b8c\u6574\u6846\u67b6, \u5305\u62ec\u6570\u636e\u51c6\u5907, \u8bad\u7ec3, \u5fae\u8c03, \u8bc4\u4f30\u8fc7\u7a0b, \u4ee5\u53ca\u591a\u4e2a\u9884\u8bad\u7ec3\u68c0\u67e5\u70b9\u548c\u8bad\u7ec3\u65e5\u5fd7, \u4ee5\u4fc3\u8fdb\u5f00\u653e\u7814\u7a76. \u91cd\u8981\u7684\u662f, OpenELM \u7684\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u7684\u5728\u516c\u5f00\u53ef\u7528\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b, \u53c2\u9605\u8868\u683c 01. \u4f8b\u5982, 1.1 \u4ebf\u53c2\u6570\u7684 OpenELM \u4f18\u4e8e 1.2 \u4ebf\u53c2\u6570\u7684 OLMo, \u4ec5\u9700 2 \u500d\u5c11\u4e8e OLMo \u7684\u9884\u8bad\u7ec3\u6570\u636e, \u4e14\u51c6\u786e\u7387\u63d0\u9ad8 2.36%.</p> <p></p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#2pre-training","title":"2.Pre-Training\u00b7\u9884\u8bad\u7ec3","text":"<p>This section describes the framework, including model architecture (2.1), pre-training data (2.2), training hyper-parameters (2.3), and evaluation (2.4).</p> <p>\u672c\u8282\u4ecb\u7ecd\u4e86\u6574\u4f53\u6846\u67b6, \u5305\u62ec\u6a21\u578b\u67b6\u6784, \u9884\u8bad\u7ec3\u6570\u636e, \u8bad\u7ec3\u8d85\u53c2\u6570\u548c\u8bc4\u4f30.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#21openelm-architecture","title":"2.1.OpenELM Architecture\u00b7\u67b6\u6784","text":"<p>We adopt the decoder-only Transformer-based architecture. Following state-of-the-art LLMs, we: 1. do not use learnable bias parameters in any fully-connected (a.k.a., linear) layers,  2. apply pre-normalization using RMSNorm [53] and also, use rotatory positional embedding (ROPE) [43] for encoding positional information,  3. use Grouped Query Attention (GQA) [1] instead of Multi-Head Attention (MHA),  4. replace the feed forward network (FFN) with SwiGLU FFN [41],  5. use flash attention [13] for computing the scaled dot-product attention, 6. use the same tokenizer as LLaMA.</p> <p>\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u4ec5\u7528 Transformer \u89e3\u7801\u5668\u7684\u67b6\u6784. \u9075\u5faa\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b, \u6211\u4eec: 1. \u4e0d\u5728\u4efb\u4f55\u5168\u8fde\u63a5\u5c42\u4e2d\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u504f\u7f6e\u53c2\u6570; 2. \u4f7f\u7528 RMSNorm \u8fdb\u884c\u9884\u5f52\u4e00\u5316, \u5e76\u4f7f\u7528\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165 (ROPE) \u6765\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f; 3. \u4f7f\u7528 Grouped Query Attention (GQA) \u800c\u4e0d\u662f\u591a\u5934\u6ce8\u610f\u529b; 4. \u5c06\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u66ff\u6362\u4e3a SwiGLU FFN; 5. \u4f7f\u7528 Flash Attention \u8ba1\u7b97\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b; 6. \u4f7f\u7528\u548c LLaMA \u76f8\u540c\u7684\u5206\u8bcd\u5668.</p> <p>Existing LLMs use the same configuration for each Transformer layer in the model, resulting in a uniform allocation of parameters across layers. Unlike these models, each Transformer layer in ==OpenELM== has a different configuration (e.g., number of heads and feed forward network dimension), resulting in variable number of parameters in each layer of the model. This lets ==OpenELM== to better utilize the available parameter budget for achieving higher accuracies. We implement this non-uniform allocation of parameters across layers using layer-wise scaling (also referred as block-wise scaling in [30]).</p> <p>\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u6bcf\u4e2a Transformer \u5c42\u90fd\u4f7f\u7528\u76f8\u540c\u7684\u914d\u7f6e, \u4f7f\u5f97\u53c2\u6570\u5206\u914d\u5728\u5404\u5c42\u4e0a\u662f\u5747\u5300\u7684. \u548c\u8fd9\u4e9b\u6a21\u578b\u4e0d\u540c, OpenELM \u4e2d\u6bcf\u4e2a Transformer \u5c42\u90fd\u6709\u4e0d\u540c\u914d\u7f6e (\u4f8b\u5982, \u6ce8\u610f\u529b\u5934\u6570\u548c\u524d\u9988\u7f51\u7edc\u7ef4\u5ea6), \u4f7f\u5f97\u6a21\u578b\u6bcf\u5c42\u90fd\u5177\u6709\u4e0d\u540c\u53c2\u6570\u6570\u91cf. \u8fd9\u4f7f\u5f97 OpenELM \u80fd\u591f\u66f4\u597d\u5730\u5229\u7528\u53ef\u7528\u53c2\u6570\u9884\u7b97\u5b9e\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u7387. \u6211\u4eec\u4f7f\u7528\u9010\u5c42\u7f29\u653e (\u4e5f\u79f0\u4e3a\u5757\u7ea7\u7f29\u653e) \u6765\u5b9e\u73b0\u975e\u5747\u5300\u5206\u914d\u53c2\u6570.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#layer-wise-scaling","title":"Layer-Wise Scaling\u00b7\u9010\u5c42\u7f29\u653e","text":"<p>A standard Transformer layer is composed of multi-head attention (MHA) and feed-forward network (FFN). For non-uniform allocation of parameters in the Transformer layer, we adjust the number of attention heads and the FFN multiplier in each Transformer layer.</p> <p>\u4e00\u4e2a\u6807\u51c6\u7684 Transformer \u5c42\u7531\u591a\u5934\u6ce8\u610f\u529b (MHA) \u548c\u524d\u9988\u7f51\u7edc (FFN) \u7ec4\u6210. \u4e3a\u4e86\u5b9e\u73b0\u975e\u5747\u5300\u5206\u914d\u53c2\u6570, \u6211\u4eec\u8c03\u6574\u6bcf\u4e2a Transformer \u5c42\u7684\u6ce8\u610f\u529b\u5934\u6570\u548c FFN \u4e58\u6570.</p> <p>Assume that the standard Transformer model with uniform parameter allocation has $N$ Transformer layers and the dimensionality of the input to each layer is $d_{model}$. The MHA has $n_h$ heads and dimension of each head is $d_h=\\dfrac{d_{model}}{n_h}$. Also, the hidden dimension for FFN is $d_{FFN}= m\\cdot d_{model}$, where $m$ is a scalar FFN multiplier.</p> <p>\u5047\u8bbe\u5747\u5300\u53c2\u6570\u5206\u914d\u7684\u6807\u51c6\u7684 Transformer \u6a21\u578b\u6709 $N$ \u4e2a Transformer \u5c42, \u8f93\u5165\u5230\u6bcf\u4e2a\u5c42\u7684\u7ef4\u5ea6\u4e3a $d_{model}$. \u591a\u5934\u6ce8\u610f\u529b\u6709 $n_h$ \u4e2a\u6ce8\u610f\u529b\u5934, \u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u7684\u7ef4\u5ea6\u4e3a $d_h=\\dfrac{d_{model}}{n_h}$. \u524d\u9988\u7f51\u7edc\u7684\u9690\u85cf\u5c42\u7ef4\u5ea6\u4e3a $d_{FFN}= m\\cdot d_{model}$, \u5176\u4e2d $m$ \u662f\u6807\u91cf\u7684 FFN \u4e58\u6570.</p> <p>We introduce parameters $\\alpha$ and $\\beta$ to scale the number of attention heads $n_h$ and FFN multiplier $m$ per layer respectively. For the $i$-th layer, $n_h$ and $m$ are computed as </p> <p>\u6211\u4eec\u5f15\u5165\u53c2\u6570 $\\alpha$ \u548c $\\beta$ \u6765\u5206\u522b\u7f29\u653e\u6bcf\u4e2a\u5c42\u7684\u6ce8\u610f\u529b\u5934\u6570 $n_h$ \u548c\u524d\u9988\u7f51\u7edc\u4e58\u6570 $m$. \u5bf9\u4e8e\u7b2c $i$ \u5c42, $n_h$ \u548c $m$ \u8ba1\u7b97\u5982\u4e0b:</p> <p>$$   n_h^i = \\dfrac{\\alpha^i\\cdot d_{model}}{d_h},\\quad m^i = \\beta^i $$</p> <p>where</p> <p>\u5176\u4e2d</p> <p>$$   \\alpha^i = \\alpha_{min} + \\dfrac{\\alpha_{max} - \\alpha_{min}}{N-1}\\cdot i $$</p> <p>$$   \\beta^i = \\beta_{min} + \\dfrac{\\beta_{max} - \\beta_{min}}{N-1}\\cdot i $$</p> <p>$$   0\\leq i &lt;N $$</p> <p>Here, $\\alpha_{min}$ and $\\alpha_{max}$ are the hyper-parameters that allow us to scale the attention heads. Similarly, $\\beta_{min}$ and $\\beta_{max}$ let us to vary the width of FFN layers. Therefore, varying the configuration of standard Transformer layers using $\\alpha$ and $\\beta$ results in non-uniform allocation of parameters in the model. Note, setting $\\alpha_{min}=\\alpha_{max}= 1.0$ and $m_i= m$ produces the standard uniform Transformer model.</p> <p>\u8fd9\u91cc\u7684 $\\alpha_{min}$ \u548c $\\alpha_{max}$ \u662f\u5141\u8bb8\u6211\u4eec\u7f29\u653e\u6ce8\u610f\u529b\u5934\u7684\u8d85\u53c2\u6570. \u7c7b\u4f3c\u5730, $\\beta_{min}$ \u548c $\\beta_{max}$ \u5141\u8bb8\u6211\u4eec\u8c03\u6574 FFN \u5c42\u7684\u5bbd\u5ea6. \u56e0\u6b64, \u4f7f\u7528 $\\alpha$ \u548c $\\beta$ \u8c03\u6574\u6807\u51c6 Transformer \u5c42\u7684\u914d\u7f6e, \u4f7f\u5f97\u6a21\u578b\u4e2d\u7684\u53c2\u6570\u5206\u914d\u4e0d\u5747\u5300. \u6ce8\u610f, \u8bbe\u7f6e $\\alpha_{min}=\\alpha_{max}= 1.0$ \u548c $m_i= m$ \u5c06\u5f97\u5230\u6807\u51c6\u5747\u5300\u5206\u914d\u53c2\u6570\u7684 Transformer \u6a21\u578b.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#22pre-training-data","title":"2.2.Pre-Training Data\u00b7\u9884\u8bad\u7ec3\u6570\u636e","text":"<p>For pre-training, we use public datasets. Specifically, our pre-training dataset contains RefinedWeb [35], deduplicated PILE [15], a subset of RedPajama [11], and a subset of Dolma v1.6 [42], totaling approximately 1.8 trillion tokens. These details are also summarized in Tab.02.</p> <p>\u5bf9\u4e8e\u9884\u8bad\u7ec3, \u6211\u4eec\u4f7f\u7528\u516c\u5f00\u6570\u636e\u96c6. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5305\u62ec RefinedWeb, \u53bb\u91cd\u7684 PILE, RedPajama \u7684\u5b50\u96c6, Dolma v1.6 \u7684\u5b50\u96c6, \u603b\u8ba1\u7ea6 1.8 \u4e07\u4ebf\u4e2a Token. \u8fd9\u4e9b\u7ec6\u8282\u5728\u8868\u683c 02 \u4e2d\u8fdb\u884c\u4e86\u603b\u7ed3.</p> Source Subset Tokens RefinedWeb 665 B RedPajama GithubBooksArXivWikipediaStackExchangeC4 59 B26 B28 B24 B20 B175 B PILE 207 B Dolma The StackRedditPeS2oProject GutenbergWikipedia + Wikibooks 411 B89 B70 B6 B4.3B","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#on-the-fly-tokenization-data-filtering","title":"On-the-Fly Tokenization &amp; Data Filtering\u00b7\u5373\u65f6\u5206\u8bcd\u548c\u6570\u636e\u8fc7\u6ee4","text":"<p>Unlike previous approaches that utilize pre-tokenized data [5,17], we filter and tokenize text data on-the-fly. This facilitates seamless experimentation with various tokenizers, thereby significantly simplifying prototyping and research endeavors. In our experiments, we use the same tokenizer as used in LLaMA.</p> <p>\u4e0e\u4e4b\u524d\u4f7f\u7528\u9884\u5206\u8bcd\u6570\u636e\u7684\u65b9\u6cd5\u4e0d\u540c, \u6211\u4eec\u8fc7\u6ee4\u5e76\u5373\u65f6\u5206\u8bcd\u6587\u672c\u6570\u636e. \u8fd9\u6837\u53ef\u4ee5\u4fc3\u8fdb\u4e0d\u540c\u5206\u8bcd\u5668\u7684\u65e0\u7f1d\u5b9e\u9a8c, \u4ece\u800c\u5927\u5927\u7b80\u5316\u539f\u578b\u8bbe\u8ba1\u548c\u7814\u7a76\u5de5\u4f5c. \u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u91c7\u7528 LLaMA \u7684\u5206\u8bcd\u5668.</p> <p>To filter out low-length sequences, we apply two filtering methods. The first method operates at the character-level, checking if the number of characters in the sequence is below a specified threshold. The second method operates at the token-level, where it examines whether the sequence contains fewer tokens than a specified threshold. Sequences that are shorter than either of these thresholds are skipped. In our experiments, we use 200 characters and 256 tokens as character and token-level filtering thresholds.</p> <p>\u4e3a\u4e86\u8fc7\u6ee4\u6389\u4f4e\u957f\u5ea6\u7684\u5e8f\u5217, \u6211\u4eec\u91c7\u7528\u4e24\u79cd\u8fc7\u6ee4\u65b9\u6cd5. \u7b2c\u4e00\u79cd\u65b9\u6cd5\u5728\u5b57\u7b26\u7ea7\u522b\u4e0a\u8fdb\u884c, \u68c0\u67e5\u5e8f\u5217\u4e2d\u5b57\u7b26\u7684\u6570\u91cf\u662f\u5426\u4f4e\u4e8e\u6307\u5b9a\u9608\u503c. \u7b2c\u4e8c\u79cd\u65b9\u6cd5\u5728 Token \u7ea7\u522b\u4e0a\u8fdb\u884c, \u5b83\u68c0\u67e5\u5e8f\u5217\u4e2d Token \u7684\u6570\u91cf\u662f\u5426\u5305\u542b\u5c11\u4e8e\u6307\u5b9a\u9608\u503c. \u5982\u679c\u5e8f\u5217\u957f\u5ea6\u4f4e\u4e8e\u8fd9\u4e24\u79cd\u9608\u503c\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a, \u5219\u8df3\u8fc7\u8be5\u5e8f\u5217. \u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u4f7f\u7528 200 \u4e2a\u5b57\u7b26\u548c 256 \u4e2a Token \u4f5c\u4e3a\u5b57\u7b26\u7ea7\u522b\u548c Token \u7ea7\u522b\u7684\u8fc7\u6ee4\u9608\u503c.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#23training-details","title":"2.3.Training Details\u00b7\u8bad\u7ec3\u7ec6\u8282","text":"<p>We train ==OpenELM== variants for 350k iterations (or training steps) using CoreNet (formerly CVNets [29]). We use AdamW as an optimizer. We use a cosine learning rate schedule [27], with warm up of 5k iterations, and decay the final learning rate down to 10% of maximum learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. We train four variants of ==OpenELM== (270M, 450M, 1.1B, and 3B), and for some, we use FSDP [56] and activation checkpointing [8]. Please refer to Appendix A for additional pre-training details.</p> <p>\u6211\u4eec\u4f7f\u7528 CoreNet (\u524d\u8eab\u4e3a CVNets) \u8bad\u7ec3 350k \u6b21\u8fed\u4ee3/\u8bad\u7ec3\u6b65\u6570\u5f97\u5230 OpenELM \u53d8\u4f53. \u6211\u4eec\u4f7f\u7528 AdamW \u4f5c\u4e3a\u4f18\u5316\u5668. \u6211\u4eec\u4f7f\u7528\u4f59\u5f26\u5b66\u4e60\u7387\u8c03\u5ea6, \u5176\u4e2d\u4e94\u5343\u6b21\u8fed\u4ee3\u9884\u70ed, \u7136\u540e\u8870\u51cf\u6700\u7ec8\u5b66\u4e60\u7387\u5230\u6700\u5927\u5b66\u4e60\u7387\u7684 10%. \u6211\u4eec\u4f7f\u7528\u6743\u91cd\u8870\u51cf 0.1 \u548c\u68af\u5ea6\u88c1\u526a 1.0. \u6211\u4eec\u8bad\u7ec3\u56db\u4e2a OpenELM \u53d8\u4f53 (270M, 450M, 1.1B, 3B), \u5176\u4e2d\u4e00\u4e9b\u4f7f\u7528 FSDP \u548c\u6fc0\u6d3b\u68c0\u67e5\u70b9. \u8bf7\u53c2\u9605\u9644\u5f55 A (\u8868\u683c 09) \u83b7\u53d6\u66f4\u591a\u9884\u8bad\u7ec3\u7ec6\u8282.</p> <p></p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#24evaluation-details","title":"2.4.Evaluation Details\u00b7\u8bc4\u4f30\u7ec6\u8282","text":"<p>Following previous works, we evaluate the performance across different tasks using LM Evaluation Harness [16]:</p> <p>\u9075\u5faa\u4e4b\u524d\u7684\u5de5\u4f5c, \u6211\u4eec\u4f7f\u7528 LM Evaluation Harness [16] \u8bc4\u4f30\u5728\u4e0d\u540c\u4efb\u52a1\u7684\u6027\u80fd:</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#standard-zero-shot-tasks","title":"Standard Zero-Shot Tasks\u00b7\u6807\u51c6\u96f6\u6837\u672c\u4efb\u52a1","text":"<p>We consider 7 standard common-sense reasoning tasks: ARC easy and challenge [10], BoolQ [9], HellaSwag [52], PIQA [6], SciQ [49], and WinoGrande [39].</p> <p>\u6211\u4eec\u8003\u8651\u4e03\u4e2a\u6807\u51c6\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1: - ARC easy - ARC challenge - BoolQ - HellaSwag - PIQA - SciQ - WinoGrande</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#openllm-leaderboard-tasksopenllm","title":"OpenLLM Leaderboard Tasks\u00b7OpenLLM \u6392\u884c\u699c\u4efb\u52a1","text":"<p>We use 5 tasks from OpenLLM leaderboard [4]: ARC challenge, HellaSwag, MMLU [20], TruthfulQA [24], and WinoGrande.</p> <p>\u6211\u4eec\u4f7f\u7528 OpenLLM \u6392\u884c\u699c\u7684\u4e94\u4e2a\u4efb\u52a1: - ARC challenge - HellaSwag - MMLU - TruthfulQA - WinoGrande</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#llm360-leaderboard-tasksllm360","title":"LLM360 Leaderboard Tasks\u00b7LLM360 \u6392\u884c\u699c\u4efb\u52a1","text":"<p>We use 7 tasks from LLM360 leaderboard [26] for evaluation: ARC challenge, CrowS-Pairs (English version) [32], HellaSwag, WinoGrande, MMLU, PIQA, and RACE [23].</p> <p>\u6211\u4eec\u4f7f\u7528 LLM360 \u6392\u884c\u699c\u7684\u4e03\u4e2a\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30: - ARC challenge - CrowS-Pairs (\u82f1\u6587\u7248\u672c) - HellaSwag - WinoGrande - MMLU - PIQA - RACE</p> <p>These evaluation frameworks, built on top of LM Evaluation Harness, allows us to comprehensively evaluate ==OpenELM== in terms of reasoning (e.g., ARC-c, HellaSwag, and PIQA), knowledge understanding (e.g., MMLU and RACE), and misinformation &amp; bias (e.g., TruthfulQA and CrowS-Pairs).</p> <p>\u8fd9\u4e9b\u5efa\u7acb\u5728 LM Evaluation Harness \u4e4b\u4e0a\u7684\u8bc4\u4f30\u6846\u67b6, \u5141\u8bb8\u6211\u4eec\u5168\u9762\u8bc4\u4f30 OpenELM \u5728\u63a8\u7406 (ARC-c, HellaSwag, PIQA), \u77e5\u8bc6\u7406\u89e3 (MMLU \u548c RACE) \u548c\u8bef\u5bfc\u6027\u4fe1\u606f\u548c\u504f\u89c1 (TruthfulQA \u548c CrowS-Pairs) \u65b9\u9762\u7684\u80fd\u529b.</p> <p>While there may be some overlap in tasks among these frameworks, they primarily differ in the few-shot settings, as outlined in Tab.03.</p> <p>\u5c3d\u7ba1\u8fd9\u4e9b\u6846\u67b6\u4e2d\u7684\u4efb\u52a1\u53ef\u80fd\u5b58\u5728\u91cd\u53e0, \u4f46\u5b83\u4eec\u4e3b\u8981\u533a\u522b\u5728\u4e8e\u5c11\u6837\u672c\u8bbe\u7f6e, \u5982\u8868\u683c 03 \u6240\u793a.</p> <p></p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#3experimental-results","title":"3.Experimental Results\u00b7\u5b9e\u9a8c\u7ed3\u679c","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#pre-training-results","title":"Pre-Training Results\u00b7\u9884\u8bad\u7ec3\u7ed3\u679c","text":"<p>We evaluate the performance of ==OpenELM== on zero-shot and few-shot settings (Tab.03). We compare ==OpenELM== with publicly available LLMs, namely PyThia [5], Cerebras-GPT [14], TinyLlama [54], OpenLM [18], MobiLlama [44], and OLMo [17]. The works most closely related to ours are MobiLlama and OLMo. These models are trained on comparable dataset mixtures, with similar or larger number of pre-training tokens.</p> <p>\u6211\u4eec\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30 OpenELM \u7684\u6027\u80fd (\u8868\u683c 03). \u6211\u4eec\u6bd4\u8f83 OpenELM \u4e0e\u516c\u5f00\u53ef\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b, \u5305\u62ec PyThia, Cerebras-GPT, TinyLlama, OpenLM, MobiLlama, \u548c OLMo. \u4e0e\u6211\u4eec\u7684\u5de5\u4f5c\u6700\u63a5\u8fd1\u7684\u6a21\u578b\u662f MobiLlama \u548c OLMo. \u8fd9\u4e9b\u6a21\u578b\u90fd\u5728\u89c4\u6a21\u5dee\u8ddd\u4e0d\u5927\u7684\u6df7\u5408\u6570\u636e\u96c6\u4e2d\u8fdb\u884c\u8bad\u7ec3, \u5177\u6709\u76f8\u4f3c\u6216\u66f4\u5927\u7684\u9884\u8bad\u7ec3 Token \u6570\u91cf.</p> <p>In Fig.01, the accuracy of ==OpenELM== is plotted against training iterations for 7 standard zero-shot tasks. We observe an overall increase in accuracy with longer training durations across most tasks. Additionally, the checkpoint obtained by averaging the last five checkpoints, collected at intervals of 5000 iterations, demonstrates comparable or slightly better accuracy compared to the final checkpoint obtained after 350k iterations. This improvement is likely due to noise reduction through weight averaging. Consequently, we use the averaged checkpoint for our main evaluations in Tab.04, instruction tuning experiments in Tab.05, and parameter-efficient tuning experiments in Tab.06.</p> <p></p> <p>\u5728\u56fe 01 \u4e2d, \u6211\u4eec\u7ed8\u5236\u4e86 OpenELM \u5728 7 \u4e2a\u6807\u51c6\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u5ea6\u968f\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u7684\u53d8\u5316\u66f2\u7ebf. \u6211\u4eec\u89c2\u5bdf\u5230, \u5728\u5927\u591a\u6570\u4efb\u52a1\u4e2d, \u8bad\u7ec3\u6301\u7eed\u65f6\u95f4\u8d8a\u957f, \u51c6\u786e\u5ea6\u8d8a\u9ad8. \u6b64\u5916, \u6211\u4eec\u6536\u96c6\u7684\u6700\u540e\u4e94\u4e2a\u68c0\u67e5\u70b9\u7684\u5e73\u5747\u503c, \u95f4\u9694\u4e3a 5000 \u6b21\u8fed\u4ee3, \u4e0e 350k \u6b21\u8fed\u4ee3\u540e\u83b7\u5f97\u7684\u6700\u7ec8\u68c0\u67e5\u70b9\u7684\u51c6\u786e\u5ea6\u76f8\u5f53\u6216\u7a0d\u597d. \u8fd9\u79cd\u6539\u8fdb\u53ef\u80fd\u662f\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u800c\u5bfc\u81f4\u7684\u566a\u58f0\u964d\u4f4e. \u56e0\u6b64, \u6211\u4eec\u91c7\u7528\u5e73\u5747\u68c0\u67e5\u70b9\u7528\u4e8e\u8868\u683c 04 \u7684\u4e3b\u8981\u8bc4\u4f30, \u8868\u683c 05 \u7684\u6307\u4ee4\u8c03\u4f18\u5b9e\u9a8c, \u4ee5\u53ca\u8868\u683c 06 \u7684\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\u5b9e\u9a8c.</p> <p>The results in Tab.04 span across various evaluation frameworks, and highlights ==OpenELM==\u2019s effectiveness over existing methods.</p> <p>\u8868\u683c 04 \u4e2d\u7684\u7ed3\u679c\u6db5\u76d6\u4e86\u5404\u79cd\u8bc4\u4f30\u6846\u67b6, \u5e76\u7a81\u51fa\u4e86 OpenELM \u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6709\u6548\u6027.</p> <p></p> <p>For instance, an ==OpenELM== variant with 1.1 billion parameters achieves 1.28% (Tab.04a), 2.36% (Tab.04b), and 1.72% (Tab.04c) higher accuracy compared to OLMo with 1.2 billion parameters. Remarkably, ==OpenELM== achieves this level of accuracy while using 2\u00d7 less pretraining data.</p> <p>\u4f8b\u5982 1.1 \u4ebf\u53c2\u6570\u7684 OpenELM \u53d8\u4f53\u76f8\u5bf9\u4e8e 1.2 \u4ebf\u53c2\u6570\u7684 OLMo, \u51c6\u786e\u5ea6\u63d0\u9ad8\u4e86 1.28%, 2.36%, \u548c 1.72% \u70b9. \u9700\u8981\u6ce8\u610f\u7684\u662f, OpenELM \u53d6\u5f97\u4e86\u5982\u6b64\u9ad8\u7684\u51c6\u786e\u5ea6, \u800c\u4ec5\u4f7f\u7528\u5176\u4e00\u534a\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u6570\u636e.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#instruction-tuning-results","title":"Instruction Tuning Results\u00b7\u6307\u4ee4\u8c03\u4f18\u7ed3\u679c","text":"<p>We use the cleaned variant of UltraFeedback [3, 12] dataset that consists of 60k prompts for instruction tuning. We do instruction tuning using Alignment Handbook library [47]. For optimization, we use either the statistical rejection sampling method [25] or the direct preference optimization method [37]. These sampling method details along with other hyper-parameters and fine-tuning details are given in Appendix B.</p> <p>\u6211\u4eec\u4f7f\u7528 UltraFeedback \u6570\u636e\u96c6\u7684\u6e05\u6d17\u7248\u672c, \u5305\u542b\u516d\u4e07\u4e2a\u63d0\u793a\u7528\u4e8e\u6307\u4ee4\u8c03\u4f18. \u6211\u4eec\u4f7f\u7528 Alignment Handbook \u5e93\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18. \u5bf9\u4e8e\u4f18\u5316, \u6211\u4eec\u91c7\u7528\u7edf\u8ba1\u62d2\u7edd\u91c7\u6837\u65b9\u6cd5\u6216\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5. \u8fd9\u4e9b\u91c7\u6837\u65b9\u6cd5\u7684\u8be6\u7ec6\u4fe1\u606f\u4ee5\u53ca\u5176\u4ed6\u8d85\u53c2\u6570\u548c\u5fae\u8c03\u7ec6\u8282, \u8bf7\u53c2\u9605\u9644\u5f55 B.</p> <p>We conducted a grid search to determine optimal values for the learning rate and training epochs.  For the learning rate, we explored values in the range of [2e-5, 3e-5, 5e-5, 8e-5, 1e-4], while for training epochs, we investigated the range of [3, 5, 8, 10].  The final recipe selected is the one that yielded the highest average accuracy across various tasks as presented in Tab.03a and Tab.03c.</p> <p>\u6211\u4eec\u91c7\u7528\u7f51\u683c\u641c\u7d22\u786e\u5b9a\u6700\u4f73\u7684\u5b66\u4e60\u7387\u548c\u8bad\u7ec3 Epoch \u6570. \u5bf9\u4e8e\u5b66\u4e60\u7387, \u6211\u4eec\u63a2\u7d22\u8303\u56f4\u4e3a [2e-5, 3e-5, 5e-5, 8e-5, 1e-4], \u800c\u5bf9\u4e8e\u8bad\u7ec3 Epoch, \u6211\u4eec\u8c03\u67e5\u8303\u56f4\u4e3a [3, 5, 8, 10]. \u6700\u7ec8\u7684\u9009\u62e9\u662f\u90a3\u4e9b\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u83b7\u5f97\u5e73\u5747\u51c6\u786e\u5ea6\u6700\u9ad8\u7684\u914d\u7f6e.</p> <p>We fine-tune all the models with BFloat16 as a data type. We use activation checkpointing along with gradient accumulation with a step size of two.  We use the AdamW optimizer with default beta values.  We use the cosine learning rate scheduler with a warm-up ratio of 0.1, and we set the weight decay to 0 and loss temperature beta to 0.01. We set the maximum context length to 1024 and maximum prompt length to 512. Other hyper-parameters are included in Tab.10.</p> <p>\u6211\u4eec\u4f7f\u7528 BFloat16 \u4f5c\u4e3a\u6570\u636e\u7c7b\u578b\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03. \u6211\u4eec\u4f7f\u7528\u6fc0\u6d3b\u68c0\u67e5\u70b9\u548c\u68af\u5ea6\u7d2f\u79ef, \u6b65\u957f\u4e3a 2. \u6211\u4eec\u4f7f\u7528 AdamW \u4f18\u5316\u5668, \u5e76\u4f7f\u7528\u9ed8\u8ba4\u7684\u53c2\u6570. \u6211\u4eec\u4f7f\u7528\u4f59\u5f26\u5b66\u4e60\u7387\u8c03\u5ea6, \u9884\u70ed\u7387\u4e3a 0.1, \u6743\u91cd\u8870\u51cf\u4e3a 0, \u635f\u5931\u6e29\u5ea6\u53c2\u6570\u4e3a 0.01. \u6211\u4eec\u8bbe\u7f6e\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e3a 1024, \u6700\u5927\u63d0\u793a\u957f\u5ea6\u4e3a 512. \u5176\u4ed6\u8d85\u53c2\u6570\u8bf7\u53c2\u9605\u8868\u683c 10.</p> <p></p> <p>Tab.05 shows that instruction tuning consistently improves ==OpenELM==\u2019s average accuracy by 1-2% across different evaluation frameworks.</p> <p>\u8868\u683c 05 \u5c55\u793a\u4e86\u6307\u4ee4\u8c03\u4f18\u5728\u4e0d\u540c\u8bc4\u4f30\u6846\u67b6\u4e0b\u7684\u5e73\u5747\u51c6\u786e\u5ea6\u63d0\u9ad8 1-2% \u5de6\u53f3.</p> <p></p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#parameter-efficient-fine-tuning-peft-results","title":"Parameter-Efficient Fine-Tuning (PEFT) Results\u00b7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7ed3\u679c","text":"<p>We use the CommonSense reasoning training and evaluation setup [22]. This setup provides 170k training samples across 8 multiple-choice datasets for PEFT studies with different methods, including LoRA [21] and DoRA [51]. We integrate ==OpenELM== with these methods, and finetune the resulting model for three epochs using 8 NVIDIA H100 GPUs. Tab.06 shows that PEFT methods can be applied to ==OpenELM==. LoRA and DoRA deliver similar accuracy on average across the given CommonSense reasoning datasets.</p> <p>\u6211\u4eec\u4f7f\u7528 CommonSense \u63a8\u7406\u8bad\u7ec3\u548c\u8bc4\u4f30\u8bbe\u7f6e. \u8be5\u8bbe\u7f6e\u63d0\u4f9b\u4e86 170k \u8bad\u7ec3\u6837\u672c, 8 \u4e2a\u591a\u9009\u6570\u636e\u96c6, \u7528\u4e8e PEFT \u7814\u7a76, \u5305\u62ec LoRA \u548c DoRA. \u6211\u4eec\u5c06 OpenELM \u4e0e\u8fd9\u4e9b\u65b9\u6cd5\u96c6\u6210, \u5e76\u4f7f\u7528 8 \u4e2a NVIDIA H100 GPU \u5fae\u8c03\u7ed3\u679c\u6a21\u578b, \u5fae\u8c03 3 \u4e2a Epoch. \u8868\u683c 06 \u5c55\u793a\u4e86 PEFT \u65b9\u6cd5\u53ef\u4ee5\u5e94\u7528\u4e8e OpenELM. LoRA \u548c DoRA \u5728\u7ed9\u5b9a\u7684 CommonSense \u63a8\u7406\u6570\u636e\u96c6\u4e0a, \u5e73\u5747\u51c6\u786e\u5ea6\u76f8\u5f53.</p> <p></p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#4benchmarking","title":"4.Benchmarking\u00b7\u57fa\u51c6\u6d4b\u8bd5","text":"","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#hardware","title":"Hardware\u00b7\u786c\u4ef6","text":"<p>We benchmark on modern, consumer-grade hardware with BFloat16 as the data type. Specifically, CUDA benchmarks were performed on a workstation with an Intel i9-13900KF CPU, equipped with 64 GB of DDR5-4000 DRAM, and an NVIDIA RTX 4090 GPU with 24 GB of VRAM, running Ubuntu 22.04. PyTorch v2.2.2 [34] was used, with the most recent versions of models and the associated libraries. HuggingFace Transformers v4.39.3 [50] was used to benchmark HuggingFace models. We did not use Torch Inductor for model compilation.</p> <p>\u6211\u4eec\u5728\u73b0\u4ee3\u6d88\u8d39\u8005\u7ea7\u786c\u4ef6\u4e0a\u91c7\u7528 BFloat16 \u4f5c\u4e3a\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5. \u5177\u4f53\u6765\u8bf4, CUDA \u57fa\u51c6\u662f\u5728\u4e00\u53f0\u5de5\u4f5c\u7ad9\u4e0a\u8fdb\u884c\u7684, \u5176 CPU \u4e3a Intel i9-13900KF, \u5185\u5b58\u4e3a 64 GB DDR5-4000, GPU \u4e3a NVIDIA RTX 4090, \u64cd\u4f5c\u7cfb\u7edf\u4e3a Ubuntu 22.04. \u6211\u4eec\u4f7f\u7528 PyTorch v2.2.2, \u5e76\u4f7f\u7528\u6700\u65b0\u7248\u672c\u7684\u6a21\u578b\u548c\u76f8\u5173\u5e93. \u6211\u4eec\u4f7f\u7528 HuggingFace Transformers v4.39.3 \u4f5c\u4e3a HuggingFace \u6a21\u578b\u57fa\u51c6. \u6211\u4eec\u6ca1\u6709\u4f7f\u7528 Torch Inductor \u8fdb\u884c\u6a21\u578b\u7f16\u8bd1.</p> <p>To benchmark ==OpenELM== models on the Apple silicon, we used an Apple MacBook Pro with an M2 Max system-on-chip and 64GiB of RAM, running macOS 14.4.1. We ported the code and the weights of ==OpenELM== to Apple MLX v0.10.0 [19]. To maximize the throughput, lazy evaluation was used in MLX with 8 tokens evaluated at a time.</p> <p>\u4e3a\u4e86\u5728 Apple silicon \u4e0a\u5bf9 OpenELM \u6a21\u578b\u8fdb\u884c\u57fa\u51c6, \u6211\u4eec\u4f7f\u7528\u4e00\u53f0 Apple MacBook Pro, \u5176\u7cfb\u7edf\u82af\u7247\u4e3a M2 Max, \u5185\u5b58\u4e3a 64GiB, \u64cd\u4f5c\u7cfb\u7edf\u4e3a macOS 14.4.1. \u6211\u4eec\u5c06 OpenELM \u4ee3\u7801\u548c\u6743\u91cd\u79fb\u690d\u5230 Apple MLX v0.10.0. \u4e3a\u4e86\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u541e\u5410\u91cf, \u6211\u4eec\u5728 MLX \u4e2d\u4f7f\u7528\u60f0\u6027\u8bc4\u4f30, \u4e00\u6b21\u8bc4\u4f30 8 \u4e2a Token.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#evaluation","title":"Evaluation\u00b7\u8bc4\u4f30","text":"<p>We provide two separate measurements for token throughput (measured in terms of tokens processed per second): (1) prompt processing (pre-fill), and (2) token generation. Additionally, we also report the total combined throughput. We benchmark all models sequentially, and execute one full \u201cdry run\u201d generating 1024 tokens for the first model, since we found that this significantly increases the throughput of generation for subsequent models. Before measurement for each individual model, we warm up the model by executing a single forward pass to allow the frameworks to perform further auto-tuning, if any. In all experiments, we use key-value caching and generate 1024 tokens in addition to the prompt tokens in all tests. Static key-value cache was used whenever supported. The same prompt was used for all runs, resulting in prompt lengths of 35-36 tokens (depending on the tokenizer).</p> <p>\u6211\u4eec\u63d0\u4f9b\u4e24\u4e2a\u5355\u72ec\u7684\u5ea6\u91cf\u7528\u4e8e Token \u541e\u5410\u91cf (\u4ee5\u6bcf\u79d2\u5904\u7406 Token \u4e3a\u5355\u4f4d): 1. \u63d0\u793a\u5904\u7406 (\u9884\u586b\u5145) 2. Token \u751f\u6210</p> <p>\u6b64\u5916, \u6211\u4eec\u8fd8\u62a5\u544a\u603b\u4f53\u541e\u5410\u91cf. \u6211\u4eec\u6309\u987a\u5e8f\u5bf9\u6240\u6709\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5, \u5e76\u5bf9\u7b2c\u4e00\u4e2a\u6a21\u578b\u6267\u884c\u5b8c\u6574\u7684 \"\u5e72\u51c0\u8fd0\u884c\", \u56e0\u4e3a\u6211\u4eec\u53d1\u73b0, \u8fd9\u6837\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u540e\u7eed\u6a21\u578b\u751f\u6210\u7684\u541e\u5410\u91cf. \u5728\u5bf9\u6bcf\u4e2a\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u4e4b\u524d, \u6211\u4eec\u901a\u8fc7\u6267\u884c\u4e00\u6b21\u524d\u5411\u4f20\u64ad\u6765\u9884\u70ed\u6a21\u578b, \u4ee5\u4fbf\u6846\u67b6\u5b8c\u6210\u8fdb\u4e00\u6b65\u7684\u81ea\u52a8\u8c03\u4f18, \u5982\u679c\u6709. \u5728\u6240\u6709\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u90fd\u4f7f\u7528\u952e\u503c\u7f13\u5b58, \u5e76\u5728\u6240\u6709\u6d4b\u8bd5\u4e2d\u751f\u6210 1024 \u4e2a Token, \u800c\u63d0\u793a Token \u6570\u91cf\u53d6\u51b3\u4e8e Tokenizer. \u9759\u6001\u952e\u503c\u7f13\u5b58\u662f\u53ef\u9009\u7684. \u6240\u6709\u8fd0\u884c\u90fd\u662f\u7528\u76f8\u540c\u7684\u63d0\u793a, \u56e0\u6b64\u63d0\u793a\u957f\u5ea6\u4e3a 35-36 \u4e2a Token.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#results","title":"Results\u00b7\u7ed3\u679c","text":"<p>Tab.07a and Tab.07b shows the benchmarking results on GPU and MacBook Pro respectively. Despite ==OpenELM==\u2019s higher accuracy for a similar parameter count, we observe that it is slower than OLMo. While the primary focus of this study is reproducibility rather than inference performance, we did comprehensive profiling to understand the bottlenecks. Our analysis reveals that a significant portion of ==OpenELM==\u2019s processing time can be attributed to our naive implementation of RMSNorm (Tab.08). Specifically, naive RMSNorm implementation results in many individual kernel launches each of which processes a small input, rather than a launch of a single, fused kernel, as would be the case with e.g. LayerNorm. By replacing the naive RMSNorm with Apex\u2019s RMSNorm [33], we observe a notable increase in ==OpenELM==\u2019s throughput. However, a substantial performance gap persists compared to the models that use optimized LayerNorm, in part because (1) ==OpenELM== has 113 RMSNorm layers as compared to 33 LayerNorm layers in OLMo and (2) Apex\u2019s RMSNorm is not optimized for small inputs. To further illustrate the performance degradation attributable to RMSNorm, we replaced the LayerNorm in OLMo with RMSNorm, and observed a significant drop in generation throughput. In future work, we plan to explore optimization strategies to further improve the inference efficiency of ==OpenELM==.</p> <p>\u8868\u683c 07a \u548c\u8868\u683c 07b \u5206\u522b\u5c55\u793a\u4e86 GPU \u548c MacBook Pro \u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c. \u5c3d\u7ba1\u5728\u76f8\u4f3c\u7684\u53c2\u6570\u6570\u91cf\u4e0b OpenELM \u7684\u51c6\u786e\u5ea6\u66f4\u9ad8, \u4f46\u6211\u4eec\u89c2\u5bdf\u5230\u5b83\u6bd4 OLMo \u6162. \u5c3d\u7ba1\u672c\u7814\u7a76\u7684\u4e3b\u8981\u5173\u6ce8\u70b9\u662f\u53ef\u590d\u73b0\u6027\u800c\u4e0d\u662f\u63a8\u7406\u6027\u80fd, \u4f46\u6211\u4eec\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790, \u4ee5\u7406\u89e3\u74f6\u9888. \u6211\u4eec\u7684\u5206\u6790\u8868\u660e, OpenELM \u7684\u5927\u90e8\u5206\u5904\u7406\u65f6\u95f4\u90fd\u548c\u6211\u4eec\u5bf9 RMSNorm \u7684\u539f\u59cb\u5b9e\u73b0\u6709\u5173 (\u8868\u683c 08). \u5177\u4f53\u6765\u8bf4, \u539f\u59cb RMSNorm \u5b9e\u73b0\u4f7f\u5f97\u542f\u52a8\u8bb8\u591a\u5904\u7406\u5c0f\u8f93\u5165\u7684\u6838, \u800c\u4e0d\u662f\u542f\u52a8\u5355\u4e2a\u7684\u878d\u5408\u6838, \u5982 LayerNorm. \u901a\u8fc7\u5c06\u539f\u59cb RMSNorm \u66ff\u6362\u4e3a Apex \u7684 RMSNorm, \u6211\u4eec\u89c2\u5bdf\u5230 OpenELM \u7684\u541e\u5410\u91cf\u663e\u8457\u63d0\u9ad8. \u7136\u800c, \u548c\u4f7f\u7528 LayerNorm \u7684\u6a21\u578b\u76f8\u6bd4\u4ecd\u7136\u5b58\u5728\u5b9e\u8d28\u7684\u6027\u80fd\u5dee\u8ddd, \u90e8\u5206\u539f\u56e0\u662f (1) OpenELM \u6709 113 \u4e2a RMSNorm \u5c42, \u800c OLMo \u6709 33 \u4e2a LayerNorm \u5c42 (2) Apex \u7684 RMSNorm \u5e76\u672a\u9488\u5bf9\u5c0f\u8f93\u5165\u8fdb\u884c\u4f18\u5316. \u4e3a\u4e86\u8fdb\u4e00\u6b65\u8bf4\u660e RMSNorm \u5bfc\u81f4\u7684\u6027\u80fd\u9000\u5316, \u6211\u4eec\u5c06 OLMo \u4e2d\u7684 LayerNorm \u66ff\u6362\u4e3a RMSNorm, \u5e76\u89c2\u5bdf\u5230\u751f\u6210\u541e\u5410\u91cf\u663e\u8457\u4e0b\u964d. \u5728\u672a\u6765\u5de5\u4f5c\u4e2d, \u6211\u4eec\u8ba1\u5212\u63a2\u7d22\u4f18\u5316\u7b56\u7565, \u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8 OpenELM \u7684\u63a8\u7406\u6548\u7387.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#5conclusion","title":"5.Conclusion\u00b7\u7ed3\u8bba","text":"<p>This work releases ==OpenELM==, a decoder-only Transformer-based open language model. The ==OpenELM== uses a layer-wise scaling method for efficient parameter allocation within the Transformer model,resulting in improved accuracy compared to existing models. Additionally, we have made the entire framework open-source, including training logs, multiple checkpoints, pre-training configurations, and MLX inference code. This extensive release aims to empower and strengthen the open research community, facilitating future research efforts.</p> <p>\u672c\u9879\u5de5\u4f5c\u53d1\u5e03\u4e86 OpenELM, \u4e00\u4e2a\u57fa\u4e8e\u4ec5\u6709 Transformer \u89e3\u7801\u5668\u7684\u5f00\u6e90\u8bed\u8a00\u6a21\u578b. OpenELM \u4f7f\u7528\u9010\u5c42\u7f29\u653e\u65b9\u6cd5, \u6709\u6548\u5730\u5728 Transformer \u6a21\u578b\u4e2d\u5206\u914d\u53c2\u6570, \u4ece\u800c\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u6bd4, \u51c6\u786e\u5ea6\u5f97\u5230\u63d0\u9ad8. \u6b64\u5916, \u6211\u4eec\u5df2\u7ecf\u5c06\u6574\u4e2a\u6846\u67b6\u5f00\u6e90, \u5305\u62ec\u8bad\u7ec3\u65e5\u5fd7, \u591a\u4e2a\u68c0\u67e5\u70b9, \u9884\u8bad\u7ec3\u914d\u7f6e, \u548c MLX \u63a8\u7406\u4ee3\u7801. \u8fd9\u4e00\u8be6\u5c3d\u53d1\u5e03\u65e8\u5728\u6fc0\u53d1\u548c\u52a0\u5f3a\u5f00\u653e\u7814\u7a76\u793e\u533a, \u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u5de5\u4f5c.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/LLM/2024.04_OpenELM/2024.04_OpenELM/#broader-impact","title":"Broader Impact\u00b7\u66f4\u5e7f\u6cdb\u7684\u5f71\u54cd","text":"<p>The release of ==OpenELM== models aims to empower and enrich the open research community by providing access to state-of-the-art language models. Trained on publicly available datasets, these models are made available without any safety guarantees. Consequently, there exists the possibility of these models producing outputs that are inaccurate, harmful, biased, or objectionable in response to user prompts. Thus, it is imperative for users and developers to undertake thorough safety testing and implement appropriate filtering mechanisms tailored to their specific requirements.</p> <p>OpenELM \u6a21\u578b\u7684\u53d1\u5e03\u65e8\u5728\u6fc0\u53d1\u548c\u4e30\u5bcc\u5f00\u653e\u7814\u7a76\u793e\u533a, \u63d0\u4f9b\u5bf9\u6700\u65b0\u8bed\u8a00\u6a21\u578b\u7684\u8bbf\u95ee. \u8fd9\u4e9b\u6a21\u578b\u5728\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3, \u65e0\u4efb\u4f55\u5b89\u5168\u4fdd\u8bc1. \u6240\u4ee5\u7528\u6237\u548c\u5f00\u53d1\u8005\u5e94\u8ba4\u771f\u6d4b\u8bd5\u8fd9\u4e9b\u6a21\u578b\u7684\u5b89\u5168\u6027, \u5e76\u6839\u636e\u5177\u4f53\u9700\u6c42\u5b9e\u73b0\u9002\u5f53\u7684\u8fc7\u6ee4\u673a\u5236. \u5bf9\u4e8e\u7528\u6237\u548c\u5f00\u53d1\u8005\u6765\u8bf4, \u4fdd\u8bc1\u6a21\u578b\u7684\u51c6\u786e\u6027, \u5b89\u5168\u6027, \u516c\u5e73\u6027, \u4ee5\u53ca\u4e0d\u53d7\u7528\u6237\u63d0\u793a\u7684\u5e72\u6270\u662f\u81f3\u5173\u91cd\u8981\u7684.</p>","tags":["\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/","title":"DL Based Expressive Speech Synthesis","text":"<p>@import \"../style.less\"</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#deep-learning-based-expressive-speech-synthesis-a-systematic-review-of-approaches-challenges-and-resources","title":"Deep Learning-Based Expressive Speech Synthesis: A Systematic Review of Approaches,  Challenges, and Resources\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8868\u73b0\u6027\u8bed\u97f3\u5408\u6210: \u65b9\u6cd5, \u6311\u6218, \u8d44\u6e90\u7684\u7cfb\u7edf\u6027\u56de\u987e","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#a","title":"A.\u6458\u8981","text":"<p>Speech synthesis has made significant strides thanks to the transition from machine learning to deep learning models.  Contemporary text-to-speech (TTS) models possess the capability to generate speech of exceptionally high quality, closely mimicking human speech. Nevertheless, given the wide array of applications now employing TTS models, mere high-quality speech generation is no longer sufficient.  Present-day TTS models must also excel at producing expressive speech that can convey various speaking styles and emotions, akin to human speech. Consequently,researchers have concentrated their efforts on developing more efficient models for expressive speech synthesis in recent years. This paper presents a systematic review of the literature on expressive speech synthesis models published within the last 5 years, with a particular emphasis on approaches based on deep learning. We offer a comprehensive classification scheme for these models and provide concise descriptions of models falling into each category. Additionally, we summarize the principal challenges encountered in this research domain and outline the strategies employed to tackle these challenges as documented in the literature. In the Section 8, we pinpoint some research gaps in this field that necessitate further exploration. Our objective with this work is to give an all-encompassing overview of this hot research area to offer guidance to interested researchers and future endeavors in this field.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#1","title":"1.\u5f15\u8a00","text":"<p>Since the late 1950s, computer-based text-to-speech systems (TTS) have undergone significant advancements, culminating in the production of models that generate speech almost indistinguishable from that of a human. This progress has followed a path consisting of several stages, beginning with conventional methods named as concatenative synthesis and progressing to more advanced approaches known as Statistical Parametric Speech Synthesis (SPSS). Advanced approaches are mainly based on machine learning algorithms like hidden Markov models (HMMs) and gaussian mixture models (GMMs). Despite this progress, speech generated by these methods was still noticeably artificial. However, the emergence of deep learning (DL) as a new branch under machine learning (ML) in 2006 has led to significant improvements. Speech synthesis researchers,like many in other research fields, started incorporating deep neural networks (DNN) in their models. Initially,DNNs replaced HMMs and GMMs in SPSS models while the main structure still follows the primary framework of SPSS models as shown in Fig. 1. As discussed in \"Statistical Parametric Speech Synthesis Using Deep Neural Networks\", the deep learning-based models have overcome many limitations and problems associated with machine learning-based models.</p> <p>Researchers continue to aim for improved speech quality and more human-like speech despite past advancements. Additionally, they seek to simplify the framework of the text-to-speech models due to the intricate nature of the SPSS structure, which limits progress in this field to those with extensive linguistic knowledge and expertise. Deep learning advancements have brought about the simple encoder-decoder structure for TTS models as sequence-to-sequence (Seq2Seq) approaches. The pro-posed approaches have simplified the structure of conventional TTS with multiple components into training a single network that converts a set of input text characters/phonemes into a set of acoustic features (mel-spectrograms). A main concern in these advanced TTS models is the mapping process between the input and output sequences, which is a one-to-many problem, as the single input text can have multiple speech variations as output. In fact, there are two groups of recent TTS models, as shown in Fig.02. The first group generates mel-spectrograms in a sequential (autoregressive) manner using soft and automatic attention alignments between input and output sequences, such as the Tacotron model (Tacotron (2017) Tacotron2 (2017)). The second group utilizes hard alignments between the phonemes/characters and mel-spectro-grams, and thus its speech generation process is parallel(non-autoregressive), as in the FastSpeech model (FastSpeech (2019) FastSpeech2 (2020)).This improvement in the structure of the TTS model has encouraged rapid development in the field within the last few years, during which the proposed models produced speech that is nearly indistinguishable from human speech.</p> <p>Human speech is highly expressive and reflects various factors, such as the speaker\u2019s identity, emotion, and speaking style. In addition, there are many applications in which speech synthesis can be utilized, especially expressive speech synthesis. For instance, audiobooks and podcast applications that create audio versions of eBooks and podcasts, translation applications which provide real-time translation of foreign language text, dubbing applications that generate an alternative audio track for a video with different content, speaker, or language, and content creation applications which help produce audio versions of textual content, such as blogs and news articles. E-learning applications that allow for adding voice-over audio to e-learning courses, and conversational AI applications enable machines to communicate with users in a human-like manner, such as AI chatbots and virtual assistants.</p> <p>As spoken language is a crucial component in such applications, users must feel as if they are communicating with a real human rather than a machine. Therefore, the speech generated by these applications should convey appropriate emotion, intonation, stress, and speaking style to match the ongoing conversation or the content type and context of the text being read.</p> <p>As a result, there has been a recent attention towards building efficient expressive speech synthesis models as another step forward in achieving human-like speech. Therefore, many studies have been devoted to expressive speech synthesis (ETTS) as a hot research area, particularly over the last 5 years. In this work, we present the findings of our systematic literature review on ETTS field from the past 5 years. Firstly, we suggest a classification schema of deep learning-based ETTS models that are proposed during this period, based on structures,and learning methods followed in each study. A summary is then provided for each category in the classification schema and main papers related to this category. After that, we outline the main challenges in the ETTS area and solutions that have been proposed to solve them from literature. Finally, we conclude with a discussion of the implications of our work and a highlight of some gaps that require further research in this area.</p> <p>During our work on this review of expressive speech synthesis literature, we came across several review papers that focus on different stages of development in the speech synthesis field. The majority of these reviews concentrate on DL-based TTS approaches, while only a few papers cover recent TTS approaches in addition to early conventional ones. However, to the best of our knowledge, there are no review papers that cover the fast growth in the (expressive) speech synthesis area, especially in the last few years. Therefore, our main goal in this review is to provide an overview of research trends, techniques, and challenges in this area during this period. We hope that our work will offer researchers a comprehensive understanding of how and what has been accomplished in this field and the gaps that need to be filled as guidance for their future efforts.</p> <p>While we were writing this paper, we came across an interesting recent review paper \"An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era\" that is similar to our work. However, the review covers emotional speech synthesis (ESS) as a sub-field of voice transformation while our work is more comprehensive as a systematic literature review that discusses approaches,challenges, and resources. Furthermore, the taxonomy we provide for the reviewed approaches differs from the one given in as elaborated in the next section.</p> <p>The remaining sections of this paper are structured as follows: Section 2 provides an explanation of the methodology employed for conducting this review. Sections 3and 4 describe the different main and sub-categories of the proposed classification schema for DL-based expressive TTS models. Main challenges facing ETTS models and how they have been tackled in the literature are covered in Section 5. We then give a brief description of ETTS datasets and applied evaluation metrics in Sections 6 and 7, respectively. Finally, Section 8 concludes the paper.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#2","title":"2.\u65b9\u6cd5","text":"<p>The last few years have seen rapid growth in expressive and emotional speech synthesis approaches, resulting in a large number of papers and publications in this area. Here, we present the outcomes of a systematic literature review of the last 5 years\u2019 publications within this active research area. This section describes the methodology used to conduct the review, illustrated by Fig.03, which consists of three main stages: paper selection, paper exclusion, and paper classification.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#21","title":"2.1.\u8bba\u6587\u9009\u62e9","text":"<p>For our review, we used the Scopus database to retrieve papers as it encompasses most of the significant journals and conferences pertaining to the speech synthesis field. Our query criteria to find relevant papers on Scopus were twofold: (1) the paper title must include at least one of four words (emotion OR expressive OR prosod OR style) that denote expressive speech, and (2) the paper title, abstract, or keywords must comprise the terms \u201cspeech\u201d AND \u201csynthesis,\u201d in addition to at least one of the above-mentioned words for expressive speech. We considered all papers written in English and published in journals or conferences since 2018. The search query was conducted in January 2023, and it yielded 356papers. Scopus provides an Excel file containing all the primary information of the retrieved papers, which we used in the second stage of our review.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#22","title":"2.2.\u8bba\u6587\u6392\u9664","text":"<p>The exclusion of papers occurred in two phases. In the first phase, we screened the abstract text, while in the second phase, we screened the full text of the paper. Five main constraints were used to exclude papers, including (1) papers that were not related to the TTS field, (2)papers that were not DL-based models, (3) papers that did not focus on expressive or emotional TTS models, (4)papers that were too specific to non-English languages,and (5) papers that lacked details about the applied method. After screening the paper abstracts, we excluded180 papers, mostly based on the first exclusion criterion. During the second exclusion phase, in which we read the full text of each paper, we identified another 65 papers that met at least one of the five exclusion criteria. Consequently, 111 papers were included in the third stage of our review. Additionally, a group of recently published papers in this area ([17,18,19,20] InstructTTS (2023), [22,23, DiffProsody (2023), VoiceBox (2023)) was hand-picked and added to the final set of selected papers. While most of the reviewed papers trained their models on English data, a few other papers used data in other languages as listed in Table 1.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#23","title":"2.3.\u8bba\u6587\u5206\u7c7b","text":"<p>After summarizing the approach proposed for generating expressive speech in each selected paper, we categorized the papers based on the learning approach applied in each one. Accordingly, papers are divided into two main categories, including supervised and unsupervised approaches. Under the supervised category, where labeled data is utilized, we identified three subcategories based on how models are employed expressive speech synthesis. The three proposed subcategories are (1)labels as input features, (2) labels as separate layers or models, and (3) labels for emotion predictors/classifiers.</p> <p>Papers in the unsupervised approaches category are grouped into four different subcategories based on the main structure or method used for modeling expressivity in these papers. From our observation, most of the proposed methods in the last 5 years are based on three main early works in this field, namely, reference encoder [74], global style tokens[75], and latent features via variational autoencoders (VAE)[76] [77]. Specifically, proposed models in most of the papers under this category can be considered as an extension or enhancement of one of the three previously mentioned methods. Besides, we identify a fourth subcategory that includes the recent TTS models representing the new trend in the TTS area, which utilizes in-context learning. There is one factor common to all these four unsupervised models,which is that they are all based on using an audio reference/prompt. Additionally, we added a fifth subcategory (named other approaches) in which we include approaches outside the previous four main unsupervised approaches. Fig.04 illustrates the proposed classification schema for the DL-based expressive speech synthesis models.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#3","title":"3.\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5","text":"<p>Supervised approaches refer to models that are trained on datasets with emotion labels. Those labels guide model training, enabling it to learn accurate weights. Early deep learning-based expressive speech synthesis systems were primarily supervised models that utilized labeled speech exhibiting various emotions (such as sadness, happiness, and anger) or speaking styles (such as talk-show, newscaster, and call-center). Note that the term style has also been used to refer to a set of emotions or a mixture of emotions and speaking styles (ST-TTS, [68] [78] [79].  Generally, the structure of early conventional TTS models was built upon two primary networks: one for predicting duration and the other for predicting acoustic features. These acoustic features were then converted to speech using vocoders. Both networks receive linguistic features extracted from the input text.  In supervised ETTS approaches, speech labels (emotions and/or styles) are represented in the TTS model as either input features or as separate layers, models, or sets of neurons for each specific label. The following sections explain these three representations in detail then we provide a general summary of the supervised approaches reviewed in this work in Table 2.</p> <p>\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u662f\u6307\u5728\u5e26\u6709\u60c5\u611f\u6807\u7b7e\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u8fd9\u4e9b\u6807\u7b7e\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\u4ee5\u786e\u4fdd\u80fd\u591f\u5b66\u4e60\u5230\u51c6\u786e\u7684\u6743\u91cd\u3002 \u65e9\u671f\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u4e3b\u8981\u662f\u76d1\u7763\u6a21\u578b\uff0c\u4ed6\u4eec\u4f7f\u7528\u5e26\u6709\u5404\u79cd\u60c5\u611f \uff08\u5982\u60b2\u4f24\u3001\u5feb\u4e50\u548c\u6124\u6012\uff09\u6216\u8bf4\u8bdd\u98ce\u683c\uff08\u5982\u8131\u53e3\u79c0\u3001\u65b0\u95fb\u64ad\u97f3\u548c\u547c\u53eb\u4e2d\u5fc3\uff09\u6807\u7b7e\u7684\u8bed\u97f3\u3002\u6ce8\u610f\uff0c\u201c\u98ce\u683c\u201d\u8fd9\u4e00\u672f\u8bed\u540c\u6837\u7528\u4e8e\u6307\u4ee3\u4e00\u7ec4\u60c5\u611f\u6216\u60c5\u611f\u548c\u8bf4\u8bdd\u98ce\u683c\u7684\u6df7\u5408\u3002 \u901a\u5e38\u65e9\u671f\u4f20\u7edf TTS \u6a21\u578b\u7684\u7ed3\u6784\u662f\u5efa\u7acb\u5728\u4e24\u4e2a\u4e3b\u8981\u7f51\u7edc\u4e4b\u4e0a\uff1a\u4e00\u4e2a\u7528\u4e8e\u9884\u6d4b\u65f6\u957f\uff1b\u53e6\u4e00\u4e2a\u7528\u4e8e\u9884\u6d4b\u58f0\u5b66\u7279\u5f81\uff0c\u8fd9\u4e9b\u58f0\u5b66\u7279\u5f81\u4e4b\u540e\u901a\u8fc7\u58f0\u7801\u5668\u8f6c\u6362\u4e3a\u8bed\u97f3\u3002\u8fd9\u4e24\u4e2a\u7f51\u7edc\u90fd\u63a5\u6536\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u51fa\u7684\u7684\u8bed\u4e49\u7279\u5f81\u3002 \u5728\u76d1\u7763 ETTS \u65b9\u6cd5\u4e2d\uff0c\u8bed\u97f3\u6807\u7b7e\uff08\u60c5\u611f\u4e0e\u3001\u6216\u98ce\u683c\uff09\u5728 TTS \u6a21\u578b\u4e2d\u88ab\u8868\u793a\u4e3a\u8f93\u5165\u7279\u5f81\u6216\u4e3a\u6bcf\u4e2a\u7279\u5b9a\u6807\u7b7e\u7684\u5355\u72ec\u5c42\u3001\u6a21\u578b\u6216\u795e\u7ecf\u5143\u96c6\u5408\u3002\u4ee5\u4e0b\u5c0f\u8282\u5c06\u8be6\u7ec6\u89e3\u91ca\u8fd9\u4e09\u79cd\u8868\u793a\uff0c\u7136\u540e\u5728\u8868\u683c\u4e8c\u4e2d\u63d0\u4f9b\u672c\u6587\u56de\u987e\u7684\u76d1\u7763\u65b9\u6cd5\u7684\u6982\u8ff0\u3002</p> \u5f15\u7528\u5e8f\u53f7 \u7b97\u6cd5\u7b80\u79f0 \u8f93\u5165 \u60c5\u611f\u6807\u7b7e\u8868\u793a \u662f\u5426\u652f\u6301\u60c5\u7eea\u8f6c\u79fb TTS \u6a21\u578b 80 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801 DL-SPSS, HMM 65 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801/\u5355\u72ec\u5c42 \u221a DL-SPSS 66 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u611f\u77e5\u5411\u91cf/\u77e9\u9635 DL-SPSS 41 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801 DL-SPSS 42 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u4f9d\u8d56\u5c42 DL-SPSS 81 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801/\u795e\u7ecf\u5143\u96c6\u5408 \u221a DL-SPSS 43 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801/\u4f9d\u8d56\u5c42/\u72ec\u7acb\u6a21\u578b DL-SPSS 82 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801 \u221a DL-SPSS 83 \u97f3\u7d20\u5e8f\u5217+\u8bed\u8a00\u6a21\u578b\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u5d4c\u5165\u5411\u91cf Encoder-Dttention-Decoder 28, 78 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801/\u4f9d\u8d56\u5c42/\u72ec\u7acb\u6a21\u578b \u221a DL-SPSS 26 \u97f3\u7d20\u5e8f\u5217+\u6885\u5c14\u9891\u8c31+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801 GSTs \u6743\u91cd\u7684\u771f\u5b9e\u503c Tacotron2 27 \u97f3\u7d20\u5e8f\u5217+\u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u5d4c\u5165\u5411\u91cf Tacotron2 84 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u5d4c\u5165\u4e0e\u5176\u4ed6\u6570\u636e\u6807\u7b7e\u8054\u5408 DL-SPSS 85 \u8bed\u8a00\u7279\u5f81+\u97f5\u5f8b\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c DL-SPSS 86 \u97f3\u7d20\u5e8f\u5217+\u60c5\u611f\u6807\u7b7e \u5d4c\u5165\u5411\u91cf Transformer TTS 32, 36 \u5b57\u7b26\u5e8f\u5217+\u6885\u5c14\u9891\u8c31+\u60c5\u611f\u6807\u7b7e \u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c Tacotron2 69 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801/\u4f9d\u8d56\u5c42 \u221a DL-SPSS 34 \u97f3\u7d20\u5e8f\u5217+\u6885\u5c14\u9891\u8c31+\u60c5\u611f\u6807\u7b7e \u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c Tacotron2 64 \u5b57\u7b26\u5e8f\u5217+\u8bed\u8a00\u6a21\u578b\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c Tacotron2 39, 87 \u97f3\u7d20\u5e8f\u5217+\u6885\u5c14\u9891\u8c31+\u60c5\u611f\u6807\u7b7e \u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c Tacotron2","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#31-labels-as-input-features","title":"3.1 Labels as Input Features \u6807\u7b7e\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81","text":"<p>The most straightforward method for representing emotion labels of annotated datasets as input to the TTS model is by using a one-hot vector. This approach entails using a vector with a size equivalent to the number of available labels. In this vector, a value of (1) is assigned to the index corresponding to the label ID, while all other values are set to (0). Many early ETTS models [43] [56] [65] [69] [78] [80] [82] [84] advocated for this direct representation of emotion labels in order to generate speech encompassing various emotions.</p> <p>\u7528\u4e8e\u8868\u793a\u5e26\u6709\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u7684\u60c5\u611f\u6807\u7b7e\u4f5c\u4e3a TTS \u6a21\u578b\u8f93\u5165\u7684\u6700\u76f4\u63a5\u65b9\u6cd5\u662f\u4f7f\u7528\u72ec\u70ed\u7f16\u7801\u3002\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u4f7f\u7528\u548c\u53ef\u7528\u6807\u7b7e\u6570\u91cf\u957f\u5ea6\u76f8\u540c\u7684\u5411\u91cf\u3002\u5728\u8fd9\u4e2a\u5411\u91cf\u5185\uff0c\u5c06\u503c\u4e3a 1 \u5206\u914d\u7ed9\u6807\u7b7e ID \u5bf9\u5e94\u7684\u7d22\u5f15\uff0c\u5176\u4ed6\u503c\u4e3a 0. \u8bb8\u591a\u65e9\u671f ETTS \u6a21\u578b\u63d0\u5021\u8fd9\u79cd\u76f4\u63a5\u8868\u793a\u60c5\u611f\u6807\u7b7e\u7684\u65b9\u6cd5\u4ee5\u751f\u6210\u5305\u542b\u5404\u79cd\u60c5\u7eea\u7684\u8bed\u97f3\u3002</p> <p>The one-hot emotion vector, also referred to as a style/emotion code in some studies [43] [78] [80] [82], is concatenated with the input linguistic features of the model.</p> <p>\u72ec\u70ed\u7f16\u7801\u5728\u67d0\u4e9b\u6587\u732e\u4e2d\u4e5f\u88ab\u79f0\u4e3a\u98ce\u683c/\u60c5\u611f\u7f16\u7801\uff0c\u548c\u6a21\u578b\u7684\u8f93\u5165\u8bed\u8a00\u7279\u5f81\u8fdb\u884c\u62fc\u63a5\u3002</p> <p>When dealing with large number of labels, the one-hot representation becomes both high-dimensional and sparse. Moreover, in other scenarios, merging label vectors with input features instead of concatenation can lead to length mismatch issues. </p> <p>\u5f53\u5904\u7406\u5927\u91cf\u6807\u7b7e\u65f6\uff0c\u72ec\u70ed\u7f16\u7801\u8868\u793a\u53d8\u5f97\u9ad8\u7ef4\u4e14\u7a00\u758f\u3002\u800c\u4e14\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u5c06\u6807\u7b7e\u5411\u91cf\u548c\u8f93\u5165\u7279\u5f81\u5408\u5e76\u800c\u4e0d\u662f\u62fc\u63a5\u4f1a\u5bfc\u81f4\u957f\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\u3002</p> <p>In both situations, the embedding layer offers a solution by creating a continuous representation for each label, known as embedding vectors. Unlike the one-hot vector, which is constrained in size based on the number of labels, an emotion embedding can have any dimension, regardless of the number of available labels.</p> <p>\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u5d4c\u5165\u5c42\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff1a\u901a\u8fc7\u7ed9\u6bcf\u4e2a\u6807\u7b7e\u521b\u5efa\u4e00\u4e2a\u8fde\u7eed\u7684\u8868\u793a\uff0c\u5373\u5d4c\u5165\u5411\u91cf\u3002\u548c\u72ec\u70ed\u7f16\u7801\u53d7\u5230\u6807\u7b7e\u6570\u91cf\u7684\u9650\u5236\u4e0d\u540c\uff0c\u60c5\u611f\u5d4c\u5165\u53ef\u4ee5\u6709\u4efb\u610f\u7684\u7ef4\u5ea6\uff0c\u548c\u53ef\u7528\u6807\u7b7e\u6570\u91cf\u65e0\u5173\u3002</p> <p>For instance, in [84], each sample in the training dataset has three separated labels including speaker, style(emotion), and cluster. In this context, the cluster value indicates the consistency in speech quality of a given speaker and style pair. If one-hot vector is used to represent each unique combined label of each sample, the resulting label vector will be high dimensional (which in this case is 67). Therefore, the three one-hot vectors representing the given three labels are combined and passed as input to an embedding layer to reduce its dimension (in this case 15). On a different note, [41] utilizes an embedding layer to expand concise binary one-hot label vectors to match with the dimensions of the input features to be added together as input to the TTS model.</p> <p>\u4f8b\u5982, \u5728\u6587\u732e [84] \u4e2d, \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6bcf\u4e2a\u6837\u672c\u6709\u4e09\u4e2a\u72ec\u7acb\u7684\u6807\u7b7e, \u5305\u62ec\u8bf4\u8bdd\u4eba, \u98ce\u683c (\u60c5\u611f) \u548c\u805a\u7c7b\u7c7b\u522b. \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u805a\u7c7b\u7c7b\u522b\u503c\u8868\u660e\u4e86\u540c\u65f6\u7ed9\u5b9a\u8bf4\u8bdd\u4eba\u548c\u98ce\u683c\u5728\u8bed\u97f3\u8d28\u91cf\u65b9\u9762\u7684\u4e00\u81f4\u6027. \u5982\u679c\u72ec\u70ed\u7f16\u7801\u7528\u4e8e\u8868\u793a\u6bcf\u4e00\u4e2a\u552f\u4e00\u62fc\u63a5\u7684\u6807\u7b7e, \u90a3\u4e48\u6807\u7b7e\u5411\u91cf\u4f1a\u53d8\u5f97\u975e\u5e38\u9ad8\u7ef4. \u56e0\u6b64\u8fd9\u4e09\u4e2a\u72ec\u70ed\u7f16\u7801\u5411\u91cf\u5206\u522b\u8868\u793a\u7ed9\u5b9a\u7684\u4e09\u4e2a\u6807\u7b7e\u7ed3\u5408\u5e76\u8f93\u5165\u5230\u5d4c\u5165\u5c42\u8fdb\u884c\u964d\u7ef4. \u800c\u6587\u732e [41] \u4f7f\u7528\u5d4c\u5165\u5c42\u5c06\u7b80\u6d01\u7684\u4e8c\u8fdb\u5236\u72ec\u70ed\u6807\u7b7e\u5411\u91cf\u6269\u5c55\u5230\u5339\u914d\u8f93\u5165\u7279\u5f81\u7684\u7ef4\u5ea6, \u4ee5\u4fbf\u76f8\u52a0\u4f5c\u4e3a TTS \u6a21\u578b\u7684\u8f93\u5165.</p> <p>To address the potential disparities between a talker\u2019s intent and a listener\u2019s perception when annotating emotional samples, in [66], a different methodology for representing labels is introduced. In the context of N emotion classes, each sample from the talker may be perceived by the listener as one of the N emotions. In response to this, the paper suggests the adoption of a singular vector termed the \u2019perception vector,\u2019 with N dimensions. This vector represents how samples from a specific emotion class are distributed among the N emotions, based on the listener\u2019s perception. Furthermore, in the context of multiple listeners, each emotion class can be represented as a confusion matrix that captures the diverse perceptions of samples belonging to that emotion class by multiple listeners.</p> <p>\u4e3a\u4e86\u89e3\u51b3\u5728\u6ce8\u91ca\u60c5\u611f\u6837\u672c\u65f6\u8bf4\u8bdd\u4eba\u610f\u56fe\u548c\u503e\u542c\u8005\u7684\u611f\u77e5\u4e4b\u95f4\u7684\u6f5c\u5728\u5dee\u5f02, \u6587\u732e [66] \u5f15\u5165\u4e86\u8868\u793a\u6807\u7b7e\u7684\u4e0d\u540c\u65b9\u6cd5. \u5728\u5177\u6709 N \u4e2a\u60c5\u7eea\u7c7b\u522b\u7684\u60c5\u51b5\u4e0b, \u6765\u81ea\u8bf4\u8bdd\u4eba\u7684\u6bcf\u4e2a\u6837\u672c\u53ef\u80fd\u88ab\u503e\u542c\u8005\u611f\u77e5\u4e3a\u8fd9 $N$ \u4e2a\u60c5\u7eea\u7684\u5176\u4e2d\u4e4b\u4e00. \u5bf9\u6b64, \u6587\u732e [66] \u5efa\u8bae\u91c7\u6837\u4e00\u4e2a\u540d\u4e3a\"\u611f\u77e5\u5411\u91cf\"\u7684\u5355\u4e2a N \u7ef4\u5411\u91cf. \u8fd9\u4e00\u5411\u91cf\u8868\u793a\u7279\u5b9a\u60c5\u7eea\u7c7b\u522b\u7684\u6837\u672c\u5982\u4f55\u6839\u636e\u503e\u542c\u8005\u7684\u611f\u77e5\u5728 N \u4e2a\u60c5\u7eea\u4e0a\u5206\u5e03. \u6b64\u5916, \u5728\u591a\u503e\u542c\u8005\u7684\u60c5\u51b5\u4e0b, \u6bcf\u4e2a\u60c5\u7eea\u7c7b\u53ef\u4ee5\u8868\u793a\u4e3a\u4e00\u4e2a\u6df7\u6dc6\u77e9\u9635, \u6355\u83b7\u7531\u591a\u4e2a\u503e\u542c\u8005\u63d0\u4f9b\u7684\u5c5e\u4e8e\u8be5\u60c5\u7eea\u7c7b\u522b\u7684\u6837\u672c\u7684\u591a\u6837\u6027\u611f\u77e5.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#32-labels-as-separate-layersmodels","title":"3.2 Labels as Separate Layers/Models \u6807\u7b7e\u4f5c\u4e3a\u5355\u72ec\u5c42/\u6a21\u578b","text":"<p>In this approach, to represent emotion or style labels in TTS models, each label is associated with either a separate instance of the DNN model, an emotion-specific layers, or a set of emotion-specific neurons within a layer. Initially, the model is trained using neutral data, which typically has larger size. Subsequently, in the first approach, multiple copies of the trained model are fine-tuned using emotion-specific data of small size [43] [78]. In the second approach, instead of creating an individual model for each emotion, only specific model layers (usually the uppermost or final layers) from the employed DNN model are assigned to each emotion [43] [65] [69] [78] as shown by Fig. 5. While shared layers are adjusted during training using neutral data, output layers corresponding to each emotion are modified exclusively when the model is trained with data from the respective emotion.</p> <p>\u5728\u8fd9\u79cd\u65b9\u6cd5\u4e2d, TTS \u6a21\u578b\u5185\u4e3a\u4e86\u8868\u793a\u60c5\u611f\u6216\u98ce\u683c\u6807\u7b7e, \u6bcf\u4e2a\u6807\u7b7e\u8981\u4e48\u548c DNN \u6a21\u578b\u7684\u5355\u72ec\u793a\u4f8b\u5373\u4e00\u4e2a\u7279\u5b9a\u60c5\u611f\u5c42, \u8981\u4e48\u548c\u4e00\u5c42\u5185\u7684\u7279\u5b9a\u60c5\u611f\u795e\u7ecf\u5143\u96c6\u5408\u76f8\u5173\u8054.  \u9996\u5148, \u6a21\u578b\u4f7f\u7528\u901a\u5e38\u5c3a\u5bf8\u8f83\u5927\u7684\u4e2d\u6027\u6570\u636e\u8fdb\u884c\u8bad\u7ec3.  \u7b2c\u4e00\u79cd\u65b9\u6cd5, \u5bf9\u591a\u4e2a\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u7684\u526f\u672c\u5206\u522b\u4f7f\u7528\u5c0f\u5c3a\u5bf8\u7684\u7279\u5b9a\u60c5\u611f\u6570\u636e\u8fdb\u884c\u5fae\u8c03;  \u7b2c\u4e8c\u79cd\u65b9\u6cd5, \u4e0d\u4e3a\u6bcf\u79cd\u60c5\u611f\u521b\u5efa\u5355\u72ec\u6a21\u578b, \u800c\u662f\u53ea\u5c06\u6240\u4f7f\u7528\u7684 DNN \u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u5c42 (\u901a\u5e38\u662f\u6700\u4e0a\u5c42/\u6700\u7ec8\u5c42) \u5206\u914d\u7ed9\u6bcf\u79cd\u60c5\u611f. \u5982\u56fe\u4e94\u6240\u793a. \u4f7f\u7528\u4e2d\u6027\u6570\u636e\u8bad\u7ec3\u65f6\u5171\u4eab\u5c42\u4f1a\u8fdb\u884c\u8c03\u6574, \u5bf9\u5e94\u6bcf\u79cd\u60c5\u611f\u7684\u8f93\u51fa\u5c42\u4ec5\u5728\u6a21\u578b\u4f7f\u7528\u76f8\u5e94\u60c5\u611f\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u662f\u8fdb\u884c\u4fee\u6539.</p> <p>Alternatively, when dealing with limited data for certain emotions/styles, the model can initially undergo training for emotions with large amount of data. Following this step, the weights of the shared layers within the model are fixed, and only the weights of the top layers are fine-tuned using the limited, emotion-specific data [42]. </p> <p>\u5f53\u5904\u7406\u67d0\u4e9b\u60c5\u611f\u6216\u98ce\u683c\u7684\u6709\u9650\u6570\u636e\u65f6, \u6a21\u578b\u53ef\u4ee5\u5148\u4e3a\u5177\u6709\u5927\u91cf\u6570\u636e\u7684\u60c5\u611f\u8fdb\u884c\u8bad\u7ec3. \u5b8c\u6210\u540e\u5c06\u5171\u4eab\u5c42\u7684\u6743\u91cd\u56fa\u5b9a, \u53ea\u6709\u6700\u9876\u5c42\u7684\u6743\u91cd\u4f7f\u7528\u6709\u9650\u7684, \u7279\u5b9a\u60c5\u611f\u7684\u6570\u636e\u8fdb\u884c\u5fae\u8c03. \u5982\u6587\u732e [42]. </p> <p>Another method for representing emotion labels involves allocating specific neurons from a layer within the DNN model for each emotion. In this approach, the hidden layers of the model could be expanded by introducing new neurons. Then, as outlined in [81], particular neurons from this expanded set are assigned to represent each distinct emotion. Importantly, the associated weights of these specific neuron subsets are adjusted solely during the processing of data relevant to the corresponding emotion. Furthermore, by substituting the subset of neurons dedicated to a particular emotional class with a different set, the model becomes capable of generating speech imbued with the desired emotional class. This capability holds true even for new speakers who only possess neutral data, and in this case, it is known as expression/emotion transplantation.</p> <p>\u5176\u4ed6\u8868\u793a\u60c5\u611f\u6807\u7b7e\u7684\u65b9\u6cd5\u662f\u5c06 DNN \u6a21\u578b\u5c42\u4e2d\u7279\u5b9a\u7684\u795e\u7ecf\u5143\u5206\u914d\u7ed9\u6bcf\u79cd\u60c5\u611f. \u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u795e\u7ecf\u5143\u6765\u6269\u5c55\u6a21\u578b\u7684\u9690\u85cf\u5c42. \u5982\u6587\u732e [81] \u4ece\u6269\u5c55\u7684\u795e\u7ecf\u5143\u96c6\u5408\u4e2d\u5206\u914d\u7279\u5b9a\u795e\u7ecf\u5143\u6765\u8868\u793a\u6bcf\u79cd\u4e0d\u540c\u7684\u60c5\u611f. \u91cd\u8981\u7684\u662f\u53ea\u6709\u5728\u5904\u7406\u548c\u76f8\u5e94\u60c5\u611f\u76f8\u5173\u7684\u6570\u636e\u65f6, \u8fd9\u4e9b\u7279\u5b9a\u795e\u7ecf\u5143\u5b50\u96c6\u7684\u5173\u8054\u6743\u91cd\u624d\u4f1a\u8fdb\u884c\u8c03\u6574. \u6b64\u5916, \u901a\u8fc7\u52a0\u5165\u4e13\u95e8\u7528\u4e8e\u67d0\u79cd\u7279\u5b9a\u60c5\u611f\u7c7b\u522b\u7684\u795e\u7ecf\u5143, \u6a21\u578b\u53ef\u4ee5\u751f\u6210\u5177\u6709\u6240\u9700\u60c5\u611f\u7c7b\u7684\u8bed\u97f3. \u8fd9\u79cd\u80fd\u529b\u5bf9\u4ec5\u6709\u4e2d\u6027\u6570\u636e\u7684\u65b0\u8bf4\u8bdd\u4eba\u4e5f\u6210\u7acb, \u8fd9\u79f0\u4e3a\u8868\u8fbe/\u60c5\u611f\u79fb\u690d.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#33-labels-for-emotion-predictorsclassifiers","title":"3.3 Labels for Emotion Predictors/Classifiers \u6807\u7b7e\u7528\u4e8e\u60c5\u611f\u9884\u6d4b\u5668/\u5206\u7c7b\u5668","text":"<p>Another common approach to utilize emotion labels is to use them directly or via emotion predictor or classifier to support the process of extracting emotion/prosody embedding. </p> <p>\u53e6\u4e00\u79cd\u5e38\u89c1\u7684\u4f7f\u7528\u60c5\u611f\u6807\u7b7e\u7684\u65b9\u6cd5\u662f\u76f4\u63a5\u4f7f\u7528\u5b83\u4eec\u6216\u8005\u901a\u8fc7\u60c5\u611f\u9884\u6d4b\u5668/\u5206\u7c7b\u5668\u4ee5\u652f\u6301\u63d0\u53d6\u60c5\u611f/\u97f5\u5f8b\u5d4c\u5165\u7684\u8fc7\u7a0b.</p> <p>For example, in \"End-to-End Emotional Speech Synthesis Using Style Tokens  and Semi-Supervised Training\" emotion labels represented as one-hot vectors are used as targets for the weight vectors of GSTs (explained in Section 4.3) where a cross entropy loss between the two vectors is added to the total loss function. Yoon et al. [64] proposes a joint emotion predictor based on the Generative Pre-trained Transformer (GPT)-3 [88]. The proposed predictor produces two outputs including emotion class and emotion strength based on features extracted from input text by (GPT)-3. A joint emotion encoder is then used to encode the predictor outputs into a joint emotion embedding. The joint emotion predictor is trained with the guidance of the emotion labels and emotion strength values obtained via a ranking support vector machine (RankSVM) [89].</p> <p>\u6587\u732e [026] \u4e2d\u60c5\u611f\u6807\u7b7e\u8868\u793a\u6210\u72ec\u70ed\u5411\u91cf, \u4f5c\u4e3a GSTs \u6743\u91cd\u5411\u91cf\u7684\u76ee\u6807\u503c, \u8fd9\u4e24\u4e2a\u5411\u91cf\u4e4b\u95f4\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u88ab\u6dfb\u52a0\u5230\u603b\u635f\u5931\u51fd\u6570\u4e2d; \u6587\u732e [64] \u63d0\u51fa\u4e86\u57fa\u4e8e GPT-3 \u7684\u8054\u5408\u60c5\u611f\u9884\u6d4b\u5668, \u8fd9\u4e2a\u9884\u6d4b\u5668\u57fa\u4e8e GPT-3 \u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u4ea7\u751f\u4e24\u4e2a\u8f93\u51fa\u5305\u62ec\u60c5\u611f\u7c7b\u522b\u548c\u60c5\u611f\u5f3a\u5ea6. \u8054\u5408\u60c5\u611f\u7f16\u7801\u5668\u5c06\u9884\u6d4b\u5668\u7684\u8f93\u51fa\u7f16\u7801\u4e3a\u4e00\u4e2a\u8054\u5408\u60c5\u611f\u5d4c\u5165. \u8054\u5408\u60c5\u611f\u9884\u6d4b\u5668\u5728\u901a\u8fc7 RankSVM \u83b7\u5f97\u7684\u60c5\u611f\u6807\u7b7e\u548c\u60c5\u611f\u5f3a\u5ea6\u503c\u5f97\u6307\u5bfc\u4e0b\u8fdb\u884c\u8bad\u7ec3.</p> <p>In [32], an emotion classifier is used to produce more discriminative emotion embeddings. Initially, the input Mel-spectrogram features from the reference-style audio and those predicted by the proposed TTS model are passed to two reference encoders (explained in Section 4.1) to generate reference embeddings. Both embeddings are then fed to two emotion classifiers, which consist of intermediate fully connected (FC) layers. The output of the second FC layer from both classifiers is considered as the emotion embedding. Apart from the loss of the classifiers, an additional loss function is established between the resulting emotion embeddings from the two classifiers. Similarly, an emotion classifier is also employed in [36] to reduce irrelevant information in the generated emotion embedding from an emotion encoder with reference speech (Mel-spectrogram) as input.</p> <p>\u6587\u732e [32] \u4f7f\u7528\u60c5\u611f\u5206\u7c7b\u5668\u7528\u4e8e\u4ea7\u751f\u66f4\u5177\u6709\u533a\u5206\u6027\u5f97\u60c5\u611f\u5d4c\u5165. \u9996\u5148\u4ece\u53c2\u8003\u98ce\u683c\u97f3\u9891\u7684\u8f93\u5165\u6885\u5c14\u9891\u8c31\u7279\u5f81\u548c TTS \u6a21\u578b\u7684\u76f8\u5e94\u9884\u6d4b\u4f20\u9012\u5230\u4e24\u4e2a\u53c2\u8003\u7f16\u7801\u5668\u4e2d\u4ee5\u751f\u6210\u53c2\u8003\u5d4c\u5165. \u4e24\u4e2a\u5d4c\u5165\u4e4b\u540e\u90fd\u8f93\u5165\u5230\u4e24\u4e2a\u60c5\u611f\u5206\u7c7b\u5668\u4e2d, \u7531\u4e2d\u95f4\u5168\u8fde\u63a5\u5c42\u7ec4\u6210. \u5206\u7c7b\u5668\u7684\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u88ab\u89c6\u4e3a\u60c5\u611f\u5d4c\u5165. \u9664\u4e86\u5206\u7c7b\u5668\u7684\u635f\u5931\u4e4b\u5916, \u8fd8\u5efa\u7acb\u4e86\u4e24\u4e2a\u5206\u7c7b\u5668\u4ea7\u751f\u7684\u60c5\u611f\u5d4c\u5165\u7ed3\u679c\u4e4b\u95f4\u7684\u9644\u52a0\u635f\u5931\u51fd\u6570.  \u6587\u732e [36] \u4f7f\u7528\u4e86\u60c5\u611f\u5206\u7c7b\u5668\u7528\u4e8e\u51cf\u5c11\u5e26\u6709\u53c2\u8003\u8bed\u97f3 (\u6885\u5c14\u9891\u8c31) \u7684\u60c5\u611f\u7f16\u7801\u5668\u751f\u6210\u7684\u60c5\u611f\u5d4c\u5165\u7684\u65e0\u5173\u4fe1\u606f.</p> <p>Several other studies [34] [36] [39] that support multiple speakers also suggest utilizing a speaker classifier in addition to the emotion classifier. This approach aims to improved the speaker embedding derived from speaker encoders. Moreover, these studies introduce an adversarial loss between the speaker encoder and the emotion classifier using a gradient reversal layer (GRL) [90].The purpose of this is to minimize the potential transfer of emotion-related information into the speaker embedding. The GRL technique involves updating the weights of the speaker encoder by utilizing the inverse of the gradient values obtained from the emotion classifier during the training process.</p> <p>\u6587\u732e [34] [36] [39] \u652f\u6301\u591a\u8bf4\u8bdd\u4eba\u7684\u7814\u7a76\u4e5f\u5efa\u8bae\u4f7f\u7528\u8bf4\u8bdd\u4eba\u5206\u7c7b\u5668\u4ee5\u53ca\u60c5\u611f\u5206\u7c7b\u5668. \u8fd9\u4e00\u65b9\u6cd5\u65e8\u5728\u6539\u5584\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u5bfc\u51fa\u7684\u8bf4\u8bdd\u4eba\u5d4c\u5165. \u6b64\u5916\u8fd9\u4e9b\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u4e00\u4e2a GRL \u5f15\u5165\u4e86\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u548c\u60c5\u611f\u5206\u7c7b\u5668\u7684\u5bf9\u6297\u635f\u5931. \u76ee\u7684\u662f\u6700\u5c0f\u5316\u60c5\u611f\u76f8\u5173\u7684\u4fe1\u606f\u8f6c\u79fb\u5230\u8bf4\u8bdd\u4eba\u5d4c\u5165\u4e2d. GRL \u6280\u672f\u6d89\u53ca\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4ece\u60c5\u611f\u5206\u7c7b\u5668\u83b7\u5f97\u7684\u68af\u5ea6\u503c\u7684\u9006\u6765\u66f4\u65b0\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u7684\u6743\u91cd.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#4","title":"4.\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5","text":"<p>Due to the limited availability and challenges associated with collecting or preparing labeled datasets of expressive speech, as discussed in Section 6, many researchers tend to resort to unsupervised approaches for generating expressive speech. Within these approaches, models are trained to extract speaking styles or emotions from expressive speech data through unsupervised methods. Unsupervised models typically utilize reference speech as an input to the TTS model, which extracts a style or prosody embedding which is then used to synthesize speech resembling the input style reference. In the literature, three primary structures emerge as baseline models for unsupervised ETTS models: including reference encoders, global style tokens, and variational autoencoders, which are explained in the following three sections. In addition, we identify the recent TTS models that utilize in-context learning as another group of unsupervised approaches. The last subcategory under the unsupervised approaches involves other individual approaches. We then provide a general summary of all the unsupervised approaches reviewed in this work in Table 3.</p> <p>\u7531\u4e8e\u8868\u8fbe\u6027\u8bed\u97f3\u7684\u6807\u6ce8\u6570\u636e\u96c6\u7684\u6536\u96c6\u6216\u51c6\u5907\u76f8\u5173\u7684\u6709\u9650\u53ef\u7528\u6027\u548c\u6311\u6218, \u8bb8\u591a\u7814\u7a76\u4eba\u5458\u503e\u5411\u4e8e\u91c7\u7528\u65e0\u76d1\u7763\u65b9\u6cd5\u7528\u4e8e\u751f\u6210\u8868\u8fbe\u6027\u8bed\u97f3. \u5728\u8fd9\u4e9b\u65b9\u6cd5\u4e2d, \u6a21\u578b\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u6cd5\u88ab\u8bad\u7ec3\u7528\u4e8e\u4ece\u8868\u8fbe\u6027\u8bed\u97f3\u6570\u636e\u4e2d\u63d0\u53d6\u8bf4\u8bdd\u98ce\u683c\u6216\u60c5\u611f. \u65e0\u76d1\u7763\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u53c2\u8003\u8bed\u97f3\u4f5c\u4e3a\u8f93\u5165\u4f20\u9012\u7ed9 TTS \u6a21\u578b, \u8be5\u6a21\u578b\u63d0\u53d6\u98ce\u683c\u6216\u97f5\u5f8b\u5d4c\u5165, \u4e4b\u540e\u7528\u4e8e\u5408\u6210\u7c7b\u4f3c\u4e8e\u8f93\u5165\u98ce\u683c\u53c2\u8003\u7684\u8bed\u97f3.  \u73b0\u6709\u7684\u6587\u732e\u4e2d\u51fa\u73b0\u4e86\u4e09\u4e2a\u4e3b\u8981\u7ed3\u6784\u4f5c\u4e3a\u65e0\u76d1\u7763 ETTS \u6a21\u578b\u7684\u57fa\u7ebf\u6a21\u578b: Reference Encoders, Global Style Tokens, VAEs. \u6b64\u5916\u6211\u4eec\u8ba4\u4e3a\u8fd1\u671f\u4f7f\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684 TTS \u6a21\u578b\u4e3a\u5176\u4ed6\u4e00\u7ec4\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5.\u6700\u540e\u4e00\u4e2a\u5b50\u7c7b\u522b\u8fd8\u6d89\u53ca\u5230\u5176\u4ed6\u4e2a\u522b\u65b9\u6cd5. \u6211\u4eec\u5728\u8868\u683c\u4e09\u79cd\u63d0\u4f9b\u4e86\u672c\u6587\u56de\u987e\u7684\u6240\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u4e00\u822c\u6027\u603b\u7ed3.</p> <p>|\u5e8f\u53f7|\u7ec4\u522b|TTS \u6a21\u578b|\u97f5\u5f8b\u7ea7\u522b|</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#41-direct-reference-encoding","title":"4.1 Direct Reference Encoding","text":"<p>The main approach, based on a reference or prosody encoder, can be traced back to an early Google paper[74]. The paper suggests using a reference encoder to produce a low-dimensional embedding for a given style reference audio, which is called a prosody embedding. This encoder takes spectrograms as input to represent the reference audio. The generated prosody embedding is then concatenated with the text embedding derived from the text encoder of a Seq2Seq TTS model such as Tacotron, Tacotron2. Figure 6 shows reference encoder integrated to the TTS model.</p> <p>\u57fa\u4e8e\u53c2\u8003\u6216\u97f5\u5f8b\u7f16\u7801\u5668\u7684\u4e3b\u8981\u65b9\u6cd5\u53ef\u4ee5\u56de\u6eaf\u5230 Google \u7684\u4e00\u7bc7\u8bba\u6587 [74] Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron. \u8be5\u6587\u732e\u5efa\u8bae\u4f7f\u7528\u53c2\u8003\u7f16\u7801\u5668\u4ee5\u4e3a\u7ed9\u5b9a\u98ce\u683c\u53c2\u8003\u97f3\u9891\u751f\u6210\u4f4e\u7ef4\u5d4c\u5165, \u79f0\u4e3a\u97f5\u5f8b\u5d4c\u5165. \u8fd9\u4e00\u7f16\u7801\u5668\u5c06\u9891\u8c31\u4f5c\u4e3a\u8f93\u5165\u6765\u8868\u793a\u53c2\u8003\u97f3\u9891. \u751f\u6210\u7684\u97f5\u5f8b\u5d4c\u5165\u4f1a\u4e0e Seq2Seq TTS \u6a21\u578b\u5982 Tacotron \u7684\u6587\u672c\u7f16\u7801\u5668\u5bfc\u51fa\u7684\u6587\u672c\u5d4c\u5165\u76f8\u62fc\u63a5. \u56fe\u516d\u5c55\u793a\u4e86\u53c2\u8003\u7f16\u7801\u5668\u96c6\u6210\u5230 TTS \u6a21\u578b.</p> <p>Various features have been employed in the literature as inputs for the reference encoder. For example, in the work [85], MFCC features extracted using the openSMILE toolkit [139] are fed into one of the encoders within its style extraction model, which is composed of a multi-modal dual recurrent encoder (MDRE). In another study [31], the reference encoder is proposed as a ranking function model, aimed at learning emotion strength at the phoneme level. This model leverages the OpenSMILE toolkit to extract 384-dimensional emotion-related features from segments of reference audio, derived using a forced alignment model for phoneme boundaries. Furthermore, in work [63], a word-level prosody embedding is generated. This is achieved by extracting phoneme-level F0 features from reference speech using the WORLD vocoder [140] and an internal aligner operating with the input text.</p> <p>\u6587\u732e\u4e2d\u5df2\u7ecf\u4f7f\u7528\u4e86\u5404\u79cd\u7279\u5f81\u4f5c\u4e3a\u53c2\u8003\u7f16\u7801\u5668\u7684\u8f93\u5165. - \u6587\u732e [85] \u4f7f\u7528 OpenSMILE \u5de5\u5177\u7bb1\u63d0\u53d6 MFCC \u7279\u5f81\u88ab\u8f93\u5165\u5230\u5176\u98ce\u683c\u63d0\u53d6\u6a21\u578b\u7684\u4e00\u4e2a\u7f16\u7801\u5668\u4e2d. \u8be5\u6a21\u578b\u6709\u4e00\u4e2a\u591a\u6a21\u6001\u5bf9\u5076\u5faa\u73af\u7f16\u7801\u5668\u7ec4\u6210. - \u6587\u732e [31] \u53c2\u8003\u7f16\u7801\u5668\u88ab\u4f5c\u4e3a\u4e00\u4e2a\u6392\u5e8f\u51fd\u6570\u6a21\u578b, \u65e8\u5728\u97f3\u7d20\u7ea7\u522b\u5b66\u4e60\u60c5\u611f\u5f3a\u5ea6. \u8fd9\u4e2a\u6a21\u578b\u5229\u7528 OpenSMILE \u5de5\u5177\u7bb1\u4ece\u53c2\u8003\u97f3\u9891\u7247\u6bb5\u4e2d\u63d0\u53d6\u548c\u60c5\u611f\u76f8\u5173\u7684 384 \u7ef4\u7279\u5f81, \u8fd9\u4e9b\u7247\u6bb5\u662f\u5bf9\u97f3\u7d20\u8fb9\u754c\u91c7\u7528\u5f3a\u5236\u5bf9\u9f50\u6a21\u578b\u83b7\u5f97\u7684. - \u6587\u732e [63] \u751f\u6210\u4e86\u57fa\u4e8e\u5355\u8bcd\u7ea7\u522b\u7684\u97f5\u5f8b\u5d4c\u5165. \u8fd9\u662f\u901a\u8fc7\u4f7f\u7528 WORLD \u58f0\u7801\u5668\u548c\u4e0e\u8f93\u5165\u95ee\u9898\u4e00\u8d77\u64cd\u4f5c\u7684\u5185\u7f6e\u5bf9\u9f50\u5668\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u63d0\u53d6\u97f3\u7d20\u7ea7\u522b\u7684 F0 \u7279\u5f81\u6765\u5b9e\u73b0\u7684.</p> <p>A prosody-aware module is proposed in [37] which extracts other prosody-related features. The prosody-aware module consists of an encoder, an extractor, and a predictor. The encoder receives the three phoneme-level features including logarithmic fundamental frequency(LF0), intensity, and duration from the extractor as input and generates the paragraph prosody embedding with the assistance of an attention unit. Simultaneously, the predictor is trained to predict these features at inference time based on the input text embedding only.</p> <p>\u6587\u732e [37] \u63d0\u51fa\u4e86\u97f5\u5f8b\u611f\u77e5\u6a21\u5757\u7528\u4e8e\u63d0\u53d6\u5176\u4ed6\u97f5\u5f8b\u76f8\u5173\u7279\u5f81. \u8be5\u6a21\u5757\u7531\u4e00\u4e2a\u7f16\u7801\u5668, \u4e00\u4e2a\u63d0\u53d6\u5668\u548c\u4e00\u4e2a\u9884\u6d4b\u5668\u7ec4\u6210. \u7f16\u7801\u5668\u63a5\u6536\u6765\u81ea\u63d0\u53d6\u5668\u7684\u4e09\u4e2a\u97f3\u7d20\u7ea7\u522b\u7279\u5f81\u5305\u62ec\u5bf9\u6570\u57fa\u9891 LF0, \u5f3a\u5ea6\u548c\u65f6\u957f\u4f5c\u4e3a\u8f93\u5165, \u5e76\u501f\u52a9\u6ce8\u610f\u529b\u5355\u5143\u751f\u6210\u6bb5\u843d\u97f5\u5f8b\u5d4c\u5165. \u540c\u65f6\u9884\u6d4b\u5668\u88ab\u8bad\u7ec3\u7528\u4e8e\u5728\u63a8\u7406\u65f6\u4ec5\u8f93\u5165\u6587\u672c\u5d4c\u5165\u6765\u9884\u6d4b\u8fd9\u4e9b\u7279\u5f81.</p> <p>In Daft-Exprt TTS model [118], the prosody encoder receives pitch, energy and spectrogram as input. The prosody encoder then uses FiLM conditioning layers[141] to carry out affine transformations to the intermediate features of specific layers in the TTS model. A slightly modified version of the FastSpeech2 model is utilized in this work where the phoneme encoder,prosody predictor and the decoder are the conditioned components. The prosody predictor is similar to the variance adaptor of FastSpeech2 but without the length regulator, and it estimates pitch, energy and duration at phoneme-level.</p> <p>\u5728 Draft-Exprt TTS \u6a21\u578b\u4e2d, \u97f5\u5f8b\u7f16\u7801\u5668\u63a5\u53d7\u97f3\u9ad8, \u80fd\u91cf\u548c\u9891\u8c31\u4f5c\u4e3a\u8f93\u5165. \u7136\u540e\u97f5\u5f8b\u7f16\u7801\u5668\u4f7f\u7528 FiLM \u6761\u4ef6\u5c42\u5bf9 TTS \u6a21\u578b\u7684\u7279\u5b9a\u5c42\u7684\u4e2d\u95f4\u7279\u5f81\u6267\u884c\u4eff\u5c04\u53d8\u6362. \u8fd9\u9879\u5de5\u4f5c\u4f7f\u7528\u4e86 FastSpeech2 \u7684\u7a0d\u5fae\u4fee\u6539\u7248\u672c, \u5176\u4e2d\u97f3\u7d20\u7f16\u7801\u5668, \u97f5\u5f8b\u9884\u6d4b\u5668\u548c\u89e3\u7801\u5668\u90fd\u662f\u6761\u4ef6\u5316\u7ec4\u4ef6. \u97f5\u5f8b\u9884\u6d4b\u5668\u548c FastSpeech2 \u7684\u65b9\u5dee\u9002\u914d\u5668\u76f8\u4f3c\u4f46\u6ca1\u6709\u957f\u5ea6\u8c03\u8282\u5668, \u4e14\u5176\u5728\u97f3\u7d20\u6c34\u5e73\u4f30\u8ba1\u97f3\u9ad8, \u80fd\u91cf\u548c\u65f6\u957f.</p> <p>A pre-trained Wav2Vec model [142] has also been utilized for extracting features from the reference waveform.</p> <p>\u6587\u732e [142] \u91c7\u7528\u9884\u8bad\u7ec3 Wav2Vec \u6a21\u578b\u7528\u4e8e\u4ece\u53c2\u8003\u6ce2\u5f62\u4e2d\u63d0\u53d6\u7279\u5f81. </p> <p>These features serve as input to the reference encoders of the proposed Emo-VITS model, which integrates an emotion network into the VITS model [143] to enhance expressive speech synthesis. In fact, the emotion network in the Emo-VITS model comprises two reference encoders. The resulting emotion embeddings from these encoders are then combined through a feature fusion module that employs an attention mechanism. Wav2vec2.0-derived features from the reference waveform in this work are particularly suitable for attention-based fusion and contribute to reducing the textual content within the resulting embeddings.</p> <p>Emo-VITS \u5c06\u8fd9\u4e9b\u7279\u5f81\u4f5c\u4e3a Emo-VITS \u7684\u53c2\u8003\u7f16\u7801\u5668\u7684\u8f93\u5165, \u5b83\u5c06\u60c5\u611f\u7f51\u7edc\u96c6\u6210\u5230 VITS \u6a21\u578b\u4e2d\u7528\u4e8e\u589e\u5f3a\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210. \u5b9e\u9645\u4e0a, \u60c5\u611f\u7f51\u7edc\u5305\u542b\u4e86\u4e24\u4e2a\u53c2\u8003\u7f16\u7801\u5668. \u8fd9\u4e9b\u7f16\u7801\u5668\u7684\u60c5\u611f\u5d4c\u5165\u8f93\u51fa\u4e4b\u540e\u901a\u8fc7\u4e00\u4e2a\u7279\u5f81\u878d\u5408\u6a21\u5757\u8fdb\u884c\u7ed3\u5408\u7136\u540e\u5e94\u7528\u6ce8\u610f\u529b\u673a\u5236. \u7531 Wave2Vec 2.0 \u4ece\u53c2\u8003\u97f3\u9891\u5bfc\u51fa\u7684\u7279\u5f81\u5c24\u5176\u9002\u5408\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u878d\u5408, \u4e14\u6709\u52a9\u4e8e\u5728\u7ed3\u679c\u5d4c\u5165\u4e2d\u51cf\u5c11\u6587\u672c\u5185\u5bb9.</p> <p>In contrast, [60] proposes a an image style transfer module to generate input for reference encoder. The concept of image style transfer involves altering the artistic style of an image from one domain to another while retaining the image\u2019s original content [144]. In specific research, the style reconstruction module from VGG-19[145], a deep neural network primarily used for image classification, is employed to extract style-related information from the Mel-spectrogram used as input image. Subsequently, the output of this module is fed into the reference encoder to generate the style embedding.</p> <p>\u6587\u732e [60] \u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u6a21\u5757\u7528\u4e8e\u751f\u6210\u53c2\u8003\u7f16\u7801\u5668\u7684\u8f93\u5165. \u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u7684\u6982\u5ff5\u6d89\u53ca\u5230\u56fe\u50cf\u7684\u827a\u672f\u98ce\u683c\u4ece\u4e00\u4e2a\u9886\u57df\u7684\u8f6c\u5316\u5230\u53e6\u4e00\u4e2a\u9886\u57df, \u540c\u65f6\u4fdd\u7559\u56fe\u50cf\u539f\u59cb\u5185\u5bb9. \u5177\u4f53\u800c\u8a00\u6765\u81ea VGG-19 \u7684\u98ce\u683c\u91cd\u6784\u6a21\u5757, \u4e00\u4e2a\u4e3b\u8981\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edc, \u5c06\u6885\u5c14\u9891\u8c31\u89c6\u4e3a\u8f93\u5165\u56fe\u50cf\u4ece\u4e2d\u63d0\u53d6\u548c\u98ce\u683c\u76f8\u5173\u7684\u4fe1\u606f. \u4e4b\u540e\u8fd9\u4e2a\u6a21\u5757\u7684\u8f93\u51fa\u4f20\u9012\u5230\u53c2\u8003\u7f16\u7801\u5668\u4e2d\u4ee5\u751f\u6210\u98ce\u683c\u5d4c\u5165.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#42-latent-features-via-variational-autoencoders-vae","title":"4.2 Latent Features via Variational Auto\u2011Encoders \u901a\u8fc7 VAE \u83b7\u53d6\u9690\u7279\u5f81","text":"<p>The goal of TTS models under this is to map input speech from the higher dimensional space to a well-organized and lower-dimensional latent space utilizing variational auto-encoders (VAEs) [146]. VAE is a generative model that is trained to learn the mapping between observed data x and continuous random vectors z in an unsupervised manner. In detail, VAEs learn a Gaussian distribution denoted as the latent space from which the latent vectors representing the given data x can be sampled. A typical variational autoencoder consists of two components. First, the encoder learns the parameters of the z vectors (latent distribution), namely the mean $\\mu(x)$ and variance $\\sigma^2(x)$, based on the input data x. Second,the decoder regenerates the input data x based on latent vectors z sampled from the distribution learned by the encoder. In addition to the reconstruction loss between the model input and the data, variational autoencoders are also trained to minimize a latent loss, which ensures that the latent space follows a Gaussian distribution.</p> <p>\u5728\u8fd9\u7c7b TTS \u6a21\u578b\u4e2d, \u76ee\u6807\u662f\u5229\u7528 VAE \u5c06\u6765\u81ea\u4e8e\u9ad8\u7ef4\u7a7a\u95f4\u7684\u8bed\u97f3\u6620\u5c04\u5230\u7ec4\u7ec7\u826f\u597d\u4e14\u7ef4\u5ea6\u8f83\u4f4e\u7684\u9690\u7a7a\u95f4. VAE \u662f\u4e00\u79cd\u751f\u6210\u6a21\u578b, \u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\u5b66\u4e60\u89c2\u5bdf\u6570\u636e $x$ \u548c\u8fde\u7eed\u968f\u673a\u5411\u91cf $z$ \u4e4b\u95f4\u7684\u6620\u5c04. \u5177\u4f53\u6765\u8bf4, VAE \u5b66\u4e60\u4e00\u4e2a\u8bb0\u4e3a\u9690\u7a7a\u95f4\u7684\u9ad8\u65af\u5206\u5e03, \u4ece\u4e2d\u53ef\u4ee5\u91c7\u6837\u5230\u80fd\u8868\u793a\u7ed9\u5b9a\u6570\u636e $x$ \u7684\u9690\u5411\u91cf. \u4e00\u4e2a\u5178\u578b\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7531\u4e24\u90e8\u5206\u7ec4\u6210: \u4e00\u662f\u7f16\u7801\u5668\u57fa\u4e8e\u8f93\u5165\u6570\u636e\u5b66\u4e60 $z$ \u5411\u91cf\u5373\u9690\u5206\u5e03\u7684\u53c2\u6570: \u5747\u503c\u548c\u65b9\u5dee, \u4e8c\u662f\u89e3\u7801\u5668\u57fa\u4e8e\u89e3\u7801\u5668\u5b66\u4e60\u5230\u7684\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u9690\u53d8\u91cf $z$ \u91cd\u65b0\u751f\u6210\u8f93\u5165\u6570\u636e. \u9664\u4e86\u6a21\u578b\u8f93\u51fa\u548c\u6570\u636e\u4e4b\u95f4\u7684\u91cd\u6784\u635f\u5931, \u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fd8\u9700\u8981\u6700\u5c0f\u5316\u4e00\u4e2a\u6f5c\u5728\u635f\u5931, \u4f7f\u5f97\u9690\u7a7a\u95f4\u670d\u4ece\u9ad8\u65af\u5206\u5e03.</p> <p>Utilizing VAEs in expressive TTS models as shown by Fig. 7, allows for mapping the various speech styles within the given dataset to be encoded as latent vectors,often referred to as prosody vectors, within this latent space. During inference, these latent vectors can be sampled directly or with the guidance of reference audio from the VAE\u2019s latent space. Furthermore, the latent vectors offer the advantage of disentangling prosody features,meaning that some specific dimensions of these vectors independently represent single prosody features such as pitch variation or speaking rate. Disentangled prosody features allow for better prosody control via manipulating the latent vectors with different operations such as interpolation and scaling [77]. </p> <p>\u5728\u8868\u8fbe\u6027 TTS \u4e2d\u4f7f\u7528 VAEs \u5982\u56fe\u4e03\u6240\u793a, \u5141\u8bb8\u5c06\u7ed9\u5b9a\u6570\u636e\u96c6\u4e2d\u5404\u79cd\u8bed\u97f3\u98ce\u683c\u7f16\u7801\u4e3a\u9690\u53d8\u91cf, \u901a\u5e38\u79f0\u4e3a\u97f5\u5f8b\u5411\u91cf. \u5728\u63a8\u7406\u65f6\u8fd9\u4e9b\u9690\u53d8\u91cf\u53ef\u4ee5\u4ece\u9690\u7a7a\u95f4\u76f4\u63a5\u91c7\u6837\u6216\u5728\u53c2\u8003\u97f3\u9891\u7684\u6307\u5bfc\u4e0b\u91c7\u6837. \u6b64\u5916, \u9690\u53d8\u91cf\u8fd8\u63d0\u4f9b\u4e86\u5206\u79bb\u97f5\u5f8b\u7279\u5f81\u7684\u4f18\u52bf, \u610f\u5473\u7740\u8fd9\u4e9b\u5411\u91cf\u67d0\u4e9b\u7279\u5b9a\u7ef4\u5ea6\u72ec\u7acb\u5730\u8868\u793a\u5355\u4e2a\u97f5\u5f8b\u7279\u5f81, \u5982\u97f3\u9ad8\u53d8\u5316\u6216\u8bed\u901f. \u5206\u79bb\u7684\u97f5\u5f8b\u7279\u5f81\u901a\u8fc7\u67d0\u4e9b\u64cd\u4f5c\u4ee5\u8fdb\u884c\u66f4\u597d\u7684\u97f5\u5f8b\u63a7\u5236, \u5982\u63d2\u503c\u548c\u7f29\u653e.</p> <p>The two early papers, [76] [77], can be regarded as the baseline for latent feature-based approaches. The former study [76] introduces VAE within the VoiceLoop model [147], while the latter [77]incorporates VAE into Tacotron2 as an end-to-end TTS model for expressive speech synthesis.</p> <p>\u6587\u732e [76] [77] \u53ef\u4ee5\u89c6\u4e3a\u57fa\u4e8e\u9690\u7279\u5f81\u65b9\u6cd5\u7684\u57fa\u7ebf\u6a21\u578b. [76] \u5c06 VAE \u5f15\u5165 VoiceLoop \u6a21\u578b, [77] \u5c06 VAE \u96c6\u6210\u5230 Tacotron2 \u4e2d\u4f5c\u4e3a\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u7684\u7aef\u5230\u7aef TTS \u6a21\u578b.</p> <p>In the same direction of modeling the variation of the prosodic features in expressive speech, studies [109] [110] propose a hierarchical structure for the baseline variational autoencoder, known as Clockwork Hierarchical Variational AutoEncoder (CHiVE). Both the encoder and decoder in the CHiVE model have several layers to capture prosody at different levels based on the input text\u2019s hierarchical structure. Accordingly, linguistic features are also used alongside acoustic features as input to the model\u2019s encoder. The model\u2019s layers are dynamically clocked at specific rates: sentence, words, syllables, and phones. The encoder hierarchy goes from syllables to the sentence level, while the decoder hierarchy is in the reversed order.</p> <p>\u8868\u8fbe\u6027\u8bed\u97f3\u4e2d\u5efa\u6a21\u97f5\u5f8b\u7279\u5f81\u53d8\u5316\u7684\u65b9\u9762, \u6587\u732e [109], [110] \u4e3a\u57fa\u7ebf VAE \u63d0\u51fa\u4e86\u4e00\u4e2a\u5c42\u6b21\u7ed3\u6784, \u5373 CHiVE. \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u90fd\u6709\u6570\u5c42, \u57fa\u4e8e\u8f93\u5165\u6587\u672c\u7684\u5c42\u6b21\u7ed3\u6784\u5728\u4e0d\u540c\u7ea7\u522b\u6355\u83b7\u97f5\u5f8b. \u56e0\u6b64\u9664\u4e86\u58f0\u5b66\u7279\u5f81\u4e4b\u5916, \u8bed\u8a00\u7279\u5f81\u4e5f\u4f5c\u4e3a\u6a21\u578b\u7f16\u7801\u5668\u7684\u8f93\u5165. \u6a21\u578b\u7684\u5c42\u4ee5\u7279\u5b9a\u7684\u901f\u7387\u52a8\u6001\u8ba1\u65f6: \u53e5\u5b50, \u5355\u8bcd, \u97f3\u8282\u548c\u97f3\u7d20. \u7f16\u7801\u5668\u7684\u5c42\u6b21\u7ed3\u6784\u4ece\u97f3\u8282\u5230\u53e5\u5b50, \u800c\u89e3\u7801\u5668\u5219\u76f8\u53cd;</p> <p>The CHiVE-BERT model in [110], differs from the main model in [109] as it utilizes BERT [148] features for input text at the word-level. Since the features extracted by the BERT model incorporate both syntactic and semantic information from a large language model, CHiVE-BERT model is expected to have improved the prosody generation.</p> <p>ChiVE-BERT \u6a21\u578b\u4f7f\u7528 BERT \u7279\u5f81\u4f5c\u4e3a\u5355\u8bcd\u7ea7\u522b\u7684\u8f93\u5165\u6587\u672c. \u7531\u4e8e BERT \u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u5305\u542b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f, \u6240\u4ee5 CHiVE-BERT \u6a21\u578b\u9884\u8ba1\u5c06\u63d0\u9ad8\u97f5\u5f8b\u7684\u751f\u6210.</p> <p>Other studies DiffProsody [53] propose Vector-Quantized Variational Auto-Encoder (VQ-VAE) to achieve discretized latent prosody vectors. In vector quantization(VQ) [149], latent representations are mapped from the prosody latent space to a codebook of a limited number of prosody codes. Specifically, during training, the nearest neighbor lookup algorithm is applied to find the nearest codebook vector to the output of the reference encoder and used to condition TTS decoder. </p> <p>DiffProsody [53] \u63d0\u51fa\u77e2\u91cf\u91cf\u5316 VAE \u7528\u4e8e\u5b9e\u73b0\u79bb\u6563\u7684\u9690\u97f5\u5f8b\u5411\u91cf. \u5728 VQ \u4e2d, \u9690\u8868\u793a\u4ece\u97f5\u5f8b\u6f5c\u5728\u7a7a\u95f4\u6620\u5c04\u5230\u6709\u9650\u6570\u91cf\u7684\u97f5\u5f8b\u4ee3\u7801\u7684\u7801\u672c. \u7279\u522b\u5730\u5728\u8bad\u7ec3\u65f6, \u6700\u8fd1\u90bb\u67e5\u627e\u7b97\u6cd5\u5e94\u7528\u4e8e\u67e5\u627e\u53c2\u8003\u7f16\u7801\u5668\u8f93\u51fa\u6700\u8fd1\u7684\u7801\u672c\u5411\u91cf, \u5e76\u7528\u4e8e\u6761\u4ef6 TTS \u6a21\u578b. </p> <p>To further improve the quality of latent prosody vectors and consequently the expressiveness of the generated speech, DiffProsody proposes a diffusion-based VQ-VAE model.</p> <p>\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9690\u97f5\u5f8b\u5411\u91cf\u7684\u8d28\u91cf\u548c\u540e\u7eed\u751f\u6210\u8bed\u97f3\u7684\u8868\u8fbe\u6027, Diff-Prosody \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684 VQ-VAE \u6a21\u578b.</p> <p>In the proposed model a prosody generator that utilizes a denoising diffusion generative adversarial networks (DDGANs) [150] is trained to generate the prosody latent vectors based only on text and speaker information. At inference time, the prosody generator is used to produce prosody vectors based on input text and with no need for an audio reference which improves both quality and speed of speech synthesis.</p> <p>\u5728\u63d0\u51fa\u7684\u6a21\u578b\u4e2d, \u4e00\u4e2a\u97f5\u5f8b\u751f\u6210\u5668\u4f7f\u7528\u53bb\u566a\u6269\u6563\u5bf9\u6297\u751f\u6210\u6a21\u578b\u4ec5\u4f7f\u7528\u6587\u672c\u548c\u8bf4\u8bdd\u4eba\u4fe1\u606f\u751f\u6210\u97f5\u5f8b\u9690\u53d8\u91cf. \u5728\u63a8\u7406\u65f6, \u97f5\u5f8b\u751f\u6210\u5668\u7528\u4e8e\u57fa\u4e8e\u8f93\u5165\u6587\u672c\u7684\u97f5\u5f8b\u5411\u91cf\u800c\u65e0\u9700\u97f3\u9891\u53c2\u8003, \u4ece\u800c\u63d0\u5347\u8bed\u97f3\u5408\u6210\u7684\u8d28\u91cf\u548c\u901f\u5ea6.</p> <p>While most of the studies in this category follow the baseline model and use mel-spectrograms to represent the reference audio, other studies extract correlated prosody features as input to the VAE. For instance, frame-level F0, energy, and duration features are extracted from the reference speech as basic input for the hierarchical encoder of the CHiVE model [109]. These same features are also used as input for the VAE encoder in work [35], but at the phoneme level. In work [68], multi-resolution VAEs are employed, each with acoustic and linguistic input vectors. The acoustic feature vectors for each encoder include 70 mel-cepstral coefficients, log F0value, a voiced/unvoiced value, and 35 mel-cepstral analysis aperiodicity measures.</p> <p>\u6b64\u7c7b\u7684\u5927\u90e8\u5206\u7814\u7a76\u90fd\u9075\u5faa\u57fa\u7ebf\u6a21\u578b\u5e76\u4f7f\u7528\u6885\u5c14\u9891\u8c31\u8868\u793a\u53c2\u8003\u97f3\u9891, \u5176\u4ed6\u7814\u7a76\u63d0\u53d6\u76f8\u5173\u97f5\u5f8b\u7279\u5f81\u4f5c\u4e3a VAE \u7684\u8f93\u5165. \u4f8b\u5982 \u5e27\u7ea7\u522b\u7684 F0, \u80fd\u91cf, \u65f6\u957f\u7279\u5f81\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u63d0\u53d6, \u4f5c\u4e3a ChiVE \u6a21\u578b\u5c42\u6b21\u7f16\u7801\u5668\u7684\u8f93\u5165. \u8fd9\u4e9b\u76f8\u540c\u7279\u5f81\u540c\u6837\u5728\u6587\u732e [35] \u4e2d\u4f7f\u7528, \u4f46\u662f\u662f\u97f3\u7d20\u7ea7\u522b.</p> <p>\u6587\u732e [68] \u91c7\u7528\u4e86\u591a\u5206\u8fa8\u7387 VAEs, \u6bcf\u4e2a\u90fd\u6709\u58f0\u5b66\u548c\u8bed\u8a00\u8f93\u5165\u5411\u91cf. \u58f0\u5b66\u7279\u5f81\u5305\u62ec 70 \u4e2a\u6885\u5c14\u9891\u8c31\u7cfb\u6570, \u5bf9\u6570 F0 \u503c, \u6709\u58f0/\u65e0\u58f0\u503c\u548c 35 \u4e2a mel-cepstral \u5206\u6790\u975e\u5468\u671f\u5ea6\u91cf.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#43-global-style-tokens","title":"4.3 Global Style Tokens","text":"<p>The Global Style Tokens (GST) approach for expressive synthesis was first introduced in [75]. The paper proposes a framework to learn various speaking styles (referred to as style tokens) in an unsupervised manner within an end-to-end TTS model. The proposed approach can be seen as a soft clustering method that learns soft style clusters for expressive styles in an unlabeled dataset. In detail, GST, as shown by Fig. 8, extends the approach introduced in [74] by passing the resulting style embedding from the reference encoder to an attention unit,which functions as a similarity measure between the style embedding and a bank of randomly initialized tokens. During training, the model learns the style tokens and a set of weights, where each style embedding is generated via a weighted sum of the learned tokens. In fact, the obtained weights represent how each token contributes to the final style embedding. Therefore, each token will represent a single style or a single prosody-related feature, such as pitch, intensity, or speaking rate. At inference time, a reference audio can be passed to the model to generate its corresponding style embedding via a weighted sum of the style tokens. Alternatively, each individual style token can be used as a style embedding. In addition, GSTs offer an enhanced control over the speaking style through various operations. These include manual weight refinement, token scaling with different values, or the ability to condition different parts of the input text with distinct style tokens.</p> <p>\u7528\u4e8e\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u7684\u5168\u5c40\u98ce\u683c\u6807\u8bb0\u65b9\u6cd5\u5728\u6587\u732e [75] \u4e2d\u88ab\u9996\u6b21\u63d0\u51fa. \u8be5\u6587\u732e\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u7528\u4e8e\u7aef\u5230\u7aef TTS \u6a21\u578b\u4e2d\u4ee5\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\u7528\u4e8e\u5b66\u4e60\u5404\u79cd\u8bf4\u8bdd\u98ce\u683c (\u79f0\u4e3a\u98ce\u683c\u6807\u8bb0). \u8be5\u65b9\u6cd5\u53ef\u4ee5\u89c6\u4e3a\u4e00\u79cd\u8f6f\u805a\u7c7b\u65b9\u6cd5\u7528\u4e8e\u65e0\u6807\u7b7e\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u8f6f\u98ce\u683c\u7c07. \u5177\u4f53\u5730 GST \u5982\u56fe\u516b\u6240\u793a, \u5c06\u6587\u732e [74] \u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6269\u5c55, \u5c06\u53c2\u8003\u7f16\u7801\u5668\u8f93\u51fa\u7684\u98ce\u683c\u5d4c\u5165\u4f20\u9012\u7ed9\u6ce8\u610f\u529b\u5355\u5143, \u4f5c\u4e3a\u98ce\u683c\u5d4c\u5165\u548c\u4e00\u7ec4\u968f\u673a\u521d\u59cb\u6807\u8bb0\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf. \u5728\u8bad\u7ec3\u65f6, \u6a21\u578b\u5b66\u4e60\u98ce\u683c\u6807\u8bb0\u548c\u4e00\u7ec4\u6743\u91cd, \u5176\u4e2d\u6bcf\u4e2a\u98ce\u683c\u5d4c\u5165\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u6807\u8bb0\u8fdb\u884c\u52a0\u6743\u548c\u83b7\u5f97. \u5b9e\u9645\u4e0a, \u83b7\u5f97\u7684\u6743\u91cd\u8868\u793a\u6bcf\u4e2a\u6807\u8bb0\u5bf9\u4e8e\u6700\u7ec8\u98ce\u683c\u5d4c\u5165\u7684\u8d21\u732e. \u56e0\u6b64\u6bcf\u4e2a\u6807\u8bb0\u5c06\u8868\u793a\u5355\u4e2a\u98ce\u683c\u6216\u5355\u4e2a\u97f5\u5f8b\u76f8\u5173\u7684\u7279\u5f81, \u4f8b\u5982\u97f3\u9ad8, \u5f3a\u5ea6\u6216\u8bed\u901f. \u5728\u63a8\u7406\u65f6, \u53c2\u8003\u97f3\u9891\u4f20\u9012\u7ed9\u6a21\u578b\u901a\u8fc7\u98ce\u683c\u6807\u8bb0\u7684\u52a0\u6743\u548c\u7528\u4e8e\u751f\u6210\u5bf9\u5e94\u98ce\u683c\u5d4c\u5165. \u6216\u8005\u6bcf\u4e2a\u5355\u72ec\u7684\u98ce\u683c\u6807\u8bb0\u4f5c\u4e3a\u98ce\u683c\u5d4c\u5165. \u6b64\u5916 GSTs \u63d0\u4f9b\u4e86\u901a\u8fc7\u5404\u79cd\u64cd\u4f5c\u589e\u5f3a\u5bf9\u8bf4\u8bdd\u98ce\u683c\u7684\u63a7\u5236, \u5305\u62ec\u624b\u52a8\u6743\u91cd\u7ec6\u5316, \u6807\u8bb0\u6309\u4e0d\u540c\u503c\u7f29\u653e\u6216\u4f7f\u7528\u4e0d\u540c\u98ce\u683c\u6807\u8bb0\u6761\u4ef6\u8bdd\u8f93\u5165\u6587\u672c\u7684\u4e0d\u540c\u90e8\u5206.</p> <p>The GST-TTS model can be further enhanced by modeling different levels of prosody to improve both expressiveness and control over the generated speech. For instance, [46] proposes a fine-grained GST-TTS model where word-level GSTs are generated to capture local style variations (WSVs) through a prosody extractor. The WSV extractor consists of a reference encoder and a style token layer, as described in [75], along with an attention unit to produce the word-level style token.</p> <p>In [133] a hierarchical structure of multi-layer GSTs with residuals is proposed. The model employs three GST layers, each with 10 tokens, resulting in a better interpretation of the tokens of each level. Upon tokens analysis, it was found that the first-layer tokens learned speaker representations, while the second-layer tokens captured various speaking style features such as pause position, duration, and stress. The third-layer tokens, however, were able to generate higher-quality samples with more distinct and interpretable styles. Similarly, in[50], a multi-scale GST extractor is proposed to extract speaking style at different levels. This extractor extracts style embeddings from the reference mel-spectrogram using three style encoders at global, sentence, and sub word levels, and combines their outputs to form the multi-scale style embedding.</p> <p>GST-TTS \u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5efa\u6a21\u4e0d\u540c\u7ea7\u522b\u7684\u97f5\u5f8b\u6765\u63d0\u5347\u751f\u6210\u8bed\u97f3\u7684\u8868\u8fbe\u6027\u548c\u63a7\u5236.</p> <ul> <li>\u6587\u732e [46] \u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6 GST-TTS \u6a21\u578b, \u901a\u8fc7\u97f5\u5f8b\u63d0\u53d6\u5668\u751f\u6210\u4e86\u8bcd\u7ea7\u522b\u7684 GSTs \u4ee5\u6355\u83b7\u5c40\u90e8\u98ce\u683c\u53d8\u5316 (WSVs). WSV \u63d0\u53d6\u5668\u7531\u53c2\u8003\u7f16\u7801\u5668\u548c\u98ce\u683c\u6807\u8bb0\u5c42\u7ec4\u6210, \u5982\u6587\u732e [75] \u6240\u8ff0, \u4ee5\u53ca\u7528\u4e8e\u751f\u6210\u5355\u8bcd\u7ea7\u522b\u7684\u6ce8\u610f\u529b\u5355\u5143.</li> <li>\u6587\u732e [133] \u5177\u6709\u6b8b\u5dee\u7684\u591a\u5c42 GSTs \u7684\u5c42\u6b21\u7ed3\u6784, \u5e94\u7528\u4e09\u5c42 GST \u5c42, \u6bcf\u4e2a\u6709 10 \u4e2a\u6807\u8bb0, \u4ece\u800c\u5f97\u5230\u5404\u4e2a\u7ea7\u522b\u6807\u8bb0\u7684\u66f4\u4f73\u89e3\u91ca. \u6807\u8bb0\u5206\u6790\u53d1\u73b0\u7b2c\u4e00\u5c42\u6807\u8bb0\u5b66\u4e60\u5230\u8bf4\u8bdd\u4eba\u8868\u793a, \u7b2c\u4e8c\u5c42\u6355\u83b7\u4e86\u5404\u79cd\u8bf4\u8bdd\u98ce\u683c\u4f8b\u5982\u505c\u987f\u4f4d\u7f6e, \u65f6\u957f\u548c\u5f3a\u8c03. \u7b2c\u4e09\u5c42\u6807\u9898\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6837\u672c, \u5177\u6709\u66f4\u660e\u663e\u548c\u53ef\u89e3\u91ca\u7684\u98ce\u683c. </li> <li>\u7c7b\u4f3c\u5730 [50], \u4e00\u4e2a\u591a\u5c3a\u5ea6 GST \u63d0\u53d6\u5668\u7528\u4e8e\u63d0\u53d6\u4e0d\u540c\u7ea7\u522b\u7684\u8bf4\u8bdd\u98ce\u683c, \u8be5\u63d0\u53d6\u5668\u4f7f\u7528\u4e09\u79cd\u98ce\u683c\u7f16\u7801\u5668\u6309\u5168\u5c40, \u53e5\u5b50\u548c\u5355\u8bcd\u7ea7\u522b\u4ece\u53c2\u8003\u6885\u5c14\u9891\u8c31\u4e2d\u63d0\u53d6\u98ce\u683c\u5d4c\u5165, \u5e76\u5c06\u5b83\u4eec\u7684\u8f93\u51fa\u7ed3\u5408\u4ee5\u5f62\u6210\u591a\u5c3a\u5ea6\u7684\u98ce\u683c\u5d4c\u5165.</li> </ul> <p>With only a small portion of the training dataset labeled with emotions, \"End-to-End Emotional Speech Synthesis Using Style Tokens  and Semi-Supervised Training\" proposes a semi-supervised GST model for generating emotional speech. The model applies a cross-entropy loss between the one-hot vectors representing the emotion labels and the weights of GSTs,in addition to the GST-TTS reconstruction loss. The semi-GST model is trained on a dataset in which only 5%of the samples are labeled with emotion classes, while the rest of the dataset is unlabeled. After training, each style token represents a specific emotion class from the training dataset and can be used to generate speech in the corresponding emotion.</p> <p>\u5f53\u8bad\u7ec3\u96c6\u53ea\u6709\u4e00\u5c0f\u90e8\u5206\u5e26\u6709\u60c5\u611f\u6807\u7b7e\u65f6, \u6587\u732e [026] \u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7684 GST \u6a21\u578b\u7528\u4e8e\u751f\u6210\u60c5\u611f\u8bed\u97f3. \u8be5\u6a21\u578b\u5e94\u7528\u8868\u793a\u60c5\u611f\u6807\u7b7e\u7684\u72ec\u70ed\u7f16\u7801\u548c GSTs \u7684\u6743\u91cd\u4e4b\u95f4\u7684\u4ea4\u53c9\u71b5\u635f\u5931, \u4ee5\u53ca GST-TTS \u91cd\u6784\u635f\u5931. \u8fd9\u4e2a semi-GST \u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u53ea\u6709 5% \u7684\u6837\u672c\u5177\u6709\u60c5\u611f\u7c7b\u522b\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3, \u8bad\u7ec3\u540e\u6bcf\u4e2a\u98ce\u683c\u6807\u8bb0\u8868\u793a\u8bad\u7ec3\u96c6\u4e2d\u4e00\u4e2a\u7279\u5b9a\u7684\u60c5\u611f\u7c7b\u522b\u5e76\u4e14\u80fd\u7528\u4e8e\u5bf9\u5e94\u60c5\u611f\u751f\u6210\u8bed\u97f3.</p> <p>Furthermore, in [92], a speech emotion recognition(SER) model is proposed with the GST-TTS to generate emotional speech while acquiring only a small labeled dataset for training. The paper formulates the training process as reinforcement learning (RL). In this frame-work, the GST-TTS model is treated as the agent, and its parameters serve as the policy. The policy aims to predict the emotional acoustic features at each time step, where these features represent the actions. The pre-trained SER model then provides feedback on the predicted features through emotion recognition accuracy, which represents the reward. The policy gradient strategy is employed to perform backpropagation and optimize the TTS model to achieve the maximum reward.</p> <p>\u6587\u732e [92] \u4e00\u4e2a\u8bed\u97f3\u60c5\u611f\u8bc6\u522b SER \u6a21\u578b\u88ab\u63d0\u51fa\u548c GST-TTS \u6a21\u578b\u7ed3\u5408\u7528\u4e8e\u751f\u6210\u60c5\u611f\u8bed\u97f3, \u53ea\u9700\u8981\u5f88\u5c0f\u90e8\u5206\u5e26\u6807\u7b7e\u7684\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3. \u8be5\u6587\u732e\u5c06\u8bad\u7ec3\u8fc7\u7a0b\u5f62\u5f0f\u5316\u4e3a\u5f3a\u5316\u5b66\u4e60. \u5728\u6b64\u67b6\u6784\u4e0b, GST-TTS \u6a21\u578b\u89c6\u4e3a\u667a\u80fd\u4f53, \u5176\u53c2\u6570\u4f5c\u4e3a\u7b56\u7565. \u7b56\u7565\u65e8\u5728\u9884\u6d4b\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u60c5\u611f\u58f0\u5b66\u7279\u5f81, \u8fd9\u4e9b\u7279\u5f81\u8868\u793a\u52a8\u4f5c. \u9884\u8bad\u7ec3 SER \u6a21\u578b\u901a\u8fc7\u60c5\u611f\u8bc6\u522b\u7cbe\u5ea6\u63d0\u4f9b\u53cd\u9988, \u5373\u5956\u52b1. \u7b56\u7565\u68af\u5ea6\u7b56\u7565\u7528\u4e8e\u4f18\u5316 TTS \u6a21\u578b\u4ee5\u8fbe\u5230\u6700\u5927\u5956\u52b1.</p> <p>In contrast, the Mellotron model [114] introduces a unique structure for the GSTs, enabling Mellotron to generate speech in various styles, including singing styles, based on pitch and duration information extracted from the reference audio. This is achieved by obtaining a set of explicit and latent variables from the reference audio. Explicit variables (text, speaker, and F0contour) capture explicit audio information, while latent variables (style tokens and attention maps) capture the latent characteristics of speech that are hard to extract explicitly.</p> <p>\u6587\u732e [114] Mellotron \u5f15\u5165 GSTs \u72ec\u7279\u7ed3\u6784, \u4f7f\u5f97 Mellotron \u80fd\u591f\u57fa\u4e8e\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u63d0\u53d6\u7684\u97f3\u9ad8\u548c\u65f6\u957f\u4fe1\u606f\u751f\u6210\u5404\u79cd\u98ce\u683c\u7684\u8bed\u97f3, \u5305\u62ec\u6b4c\u5531\u98ce\u683c. \u8fd9\u901a\u8fc7\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u83b7\u53d6\u663e\u5f0f\u548c\u9690\u5f0f\u53d8\u91cf\u5b9e\u73b0. \u663e\u5f0f\u53d8\u91cf\u6355\u83b7\u663e\u5f0f\u97f3\u9891\u4fe1\u606f, \u9690\u5f0f\u53d8\u91cf\u6355\u83b7\u8bed\u97f3\u4e2d\u96be\u4ee5\u663e\u5f0f\u63d0\u53d6\u7684\u9690\u85cf\u7279\u5f81.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#44","title":"4.4.\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b9\u6cd5","text":"<p>These is a group of recent TTS models that are trained on a large amounts of data using in-context learning strategy. During in-context learning (also called prompt engineering), the model is trained to predict missing data based its context. In other words, the model is trained with a list of input-output pairs formed in a way that represents the in-context learning task. After training, the model should be able to predict the output based on a given input.</p> <p>\u8fd1\u671f\u6709\u4e00\u7ec4 TTS \u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u5728\u5927\u91cf\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3. \u5728\u4e0a\u4e0b\u6587\u5b66\u4e60 (\u6216\u63d0\u793a\u5de5\u7a0b) \u4e2d, \u6a21\u578b\u88ab\u8bad\u7ec3\u7528\u4e8e\u57fa\u4e8e\u4e0a\u4e0b\u6587\u9884\u6d4b\u7f3a\u5931\u6570\u636e. \u6362\u53e5\u8bdd\u8bf4\u6a21\u578b\u901a\u8fc7\u8868\u793a\u4e0a\u4e0b\u6587\u5b66\u4e60\u4efb\u52a1\u7684\u8f93\u5165\u8f93\u51fa\u5bf9\u5217\u8868\u8fdb\u884c\u8bad\u7ec3, \u8bad\u7ec3\u540e\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u7ed9\u5b9a\u8f93\u5165\u7684\u8f93\u51fa.</p> <p>For the TTS task, the provided style reference (referred to as prompt) is considered as part of the entire utterance to be synthesized. The TTS model training task is to generate the rest of this utterance following the style of the provided prompt as shown by Fig. 9. By employing this training strategy, recent TTS models such as VALL-E (2023), NaturalSpeech2 (2022), and Voicebox (2023) are capable of producing zero-shot speech synthesis using only a single acoustic prompt. Furthermore, these models demonstrate the ability to replicate speech style/emotion from a provided prompt (NaturalSpeech2 (2022), VALL-E (2023)) or reference (Voicebox (2023)) to the synthesized speech.</p> <p>\u5bf9\u4e8e TTS \u4efb\u52a1, \u63d0\u4f9b\u7684\u98ce\u683c\u53c2\u8003 (\u5373\u63d0\u793a) \u88ab\u8003\u8651\u4e3a\u8981\u5408\u6210\u7684\u6574\u4e2a\u53d1\u8a00\u7684\u4e00\u90e8\u5206. TTS \u6a21\u578b\u8bad\u7ec3\u4efb\u52a1\u5373\u9075\u5faa\u63d0\u793a\u7684\u98ce\u683c\u751f\u6210\u8fd9\u4e2a\u53d1\u8a00\u5269\u4e0b\u7684\u90e8\u5206. \u901a\u8fc7\u5e94\u7528\u8fd9\u79cd\u8bad\u7ec3\u7b56\u7565, \u8fd1\u671f TTS \u6a21\u578b\u4f8b\u5982 VALL-E, NaturalSpeech2 \u548c VoiceBox \u80fd\u591f\u4f7f\u7528\u5355\u4e2a\u58f0\u5b66\u63d0\u793a\u8fdb\u884c\u96f6\u6b21\u8bed\u97f3\u5408\u6210. \u6b64\u5916, \u8fd9\u4e9b\u6a21\u578b\u8bf4\u660e\u4e86\u4ece\u63d0\u4f9b\u7684\u63d0\u793a\u6216\u53c2\u8003\u590d\u5236\u8bed\u97f3\u98ce\u683c/\u60c5\u611f\u5230\u5408\u6210\u8bed\u97f3\u7684\u80fd\u529b.</p> <p>In VALL-E (2023), a language model is trained on tokens from Encodec (2022), and the input text is used to condi-tion the language model. Specifically, the Encodec model tokenizes audio frames into discrete latent vectors/codes,where each audio frame is encoded with eight codebooks. VALL-E employs two main models: the first one is an auto-regressive (AR) model that predicts the first code of each frame, and the second is non-auto-regressive (NAR)model that predicts the other seven codes of the frame.</p> <p>VALL-E \u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u5728 Encodec \u7684\u6807\u8bb0\u4e0a\u8bad\u7ec3, \u4e14\u8f93\u5165\u6587\u672c\u7528\u4e8e\u6761\u4ef6\u5316\u8bed\u8a00\u6a21\u578b. \u7279\u522b\u5730, Encodec \u6a21\u578b\u5c06\u97f3\u9891\u5e27\u79bb\u6563\u5316\u4e3a\u79bb\u6563\u7684\u9690\u5411\u91cf/\u4ee3\u7801, \u6bcf\u4e2a\u97f3\u9891\u5e27\u7531\u516b\u4e2a\u7801\u672c\u8fdb\u884c\u7f16\u7801. VALL-E \u5e94\u7528\u4e24\u4e2a\u4e3b\u8981\u6a21\u578b\u4e00\u4e2a\u662f\u81ea\u56de\u5f52\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u6bcf\u5e27\u7684\u7b2c\u4e00\u4e2a\u7f16\u7801, \u7b2c\u4e8c\u4e2a\u662f\u975e\u81ea\u56de\u5f52\u6a21\u578b\u7528\u4e8e\u9884\u6d4b\u5176\u4ed6\u4e03\u4e2a\u7f16\u7801.</p> <p>Instead of discrete tokens used in VALL-E, NaturalSpeech2 (2022) represents speech as latent vectors from a neural audio codec with residual vector quantizers. The latent vectors are then predicted via a diffusion model,conditioned on input text, pitch from a pitch predictor,and input speech prompt.</p> <p>\u548c VALL-E \u4e0d\u540c, NaturalSpeech 2 \u4f7f\u7528\u5177\u6709\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5c06\u8bed\u97f3\u8868\u793a\u4e3a\u9690\u5411\u91cf. \u9690\u5411\u91cf\u4e4b\u540e\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9884\u6d4b, \u6839\u636e\u8f93\u5165\u95ee\u9898, \u97f3\u9ad8\u9884\u6d4b\u5668\u7684\u97f3\u9ad8\u548c\u8f93\u5165\u8bed\u97f3\u63d0\u793a\u8fdb\u884c\u6761\u4ef6\u5316.</p> <p>Another example of in-context training is Voicebox (2023) which is a versatile generative model for speech trained on a large amount of multilingual speech data. The model is trained on a text-guided speech infilling task, which gives it the flexibility to perform various speech tasks such as zero-shot TTS, noise removal, content editing,and diverse speech sampling. Voicebox is modeled as a non-autoregressive (NAR) flow-matching model with the ability to consider future context.</p> <p>\u53e6\u4e00\u4e2a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u8bad\u7ec3\u7684\u4f8b\u5b50\u662f Voicebox, \u662f\u4e00\u4e2a\u5728\u5927\u91cf\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u8bed\u97f3\u7684\u901a\u7528\u751f\u6210\u6a21\u578b. \u8be5\u6a21\u578b\u5728\u6587\u672c\u5f15\u5bfc\u7684\u8bed\u97f3\u586b\u5145\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3, \u8fd9\u4f7f\u5176\u80fd\u591f\u6267\u884c\u5404\u79cd\u8bed\u97f3\u4efb\u52a1, \u4f8b\u5982\u96f6\u6b21 TTS, \u53bb\u566a, \u5185\u5bb9\u7f16\u8f91\u548c\u591a\u6837\u5316\u7684\u8bed\u97f3\u91c7\u6837. Voicebox \u88ab\u5efa\u6a21\u4e3a\u4e00\u4e2a\u975e\u81ea\u56de\u5f52\u7684\u6d41\u5339\u914d\u6a21\u578b, \u80fd\u591f\u8003\u8651\u672a\u6765\u4e0a\u4e0b\u6587.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#45-other-approaches","title":"4.5 Other Approaches \u5176\u4ed6\u65b9\u6cd5","text":"<p>This category containes reviewed papers that propose individual techniques or methods which cannot be categorized under any of the previously mentioned unsupervised approaches. </p> <p>\u8fd9\u4e2a\u7c7b\u522b\u5305\u542b\u4e86\u4e00\u4e9b\u4e0d\u80fd\u5f52\u7c7b\u4e3a\u4e4b\u524d\u63d0\u5230\u7684\u4efb\u4f55\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u5355\u72ec\u6280\u672f\u548c\u65b9\u6cd5\u7684\u603b\u7ed3.</p> <p>For instance, in [121], a neural encoder is introduced to encode the residual error between the predictions of a trained average TTS model and the ground truth speech. The encoded error is then used as a style embedding that conditions the decoder of the TTS model to guide the synthesis process. </p> <p>\u6587\u732e [121] \u5f15\u5165\u795e\u7ecf\u7f16\u7801\u5668\u6765\u7f16\u7801\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u5e73\u5747 TTS \u6a21\u578b\u9884\u6d4b\u548c\u771f\u5b9e\u8bed\u97f3\u4e4b\u95f4\u7684\u6b8b\u5dee\u8bef\u5dee. \u7136\u540e\u7f16\u7801\u7684\u8bef\u5dee\u88ab\u7528\u4f5c\u98ce\u683c\u5d4c\u5165\u7528\u4e8e\u6761\u4ef6\u5316 TTS \u6a21\u578b\u7684\u89e3\u7801\u5668\u4ee5\u6307\u5bfc\u5408\u6210\u8fc7\u7a0b.</p> <p>Raitio and Seshadri [128] improves prosody modeling of FastSpeech2 model with an additional variance adaptor for utterance-wise prosody modeling. </p> <p>\u6587\u732e [128] \u901a\u8fc7\u4f7f\u7528\u989d\u5916\u7684\u65b9\u5dee\u9002\u914d\u5668\u7528\u4e8e\u8bed\u8c03\u97f5\u5f8b\u5efa\u6a21, \u4ee5\u63d0\u5347 FastSpeech2 \u6a21\u578b\u7684\u97f5\u5f8b\u5efa\u6a21.</p> <p>As context information is strongly related to speech expressivity, [45] proposes using multiple self-attention layers in Tacotron2 encoder to better capture the con-text information in the input text. The outputs of these layers in the encoder are combined through either direct aggregation (concatenation) or weighted aggregation using a multi-head attention layer. </p> <p>\u7531\u4e8e\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u8bed\u97f3\u8868\u8fbe\u6027\u5f3a\u76f8\u5173, \u6587\u732e [45] \u5728 Tacotron \u7f16\u7801\u5668\u4e2d\u4f7f\u7528\u591a\u4e2a\u81ea\u6ce8\u610f\u529b\u5c42\u7528\u4e8e\u66f4\u597d\u5730\u6355\u83b7\u8f93\u5165\u6587\u672c\u7684\u5185\u5bb9\u4fe1\u606f. \u8fd9\u4e9b\u5c42\u7684\u8f93\u51fa\u901a\u8fc7\u76f4\u63a5\u805a\u5408 (\u62fc\u63a5) \u6216\u52a0\u6743\u805a\u5408 (\u591a\u5934\u6ce8\u610f\u529b\u5c42) \u8fdb\u884c\u7ed3\u5408.</p> <p>Additionally, there are some papers that propose using only input text to obtain prosody-related representations/embeddings without any style references, and those are further discussed in Section 5.2.4.</p> <p>\u6b64\u5916, \u6709\u4e9b\u6587\u732e\u63d0\u51fa\u53ea\u4f7f\u7528\u8f93\u5165\u6587\u672c\u7528\u4e8e\u83b7\u5f97\u97f5\u5f8b\u76f8\u5173\u8868\u793a/\u5d4c\u5165, \u65e0\u9700\u98ce\u683c\u53c2\u8003, \u8fd9\u5728\u540e\u7eed\u7684 5.2.4 \u4e2d\u8fdb\u884c\u8ba8\u8bba.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#5","title":"5.\u8868\u73b0\u6027\u8bed\u97f3\u5408\u6210\u7684\u4e3b\u8981\u6311\u6218","text":"<p>In this section, we list and explain the most important challenges that face expressive TTS models and the main solutions that have been proposed in the literature to overcome these challenges. We then provide a summary of papers addressing each challenge in Table 5.</p> <p>\u672c\u8282\u5217\u51fa\u5e76\u5c55\u793a\u8868\u73b0\u6027\u8bed\u97f3\u5408\u6210\u6a21\u578b\u9762\u4e34\u7684\u6700\u91cd\u8981\u6311\u6218, \u4ee5\u53ca\u5728\u73b0\u6709\u6587\u732e\u4e2d\u63d0\u51fa\u7684\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u7684\u4e3b\u8981\u89e3\u51b3\u65b9\u6848. \u5728\u8868\u683c\u4e94\u79cd\u63d0\u4f9b\u4e86\u89e3\u51b3\u6bcf\u4e2a\u6311\u6218\u7684\u6587\u732e\u7684\u603b\u89c8.</p> \u53c2\u8003\u6587\u732e \u4fe1\u606f\u6cc4\u9732 \u7f3a\u5c11\u53c2\u8003\u97f3\u9891 \u97f5\u5f8b\u53ef\u63a7\u6027 \u672a\u77e5\u98ce\u683c/\u8bf4\u8bdd\u4eba \u221a \u221a \u221a \u221a \u221a \u221a \u221a 097 \u221a \u221a \u221a \u221a \u221a 102 \u221a \u221a \u221a 047 111 \u221a \u221a 019 \u221a \u221a \u221a \u221a","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#51-irrelevant-information-leakage","title":"5.1.\u65e0\u5173\u4fe1\u606f\u6cc4\u9732 (Irrelevant Information Leakage)","text":"<p>One main problem in unsupervised approaches that rely on having a style reference or a prompt, is the leakage of irrelevant information, like speaker or text related information, into the generated style or prosody embedding.  This irrelevant information within the speech style can lead to degradation in the quality of the synthesized speech.  As a result, many studies have investigated this problem, and several solutions have been proposed as outlined below.</p> <p>\u5728\u4f9d\u8d56\u98ce\u683c\u53c2\u8003\u6216\u63d0\u793a\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4e2d, \u4e00\u4e2a\u4e3b\u8981\u7684\u95ee\u9898\u662f\u65e0\u5173\u4fe1\u606f\u7684\u6cc4\u9732, \u5982\u8bf4\u8bdd\u4eba\u6216\u6587\u672c\u76f8\u5173\u7684\u4fe1\u606f\u8fdb\u5165\u5230\u751f\u6210\u7684\u98ce\u683c\u6216\u97f5\u5f8b\u5d4c\u5165. \u8fd9\u79cd\u8bed\u97f3\u98ce\u683c\u4e2d\u7684\u65e0\u5173\u4fe1\u606f\u53ef\u80fd\u5bfc\u81f4\u5408\u6210\u8bed\u97f3\u7684\u8d28\u91cf\u4e0b\u964d. \u56e0\u6b64\u8bb8\u591a\u6587\u732e\u7814\u7a76\u4e86\u8fd9\u4e00\u95ee\u9898\u5e76\u63d0\u51fa\u4e86\u4ee5\u4e0b\u51e0\u79cd\u65b9\u6848.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#511-adversarial-training","title":"5.1.1.\u5bf9\u6297\u6027\u8bad\u7ec3 (Adversarial Training)","text":"<p>Adversarial training is one of the widely used techniques to confront the information leakage problem. Typically, a classifier is trained to distinguish the type of unwanted information (such as speaker or content information) that is leaking from the prosody reference audio into the generated prosody embedding.  During the training process, the weights of the employed prosody encoder/extractor from the reference audio are modified with gradient inversion of the proposed classifier.  In other words, the classifier penalizes the prosody encoder/extractor for any undesired information in its output.  A Gradient Reversal Layer (GRL) is usually used to achieve the inversion of the classifier gradients.</p> <p>\u5bf9\u6297\u8bad\u7ec3\u662f\u5e7f\u6cdb\u7528\u4e8e\u5904\u7406\u4fe1\u606f\u6cc4\u9732\u7684\u4e00\u79cd\u6280\u672f. \u901a\u5e38, \u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668\u7528\u4e8e\u533a\u5206\u4ece\u97f5\u5f8b\u53c2\u8003\u97f3\u9891\u6cc4\u9732\u5230\u751f\u6210\u7684\u97f5\u5f8b\u5d4c\u5165\u4e2d\u7684\u4e0d\u9700\u8981\u7684\u4fe1\u606f\u7c7b\u578b (\u4f8b\u5982\u8bf4\u8bdd\u4eba\u6216\u5185\u5bb9\u4fe1\u606f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u53c2\u8003\u97f3\u9891\u4e2d\u4f7f\u7528\u7684\u97f5\u5f8b\u7f16\u7801\u5668\u6216\u63d0\u53d6\u5668\u7684\u6743\u91cd\u88ab\u5206\u7c7b\u5668\u7684\u68af\u5ea6\u53cd\u8f6c\u4fee\u6539. \u6362\u53e5\u8bdd\u8bf4, \u5206\u7c7b\u5668\u5728\u5b83\u7684\u8f93\u51fa\u4e2d\u5bf9\u4efb\u4f55\u4e0d\u9700\u8981\u7684\u4fe1\u606f\u60e9\u7f5a\u97f5\u5f8b\u7f16\u7801\u5668\u6216\u63d0\u53d6\u5668. \u901a\u5e38\u4f7f\u7528\u68af\u5ea6\u53cd\u8f6c\u5c42 (Gradient Reversal Layer, GRL) \u6765\u83b7\u5f97\u5206\u7c7b\u5668\u68af\u5ea6\u7684\u53cd\u8f6c.</p> <p>Several studies utilize adversarial training to prevent the flow of either speaker or content-related information from the given reference audio to the resulting prosody embedding.  For instance, the VAE-TTS model learns phoneme-level 3-dimensional prosody codes.  The VAE is conditioned on speaker and emotion embeddings, besides the tone sequence and mel-spectrogram from the reference audio.  Adversarial training using a Gradient Reversal Layer (GRL) is applied to disentangle speaker and tone from the resulting prosody codes. Similarly, adversarial training is introduced to the style encoder of the cross-speaker emotion transfer model to learn a speaker-independent style embedding, where the target speaker embedding is provided from a separate speaker encoder.</p> <p>\u6709\u51e0\u9879\u7814\u7a76\u5229\u7528\u5bf9\u6297\u8bad\u7ec3\u6765\u9632\u6b62\u8bf4\u8bdd\u4eba\u6216\u5185\u5bb9\u76f8\u5173\u4fe1\u606f\u4ece\u7ed9\u5b9a\u53c2\u8003\u97f3\u9891\u6d41\u52a8\u5230\u751f\u6210\u97f5\u5f8b\u5d4c\u5165. \u4f8b\u5982, VAE-TTS \u6a21\u578b\u5b66\u4e60\u97f3\u7d20\u7ea7\u522b\u7684\u4e09\u7ef4\u97f5\u5f8b\u7f16\u7801. \u7528\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u5d4c\u5165, \u53c2\u8003\u97f3\u9891\u7684\u8bed\u8c03\u5e8f\u5217\u548c\u6885\u5c14\u9891\u8c31\u6761\u4ef6\u5316 VAE. \u91c7\u7528\u68af\u5ea6\u53cd\u8f6c\u5c42\u7684\u5bf9\u6297\u5b66\u4e60\u7528\u4e8e\u89e3\u8026\u97f5\u5f8b\u7f16\u7801\u4e2d\u7684\u8bf4\u8bdd\u4eba\u548c\u8bed\u8c03. \u7c7b\u4f3c\u5730, \u8de8\u8bf4\u8bdd\u4eba\u60c5\u611f\u8f6c\u79fb\u6a21\u578b\u4e2d\u7684\u98ce\u683c\u7f16\u7801\u5668\u4e5f\u5f15\u5165\u4e86\u5bf9\u6297\u8bad\u7ec3\u7528\u4e8e\u5b66\u4e60\u8bf4\u8bdd\u4eba\u72ec\u7acb\u7684\u98ce\u683c\u5d4c\u5165, \u5176\u4e2d\u76ee\u6807\u8bf4\u8bdd\u4eba\u5d4c\u5165\u7531\u5355\u72ec\u7684\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u63d0\u4f9b.</p> <p>The STYLER model employs multiple style encoders to decompose the style reference into several components, including duration, pitch, speaker, energy, and noise.  Both channel-wise and frame-wise bottleneck layers are added to all the style encoders to eliminate content-related information from the resulting embeddings.  Furthermore, as noise is encoded individually by a separate encoder in the model, other encoders are constrained to exclude noise information by employing either domain adversarial training or residual decoding.</p> <p>\u5728 STYLE \u6a21\u578b\u4e2d\u4f7f\u7528\u4e86\u591a\u4e2a\u98ce\u683c\u7f16\u7801\u5668\u5c06\u98ce\u683c\u53c2\u8003\u5206\u89e3\u4e3a\u591a\u4e2a\u6210\u5206, \u5305\u62ec\u65f6\u957f, \u97f3\u9ad8, \u80fd\u91cf\u548c\u566a\u58f0. \u5728\u6240\u6709\u7684\u98ce\u683c\u7f16\u7801\u5668\u4e2d\u6dfb\u52a0\u901a\u9053\u7ea7\u548c\u5e27\u7ea7\u74f6\u9888\u5c42\u4ece\u5bfc\u51fa\u7684\u5d4c\u5165\u4e2d\u6392\u9664\u5185\u5bb9\u76f8\u5173\u7684\u4fe1\u606f. \u6b64\u5916, \u7531\u4e8e\u566a\u58f0\u5728\u6a21\u578b\u4e2d\u901a\u8fc7\u5355\u72ec\u7684\u7f16\u7801\u5668\u7f16\u7801, \u901a\u8fc7\u5e94\u7528\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u6216\u6b8b\u5dee\u89e3\u7801\u6765\u7ea6\u675f\u5176\u4ed6\u7f16\u7801\u5668\u4ee5\u6392\u9664\u566a\u58f0\u4fe1\u606f.</p> <p>In 111, prosody is modeled at the phone-level and utterance-level by two separate encoders.  The first encoder consists of two sub-encoders: a style encoder and a content encoder, besides two supporting classifiers.  The first classifier predicts phone identity based on the content embedding, while the other classifier makes the same prediction but based on the style embedding.  The content encoder is trained via collaborative training with the guidance of the first classifier, while adversarial training is used to train the style encoder, utilizing the second classifier.</p> <p>\u6587\u732e 111 \u4e2d\u97f5\u5f8b\u901a\u8fc7\u4e24\u4e2a\u5355\u72ec\u7684\u7f16\u7801\u5668\u5728\u97f3\u7d20\u7ea7\u522b\u548c\u8bed\u8c03\u7ea7\u522b\u8fdb\u884c\u5efa\u6a21. \u7b2c\u4e00\u4e2a\u7f16\u7801\u5668\u7531\u4e24\u4e2a\u5b50\u7f16\u7801\u5668\u7ec4\u6210: \u4e00\u4e2a\u98ce\u683c\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u5185\u5bb9\u7f16\u7801\u5668, \u4ee5\u53ca\u4e24\u4e2a\u652f\u6301\u5206\u7c7b\u5668. \u9996\u4e2a\u5206\u7c7b\u5668\u57fa\u4e8e\u5185\u5bb9\u5d4c\u5165\u9884\u6d4b\u97f3\u7d20\u6807\u8bc6, \u5176\u4ed6\u5206\u7c7b\u5668\u5219\u57fa\u4e8e\u98ce\u683c\u5d4c\u5165\u8fdb\u884c\u76f8\u540c\u7684\u9884\u6d4b. \u7ed3\u5408\u7b2c\u4e00\u4e2a\u5206\u7c7b\u5668\u7684\u6307\u5bfc\u91c7\u7528\u534f\u4f5c\u8bad\u7ec3\u6765\u8bad\u7ec3\u5185\u5bb9\u7f16\u7801\u5668, \u5229\u7528\u7b2c\u4e8c\u4e2a\u5206\u7c7b\u5668\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u6765\u8bad\u7ec3\u98ce\u683c\u7f16\u7801\u5668.</p> <p>On the other hand, 102 proposes adversarial training for the style reference by inverting the gradient of an Automatic Speech Recognition (ASR) model.  The proposed model introduces a shared layer between an ASR and a reference encoder-based model.  Specifically, a single BiLSTM layer from the listener module of a pre-trained ASR model serves as the prior layer to the reference encoder.  The process starts by passing the reference Mel-spectrogram to the shared layer to produce the shared embedding as input to both the reference encoder and the ASR model.  A Gradient Reversal Layer (GRL) is employed by the ASR model to reverse its gradient on the shared layer.  Accordingly, the reference encoder parameters are modified so that the ASR model fails to recognize the shared embedding, and thus content leakage to the style embedding from the reference encoder is reduced.</p> <p>\u53e6\u4e00\u65b9\u9762, \u6587\u732e 102 \u63d0\u51fa\u901a\u8fc7\u53cd\u8f6c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u68af\u5ea6\u5bf9\u98ce\u683c\u53c2\u8003\u8fdb\u884c\u5bf9\u6297\u6027\u8bad\u7ec3. \u8be5\u6a21\u578b\u5728 ASR \u548c\u57fa\u4e8e\u53c2\u8003\u7f16\u7801\u5668\u7684\u6a21\u578b\u4e4b\u95f4\u5f15\u5165\u4e86\u4e00\u4e2a\u5171\u4eab\u5c42. \u5177\u4f53\u5730, \u9884\u8bad\u7ec3 ASR \u6a21\u578b\u4e2d\u542c\u4f17\u6a21\u5757\u4e2d\u7684\u5355\u4e2a BiLSTM \u5c42\u4f5c\u4e3a\u53c2\u8003\u7f16\u7801\u5668\u7684\u524d\u4e00\u5c42. \u5c06\u53c2\u8003\u6885\u5c14\u9891\u8c31\u4f20\u9012\u5230\u5171\u4eab\u5c42\u7528\u4e8e\u751f\u6210\u5171\u4eab\u5d4c\u5165, \u4f5c\u4e3a\u53c2\u8003\u7f16\u7801\u5668\u548c ASR \u6a21\u578b\u7684\u8f93\u5165. ASR \u6a21\u578b\u901a\u8fc7\u4f7f\u7528\u68af\u5ea6\u53cd\u8f6c\u5c42\u6765\u53cd\u8f6c\u5171\u4eab\u5c42\u7684\u68af\u5ea6. \u56e0\u6b64, \u53c2\u8003\u7f16\u7801\u5668\u7684\u53c2\u6570\u88ab\u4fee\u6539\u4f7f\u5f97 ASR \u6a21\u578b\u65e0\u6cd5\u8bc6\u522b\u5171\u4eab\u5d4c\u5165, \u4ece\u800c\u51cf\u5c11\u4ece\u53c2\u8003\u7f16\u7801\u5668\u5230\u98ce\u683c\u5d4c\u5165\u5668\u7684\u5185\u5bb9\u6cc4\u9732.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#512-prosody-classifiers","title":"5.1.2.\u97f5\u5f8b\u5206\u7c7b\u5668 (Prosody Classifiers)","text":"<p>This is a supporting approach used by some studies to produce more discriminative prosody embeddings by passing them to a prosody classifier.  This method can be applied when the training data is labeled with emotion or style labels.  In the two consecutive studies  from the same research group, an auxiliary reference encoder is proposed and located after the decoder of the baseline TTS model.  The two reference encoders in the model are followed by emotion classifiers to further enhance the discriminative nature of their resulting embeddings. However, the emotion embedding that is passed to the TTS model is the output of an intermediate hidden layer of the classifiers.  In addition to the classification loss, an additional style loss is also applied between the two emotion embeddings from the two employed emotion classifiers.</p> <p>\u8fd9\u662f\u67d0\u4e9b\u7814\u7a76\u4e2d\u91c7\u7528\u7684\u652f\u6301\u65b9\u6cd5, \u901a\u8fc7\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9\u97f5\u5f8b\u5206\u7c7b\u5668\u7528\u4e8e\u4ea7\u751f\u66f4\u5177\u533a\u5206\u6027\u7684\u97f5\u5f8b\u5d4c\u5165. \u5f53\u8bad\u7ec3\u6570\u636e\u5e26\u6709\u60c5\u611f\u6216\u98ce\u683c\u6807\u7b7e\u65f6\u53ef\u4ee5\u91c7\u7528\u6b64\u65b9\u6cd5. \u5728\u540c\u4e00\u7814\u7a76\u5c0f\u7ec4\u7684\u4e24\u9879\u8fde\u7eed\u7814\u7a76\u4e2d, \u7ed9\u57fa\u7ebf TTS \u6a21\u578b\u7684\u89e3\u7801\u5668\u540e\u6dfb\u52a0\u4e00\u4e2a\u8f85\u52a9\u7684\u53c2\u8003\u7f16\u7801\u5668. \u6a21\u578b\u4e2d\u7684\u4e24\u4e2a\u53c2\u8003\u7f16\u7801\u5668\u540e\u8ddf\u7740\u60c5\u611f\u5206\u7c7b\u5668\u7528\u4e8e\u8fdb\u4e00\u6b65\u589e\u5f3a\u5176\u751f\u6210\u5d4c\u5165\u7684\u533a\u5206\u6027. \u7136\u800c\u4f20\u9012\u7ed9 TTS \u6a21\u578b\u7684\u60c5\u611f\u5d4c\u5165\u662f\u5206\u7c7b\u5668\u7684\u4e2d\u95f4\u9690\u85cf\u5c42\u7684\u8f93\u51fa. \u9664\u4e86\u5206\u7c7b\u5668\u635f\u5931\u5916, \u8fd8\u5e94\u7528\u4e86\u6765\u6e90\u4e8e\u4e24\u4e2a\u60c5\u611f\u5206\u7c7b\u5668\u7684\u60c5\u611f\u5d4c\u5165\u4e4b\u95f4\u7684\u9644\u52a0\u98ce\u683c\u635f\u5931.</p> <p>In [36], alongside the text encoder, two encoders are introduced to generate embeddings for speaker and emotion from a reference audio.  To further disentangle emotion, speaker, and text information, both speaker and emotion encoders are supported with a classifier to predict speaker and emotion labels, respectively.  Similarly, in paper [39], a model with two encoders and two classifiers is proposed to produce disentangled embeddings for speakers and emotions from a reference audio.  However, the paper claims that some emotional information is lost during the process of disentangling speaker identity from the emotion embedding.  As a result, an ASR model is introduced to compensate for the missing emotional information.  The emotion embedding is incorporated within a pre-trained ASR model through a Global Context (GC) block.  This block extracts global emotional features from the ASR model\u2019s intermediate features (AIF).  Subsequently, a prosody compensation encoder is utilized to generate emotion compensation information from the output of the AIF layer, which is then added to the emotion encoder output.</p> <p>\u5728\u6587\u732e [36] \u4e2d, \u9664\u4e86\u6587\u672c\u7f16\u7801\u5668\u4e4b\u5916, \u8fd8\u5f15\u5165\u4e86\u4e24\u4e2a\u7f16\u7801\u5668\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u751f\u6210\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u7684\u5d4c\u5165. \u4e3a\u4e86\u8fdb\u4e00\u6b65\u89e3\u8026\u60c5\u611f, \u8bf4\u8bdd\u4eba\u548c\u6587\u672c\u4fe1\u606f, \u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u7f16\u7801\u5668\u90fd\u7528\u4e00\u4e2a\u5206\u7c7b\u5668\u8fdb\u884c\u652f\u6301, \u5206\u522b\u7528\u4e8e\u9884\u6d4b\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u6807\u7b7e. \u7c7b\u4f3c\u5730, \u6587\u732e [39] \u4e2d, \u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u4e24\u4e2a\u7f16\u7801\u5668\u548c\u4e24\u4e2a\u5206\u7c7b\u5668\u7684\u6a21\u578b\u4ee5\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u4ea7\u751f\u89e3\u8026\u7684\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u5d4c\u5165. \u7136\u800c\u8be5\u6587\u732e\u58f0\u79f0\u518d\u4ece\u60c5\u611f\u5d4c\u5165\u4e2d\u89e3\u8026\u8bf4\u8bdd\u4eba\u6807\u8bc6\u65f6\u4e22\u5931\u4e86\u4e00\u4e9b\u60c5\u611f\u4fe1\u606f. \u56e0\u6b64\u5f15\u5165\u4e86\u4e00\u4e2a ASR \u6a21\u578b\u7528\u4e8e\u8865\u507f\u4e22\u5931\u7684\u60c5\u611f\u4fe1\u606f. \u60c5\u611f\u5d4c\u5165\u901a\u8fc7\u5168\u5c40\u6587\u672c\u5757\u88ab\u6574\u5408\u8fdb\u9884\u8bad\u7ec3 ASR \u6a21\u578b\u4e2d. \u8fd9\u4e2a\u5757\u4ece ASR \u6a21\u578b\u7684\u4e2d\u95f4\u7279\u5f81 (AIF) \u4e2d\u63d0\u53d6\u5168\u5c40\u60c5\u611f\u7279\u5f81. \u7136\u540e, \u4f7f\u7528\u97f5\u5f8b\u8865\u507f\u7f16\u7801\u5668\u4ece AIF \u5c42\u7684\u8f93\u51fa\u4e2d\u751f\u6210\u60c5\u611f\u8865\u507f\u4fe1\u606f, \u4e4b\u540e\u52a0\u5165\u5230\u60c5\u611f\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4e2d.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#513-information-bottleneck","title":"5.1.3.\u4fe1\u606f\u74f6\u9888 (Information Bottleneck)","text":"<p>The information bottleneck is a technique used to control information flow via a single layer/network.  It helps prevent information leakage as it projects input into a lower dimension so that there is not enough capacity to model additional information and only important information is passed through it.  In other words, the bottleneck can be seen as a down-sampling and up-sampling filter that restricts its output and generates a pure style embedding.  Several prosody-reference based approaches, as in [86] [93] [97] [101] [130], have employed this technique to prevent the flow of speaker or content-related information from the reference audio to the prosody embedding.</p> <p>\u4fe1\u606f\u74f6\u9888\u662f\u4e00\u79cd\u901a\u8fc7\u5355\u5c42\u6216\u7f51\u7edc\u63a7\u5236\u4fe1\u606f\u6d41\u7684\u6280\u672f\u3002 \u5b83\u6709\u52a9\u4e8e\u9632\u6b62\u4fe1\u606f\u6cc4\u9732\uff0c\u56e0\u4e3a\u5b83\u5c06\u8f93\u5165\u6295\u5c04\u5230\u8f83\u4f4e\u7ef4\u5ea6\u4ece\u800c\u6ca1\u6709\u8db3\u591f\u7684\u5bb9\u91cf\u6765\u5efa\u6a21\u989d\u5916\u4fe1\u606f\uff0c\u53ea\u6709\u91cd\u8981\u4fe1\u606f\u901a\u8fc7\u5b83\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0c\u4fe1\u606f\u74f6\u9888\u53ef\u4ee5\u89c6\u4e3a\u4e00\u4e2a\u4e0b\u91c7\u6837\u548c\u4e0a\u91c7\u6837\u6ee4\u6ce2\u5668\uff0c\u5b83\u9650\u5236\u4e86\u8f93\u51fa\u5e76\u751f\u6210\u7eaf\u98ce\u683c\u5d4c\u5165\u3002 \u6709\u51e0\u9879\u57fa\u4e8e\u97f5\u5f8b\u53c2\u8003\u7684\u7814\u7a76\u5df2\u7ecf\u5e94\u7528\u4e86\u8fd9\u4e00\u6280\u672f\u7528\u4e8e\u9632\u6b62\u8bf4\u8bdd\u4eba\u6216\u5185\u5bb9\u76f8\u5173\u7684\u4fe1\u606f\u4ece\u53c2\u8003\u97f3\u9891\u6d41\u5165\u97f5\u5f8b\u5d4c\u5165\u3002</p> <p>In [93], a bottleneck layer named sieve layer is introduced to the style encoder in GST-TTS to generate pure style embedding.  Similarly, in the multiple style encoders model STYLER [97], each encoder involves a channel-wise bottleneck block of two bidirectional-LSTM layers to eliminate content information from encoders\u2019 output.  Another example is the cross-speaker-style transfer Transformer-TTS model proposed in [86] with both speaker and style embeddings as input to the model encoder.  The speaker-style-combined output from the encoder is then passed to a prosody bottleneck sub-network, which produces a prosody embedding that involves only prosody-related features.  The proposed bottleneck sub-network consists of two CNN layers, a squeeze-and-excitation (SE) block [152], and a linear layer.  The encoder output is then concatenated with the resulting prosody embedding and used as input to the decoder.</p> <ul> <li>\u6587\u732e 093 \u5728 GST-TTS \u7684\u98ce\u683c\u7f16\u7801\u5668\u4e2d\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a\u7b5b\u5c42 (Sieve Layer) \u7684\u74f6\u9888\u5c42\u7528\u4e8e\u751f\u6210\u7eaf\u98ce\u683c\u5d4c\u5165.</li> <li>\u6587\u732e 097 \u63d0\u51fa\u7684\u591a\u98ce\u683c\u7f16\u7801\u5668\u6a21\u578b STYLER \u4e2d\uff0c\u6bcf\u4e2a\u7f16\u7801\u5668\u90fd\u5305\u542b\u4e24\u4e2a\u53cc\u5411 LSTM \u5c42\u901a\u9053\u7ea7\u522b\u74f6\u9888\u5757\u7528\u4e8e\u6d88\u9664\u7f16\u7801\u5668\u8f93\u51fa\u4e2d\u7684\u5185\u5bb9\u4fe1\u606f.</li> <li>\u6587\u732e 086 \u63d0\u51fa\u7684\u8de8\u8bf4\u8bdd\u4eba\u98ce\u683c\u8fc1\u79fb Transformer-TTS \u6a21\u578b\u4e2d, \u8bf4\u8bdd\u4eba\u548c\u98ce\u683c\u5d4c\u5165\u90fd\u4f5c\u4e3a\u6a21\u578b\u7f16\u7801\u5668\u7684\u8f93\u5165. \u7f16\u7801\u5668\u8f93\u51fa\u7684\u8bf4\u8bdd\u4eba-\u98ce\u683c\u7ec4\u5408\u88ab\u4f20\u9012\u5230\u97f5\u5f8b\u74f6\u9888\u81ea\u7f51\u7edc, \u5bfc\u51fa\u4ec5\u5305\u542b\u97f5\u5f8b\u76f8\u5173\u7279\u5f81\u7684\u97f5\u5f8b\u5d4c\u5165. \u4e4b\u540e\u5c06\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4e0e\u4ea7\u751f\u7684\u97f5\u5f8b\u5d4c\u5165\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165.</li> </ul> <p>The Copycat TTS model [130] is a prosody transfer model via VAE.  The model applies three techniques to disentangle the source speaker information from the prosody embedding.  One of these techniques is to use a temporal bottleneck encoder [153] within the reference encoder of the model.  The prosody embedding that is sampled from the latent space is passed to the bottleneck to reduce speaker identity-related information in the prosody embedding before it flows to the model decoder. Similarly, the model proposed in [101] produces a style embedding with less irrelevant style information by adding a variational information bottleneck (VIB) [154] layer to the reference encoder.  The idea behind this layer is to introduce a complexity constraint on mutual information(MI) between the reference encoder input and output so that it only flows out style-related information.</p> <ul> <li>\u6587\u732e 130 \u63d0\u51fa\u7684 Copycat TTS \u662f\u901a\u8fc7 VAE \u8fdb\u884c\u97f5\u5f8b\u8fc1\u79fb\u7684\u6a21\u578b. \u6a21\u578b\u5e94\u7528\u4e09\u79cd\u6280\u672f\u6765\u4ece\u97f5\u5f8b\u5d4c\u5165\u4e2d\u89e3\u8026\u6e90\u8bf4\u8bdd\u4eba\u4fe1\u606f. \u5176\u4e2d\u4e4b\u4e00\u662f\u5728\u6a21\u578b\u7684\u53c2\u8003\u7f16\u7801\u5668\u4e2d\u4f7f\u7528\u65f6\u5e8f\u74f6\u9888\u7f16\u7801\u5668 \u6587\u732e 153, \u5728\u4f20\u9012\u7ed9\u6a21\u578b\u89e3\u7801\u5668\u4e4b\u524d, \u4ece\u9690\u7a7a\u95f4\u4e2d\u91c7\u6837\u7684\u97f5\u5f8b\u5d4c\u5165\u88ab\u4f20\u9012\u5230\u74f6\u9888\u4ee5\u51cf\u5c11\u5176\u4e2d\u548c\u8bf4\u8bdd\u4eba\u8eab\u4efd\u76f8\u5173\u7684\u4fe1\u606f.</li> <li>\u6587\u732e 101 \u901a\u8fc7\u7ed9\u53c2\u8003\u7f16\u7801\u5668\u6dfb\u52a0\u53d8\u5206\u4fe1\u606f\u74f6\u9888 (Variational Information Bottleneck, VIB) \u5c42\u4ee5\u751f\u6210\u5177\u6709\u66f4\u5c11\u98ce\u683c\u65e0\u5173\u4fe1\u606f\u7684\u98ce\u683c\u5d4c\u5165. \u8be5\u5c42\u80cc\u540e\u7684\u601d\u60f3\u662f\u5728\u53c2\u8003\u7f16\u7801\u5668\u7684\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u5f15\u5165\u4e00\u4e2a\u590d\u6742\u5ea6\u7ea6\u675f\u4f7f\u5176\u53ea\u6d41\u51fa\u98ce\u683c\u76f8\u5173\u7684\u4fe1\u606f.</li> </ul>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#514-instance-normalization","title":"5.1.4.\u5b9e\u4f8b\u5f52\u4e00\u5316 (Instance Normalization)","text":"<p>Batch normalization (BN), first introduced in [155], is utilized in deep neural networks to accelerate the training process and increase its stability. Essentially, a batch normalization layer is added before each layer in deep neural networks to adjust the means and variances of the layer inputs, as illustrated by Eq.(1): $$   IN(x) = \\gamma \\left[\\dfrac{x-\\mu(x)}{\\sigma(x)}\\right]+\\beta, \\tag{1} $$</p> <p>where $\\gamma$, $\\beta$ are affine parameters learned from data and $\\mu$, $\\sigma$ are the mean and standard deviation which are calculated for each feature channel across the batch size. Instance normalization (IN) also follows equation (1); however, it calculates means and variances across spatial dimensions independently for each channel and each sample (instance).  In the field of computer vision, stylization approach is significantly improved by replacing (BN) layers with (IN) layers [156].  Consequently, researchers in the expressive speech field have started to apply IN to extract better prosody representations.  For example, an instance normalization (IN) layer is used at the reference encoder in [130], at the prosody extractor in [93], and at the style encoder in [96] to remove style/prosody irrelevant features (such as speaker identity features) and enhance the learned style/prosody embedding.</p> <p>\u6279\u91cf\u5f52\u4e00\u5316 (Batch Normalization, BN) \u7531\u6587\u732e 155 \u9996\u6b21\u63d0\u51fa, \u7528\u4e8e\u52a0\u901f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u5e76\u63d0\u9ad8\u5176\u7a33\u5b9a\u6027. \u672c\u8d28\u4e0a, \u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6bcf\u4e00\u5c42\u4e4b\u524d\u6dfb\u52a0\u6279\u91cf\u5f52\u4e00\u5316\u5c42\u7528\u4e8e\u8c03\u6574\u8f93\u5165\u7684\u5747\u503c\u548c\u65b9\u5dee, \u5982\u516c\u5f0f\u4e00\u6240\u793a. \u5176\u4e2d $\\gamma$ \u548c $\\beta$ \u662f\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u4eff\u5c04\u53c2\u6570, $\\mu$ \u548c $\\sigma$ \u662f\u5728\u6574\u4e2a\u6279\u91cf\u5927\u5c0f\u4e0a\u4e3a\u6bcf\u4e2a\u7279\u5f81\u901a\u9053\u8ba1\u7b97\u7684\u5747\u503c\u548c\u65b9\u5dee.</p> <p>\u5b9e\u4f8b\u5f52\u4e00\u5316 (Instance Normaliztion, IN) \u540c\u6837\u6ee1\u8db3\u516c\u5f0f\u4e00, \u7136\u800c\u5b83\u72ec\u7acb\u5730\u4e3a\u6bcf\u4e2a\u901a\u9053\u548c\u6bcf\u4e2a\u6837\u672c (\u53c8\u79f0\u5b9e\u4f8b) \u5728\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee. </p> <p>\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df, \u901a\u8fc7\u5c06\u6279\u91cf\u5f52\u4e00\u5316\u5c42\u66ff\u6362\u4e3a\u5b9e\u4f8b\u5f52\u4e00\u5316\u5c42, \u98ce\u683c\u5316\u65b9\u6cd5\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347. \u56e0\u6b64, \u8868\u8fbe\u6027\u8bed\u97f3\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u5c1d\u8bd5\u5e94\u7528\u5b9e\u4f8b\u5f52\u4e00\u5316\u7528\u4e8e\u63d0\u53d6\u66f4\u597d\u7684\u97f5\u5f8b\u8868\u793a. </p> <p>\u4f8b\u5982\u4ee5\u4e0b\u7814\u7a76\u90fd\u4f7f\u7528\u4e86\u5b9e\u4f8b\u5f52\u4e00\u5316\u5c42\u4ee5\u53bb\u9664\u98ce\u683c/\u97f5\u5f8b\u65e0\u5173\u7279\u5f81 (\u5982\u8bf4\u8bdd\u4eba\u8eab\u4efd\u7279\u5f81) \u5e76\u589e\u5f3a\u5b66\u4e60\u5230\u7684\u98ce\u683c/\u97f5\u5f8b\u5d4c\u5165.</p> <ul> <li>\u6587\u732e 130 \u7684\u53c2\u8003\u7f16\u7801\u5668;</li> <li>\u6587\u732e 093 \u7684\u97f5\u5f8b\u63d0\u53d6\u5668;</li> <li>\u6587\u732e 096 \u7684\u98ce\u683c\u7f16\u7801\u5668.</li> </ul>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#515-mutual-information-minimization","title":"5.1.5.\u4e92\u4fe1\u606f\u6700\u5c0f\u5316 (Mutual Information Minimization)","text":"<p>For a pair of random variables, mutual information (MI)is defined as the information obtained on one random variable by observing the other.  Specifically, if $X$ and $Y$ are two variables, then $MI(X; Y)$ shown by Venn diagram in Fig.10, can be seen as the KL-divergence between the joint distribution $(P_{XY})$ and the product of the marginals $(P_X, P_Y)$ as in equation (2). If the two random variables $X$ and $Y$ represent linguistic and style vectors, applying $MI$ minimization between these two vectors helps to produce style vectors with less information from the content vector. $$   MI(X;Y)=DL_{KL}(P_{(X,Y)}| P_X\\otimes P_Y),\\tag{2} $$</p> <p>For example, in [137], the Mutual Information Neural Estimation algorithm (MINE) [157] is employed to estimate the mutual information between the content and style vectors.  The algorithm uses a neural network that is trained to maximize the lower bound of the mutual information between the style and content vectors.  Simultaneously, the TTS model aims to minimize the reconstruction loss, making the overall problem a max-min problem.  Alternatively, in InstructTTS, the CLUB method [158], which computes an upper bound as the MI estimator, is used to prevent the leakage of speaker and content information into the style embedding.</p> <p>A new approach is proposed in [117] for MI estimation and minimization to reduce content/speaker information transfer to the style embedding in a VAE based approach.  Typically, the model needs to estimate MI between latent style embeddings and speaker/content embeddings.  To avoid the exponentially high statistical variance of the finite-sampling MI estimator, the paper suggests using a new algorithm for information divergence named R\u00e9nyi divergence.  Two variations from the R\u00e9nyi divergence family are proposed, including minimizing the Hellinger distance and minimizing the sum of R\u00e9nyi divergences.</p> <p>\u5bf9\u4e8e\u4e00\u5bf9\u968f\u673a\u53d8\u91cf, \u4e92\u4fe1\u606f (Mutual Information, MI) \u88ab\u5b9a\u4e49\u4e3a\u901a\u8fc7\u89c2\u5bdf\u53e6\u4e00\u4e2a\u968f\u673a\u53d8\u91cf\u83b7\u5f97\u7684\u5f53\u524d\u968f\u673a\u53d8\u91cf\u7684\u4fe1\u606f. \u5177\u4f53\u5730, \u82e5 $X$ \u548c $Y$ \u4e3a\u4e24\u4e2a\u53d8\u91cf, \u90a3\u4e48 $MI(X;Y)$ \u5982\u56fe\u5341\u663e\u793a\u7684\u97e6\u6069\u56fe\u6240\u793a, \u53ef\u4ee5\u89c6\u4e3a\u8054\u5408\u6982\u7387\u5206\u5e03 $P_{XY}$ \u548c\u8fb9\u9645\u6982\u7387\u5206\u5e03 $(P_X,P_Y)$ \u7684\u4e58\u79ef\u4e4b\u95f4\u7684 KL \u6563\u5ea6. \u82e5\u4e24\u4e2a\u968f\u673a\u53d8\u91cf $X$ \u548c $Y$ \u5206\u522b\u8868\u793a\u8bed\u8a00\u5411\u91cf\u548c\u98ce\u683c\u5411\u91cf, \u5bf9\u8fd9\u4e24\u4e2a\u5411\u91cf\u5e94\u7528\u4e92\u4fe1\u606f\u6700\u5c0f\u5316\u5c06\u6709\u52a9\u4e8e\u751f\u6210\u5177\u6709\u66f4\u5c11\u6765\u81ea\u5185\u5bb9\u5411\u91cf\u7684\u4fe1\u606f\u7684\u98ce\u683c\u5411\u91cf. $$     MI(X;Y)=DL_{KL}(P_{(X,Y)}| P_X\\otimes P_Y),\\tag{2} $$</p> <ul> <li>\u6587\u732e [137] \u5c06\u6587\u732e [157] \u7684\u4e92\u4fe1\u606f\u7f51\u7edc\u4f30\u8ba1\u7b97\u6cd5 (Mutual Information Neural Estimation, MINE) \u5e94\u7528\u4e8e\u4f30\u8ba1\u5185\u5bb9\u5411\u91cf\u548c\u98ce\u683c\u5411\u91cf\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f. \u8be5\u7b97\u6cd5\u4f7f\u7528\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc, \u88ab\u8bad\u7ec3\u4ee5\u6700\u5927\u5316\u98ce\u683c\u5411\u91cf\u548c\u5185\u5bb9\u5411\u91cf\u4e4b\u95f4\u4e92\u4fe1\u606f\u7684\u4e0b\u754c. \u540c\u65f6, TTS \u6a21\u578b\u6700\u5c0f\u5316\u91cd\u6784\u635f\u5931, \u4f7f\u5f97\u6574\u4f53\u53d8\u6210\u4e86\u6700\u5927-\u6700\u5c0f\u95ee\u9898.</li> <li>\u6587\u732e [021] \u5c06\u6587\u732e [158] \u4f7f\u7528 CLUB \u65b9\u6cd5\u8ba1\u7b97\u4e00\u4e2a\u4e0a\u754c\u4f5c\u4e3a MI \u4f30\u8ba1\u91cf, \u7528\u4e8e\u9632\u6b62\u8bf4\u8bdd\u4eba\u548c\u5185\u5bb9\u4fe1\u606f\u6cc4\u9732\u5230\u98ce\u683c\u5d4c\u5165\u4e2d.</li> <li>\u6587\u732e [117] \u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e92\u4fe1\u606f\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\u5e76\u5728\u57fa\u4e8e VAE \u65b9\u6cd5\u4e2d\u6700\u5c0f\u5316\u7528\u4e8e\u51cf\u5c11\u5185\u5bb9/\u8bf4\u8bdd\u4eba\u4fe1\u606f\u8f6c\u79fb\u5230\u98ce\u683c\u5d4c\u5165\u4e2d. \u901a\u5e38\u6a21\u578b\u9700\u8981\u4f30\u8ba1\u9690\u98ce\u683c\u5d4c\u5165\u548c\u8bf4\u8bdd\u4eba/\u5185\u5bb9\u5d4c\u5165\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f. \u4e3a\u4e86\u907f\u514d\u6709\u9650\u91c7\u6837\u4e92\u4fe1\u606f\u4f30\u8ba1\u5176\u7684\u6307\u6570\u9ad8\u7edf\u8ba1\u65b9\u5dee, \u4f5c\u8005\u5efa\u8bae\u4f7f\u7528\u4e00\u79cd\u65b0\u7b97\u6cd5\u7528\u4e8e\u4fe1\u606f\u6563\u5ea6, \u540d\u4e3a R\u00e9nyi \u6563\u5ea6. R\u00e9nyi \u6563\u5ea6\u5bfc\u51fa\u4e24\u79cd\u53d8\u4f53, \u5305\u62ec\u6700\u5c0f\u5316 Hellinger \u8ddd\u79bb\u548c\u6700\u5c0f\u5316 R\u00e9nyi \u6563\u5ea6\u4e4b\u548c.</li> </ul> <p>Fig. 10 Venn diagram of two random variables X and Y where P(X)and P(Y) represent their entropies, P(X|Y) is the conditional entropy of X given Y and P(Y|X) is the conditional entropy of Y given X, H(X,Y)is the joint entropy of X and Y and MI(X,Y) is their mutual information</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#516-wav2vec-features","title":"5.1.6.\u6ce2\u5f62\u8f6c\u5411\u91cf\u7279\u5f81 (Wav2Vec Features)","text":"<p>Wav2Vec [142] model converts speech waveform into context-dependent vectors/features. The model is trained via self-supervised or in-context training algorithms which are explained in Section 4.4. Features generated by wav2vec and similar models such as HuBERT [159] provide better representations of speech and its lexical and non-lexical information. Therefore, these models are utilized nowadays in different speech processing tasks such as speech recognition, synthesis, and downstream emotion detection.</p> <p>Wav2Vec \u6a21\u578b\u5c06\u8bed\u97f3\u6ce2\u5f62\u8f6c\u5316\u4e3a\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5411\u91cf\u6216\u7279\u5f81. \u8be5\u6a21\u578b\u901a\u8fc7\u81ea\u76d1\u7763\u6216\u4e0a\u4e0b\u6587\u8bad\u7ec3\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3. \u7531 Wav2Vec \u548c\u7c7b\u4f3c\u7684\u6a21\u578b\u5982 HuBERT \u751f\u6210\u7684\u7279\u5f81\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bed\u97f3\u53ca\u5176\u8bcd\u6c47\u548c\u975e\u8bcd\u6c47\u4fe1\u606f\u7684\u8868\u793a. \u56e0\u6b64\u8fd9\u4e9b\u6a21\u578b\u73b0\u5728\u88ab\u7528\u4e8e\u4e0d\u540c\u7684\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u4f8b\u5982\u8bed\u97f3\u8bc6\u522b, \u5408\u6210\u548c\u4e0b\u6e38\u60c5\u611f\u68c0\u6d4b.</p> <p>Some studies such as Emo-VITS [120] use Wav2vec 2.0 as a feature extractor to provide input to the reference encoder instead of spectrum features or raw audio waveform. Figure 11 illustrates the framework of the wav2vec technique and how it is utilized as a feature extractor with TTS models. The wav2vec model converts the continuous audio features into quantized finite set of discrete representations called tokens. This is done using a quantization module that maps the continuous feature vectors into a discrete set of tokens from a learned codebook. As those tokens are more abstract, they reduce the complexity of the features by retaining important features while filtering out all the irrelevant information. Because of that abstraction, it is harder to reconstruct audio from the wav2vec features, which means leakage of linguistic content into feature vectors is significantly lower compared to other features such as MFCCs.</p> <p>\u4e00\u4e9b\u7814\u7a76\u4f7f\u7528 Wav2Vec 2.0 \u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668, \u4e3a\u53c2\u8003\u7f16\u7801\u5668\u63d0\u4f9b\u8f93\u5165, \u800c\u4e0d\u662f\u539f\u59cb\u97f3\u9891\u6ce2\u5f62\u7684\u9891\u8c31\u7279\u5f81. \u56fe\u5341\u4e00\u5c55\u793a\u4e86 Wac2Vec \u6280\u672f\u53ca\u5176\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\u5982\u4f55\u4e0e TTS \u6a21\u578b\u76f8\u7ed3\u5408. Wav2Vec \u6a21\u578b\u5c06\u8fde\u7eed\u97f3\u9891\u7279\u5f81\u8f6c\u5316\u4e3a\u540d\u4e3a token \u7684\u79bb\u6563\u8868\u793a\u7684\u91cf\u5316\u6709\u9650\u96c6. \u8fd9\u901a\u8fc7\u4f7f\u7528\u91cf\u5316\u6a21\u5757\u5c06\u8fde\u7eed\u7279\u5f81\u5411\u91cf\u6620\u5c04\u5230\u53d6\u81ea\u5b66\u4e60\u597d\u7684\u7801\u672c\u7684\u79bb\u6563 token \u96c6\u5408. \u5f53\u8fd9\u4e9b token \u8d8a\u62bd\u8c61, \u5b83\u4eec\u901a\u8fc7\u4fdd\u7559\u91cd\u8981\u7279\u5f81\u5e76\u8fc7\u6ee4\u6389\u6240\u6709\u4e0d\u76f8\u5173\u7684\u4fe1\u606f\u6765\u51cf\u4f4e\u7279\u5f81\u7684\u590d\u6742\u5ea6. \u7531\u4e8e\u8fd9\u79cd\u62bd\u8c61\u6027, \u5f88\u96be\u4ece Wav2Vec \u7279\u5f81\u4e2d\u91cd\u6784\u97f3\u9891, \u8fd9\u610f\u5473\u7740\u4e0e\u5176\u4ed6\u7279\u5f81\u5982 MFCC \u76f8\u6bd4, \u8bed\u8a00\u5185\u5bb9\u6cc4\u6f0f\u5230\u7279\u5f81\u5411\u91cf\u5c06\u660e\u663e\u4e0b\u964d.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#517-orthogonality-loss","title":"5.1.7.\u6b63\u4ea4\u6027\u635f\u5931 (Orthogonality Loss)","text":"<p>Studies [34] [39] propose a model with two separate encoders to encode speaker and emotion information through speaker and emotion classification loss, along with gradient inversion of the emotion classification loss in the speaker encoder. Additionally, to disentangle the source speaker information from the emotion embedding, the emotion embedding is made orthogonal to the speaker embedding with an orthogonality loss shown in equation (3). An ablation study in [34] showed that applying an orthogonality constraint helped the encoders learn both speaker-irrelevant emotion embedding and emotion-irrelevant speaker embedding. $$     \\mathcal{L}{orth} = \\sum{i=1}^n |S_i-e_i|_F^2\\tag{3} $$</p> <p>where $|\\cdot|_F$ is the Frobenius norm, $e_i$ is the emotion embedding and $S_i$ is the speaker embedding.</p> <p>\u4e00\u4e9b\u7814\u7a76\u63d0\u51fa\u4e86\u5177\u6709\u4e24\u4e2a\u72ec\u7acb\u7f16\u7801\u5668\u7684\u6a21\u578b, \u5e76\u4e14\u901a\u8fc7\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u5206\u7c7b\u635f\u5931, \u4ee5\u53ca\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u4e2d\u7684\u60c5\u611f\u5206\u7c7b\u635f\u5931\u7684\u68af\u5ea6\u53cd\u8f6c\u6765\u7f16\u7801\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u4fe1\u606f. \u6b64\u5916, \u4e3a\u4e86\u4ece\u60c5\u611f\u5d4c\u5165\u4e2d\u89e3\u8026\u6e90\u8bf4\u8bdd\u4eba\u7684\u4fe1\u606f, \u60c5\u611f\u5d4c\u5165\u901a\u8fc7\u516c\u5f0f\u4e09\u6240\u793a\u7684\u6b63\u4ea4\u6027\u635f\u5931\u4e0e\u8bf4\u8bdd\u4eba\u5d4c\u5165\u6b63\u4ea4.  $$     \\mathcal{L}{orth} = \\sum{i=1}^n |S_i-e_i|_F^2\\tag{3} $$</p> <p>\u5176\u4e2d $|\\cdot|_F$ \u662f F \u8303\u6570, $e_i$ \u662f\u60c5\u611f\u5d4c\u5165, $S_i$ \u662f\u8bf4\u8bdd\u4eba\u5d4c\u5165.</p> <p>\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u5e94\u7528\u6b63\u4ea4\u6027\u7ea6\u675f\u6709\u52a9\u4e8e\u7f16\u7801\u5668\u5b66\u4e60\u4e0e\u8bf4\u8bdd\u4eba\u65e0\u5173\u7684\u60c5\u611f\u5d4c\u5165\u548c\u4e0e\u60c5\u611f\u65e0\u5173\u7684\u8bf4\u8bdd\u4eba\u5d4c\u5165.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#52-inference-without-reference-audio","title":"5.2.\u65e0\u53c2\u8003\u97f3\u9891\u63a8\u7406 (Inference without Reference Audio)","text":"<p>A main drawback of the unsupervised approaches (Section 4) is that they require a reference audio for the desired prosody or style of the generated speech. However, prosody references are not always available for the desired speaker, style, or text. Besides, using prosody reference introduces the leakage problem as discussed in Section 5.1. As a result, different techniques have been proposed that enable unsupervised expressive speech synthesis without prosody references. Some techniques utilize the reference audio at training phase while at inference phase speech synthesis can be done with or without a reference audio. Other techniques depend on input text only to generate prosody embedding at both training and inference phases. In the following three sections, we will describe techniques for inference without reference audio applied with each of the three main unsupervised ETTS approaches. In Section 5.2.4, we will discuss some ETTS approaches that are based on text only. Then in Table 4, we summarize main approaches that are used to extract text-based features with related papers links.</p> <p>\u7b2c\u56db\u8282\u4e2d\u603b\u7ed3\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u4e00\u4e2a\u4e3b\u8981\u7f3a\u70b9\u662f, \u5b83\u4eec\u9700\u8981\u4e00\u4e2a\u53c2\u8003\u97f3\u9891\u7528\u4e8e\u6240\u751f\u6210\u8bed\u97f3\u7684\u671f\u671b\u97f5\u5f8b\u6216\u98ce\u683c. \u7136\u800c\u6240\u9700\u8bf4\u8bdd\u4eba/\u98ce\u683c/\u6587\u672c\u7684\u97f5\u5f8b\u53c2\u8003\u5e76\u4e0d\u603b\u662f\u80fd\u591f\u83b7\u5f97. \u6b64\u5916\u4f7f\u7528\u97f5\u5f8b\u53c2\u8003\u4f1a\u5f15\u5165\u7b2c5.1\u8282\u8ba8\u8bba\u7684\u6cc4\u9732\u95ee\u9898.  \u56e0\u6b64\u51fa\u73b0\u4e86\u5404\u79cd\u6280\u672f\u4f7f\u5f97\u65e0\u76d1\u7763\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u80fd\u591f\u65e0\u97f5\u5f8b\u53c2\u8003. \u4e00\u4e9b\u6280\u672f\u5728\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u53c2\u8003\u97f3\u9891, \u800c\u63a8\u7406\u9636\u6bb5\u8bed\u97f3\u5408\u6210\u5c31\u53ef\u4ee5\u7528\u6216\u4e0d\u540c\u53c2\u8003\u97f3\u9891. \u5176\u4ed6\u6280\u672f\u53ea\u4f9d\u8d56\u4e8e\u8f93\u5165\u6587\u672c\u7528\u4e8e\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u751f\u6210\u97f5\u5f8b\u5d4c\u5165. \u5728\u4ee5\u4e0b\u4e09\u5c0f\u8282\u4e2d\u5c06\u63cf\u8ff0\u65e0\u53c2\u8003\u97f3\u9891\u7684\u63a8\u7406\u6280\u672f, \u5e94\u7528\u4e8e\u4e09\u79cd\u4e3b\u8981\u7684\u65e0\u76d1\u7763\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u65b9\u6cd5\u4e2d. \u5728\u7b2c\u56db\u5c0f\u8282\u5c06\u8ba8\u8bba\u4e00\u4e9b\u4ec5\u4f9d\u8d56\u4e8e\u6587\u672c\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u65b9\u6cd5. \u8868\u683c\u56db\u603b\u7ed3\u4e86\u76f8\u5173\u6587\u732e\u4e2d\u7528\u4e8e\u63d0\u53d6\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u7684\u4e3b\u8981\u65b9\u6cd5.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#521-direct-reference-encoding-without-reference-audio","title":"5.2.1.\u65e0\u53c2\u8003\u97f3\u9891\u7684\u76f4\u63a5\u53c2\u8003\u7f16\u7801 Direct Reference Encoding without Reference Audio","text":"<p>In several studies, prosody predictors are trained jointly with the proposed reference encoder to bypass the requirement for reference audio at inference time. The prosody predictors are trained to predict either the prosody embeddings generated by reference encoders[50] [96] [111] [116], or the acoustic features used as input to reference encoders [37] [63]. As input to these prosody predictors, most studies utilize the phoneme embeddings[37] [63] [96] [111].</p> <p>\u5728\u4e00\u4e9b\u7814\u7a76\u4e2d, \u97f5\u5f8b\u9884\u6d4b\u5668\u4e0e\u6240\u63d0\u51fa\u7684\u53c2\u8003\u7f16\u7801\u5668\u8054\u5408\u8bad\u7ec3, \u4ee5\u7ed5\u8fc7\u63a8\u7406\u65f6\u5bf9\u53c2\u8003\u97f3\u9891\u7684\u8981\u6c42. \u97f5\u5f8b\u9884\u6d4b\u5668\u88ab\u8bad\u7ec3\u7528\u4e8e\u9884\u6d4b\u53c2\u8003\u7f16\u7801\u5668\u751f\u6210\u7684\u97f5\u5f8b\u5d4c\u5165\u6216\u7528\u4f5c\u53c2\u8003\u7f16\u7801\u5668\u8f93\u5165\u7684\u58f0\u5b66\u7279\u5f81. \u4f5c\u4e3a\u8fd9\u4e9b\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8f93\u5165, \u5927\u591a\u6570\u7814\u7a76\u4f7f\u7528\u97f3\u7d20\u5d4c\u5165.</p> <p>Alternatively, features extracted from input text can also be used as input for prosody predictors. In [50], the prosody predictor has a hierarchical structure that utilizes contextual information at both the sentence and paragraph levels to predict prosody embeddings. The input features for this predictor are in the form of 768-dimensional phrase embeddings extracted by the pre-trained language model XLNet [160]. Sentence embeddings are initially predicted from the input features using an attention network. Then a second attention network is used to predict the paragraph-level prosody embedding.</p> <p>\u6216\u8005\u4ece\u8f93\u5165\u6587\u672c\u63d0\u53d6\u7684\u7279\u5f81\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8f93\u5165. \u6587\u732e [050] \u4e2d\u97f5\u5f8b\u9884\u6d4b\u5176\u6709\u4e00\u4e2a\u5c42\u6b21\u7ed3\u6784, \u5229\u7528\u53e5\u5b50\u548c\u6bb5\u843d\u7ea7\u522b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u9884\u6d4b\u97f5\u5f8b\u5d4c\u5165. \u8fd9\u4e00\u9884\u6d4b\u5668\u7684\u8f93\u5165\u7279\u5f81\u662f\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b XLNet \u63d0\u53d6\u7684 768 \u7ef4\u7684\u77ed\u8bed\u5d4c\u5165\u5f62\u5f0f. \u9996\u5148\u901a\u8fc7\u6ce8\u610f\u529b\u7f51\u7edc\u4ece\u8f93\u5165\u7279\u5f81\u4e2d\u9884\u6d4b\u53e5\u5b50\u5d4c\u5165, \u7136\u540e\u7b2c\u4e8c\u4e2a\u6ce8\u610f\u529b\u7f51\u7edc\u7528\u4e8e\u9884\u6d4b\u6bb5\u843d\u7ea7\u522b\u7684\u97f5\u5f8b\u5d4c\u5165.</p> <p>Furthermore, in [33], emotion is modelled at three levels: global, utterance, and syllable (local). The model employs three prosody encoders, each with a predictor trained to predict the corresponding prosody embedding based on input text. The global-level predictor functions as an emotion classifier, where the output of its final soft-max layer serves as the global emotion embedding. The emotion label\u2019s embedding is used as the ground truth for this emotion classifier. Both the utterance and local prosody encoders receive level-aligned mel-spectrograms as input and produce utterance prosody embedding and local prosody strength embedding, respectively. Similarly, two prosody predictors are used to predict utterance and local-level embeddings based on the output from the text encoder of the TTS model.</p> <p>\u6587\u732e [033] \u4e2d\u60c5\u611f\u88ab\u5efa\u6a21\u5728\u4e09\u4e2a\u7ea7\u522b: \u5168\u5c40, \u8bed\u8c03\u548c\u97f3\u8282 (\u5c40\u90e8). \u6a21\u578b\u5e94\u7528\u4e09\u4e2a\u97f5\u5f8b\u7f16\u7801\u5668, \u6bcf\u4e2a\u7f16\u7801\u5668\u90fd\u6709\u4e00\u4e2a\u9884\u6d4b\u5668\u8bad\u7ec3\u6210\u57fa\u4e8e\u8f93\u5165\u6587\u672c\u9884\u6d4b\u5bf9\u5e94\u7684\u97f5\u5f8b\u5d4c\u5165. \u5168\u5c40\u9884\u6d4b\u5668\u4f5c\u4e3a\u60c5\u611f\u5206\u7c7b\u5668, \u5176\u6700\u7ec8\u7684 softmax \u5c42\u8f93\u51fa\u4f5c\u4e3a\u5168\u5c40\u60c5\u611f\u5d4c\u5165. \u60c5\u611f\u6807\u7b7e\u7684\u5d4c\u5165\u4f5c\u4e3a\u8fd9\u4e00\u60c5\u611f\u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c. \u8bed\u8c03\u548c\u5c40\u90e8\u97f5\u5f8b\u7f16\u7801\u5668\u90fd\u63a5\u6536\u7ea7\u522b\u5bf9\u9f50\u7684\u6885\u5c14\u9891\u8c31\u4f5c\u4e3a\u8f93\u5165, \u5e76\u5206\u522b\u751f\u6210\u8bed\u8c03\u97f5\u5f8b\u5d4c\u5165\u548c\u5c40\u90e8\u97f5\u5f8b\u5f3a\u5ea6\u5d4c\u5165. \u7c7b\u4f3c\u5730, \u4e24\u4e2a\u97f5\u5f8b\u9884\u6d4b\u5668\u57fa\u4e8e TTS \u6a21\u578b\u7684\u6587\u672c\u7f16\u7801\u5668\u7684\u8f93\u51fa\u9884\u6d4b\u8bed\u8c03\u548c\u5c40\u90e8\u7ea7\u522b\u5d4c\u5165.</p> <p>In contrast, the prosody predictor proposed in paper [44] learns multiple mixed Gaussian distributions model (GMM) for prosody representations. Therefore, the final outputs of the prosody predictor involve three parameters: mean, variance, and weight of multiple mixed Gaussian distributions from which prosody representations can be sampled at inference time. As input, the predictor receives two phoneme-level sequences including embeddings from the text encoder and embeddings from a pre-trained language model. Similar work is proposed in [95] where only phoneme embeddings are used as input to the prosody predictor. GMM in both studies is modeled via the mixture density network [161].</p> <p>\u6587\u732e [044] \u63d0\u51fa\u7684\u97f5\u5f8b\u9884\u6d4b\u5668\u5b66\u4e60\u591a\u91cd\u6df7\u5408\u9ad8\u65af\u5206\u5e03\u6a21\u578b\u7528\u4e8e\u97f5\u5f8b\u8868\u793a. \u56e0\u6b64\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u6700\u7ec8\u8f93\u51fa\u5305\u542b\u4e09\u4e2a\u53c2\u6570: \u5747\u503c, \u65b9\u5dee\u548c\u591a\u6df7\u5408\u9ad8\u65af\u5206\u5e03\u7684\u6743\u91cd. \u4ece\u8fd9\u4e9b\u5206\u5e03\u4e2d\u53ef\u4ee5\u5728\u63a8\u7406\u65f6\u91c7\u6837\u97f5\u5f8b\u8868\u793a. \u4f5c\u4e3a\u8f93\u5165, \u9884\u6d4b\u5668\u63a5\u6536\u4e24\u4e2a\u97f3\u7d20\u7ea7\u522b\u7684\u5e8f\u5217\u5305\u62ec\u6765\u81ea\u6587\u672c\u7f16\u7801\u5668\u7684\u5d4c\u5165\u548c\u6765\u81ea\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165.</p> <p>\u6587\u732e [095] \u63d0\u51fa\u7684\u76f8\u4f3c\u5de5\u4f5c, \u53ea\u4f7f\u7528\u97f3\u7d20\u5d4c\u5165\u4f5c\u4e3a\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8f93\u5165.</p> <p>\u4e24\u9879\u5de5\u4f5c\u7684 GMM \u90fd\u901a\u8fc7\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\u8fdb\u884c\u5efa\u6a21.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#522vae-vaebased-approaches-without-reference-audio","title":"5.2.2.\u65e0\u53c2\u8003\u8bed\u97f3\u7684VAE\u7c7b\u65b9\u6cd5 (VAE\u2011Based Approaches without Reference Audio)","text":"<p>Sampling from the latent space without reference audio results in less controllability of style. In addition, it can also introduce naturalness degradation and inappropriate contextual prosody with regard to the input text [68] [129].  Therefore, to avoid sampling the latent space without a reference, authors of [131] proposed utilizing the same prosody embedding of the most similar training sentence to input sentence at inference time. The selection process is based on measuring cosine similarity between sentences\u2019 linguistic features. Three methods are proposed for extracting sentence linguistic information including  (1) calculating the syntactic distance between words in the sentence using constituency trees [162],  (2) averaging the contextual word embeddings (CWE) for the words in the sentence using BERT, and  (3) combining the previous two methods.</p> <p>\u5728\u6ca1\u6709\u53c2\u8003\u97f3\u9891\u7684\u60c5\u51b5\u4e0b\u4ece\u9690\u7a7a\u95f4\u91c7\u6837\u4f1a\u5bfc\u81f4\u98ce\u683c\u7684\u53ef\u63a7\u6027\u964d\u4f4e. \u6b64\u5916\u5b83\u8fd8\u4f1a\u5f15\u5165\u81ea\u7136\u6027\u9000\u5316\u548c\u548c\u4e0d\u9002\u5408\u8f93\u5165\u6587\u672c\u7684\u8bed\u5883\u97f5\u5f8b. \u56e0\u6b64\u4e3a\u4e86\u907f\u514d\u5728\u6ca1\u6709\u53c2\u8003\u7684\u60c5\u51b5\u4e0b\u4ece\u9690\u7a7a\u95f4\u4e2d\u91c7\u6837, \u6587\u732e [131] \u63d0\u51fa\u63a8\u7406\u65f6\u7528\u548c\u8f93\u5165\u53e5\u5b50\u6700\u76f8\u4f3c\u7684\u8bad\u7ec3\u53e5\u5b50\u7684\u76f8\u540c\u97f5\u5f8b\u5d4c\u5165. \u9009\u62e9\u8fc7\u7a0b\u57fa\u4e8e\u53e5\u5b50\u8bed\u8a00\u7279\u5f81\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6. \u4e09\u79cd\u65b9\u6cd5\u7528\u4e8e\u63d0\u53d6\u53e5\u5b50\u8bed\u8a00\u4fe1\u606f: 1. \u4f7f\u7528\u53e5\u6cd5\u6811\u8ba1\u7b97\u53e5\u5b50\u4e2d\u5355\u8bcd\u7684\u53e5\u6cd5\u8ddd\u79bb; 2. \u4f7f\u7528 BERT \u8ba1\u7b97\u53e5\u5b50\u4e2d\u5355\u8bcd\u7684\u4e0a\u4e0b\u6587\u8bcd\u5d4c\u5165 (Contextual Word Embeddings, CWE) \u7684\u5e73\u5747\u503c; 3. \u7ed3\u5408\u524d\u4e24\u79cd\u65b9\u6cd5.</p> <p>Other studies approach the problem in alternative ways, seeking to enhance the sampling process either through refining the baseline model structure or by incorporating text-based components into the baseline. Regarding the improvement of the baseline structure, study [68] suggests the combination of multiple variational autoencoders to generate latent variables at three distinct levels: utterance-level, phrase-level, and word-level. Furthermore, they apply a conditional prior (CP) to learn the latent space distribution based on the input text embedding. To account for dependencies within the input text, they employ Autoregressive (AR) latent converters to transform latent variables from coarser to finer levels. An alternative approach is proposed in [126] by replacing the conventional VAE encoder with a residual encoder that leverages phoneme embedding and a set of learnable free parameters as inputs. With this modified structure, the model learns a latent distribution that represents various prosody styles for a specific sentence (i.e.,the input text), in addition to capturing potential global biases within the applied dataset (represented by the free parameters). At the same time, with this modification, the problem of speaker and content leakage into prosody embedding is addressed.</p> <p>\u5176\u4ed6\u7814\u7a76\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898, \u5bfb\u6c42\u901a\u8fc7\u6539\u8fdb\u57fa\u7ebf\u6a21\u578b\u7ed3\u6784\u6216\u5411\u57fa\u7ebf\u6a21\u578b\u4e2d\u6dfb\u52a0\u57fa\u4e8e\u6587\u672c\u7684\u7ec4\u4ef6\u6765\u589e\u5f3a\u91c7\u6837\u8fc7\u7a0b. \u5173\u4e8e\u57fa\u7ebf\u7ed3\u6784\u7684\u6539\u8fdb: - \u6587\u732e [068] \u5efa\u8bae\u7ec4\u5408\u591a\u4e2a\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u751f\u6210\u4e09\u4e2a\u4e0d\u540c\u7ea7\u522b\u7684\u9690\u53d8\u91cf: \u8bed\u8c03\u7ea7, \u77ed\u8bed\u7ea7\u548c\u5355\u8bcd\u7ea7. \u6b64\u5916\u5e94\u7528\u4e86\u6761\u4ef6\u5148\u9a8c\u57fa\u4e8e\u8f93\u5165\u6587\u672c\u5d4c\u5165\u5b66\u4e60\u9690\u7a7a\u95f4\u5206\u5e03. \u4e3a\u4e86\u8003\u8651\u8f93\u5165\u6587\u672c\u5185\u7684\u4f9d\u8d56\u6027, \u4ed6\u4eec\u5e94\u7528\u81ea\u56de\u5f52\u9690\u8f6c\u5316\u5668\u5c06\u9690\u53d8\u91cf\u4ece\u7c97\u7cd9\u7ea7\u522b\u8f6c\u5316\u4e3a\u7cbe\u7ec6\u7ea7\u522b. - \u6587\u732e [126] \u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5, \u901a\u8fc7\u5c06\u4f20\u7edf VAE \u66ff\u6362\u4e3a\u4e00\u4e2a\u5229\u7528\u97f3\u7d20\u5d4c\u5165\u548c\u4e00\u7ec4\u53ef\u5b66\u4e60\u81ea\u7531\u53c2\u6570\u4f5c\u4e3a\u8f93\u5165\u7684\u6b8b\u5dee\u7f16\u7801\u5668. \u91c7\u7528\u8fd9\u4e00\u7ed3\u6784, \u6a21\u578b\u4e3a\u4e00\u4e2a\u5177\u4f53\u53e5\u5b50\u5373\u8f93\u5165\u6587\u672c\u5b66\u4e60\u4e00\u4e2a\u9690\u5206\u5e03\u8868\u793a\u5404\u79cd\u97f5\u5f8b\u98ce\u683c, \u540c\u65f6\u6355\u6349\u5e94\u7528\u6570\u636e\u96c6\u7684\u6f5c\u5728\u5168\u5c40\u504f\u5dee (\u7531\u81ea\u7531\u53c2\u6570\u8868\u793a). \u540c\u65f6\u8bf4\u8bdd\u4eba\u548c\u5185\u5bb9\u6cc4\u9732\u5230\u97f5\u5f8b\u5d4c\u5165\u7684\u95ee\u9898\u4e5f\u5f97\u5230\u89e3\u51b3.</p> <p>Various studies propose training a predictor for the latent prosody vectors based on features extracted from the input text [35] [47]. The proposed model in [47] generates fine-grained prosody latent codes of three dimensions at phoneme-level. These prosody codes are then used to guide the training process of a prosody predictor that receives phoneme embeddings as input, in addition to emotion and speaker embeddings as sentence-level conditions. In [35], the predicted mean values of the latent space distribution are employed as prosody codes. Similarly, a prosody predictor is trained to predict these prosody codes using two text-based inputs, including sentence-level embeddings from a pre-trained BERT model and contextual information considering BERT embeddings of a few of surrounding k sentences given the current sentence.</p> <p>\u591a\u9879\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u8bad\u7ec3\u4e00\u4e2a\u9690\u97f5\u5f8b\u5411\u91cf\u7684\u9884\u6d4b\u5668. - \u6587\u732e [047] \u63d0\u51fa\u7684\u6a21\u578b\u5728\u97f3\u7d20\u7ea7\u522b\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u4e09\u7ef4\u97f5\u5f8b\u9690\u4ee3\u7801. \u8fd9\u4e9b\u97f5\u5f8b\u4ee3\u7801\u4e4b\u540e\u7528\u4e8e\u6307\u5bfc\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8bad\u7ec3. \u9884\u6d4b\u5668\u63a5\u6536\u97f3\u7d20\u5d4c\u5165\u4f5c\u4e3a\u8f93\u5165, \u6b64\u5916\u60c5\u611f\u548c\u8bf4\u8bdd\u4eba\u5d4c\u5165\u4f5c\u4e3a\u53e5\u5b50\u7ea7\u522b\u6761\u4ef6. - \u6587\u732e [035] \u4e2d\u9690\u7a7a\u95f4\u5206\u5e03\u7684\u9884\u6d4b\u5747\u503c\u4f5c\u4e3a\u97f5\u5f8b\u7f16\u7801. \u7c7b\u4f3c\u5730\u8bad\u7ec3\u4e00\u4e2a\u97f5\u5f8b\u9884\u6d4b\u5668\u4f7f\u7528\u4e24\u4e2a\u57fa\u4e8e\u6587\u672c\u7684\u8f93\u5165\u5305\u62ec\u6765\u81ea\u9884\u8bad\u7ec3 BERT \u6a21\u578b\u7684\u53e5\u5b50\u7ea7\u522b\u5d4c\u5165\u548c\u8003\u8651\u5f53\u524d\u53e5\u5b50\u5468\u56f4\u7684 k \u4e2a\u53e5\u5b50\u7684 BERT \u5d4c\u5165\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u7528\u4e8e\u9884\u6d4b\u8fd9\u4e9b\u97f5\u5f8b\u7f16\u7801.</p> <p>Alternatively, study [129] proposed training a sampler, i.e., Gaussian parameters, to sample the latent space using features extracted from the input text. Three different structures are investigated for the sampler based on the input features it receives. The applied text-based features include BERT representations of a sentence (semantic information), the parsing tree of the sentence (syntactic information) after it is fed to a graph attention network, and the concatenation of outputs from the previous two samplers.</p> <p>\u6587\u732e [129] \u63d0\u51fa\u8bad\u7ec3\u4e00\u4e2a\u91c7\u6837\u5668, \u5373\u9ad8\u65af\u53c2\u6570, \u4f7f\u7528\u8f93\u5165\u6587\u672c\u63d0\u53d6\u7684\u7279\u5f81\u5bf9\u9690\u7a7a\u95f4\u8fdb\u884c\u91c7\u6837. \u6839\u636e\u91c7\u6837\u5668\u63a5\u6536\u7684\u8f93\u5165\u7279\u5f81, \u7814\u7a76\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u7ed3\u6784. \u5e94\u7528\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u5305\u62ec\u53e5\u5b50\u7684 BERT \u8868\u793a (\u8bed\u4e49\u4fe1\u606f), \u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u8f93\u51fa\u7684\u53e5\u5b50\u7684\u89e3\u6790\u6811 (\u8bed\u6cd5\u4fe1\u606f) \u548c\u524d\u4e24\u4e2a\u91c7\u6837\u5668\u7684\u8f93\u51fa\u7684\u62fc\u63a5.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#523-gst-gstbased-approaches-without-reference-audio","title":"5.2.3.\u65e0\u53c2\u8003\u97f3\u9891\u7684 GST \u7c7b\u65b9\u6cd5 (GST\u2011Based Approaches without Reference Audio)","text":"<p>There are GST-TTS models that utilize text-based features from pre-trained language models such as BERT to guide expressive speech synthesis at inference time without a reference. In ST-TTS, the training dataset is labeled with short phrases that describe the style of the utterance and are known as style tags. A pre-trained Sentence BERT (SBERT) model is used to produce embeddings for each style tag as input to a style tag encoder. The style embedding from the GST-TTS model is used as ground truth for the style tag encoder. During inference, either a reference audio or a style tag can be used to generate speech.</p> <p>\u6709\u4e9b GST-TTS \u6a21\u578b\u4f7f\u7528\u6765\u81ea\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (\u5982 BERT) \u7684\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u7528\u4e8e\u6307\u5bfc\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u65e0\u53c2\u8003\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210. - ST-TTS \u4e2d\u8bad\u7ec3\u96c6\u7528\u63cf\u8ff0\u8bed\u8c03\u98ce\u683c\u7684\u77ed\u8bed\u8fdb\u884c\u6807\u6ce8, \u5373\u98ce\u683c\u6807\u7b7e. \u7528\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u53e5\u5b50 BERT \u6a21\u578b (SBERT) \u4e3a\u6bcf\u4e2a\u98ce\u683c\u6807\u7b7e\u751f\u6210\u5d4c\u5165\u4f5c\u4e3a\u98ce\u683c\u6807\u7b7e\u7f16\u7801\u5668\u7684\u8f93\u5165. \u6765\u81ea GST-TTS \u6a21\u578b\u7684\u98ce\u683c\u5d4c\u5165\u4f5c\u4e3a\u98ce\u683c\u6807\u7b7e\u7f16\u7801\u5668\u7684\u771f\u5b9e\u503c. \u5728\u63a8\u7406\u65f6, \u53ef\u4ee5\u4f7f\u7528\u53c2\u8003\u97f3\u9891\u6216\u98ce\u683c\u6807\u7b7e\u7528\u4e8e\u751f\u6210\u8bed\u97f3.</p> <p>Alternatively, pre-trained language models are used to extract features from the input text and train a prosody predictor to predict the style embedding based on these text-based features (Context-Aware Style Predictor, [46] [50] [73] [91] [94]. In [94], the baseline model [75] is extended with a prosody predictor module that extracts time-aggregated features from the output of the baseline text encoder. Two pathways are suggested for the targets of the predictor output: either using the weights of the GSTs or the final style embedding. Similarly, in [73], two prosody predictors are investigated, using different inputs from a pre-trained multi-language BERT model. While the first predictor utilizes BERT embeddings for the sub-word sequence of input text, the other predictor employs only the CLS token from the sentence-level information extracted by the BERT model. Both inputs provide rich information for the predictors to synthesize prosodic speech based solely on input text.</p> <p>\u6216\u8005\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u6765\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u7279\u5f81\u5e76\u8bad\u7ec3\u4e00\u4e2a\u97f5\u5f8b\u9884\u6d4b\u5668, \u57fa\u4e8e\u8fd9\u4e9b\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u6765\u9884\u6d4b\u98ce\u683c\u5d4c\u5165. - \u6587\u732e [094] \u4e2d\u5c06\u57fa\u7ebf\u6a21\u578b [075] \u7528\u97f5\u5f8b\u9884\u6d4b\u5668\u6a21\u5757\u8fdb\u884c\u6269\u5c55, \u7528\u4e8e\u4ece\u57fa\u7ebf\u6587\u672c\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4e2d\u63d0\u53d6\u65f6\u95f4\u805a\u5408\u7279\u5f81. \u5bf9\u4e8e\u9884\u6d4b\u5668\u8f93\u51fa\u7684\u76ee\u6807\u6709\u4e24\u4e2a\u5efa\u8bae: \u4f7f\u7528 GSTs \u7684\u6743\u91cd\u6216\u6700\u7ec8\u98ce\u683c\u5d4c\u5165. - \u6587\u732e [073] \u7814\u7a76\u4e86\u4e24\u4e2a\u97f5\u5f8b\u9884\u6d4b\u5668, \u4f7f\u7528\u6765\u81ea\u9884\u8bad\u7ec3\u591a\u8bed\u8a00 BERT \u6a21\u578b\u7684\u4e0d\u540c\u8f93\u5165. \u7b2c\u4e00\u4e2a\u9884\u6d4b\u5668\u4f7f\u7528\u8f93\u5165\u6587\u672c\u7684\u5b50\u8bcd\u5e8f\u5217\u7684 BERT \u5d4c\u5165, \u7b2c\u4e8c\u4e2a\u9884\u6d4b\u5668\u4ec5\u4f7f\u7528\u4ece BERT \u6a21\u578b\u63d0\u53d6\u7684\u53e5\u5b50\u7ea7\u522b\u4fe1\u606f\u4e2d\u7684 CLS token. \u8fd9\u4e24\u4e2a\u8f93\u5165\u90fd\u4e3a\u9884\u6d4b\u5668\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u4fe1\u606f\u4ece\u800c\u4ec5\u4f9d\u9760\u8f93\u5165\u6587\u672c\u5408\u6210\u97f5\u5f8b\u8bed\u97f3.</p> <p>The multi-scale GST-TTS proposed in [50] which employs three style encoders, also introduces three style predictors that employ hierarchical context encoders (HCE). The input to the first predictor is the BERT sub-word-level semantic embedding sequence. The attention units in the HCE, however, are used to aggregate the resulting context embedding sequence from lower-level as input to higher-level predictors. Additionally, the output of the higher-level predictor is used to condition the lower-level predictor. BERT embeddings are also used in [46] but at word level and are passed as input to the proposed prosody predictor. The style embedding which is generated via word-level GSTs is used to guide the prosody predictor during model training.</p> <ul> <li>\u6587\u732e [050] \u63d0\u51fa\u7684\u591a\u5c3a\u5ea6 GST-TTS \u91c7\u7528\u4e86\u4e09\u4e2a\u98ce\u683c\u7f16\u7801\u5668, \u4e5f\u5f15\u5165\u4e86\u4e09\u4e2a\u91c7\u7528\u5c42\u6b21\u4e0a\u4e0b\u6587\u7f16\u7801\u5668 (Hierarchical Context Encoders, HCE) \u7684\u98ce\u683c\u9884\u6d4b\u5668. \u7b2c\u4e00\u4e2a\u9884\u6d4b\u5668\u7684\u8f93\u5165\u662f BERT \u5b50\u8bcd\u7ea7\u522b\u8bed\u4e49\u5d4c\u5165\u5e8f\u5217. HCE \u4e2d\u7684\u6ce8\u610f\u529b\u5355\u5143\u7528\u4e8e\u805a\u5408\u7531\u4f4e\u5c42\u6b21\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u5e8f\u5217\u4f5c\u4e3a\u9ad8\u5c42\u6b21\u9884\u6d4b\u5668\u7684\u8f93\u5165. \u6b64\u5916\u9ad8\u5c42\u6b21\u9884\u6d4b\u5668\u7684\u8f93\u51fa\u7528\u4e8e\u6761\u4ef6\u5316\u4f4e\u5c42\u6b21\u9884\u6d4b\u5668. </li> <li>\u6587\u732e [044] \u540c\u6837\u4f7f\u7528 BERT \u5d4c\u5165\u4f46\u662f\u5728\u5355\u8bcd\u7ea7\u522b, \u5e76\u4e14\u4f5c\u4e3a\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8f93\u5165. \u901a\u8fc7\u8bcd\u7ea7 GSTs \u751f\u6210\u7684\u98ce\u683c\u5d4c\u5165\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\u7528\u4e8e\u6307\u5bfc\u97f5\u5f8b\u9884\u6d4b\u5668.</li> </ul> <p>A Context-aware prosody predictor is proposed in \"Context-Aware Coherent Speaking Style Prediction with Hierarchical Transformers for Audiobook Speech Synthesis\" which considers both text-side context information and speech-side style information from preceding speech. This predictor comprises two hierarchical components: a sentence encoder and a fusion context encoder. The context-aware input to the predictor includes word-level embeddings from XLNet [160] for each word in the current sentence, as well as the N preceding and following sentences. The sentence encoder focuses on learning low-level word meanings within each sentence, while the fusion context encoder captures high-level contextual semantics between the sentences. Additionally, style embeddings from previous sentences are integrated into the fusion context encoder input to account for speech-side information.</p> <p>\u6587\u732e\u63d0\u51fa\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u97f5\u5f8b\u9884\u6d4b\u5668, \u8003\u8651\u6765\u6e90\u4e8e\u4e4b\u524d\u8bed\u97f3\u7684\u6587\u672c\u4fa7\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u8bed\u97f3\u4fa7\u7684\u98ce\u683c\u4fe1\u606f. \u8fd9\u4e00\u9884\u6d4b\u5668\u5305\u542b\u4e24\u4e2a\u5c42\u6b21\u7ec4\u4ef6: \u4e00\u4e2a\u53e5\u5b50\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u878d\u5408\u5185\u5bb9\u7f16\u7801\u5668. \u9884\u6d4b\u5668\u7684\u5185\u5bb9\u611f\u77e5\u8f93\u5165\u5305\u62ec XLNet \u5bf9\u5f53\u524d\u53e5\u5b50\u6bcf\u4e2a\u5355\u8bcd\u7684\u7684\u8bcd\u7ea7\u5d4c\u5165\u548c N \u4e2a\u524d\u540e\u53e5\u5b50. \u53e5\u5b50\u7f16\u7801\u5668\u91cd\u70b9\u5b66\u4e60\u53e5\u5b50\u5185\u4f4e\u5c42\u6b21\u5355\u8bcd\u542b\u4e49, \u800c\u878d\u5408\u5185\u5bb9\u7f16\u7801\u5668\u6355\u83b7\u53e5\u5b50\u95f4\u7684\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u8bed\u4e49. \u6b64\u5916\u524d\u9762\u53e5\u5b50\u7684\u98ce\u683c\u5d4c\u5165\u6574\u5408\u5230\u878d\u5408\u5185\u5bb9\u7f16\u7801\u5668\u7684\u8f93\u5165\u4ee5\u8003\u8651\u8bed\u97f3\u4fa7\u7684\u4fe1\u606f.</p> <p>In [91] Speech emotion recognition model (SER) is employed as a style descriptor to learn the implicit connection between style features and input text. Deep style features for both synthesized speech and reference speech are obtained from a small intermediate fully connected layer of a pre-trained SER model during training. The extracted style features are compared where an additional loss is introduced to the GST-TTS model loss. At inference time only text is used to synthesize expressive speech.</p> <p>\u6587\u732e [091] \u8bed\u97f3\u60c5\u611f\u8bc6\u522b (Speech Emotion Recognition, SER) \u6a21\u578b\u4f5c\u4e3a\u98ce\u683c\u63cf\u8ff0\u5668, \u5b66\u4e60\u98ce\u683c\u7279\u5f81\u548c\u8f93\u5165\u6587\u672c\u4e4b\u95f4\u7684\u9690\u5f0f\u8054\u7cfb. \u5408\u6210\u8bed\u97f3\u548c\u53c2\u8003\u8bed\u97f3\u7684\u6df1\u5ea6\u98ce\u683c\u7279\u5f81\u5728\u8bad\u7ec3\u65f6\u4ece\u9884\u8bad\u7ec3 SER \u6a21\u578b\u7684\u4e00\u4e2a\u5c0f\u7684\u4e2d\u95f4\u5168\u8fde\u63a5\u5c42\u4e2d\u83b7\u5f97. \u5f53\u7ed9 GST-TTS \u6a21\u578b\u635f\u5931\u5f15\u5165\u989d\u5916\u7684\u635f\u5931\u540e, \u5bf9\u63d0\u53d6\u7684\u98ce\u683c\u7279\u5f81\u8fdb\u884c\u4e86\u5bf9\u6bd4. \u5728\u63a8\u7406\u65f6\u4ec5\u8f93\u5165\u6587\u672c\u7528\u4e8e\u5408\u6210\u8868\u8fbe\u6027\u8bed\u97f3.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#524-etts-approaches-based-only-on-text","title":"5.2.4.\u4ec5\u4f9d\u8d56\u4e8e\u6587\u672c\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u65b9\u6cd5 (ETTS Approaches Based only on Text)","text":"<p>This category involves approaches that depend solely on input text to obtain prosody-related representations/embeddings during TTS model training. Several features related to speech prosody have been proposed by various studies for extraction from input text and subsequent transmission to a DNN-based module to generate prosody representations. For instance, the features extracted by the pre-trained language models can capture both semantic and syntactic relationships with the input text, making them effective representations for prosody. In [83], input text word-level embeddings are extracted by the Embeddings from Language Models (ELMo) model [163] and used to generate context-related embeddings via a context encoder. Similarly, in [29], BERT is employed to extract embeddings for utterance sentences and pass them to a specific context-encoder to aggregate these embeddings and form a final context vector.</p> <p>\u8fd9\u7c7b\u65b9\u6cd5\u5305\u542b\u5728\u8bad\u7ec3\u9636\u6bb5\u4ec5\u4f9d\u8d56\u8f93\u5165\u6587\u672c\u4ee5\u83b7\u53d6\u97f5\u5f8b\u76f8\u5173\u8868\u793a\u6216\u5d4c\u5165\u7684\u65b9\u6cd5. \u5404\u79cd\u7814\u7a76\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u51fa\u548c\u8bed\u97f3\u97f5\u5f8b\u76f8\u5173\u7684\u82e5\u5e72\u7279\u5f81, \u7136\u540e\u4f20\u8f93\u5230\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u5757\u4ee5\u751f\u6210\u97f5\u5f8b\u8868\u793a. \u4f8b\u5982\u7531\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u53ef\u4ee5\u6355\u83b7\u8f93\u5165\u6587\u672c\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u5173\u7cfb, \u4f7f\u4e4b\u6210\u4e3a\u97f5\u5f8b\u7684\u6709\u6548\u8868\u793a. - \u6587\u732e [083] \u901a\u8fc7 Embeddings from Language Models, ELMo \u63d0\u53d6\u8f93\u5165\u6587\u672c\u8bcd\u7ea7\u5d4c\u5165\u5e76\u901a\u8fc7\u5185\u5bb9\u7f16\u7801\u5668\u751f\u6210\u5185\u5bb9\u76f8\u5173\u7684\u5d4c\u5165. - \u6587\u732e [029] \u4f7f\u7528 BERT \u63d0\u53d6\u8bed\u8c03\u5e8f\u5217\u7684\u5d4c\u5165\u5e76\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9\u5177\u4f53\u6587\u672c\u7f16\u7801\u5668\u4ee5\u805a\u5408\u8fd9\u4e9b\u5d4c\u5165\u5e76\u5f62\u6210\u6700\u7ec8\u7684\u4e0a\u4e0b\u6587\u5411\u91cf.</p> <p>Other studies, such as [30] [40] [54], utilize graph representations of input text, which can also reflect semantic and syntactic information about the given text. In [30], the graphical representations of prosody boundaries in Chinese text are passed to a graph encoder based on Graph Neural Networks (GNN) to generate prosodic information for the input text. The prosody boundaries of the Chinese language can be manually annotated or predicted using a pre-trained model. In contrast, [54] combines BERT-extracted features for input text with its graph dependency tree to produce word-level prosody representations. Specifically, the input text is passed through both BERT and a dependency parsing model to extract the dependency tree for word-level BERT embedding. A Relational Gated Graph Network (RGGN) is used to convert this dependency tree into word-level semantic representations upon which the decoder of the TTS model is conditioned.</p> <p>\u5176\u4ed6\u7814\u7a76\u5229\u7528\u8f93\u5165\u6587\u672c\u7684\u56fe\u8868\u793a, \u540c\u6837\u53ef\u4ee5\u53cd\u6620\u7ed9\u5b9a\u6587\u672c\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u4fe1\u606f. - \u6587\u732e [030] \u5c06\u6c49\u8bed\u6587\u672c\u7684\u97f5\u5f8b\u8fb9\u754c\u7684\u56fe\u5f62\u8868\u793a\u8f6c\u9012\u7ed9\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u7f16\u7801\u5668\u4ee5\u751f\u6210\u8f93\u5165\u6587\u672c\u7684\u97f5\u5f8b\u4fe1\u606f. \u4e2d\u6587\u7684\u97f5\u5f8b\u8fb9\u754c\u53ef\u4ee5\u4eba\u5de5\u6807\u6ce8\u6216\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b.  - \u6587\u732e [054] \u5c06 BERT \u4ece\u8f93\u5165\u6587\u672c\u63d0\u53d6\u7684\u7279\u5f81\u548c\u5b83\u7684\u56fe\u4f9d\u8d56\u6811\u7ed3\u5408\u7528\u4e8e\u4ea7\u751f\u8bcd\u7ea7\u97f5\u5f8b\u8868\u793a. \u5177\u4f53\u5730, \u8f93\u5165\u6587\u672c\u4f20\u8f93\u5230 BERT \u548c\u4e00\u4e2a\u4f9d\u8d56\u89e3\u6790\u6a21\u578b\u7528\u4e8e\u63d0\u53d6\u8bcd\u7ea7 BERT \u5d4c\u5165\u7684\u4f9d\u8d56\u6811. \u4f7f\u7528\u76f8\u5173\u95e8\u63a7\u56fe\u7f51\u7edc (Relational Gated Graph Network, RGGN) \u5c06\u4f9d\u8d56\u6811\u8f6c\u5316\u4e3a\u8bcd\u7ea7\u8bed\u4e49\u8868\u793a, \u5e76\u4ee5\u6b64\u6761\u4ef6\u5316 TTS \u6a21\u578b\u7684\u89e3\u7801\u5668.</p> <p>Different text-based features have been extracted from input text to obtain prosody (style) embeddings in [40]. The paper utilizes an emotion lexicon to extract word-level emotion features, including VAD (valence, arousal, dominance) and BE5 (joy, anger, sadness, fear, disgust). Additionally, the [CLS] embedding by BERT for each utterance is also extracted. The obtained features are then passed to a style encoder to produce a style embedding.</p> <ul> <li>\u6587\u732e [40] \u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u4e0d\u540c\u7684\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u7528\u4e8e\u83b7\u5f97\u97f5\u5f8b/\u98ce\u683c\u5d4c\u5165. \u4f7f\u7528\u4e00\u4e2a\u60c5\u611f\u8bcd\u5178\u7528\u4e8e\u63d0\u53d6\u8bcd\u7ea7\u60c5\u611f\u7279\u5f81, \u5305\u62ec VAD (\u6548\u4ef7, \u5524\u9192, \u652f\u914d) \u548c BES (\u559c\u60a6, \u6124\u6012, \u60b2\u4f24, \u6050\u60e7, \u538c\u6076). \u6b64\u5916\u7531 BERT \u5bf9\u6bcf\u4e2a\u8bed\u8c03\u63d0\u53d6 [CLS] \u5d4c\u5165. \u83b7\u5f97\u7684\u7279\u5f81\u4e4b\u540e\u4f20\u9012\u7ed9\u98ce\u683c\u7f16\u7801\u5668\u4ee5\u4ea7\u751f\u98ce\u683c\u5d4c\u5165.</li> </ul> <p>Other models under this category train a prosody encoder/predictor jointly with an autoregressive TTS model such as Tacotron 2, to encode some prosody-related features utilizing text-based features. The trained encoder is then used at inference time to encode prosody-related features based on input text to the TTS model. The text-based input to these prosody encoders in most of the studies is the text\u2019s character/phoneme embeddings [20] [48] [71] [72] [103], while some studies use features extracted from the input text [64] [125]. For instance, [125] employs four ToBI (Tones and Break Indices) features as word-level prosody tags that are combined with the phoneme embedding as input to the TTS model. A ToBI predictor is jointly trained to predict four ToBI features based on grammatical and semantic information extracted from the input text using a self-supervised language representation model ELECTRA [164].</p> <p>\u5176\u4ed6\u6a21\u578b\u8bad\u7ec3\u4e00\u4e2a\u97f5\u5f8b\u7f16\u7801\u5668\u6216\u9884\u6d4b\u5668, \u548c\u81ea\u56de\u5f52 TTS \u6a21\u578b (\u5982 Tacotron 2) \u8054\u5408\u8bad\u7ec3, \u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u4ee5\u7f16\u7801\u4e00\u4e9b\u97f5\u5f8b\u76f8\u5173\u7684\u7279\u5f81. \u8bad\u7ec3\u7684\u7f16\u7801\u5668\u5728\u63a8\u7406\u65f6\u6839\u636e TTS \u6a21\u578b\u7684\u8f93\u5165\u6587\u672c\u7f16\u7801\u97f5\u5f8b\u76f8\u5173\u7684\u7279\u5f81. \u8fd9\u4e9b\u7528\u4e8e\u97f5\u5f8b\u7f16\u7801\u5668\u7684\u57fa\u4e8e\u6587\u672c\u7684\u8f93\u5165\u5728\u5927\u591a\u6570\u7814\u7a76\u4e2d\u91c7\u7528\u6587\u672c\u7684\u5b57\u7b26/\u97f3\u7d20\u5d4c\u5165, \u5176\u4ed6\u4e00\u4e9b\u7814\u7a76\u4f7f\u7528\u4ece\u8f93\u5165\u6587\u672c\u63d0\u53d6\u7684\u7279\u5f81. - \u6587\u732e [125] \u91c7\u7528\u56db\u4e2a ToBI \u7279\u5f81\u4f5c\u4e3a\u8bcd\u7ea7\u97f5\u5f8b\u53d8\u8fc1, \u548c\u97f3\u7d20\u5d4c\u5165\u7ed3\u5408\u4f5c\u4e3a TTS \u6a21\u578b\u7684\u8f93\u5165. ToBI \u9884\u6d4b\u5668\u8054\u5408\u8bad\u7ec3\u57fa\u4e8e\u8bed\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\u6765\u9884\u6d4b\u8fd9\u56db\u4e2a ToBI \u7279\u5f81, \u8fd9\u4e9b\u4fe1\u606f\u662f\u4f7f\u7528\u81ea\u76d1\u7763\u8bed\u8a00\u8868\u793a\u6a21\u578b ELECTRA \u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u51fa\u6765\u7684.</p> <p>In addition to the previously mentioned features, several other prosodic features are also proposed as the output of the prosody predictors in other studies. For example, the prosody predictor in [103] predicts a set of utterance-wise acoustic features, including log-pitch, log-pitch range, log-phone duration, log-energy, and spectral tilt. In [48], the proposed pitch predictor outputs a continuous pitch representation, which is converted into discrete values using Vector Quantization (VQ) [149]. Furthermore, studies [20] [71] propose predicting the three prosody-related features, i.e., F0, energy, and duration, either by a single acoustic features predictor (AFP)[71] or via three separated predictors [20]. </p> <p>\u9664\u4e86\u4e4b\u524d\u63d0\u5230\u7684\u7279\u5f81, \u5176\u4ed6\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u5176\u4ed6\u97f5\u5f8b\u7279\u5f81\u4f5c\u4e3a\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8f93\u51fa.  - \u6587\u732e [103] \u7684\u97f5\u5f8b\u9884\u6d4b\u5668\u9884\u6d4b\u4e00\u7ec4\u8bed\u8c03\u58f0\u5b66\u7279\u5f81, \u5305\u62ec\u5bf9\u6570\u97f3\u9ad8, \u5bf9\u6570\u97f3\u9ad8\u8303\u56f4, \u5bf9\u6570\u97f3\u7d20\u65f6\u957f, \u5bf9\u6570\u80fd\u91cf\u548c\u9891\u8c31\u503e\u659c. - \u6587\u732e [048] \u7684\u97f3\u9ad8\u9884\u6d4b\u5668\u8f93\u51fa\u4e00\u4e2a\u8fde\u7eed\u7684\u97f3\u9ad8\u8868\u793a, \u4f7f\u7528\u77e2\u91cf\u91cf\u5316\u6280\u672f\u8f6c\u5316\u4e3a\u79bb\u6563\u503c. - \u6587\u732e [020] [071] \u901a\u8fc7\u5355\u4e2a\u58f0\u5b66\u7279\u5f81\u9884\u6d4b\u5668 (AFP) \u6216\u4e09\u4e2a\u5355\u72ec\u7684\u9884\u6d4b\u5668\u9884\u6d4b\u4e09\u4e2a\u97f5\u5f8b\u76f8\u5173\u7684\u7279\u5f81, \u5373 F0, \u80fd\u91cf\u548c\u65f6\u957f.</p> <p>Another type of emotion embedding is sentiment feature embedding, which is utilized to produce expressive speech by extracting sentiment information from the input text. This is demonstrated in work [135], where the Stanford Sentiment Parser is used to generate vector embeddings or sentiment probabilities based on the tree structure of the sentence. To synthesize expressive speech, different combinations of probabilities and vector embeddings (for individual words or word-context) are added to the linguistic features as inputs to the TTS model.</p> <p>\u53e6\u4e00\u79cd\u60c5\u611f\u5d4c\u5165\u662f\u60c5\u611f\u7279\u5f81\u5d4c\u5165, \u5b83\u7528\u4e8e\u901a\u8fc7\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u60c5\u611f\u4fe1\u606f\u6765\u4ea7\u751f\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8bed\u97f3. \u8fd9\u5728\u6587\u732e [135] \u4e2d\u5f97\u5230\u4e86\u8bc1\u660e, \u5176\u4e2d\u4f7f\u7528\u4e86\u65af\u5766\u798f\u60c5\u611f\u89e3\u6790\u5668\u6765\u6839\u636e\u53e5\u5b50\u7684\u6811\u7ed3\u6784\u751f\u6210\u5411\u91cf\u5d4c\u5165\u6216\u60c5\u611f\u6982\u7387. \u4e3a\u4e86\u5408\u6210\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8bed\u97f3, \u5c06\u4e0d\u540c\u7ec4\u5408\u7684\u6982\u7387\u548c\u5411\u91cf\u5d4c\u5165\uff08\u5bf9\u4e8e\u5355\u4e2a\u5355\u8bcd\u6216\u5355\u8bcd\u4e0a\u4e0b\u6587\uff09\u6dfb\u52a0\u5230\u4f5c\u4e3a TTS \u6a21\u578b\u8f93\u5165\u7684\u8bed\u8a00\u7279\u5f81\u4e2d.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#53-prosody-controllability","title":"5.3 Prosody Controllability \u97f5\u5f8b\u53ef\u63a7\u6027","text":"<p>Text-to-speech is a one-to-many mapping problem, i.e.,for one piece of text there could be many valid prosody patterns because of speaker-specific variations. Accord-ingly, providing a kind of controllability over prosody-related features in synthesized speech is essential for generating expressive speech with different variations. However, it\u2019s not always easy to mark-up prosody or even to define boundaries between prosody events, i.e., dura-tion boundaries can vary depending on segmentation,pitch contour prediction is error-prone, and prosody fea-tures may not always correlate well with what listeners perceive. Several studies in literature have addressed the control-lability issue in terms of selecting an emotion/style class or intensity level and adjusting prosody-related features at different speech levels. In this section, we discuss stud-ies considering prosody controllability.</p> <p>\u6587\u672c\u5230\u8bed\u97f3\u8f6c\u6362\u662f\u4e00\u4e2a\u4e00\u4e00\u5bf9\u5e94\u7684\u95ee\u9898\uff0c\u5373\u5bf9\u4e8e\u4e00\u6bb5\u6587\u672c\uff0c\u7531\u4e8e\u8bf4\u8bdd\u4eba\u7279\u5b9a\u7684\u53d8\u5316\uff0c\u53ef\u80fd\u5b58\u5728\u8bb8\u591a\u6709\u6548\u7684\u97f5\u5f8b\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u5728\u5408\u6210\u7684\u8bed\u97f3\u4e2d\u63d0\u4f9b\u5bf9\u97f5\u5f8b\u76f8\u5173\u7279\u5f81\u7684\u63a7\u5236\u5bf9\u4e8e\u751f\u6210\u5177\u6709\u4e0d\u540c\u53d8\u5316\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u81f3\u5173\u91cd\u8981\u3002 \u7136\u800c\uff0c\u6807\u8bb0\u97f5\u5f8b\u6216\u5b9a\u4e49\u97f5\u5f8b\u4e8b\u4ef6\u4e4b\u95f4\u7684\u754c\u9650\u5e76\u4e0d\u603b\u662f\u5bb9\u6613\u7684\uff0c\u5373\u6301\u7eed\u65f6\u95f4\u754c\u9650\u53ef\u80fd\u4f1a\u6839\u636e\u5206\u6bb5\u800c\u53d8\u5316\uff0c\u97f3\u9ad8\u8f6e\u5ed3\u9884\u6d4b\u5bb9\u6613\u51fa\u9519\uff0c\u97f5\u5f8b\u7279\u5f81\u5e76\u4e0d\u603b\u662f\u4e0e\u542c\u4f17\u611f\u77e5\u76f8\u5173\u3002 \u5728\u6587\u732e\u4e2d\uff0c\u6709\u51e0\u9879\u7814\u7a76\u4ece\u9009\u62e9\u60c5\u611f/\u98ce\u683c\u7c7b\u522b\u6216\u5f3a\u5ea6\u7ea7\u522b\u4ee5\u53ca\u5728\u4e0d\u540c\u7684\u8bed\u97f3\u7ea7\u522b\u8c03\u6574\u97f5\u5f8b\u76f8\u5173\u7279\u5f81\u7684\u89d2\u5ea6\u89e3\u51b3\u4e86\u53ef\u63a7\u6027\u95ee\u9898\u3002\u5728\u672c\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u8ba8\u8bba\u8003\u8651\u97f5\u5f8b\u53ef\u63a7\u6027\u7684\u7814\u7a76\u3002</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#531-modelingspecific-prosody-styles","title":"5.3.1 Modeling\u2011specific prosody styles","text":"<p>This group of studies provides individual representa-tions of expressive styles/emotions, enabling the control of prosody in synthesized speech by offering the ability to select from available representations or adjust their values. In some studies [55] [70] [116], style is modeled at a single speech/text level, while in other studies [68] [79] [133] a multi-level or hierarchical model of expressive styles is used to allow for a better capture of prosody var-iation in expressive speech. In single-level prosody modeling approaches, [55] is one of the early studies that extends a baseline with fine-grained control over the speaking style/prosody of syn-thesized speech. The proposed modification involves adding an embedding network with temporal structure to either the speech-side or text-side of the TTS model. Accordingly, the resulting prosody embedding is of vari-able length, and it is used to condition input to either encoder or decoder based on the position of the embed-ding network. Speech-side prosody embedding provides adjustment of prosody at frame-level, while text-side prosody embedding enables phoneme-level prosody control. Single-level prosody embeddings can be converted into discrete embeddings as in [70] [116]. Discrete pros-ody representations are easier to control and analyze and provide a better interpretation of prosodic styles.</p> <p>\u8fd9\u4e9b\u7814\u7a76\u63d0\u4f9b\u4e86\u8868\u8fbe\u98ce\u683c/\u60c5\u611f\u7684\u4e2a\u4f53\u8868\u793a\uff0c\u4f7f\u5f97\u5728\u5408\u6210\u8bed\u97f3\u4e2d\u63a7\u5236\u8bed\u8c03\u6210\u4e3a\u53ef\u80fd\uff0c\u901a\u8fc7\u63d0\u4f9b\u4ece\u53ef\u7528\u8868\u793a\u4e2d\u9009\u62e9\u6216\u8c03\u6574\u5176\u503c\u7684\u80fd\u529b\u3002\u5728\u67d0\u4e9b\u7814\u7a76\u4e2d\uff0c\u98ce\u683c\u5728\u5355\u4e2a\u8bed\u97f3/\u6587\u672c\u7ea7\u522b\u5efa\u6a21\uff0c\u800c\u5728\u5176\u4ed6\u7814\u7a76\u4e2d\uff0c\u4f7f\u7528\u591a\u7ea7\u6216\u5206\u5c42\u6a21\u578b\u6765\u66f4\u597d\u5730\u6355\u6349\u8868\u8fbe\u6027\u8bed\u97f3\u4e2d\u7684\u8bed\u8c03\u53d8\u5316\u3002</p> <p>\u5728\u5355\u7ea7\u8bed\u8c03\u5efa\u6a21\u65b9\u6cd5\u4e2d\uff0c[55]\u662f\u65e9\u671f\u7814\u7a76\u4e4b\u4e00\uff0c\u5b83\u6269\u5c55\u4e86\u57fa\u7ebf\uff0c\u5bf9\u5408\u6210\u8bed\u97f3\u7684\u8bf4\u8bdd\u98ce\u683c/\u8bed\u8c03\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u63d0\u51fa\u7684\u4fee\u6539\u6d89\u53ca\u5728TTS\u6a21\u578b\u7684\u8bed\u97f3\u4fa7\u6216\u6587\u672c\u4fa7\u6dfb\u52a0\u5177\u6709\u65f6\u95f4\u7ed3\u6784\u7684\u5d4c\u5165\u7f51\u7edc\u3002</p> <p>\u56e0\u6b64\uff0c\u5f97\u5230\u7684\u8bed\u8c03\u5d4c\u5165\u662f\u53ef\u53d8\u957f\u5ea6\u7684\uff0c\u5e76\u4e14\u6839\u636e\u5d4c\u5165\u7f51\u7edc\u7684\u4f4d\u7f6e\u7528\u4e8e\u6761\u4ef6\u8f93\u5165\u5230\u7f16\u7801\u5668\u6216\u89e3\u7801\u5668\u3002\u8bed\u97f3\u4fa7\u8bed\u8c03\u5d4c\u5165\u63d0\u4f9b\u5e27\u7ea7\u8bed\u8c03\u8c03\u6574\uff0c\u800c\u6587\u672c\u4fa7\u8bed\u8c03\u5d4c\u5165\u5141\u8bb8\u97f3\u7d20\u7ea7\u8bed\u8c03\u63a7\u5236\u3002</p> <p>\u5355\u7ea7\u8bed\u8c03\u5d4c\u5165\u53ef\u4ee5\u8f6c\u6362\u4e3a\u79bb\u6563\u5d4c\u5165\uff0c\u5982[70] [116]\u3002\u79bb\u6563\u8bed\u8c03\u8868\u793a\u66f4\u5bb9\u6613\u63a7\u5236\u548c\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u8bed\u8c03\u98ce\u683c\u7684\u66f4\u597d\u89e3\u91ca\u3002</p> <p>In [116], a word-level prosody embedding is proposed based on decision trees and a GMM. A word-level refer-ence encoder is first used to obtain word-level prosody embedding from reference audio. A binary decision tree is employed to cluster embeddings with their identities based on their phonetic information. Prosody embed-dings of words in each leaf node will differ only in their prosodies. Then prosody embeddings of each leaf can be clustered via a GMM model where clusters represent prosody tags. If the applied GMM consists of five com-ponents and a tree of ten leaf nodes, a set of 50 prosody tags is produced. At inference time, prosody tags can be selected manually or via a prosody predictor that is trained to select appropriate prosody tags based on input text. In [70], an audiobook speech synthesis model is pro-posed. The model uses a character-acting-style extrac-tion module based on ResCNN [165] to extract different character acting styles from the input speech. Discrete character-level styles are obtained via vector quantization(VQ) [149], which maps them to a codebook, limiting the number of styles. At inference, the discrete character-act-ing-styles are predicted via a style predictor. The charac-ter-level style predictor uses both character embeddings from Skip-Gram [166] and text-based features from RoB-ERTa [167] as input. \u5728[116]\u4e2d\uff0c\u57fa\u4e8e\u51b3\u7b56\u6811\u548cGMM\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bcd\u7ea7\u8bed\u8c03\u5d4c\u5165\u3002\u9996\u5148\u4f7f\u7528\u8bcd\u7ea7\u53c2\u8003\u7f16\u7801\u5668\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u83b7\u53d6\u8bcd\u7ea7\u8bed\u8c03\u5d4c\u5165\u3002\u4f7f\u7528\u4e8c\u53c9\u51b3\u7b56\u6811\u6839\u636e\u5176\u97f3\u7d20\u4fe1\u606f\u5bf9\u5d4c\u5165\u8fdb\u884c\u805a\u7c7b\u3002\u6bcf\u4e2a\u53f6\u8282\u70b9\u4e2d\u5355\u8bcd\u7684\u8bed\u8c03\u5d4c\u5165\u4ec5\u5728\u8bed\u8c03\u4e0a\u6709\u6240\u4e0d\u540c\u3002\u7136\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7GMM\u6a21\u578b\u5bf9\u6bcf\u4e2a\u53f6\u7684\u8bed\u8c03\u5d4c\u5165\u8fdb\u884c\u805a\u7c7b\uff0c\u5176\u4e2d\u805a\u7c7b\u8868\u793a\u8bed\u8c03\u6807\u7b7e\u3002\u5982\u679c\u5e94\u7528\u7684GMM\u5305\u542b\u4e94\u4e2a\u7ec4\u4ef6\u548c\u4e00\u4e2a\u5341\u4e2a\u53f6\u8282\u70b9\u7684\u6811\uff0c\u5219\u4f1a\u4ea7\u751f50\u4e2a\u8bed\u8c03\u6807\u7b7e\u3002\u5728\u63a8\u7406\u65f6\uff0c\u8bed\u8c03\u6807\u7b7e\u53ef\u4ee5\u624b\u52a8\u9009\u62e9\u6216\u901a\u8fc7\u8bad\u7ec3\u4ee5\u6839\u636e\u8f93\u5165\u6587\u672c\u9009\u62e9\u9002\u5f53\u8bed\u8c03\u6807\u7b7e\u7684\u8bed\u8c03\u9884\u6d4b\u5668\u9009\u62e9\u3002</p> <p>\u5728[70]\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u58f0\u8bfb\u7269\u8bed\u97f3\u5408\u6210\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u57fa\u4e8eResCNN [165]\u7684\u89d2\u8272\u626e\u6f14\u98ce\u683c\u63d0\u53d6\u6a21\u5757\u4ece\u8f93\u5165\u8bed\u97f3\u4e2d\u63d0\u53d6\u4e0d\u540c\u7684\u89d2\u8272\u626e\u6f14\u98ce\u683c\u3002\u901a\u8fc7\u77e2\u91cf\u91cf\u5316(VQ) [149]\u83b7\u5f97\u79bb\u6563\u89d2\u8272\u7ea7\u98ce\u683c\uff0c\u5c06\u5176\u6620\u5c04\u5230\u7801\u672c\uff0c\u9650\u5236\u98ce\u683c\u6570\u91cf\u3002\u5728\u63a8\u7406\u65f6\uff0c\u901a\u8fc7\u98ce\u683c\u9884\u6d4b\u5668\u9884\u6d4b\u79bb\u6563\u89d2\u8272\u626e\u6f14\u98ce\u683c\u3002\u89d2\u8272\u7ea7\u98ce\u683c\u9884\u6d4b\u5668\u4f7f\u7528\u6765\u81eaSkip-Gram [166]\u7684\u89d2\u8272\u5d4c\u5165\u548c\u6765\u81eaRoBERTa [167]\u7684\u6587\u672c\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\u3002</p> <p>Regarding multi-level prosody modeling, some stud-ies propose enhancing prosody control in the baseline models [74] [75] [77] by modifying their single-level pros-ody modeling to multiple levels. For instance, [133] pro-poses a hierarchical structure of [75] with multiple GST layers. Three GST layers are employed in the proposed model, each consisting of 10 tokens, which were found to yield better token interpretation. Tokens of the first and second layers were found to learn different speak-ers and styles, but these representations were not easily interpreted. Interestingly, the tokens in the third layer were able to generate higher quality samples with more distinct and interpretable styles. Specifically, third-layer styles exhibit clear differences in their features, includ-ing pitch, stress, speaking rate, start offset, rhythm, pause position, and duration. Model in [77] is further extended in [68] with three VAEs to generate three different levels (utterance, phrase,and word) of latent variables with varying time resolu-tions. Acoustic features and linguistic features are passed as input to the three VAEs. Initially, a conditional prior(CP) is applied to learn a distribution for sampling utter-ance-level latent variables based on linguistic features from the input text. The generated latent variables are passed to other levels via auto-regressive (AR) latent converters that convert latent variables from coarser-level to finer-level with input text condition. In fact, the utterance-level latent variables can be used to control the generated speech styles, regardless of latent variables of other levels, as they are predicted based on the utterance-level latent variables.</p> <p>The Controllable Expressive Speech Synthesis (ConEx)model in [79] proposes modeling prosody at two levels,utterance-level (global) and phone-level (local), using reference encoders [74]. However, the global prosody embedding is used to condition the local prosody embed-ding, resulting in an integrated prosody embedding. The local embeddings are 3D vectors that are converted into discrete local prosody embeddings (codes) via vec-tor quantization (VQ) [149]. At inference time, the integrated prosody embedding is predicted by an auto-regressive (AR) prior model trained to predict categori-cal distributions for each of the discrete codes utilizing global prosody embedding and the phoneme embed-ding as inputs. While global prosody embedding can be obtained from training samples or from an audio refer-ence, local prosody embeddings for a given global pros-ody embedding are achieved via the AR prior model. Fine-grained prosody control can be achieved by select-ing a specific phoneme to start adjusting prosody from. The AR prior model will first generate the top k pros-ody options for this phoneme. Then, the local prosody sequence will be generated autoregressively for each of the first top k options by the AR prior model.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#532-modelingspecific-prosody-features","title":"5.3.2 Modeling\u2011specific prosody features","text":"<p>This group of studies provides individual representations of prosody-related features. Control over prosody of the synthesized speech is provided via selecting or adjust-ing a specific representation of a specific prosody-related feature. Some studies in this direction model prosody features at the global or utterance-level [97] [128], while other studies propose modeling at fine-grained lev-els [48] [63] [71] [122] [138], such as phoneme, syllable, or word-level. The STYLER model [97], for example, employs multi-ple style encoders to factor speech style into several com-ponents, including duration, pitch, speaker, energy, and noise. This structure enables STYLER to generate con-trollable expressive speech by adjusting each of the indi-vidually modeled features. Furthermore, with the explicit noise encoding, other encoders can be constrained to exclude noise information as a style factor, and thus the model can generate clean speech even with noisy refer-ences. Adjusting the style factors, various styles of speech can be generated from STYLER.</p> <p>\u8fd9\u4e9b\u7814\u7a76\u63d0\u4f9b\u4e86\u4e0e\u97f5\u5f8b\u76f8\u5173\u7684\u7279\u5f81\u7684\u4e2a\u4f53\u8868\u793a\u3002\u901a\u8fc7\u9009\u62e9\u6216\u8c03\u6574\u7279\u5b9a\u97f5\u5f8b\u76f8\u5173\u7279\u5f81\u7684\u7279\u5b9a\u8868\u793a\u6765\u63a7\u5236\u5408\u6210\u7684\u8bed\u97f3\u7684\u97f5\u5f8b\u3002\u5728\u8fd9\u4e2a\u65b9\u5411\u4e0a\uff0c\u4e00\u4e9b\u7814\u7a76\u5728\u5168\u5c40\u6216\u53e5\u5b50\u7ea7\u522b\u5efa\u6a21\u97f5\u5f8b\u7279\u5f81[97] [128]\uff0c\u800c\u5176\u4ed6\u7814\u7a76\u5219\u63d0\u51fa\u5728\u7ec6\u7c92\u5ea6\u7ea7\u522b\u5efa\u6a21[48] [63] [71] [122] [138]\uff0c\u4f8b\u5982\u97f3\u7d20\u3001\u97f3\u8282\u6216\u5355\u8bcd\u7ea7\u522b\u3002</p> <p>\u4f8b\u5982\uff0cSTYLER\u6a21\u578b[97]\u4f7f\u7528\u591a\u4e2a\u98ce\u683c\u7f16\u7801\u5668\u5c06\u8bed\u97f3\u98ce\u683c\u5206\u89e3\u4e3a\u51e0\u4e2a\u7ec4\u6210\u90e8\u5206\uff0c\u5305\u62ec\u6301\u7eed\u65f6\u95f4\u3001\u97f3\u9ad8\u3001\u8bf4\u8bdd\u8005\u3001\u80fd\u91cf\u548c\u566a\u58f0\u3002\u8fd9\u79cd\u7ed3\u6784\u4f7fSTYLER\u80fd\u591f\u901a\u8fc7\u8c03\u6574\u6bcf\u4e2a\u5355\u72ec\u5efa\u6a21\u7684\u7279\u5f81\u6765\u751f\u6210\u53ef\u63a7\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u663e\u5f0f\u566a\u58f0\u7f16\u7801\uff0c\u5176\u4ed6\u7f16\u7801\u5668\u53ef\u4ee5\u88ab\u7ea6\u675f\u4ee5\u6392\u9664\u566a\u58f0\u4fe1\u606f\u4f5c\u4e3a\u98ce\u683c\u56e0\u7d20\uff0c\u56e0\u6b64\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u5e72\u51c0\u7684\u8bed\u97f3\uff0c\u5373\u4f7f\u4f7f\u7528\u5608\u6742\u7684\u53c2\u8003\u3002\u901a\u8fc7\u8c03\u6574\u98ce\u683c\u56e0\u7d20\uff0c\u53ef\u4ee5\u4eceSTYLER\u751f\u6210\u5404\u79cd\u98ce\u683c\u7684\u8bed\u97f3\u3002</p> <p>Adjusting several features at fine-grained levels can be a difficult task.  For example, FastSpeech2 provides fine-grained control over pitch range, duration,energy, which are modeled at the phone-level (phone-wise), and it is not easy to adjust these features to achieve a specific prosodic output.  Raitio and Seshadri [128] improves FastSpeech2 with an utterance-wise (coarse-grained) prosody model using an additional variance adaptor. That second variance adaptor is the same as the original one, but it models five features at the utterance-level: pitch, pitch range, duration, energy,and spectral tilt. These features are then concatenated with the corresponding output of the first variance adaptor. Such utterance-wise prosody model enables easier control of prosody while still allowing modification at the phone-level. To control high-level prosody,a bias is added to the corresponding utterance-wise prosody predictions. A phone-level prosody control is achieved by directly modifying the phone-wise features. Fine-grained control over a specific prosody-feature can also be required specially for strong speaking styles. To that end, in [71], a predictor is proposed to predict F0, energy, and duration features at the phoneme-level. During inference, the predicted features are generated based on the input text alone; however, they can also be provided externally and modified as desired. Furthermore, two prosody modeling levels are pro-posed in [63]: the local level (word-level) and global level (utterance-wise). The global prosody embedding is the emotion embedding obtained by a reference-based encoder. The local prosody embedding is obtained from a predictor of the F0 features at the word-level with global prosody embedding and the phoneme embed-ding as inputs. Both embeddings are then passed to a multi-style encoder to form the final multi-style pros-ody embedding. Therefore, modifying the predicted F0values can provide control of prosody at the utterance,word, and phoneme levels.</p> <p>\u5728\u7ec6\u7c92\u5ea6\u7ea7\u522b\u8c03\u6574\u591a\u4e2a\u7279\u5f81\u53ef\u80fd\u662f\u4e00\u9879\u56f0\u96be\u7684\u4efb\u52a1\u3002\u4f8b\u5982\uff0cFastSpeech2 \u5728\u97f3\u7d20\u7ea7\u522b\uff08\u97f3\u7d20\u7ea7\uff09\u63d0\u4f9b\u5bf9\u97f3\u9ad8\u8303\u56f4\u3001\u6301\u7eed\u65f6\u95f4\u548c\u80fd\u91cf\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u4e14\u4e0d\u5bb9\u6613\u8c03\u6574\u8fd9\u4e9b\u7279\u5f81\u4ee5\u5b9e\u73b0\u7279\u5b9a\u7684\u97f5\u5f8b\u8f93\u51fa\u3002Raitio\u548cSeshadri[128]\u901a\u8fc7\u4f7f\u7528\u989d\u5916\u7684\u53d8\u5f02\u9002\u914d\u5668\u6539\u8fdbFastSpeech2\uff0c\u8be5\u53d8\u5f02\u9002\u914d\u5668\u5728\u53e5\u5b50\u7ea7\u522b\uff08\u7c97\u7c92\u5ea6\uff09\u4f7f\u7528\u97f5\u5f8b\u6a21\u578b\u3002\u7b2c\u4e8c\u4e2a\u53d8\u5f02\u9002\u914d\u5668\u4e0e\u539f\u59cb\u53d8\u5f02\u9002\u914d\u5668\u76f8\u540c\uff0c\u4f46\u5728\u53e5\u5b50\u7ea7\u522b\u5efa\u6a21\u4e94\u4e2a\u7279\u5f81\uff1a\u97f3\u9ad8\u3001\u97f3\u9ad8\u8303\u56f4\u3001\u6301\u7eed\u65f6\u95f4\u3001\u80fd\u91cf\u548c\u9891\u8c31\u503e\u659c\u3002\u7136\u540e\uff0c\u5c06\u8fd9\u4e9b\u7279\u5f81\u4e0e\u7b2c\u4e00\u4e2a\u53d8\u5f02\u9002\u914d\u5668\u7684\u76f8\u5e94\u8f93\u51fa\u8fde\u63a5\u8d77\u6765\u3002\u8fd9\u79cd\u53e5\u5b50\u7ea7\u522b\u7684\u97f5\u5f8b\u6a21\u578b\u5141\u8bb8\u66f4\u5bb9\u6613\u5730\u63a7\u5236\u97f5\u5f8b\uff0c\u540c\u65f6\u4ecd\u7136\u5141\u8bb8\u5728\u97f3\u7d20\u7ea7\u522b\u8fdb\u884c\u4fee\u6539\u3002\u4e3a\u4e86\u63a7\u5236\u9ad8\u7ea7\u522b\u97f5\u5f8b\uff0c\u5411\u76f8\u5e94\u7684\u53e5\u5b50\u7ea7\u522b\u97f5\u5f8b\u9884\u6d4b\u6dfb\u52a0\u504f\u5dee\u3002\u901a\u8fc7\u76f4\u63a5\u4fee\u6539\u97f3\u7d20\u7ea7\u7279\u5f81\u6765\u5b9e\u73b0\u97f3\u7d20\u7ea7\u97f5\u5f8b\u63a7\u5236\u3002</p> <p>\u5bf9\u4e8e\u5f3a\u70c8\u7684\u8bf4\u8bdd\u98ce\u683c\uff0c\u53ef\u80fd\u9700\u8981\u5bf9\u7279\u5b9a\u97f5\u5f8b\u7279\u5f81\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u4e3a\u6b64\uff0c[71]\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u5668\u6765\u9884\u6d4b\u97f3\u7d20\u7ea7\u522b\u7684F0\u3001\u80fd\u91cf\u548c\u6301\u7eed\u65f6\u95f4\u7279\u5f81\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6839\u636e\u8f93\u5165\u6587\u672c\u751f\u6210\u9884\u6d4b\u7279\u5f81\uff1b\u4f46\u662f\uff0c\u5b83\u4eec\u4e5f\u53ef\u4ee5\u4ece\u5916\u90e8\u63d0\u4f9b\u5e76\u6839\u636e\u9700\u8981\u8fdb\u884c\u4fee\u6539\u3002\u6b64\u5916\uff0c[63]\u4e2d\u63d0\u51fa\u4e86\u4e24\u4e2a\u97f5\u5f8b\u5efa\u6a21\u7ea7\u522b\uff1a\u5c40\u90e8\u7ea7\u522b\uff08\u5355\u8bcd\u7ea7\u522b\uff09\u548c\u5168\u5c40\u7ea7\u522b\uff08\u53e5\u5b50\u7ea7\u522b\uff09\u3002\u5168\u5c40\u97f5\u5f8b\u5d4c\u5165\u662f\u4ece\u57fa\u4e8e\u53c2\u8003\u7684\u7f16\u7801\u5668\u83b7\u5f97\u7684\u60c5\u7eea\u5d4c\u5165\u3002\u5c40\u90e8\u97f5\u5f8b\u5d4c\u5165\u662f\u4ece\u5177\u6709\u5168\u5c40\u97f5\u5f8b\u5d4c\u5165\u548c\u97f3\u7d20\u5d4c\u5165\u4f5c\u4e3a\u8f93\u5165\u7684\u9884\u6d4b\u5668\u83b7\u5f97\u7684\u97f3\u7d20\u7ea7\u522b\u7684F0\u7279\u5f81\u3002\u7136\u540e\uff0c\u5c06\u8fd9\u4e24\u4e2a\u5d4c\u5165\u4f20\u9012\u5230\u4e00\u4e2a\u591a\u98ce\u683c\u7f16\u7801\u5668\uff0c\u4ee5\u5f62\u6210\u6700\u7ec8\u7684\u591a\u98ce\u683c\u97f5\u5f8b\u5d4c\u5165\u3002\u56e0\u6b64\uff0c\u4fee\u6539\u9884\u6d4b\u7684F0\u503c\u53ef\u4ee5\u63d0\u4f9b\u5bf9\u53e5\u5b50\u3001\u5355\u8bcd\u548c\u97f3\u7d20\u7ea7\u522b\u7684\u97f5\u5f8b\u7684\u63a7\u5236\u3002</p> <p>More flexibility in controlling the F0 feature is pro-vided in the controllable deep auto-regressive model(C-DAR) model [138] which allows for F0 contour adjustment by the user. To achieve this goal, three strat-egies are used: 1) context awareness by conditioning the model on the preceding and following speech dur-ing training, 2) conditioning the model on some ran-dom segments of ground truth F0, and 3) predicting F0 values in reverse order. Additionally, several text-based features are used as input to the model, includ-ing word embeddings derived from BERT, V/UV label,one-hot vector for the nearby punctuation, and pho-neme encodings. At inference, F0 values specified by the user are used as alternatives for the ground truth F0 segments, and the model predicts the rest of the utter-ance\u2019s F0 contour through context awareness. Discrete fine-grained representations for prosody features as in [48] [122] are also useful to limit the number of the obtained representations. Both studies [48] [122]utilize VQ [149] to map each prosody embedding to the closest discrete representation from a predefined code-book. In [48], a pitch predictor is used to predict charac-ter-level continuous pitch representation using character embeddings from the text encoder as input. Zhang et al.[122], however, produces syllable-level prosody embed-dings from a reference encoder that takes F0, intensity,and duration features from reference audio as input. The resulting prosody embeddings are then mapped to a pre-defined codebook to extractb discrete prosody codes.</p> <p>\u5728\u53ef\u63a7\u6df1\u5ea6\u81ea\u56de\u5f52\u6a21\u578b\uff08C-DAR\uff09\u6a21\u578b[138]\u4e2d\u63d0\u4f9b\u4e86\u5bf9F0\u7279\u5f81\u7684\u66f4\u591a\u63a7\u5236\u7075\u6d3b\u6027\uff0c\u8be5\u6a21\u578b\u5141\u8bb8\u7528\u6237\u8c03\u6574F0\u8f6e\u5ed3\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u4f7f\u7528\u4e86\u4e09\u79cd\u7b56\u7565\uff1a1\uff09\u901a\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u5c06\u6a21\u578b\u6761\u4ef6\u5316\u5728\u5148\u524d\u548c\u968f\u540e\u7684\u8bed\u97f3\u4e0a\u6765\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c2\uff09\u5c06\u6a21\u578b\u6761\u4ef6\u5316\u5728\u5730\u9762\u771fF0\u7684\u4e00\u4e9b\u968f\u673a\u7247\u6bb5\u4e0a\uff0c\u4ee5\u53ca3\uff09\u4ee5\u76f8\u53cd\u7684\u987a\u5e8f\u9884\u6d4bF0\u503c\u3002\u6b64\u5916\uff0c\u5c06\u51e0\u4e2a\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u7528\u4f5c\u6a21\u578b\u7684\u8f93\u5165\uff0c\u5305\u62ec\u4eceBERT\u6d3e\u751f\u7684\u8bcd\u5d4c\u5165\u3001V/UV\u6807\u7b7e\u3001\u9644\u8fd1\u6807\u70b9\u7b26\u53f7\u7684\u4e00\u70ed\u5411\u91cf\u548c\u97f3\u7d20\u7f16\u7801\u3002\u5728\u63a8\u7406\u65f6\uff0c\u7528\u6237\u6307\u5b9a\u7684F0\u503c\u88ab\u7528\u4f5c\u5730\u9762\u771fF0\u7247\u6bb5\u7684\u66ff\u4ee3\u54c1\uff0c\u5e76\u4e14\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u6d4b\u6574\u4e2a\u53e5\u5b50\u7684F0\u8f6e\u5ed3\u3002</p> <p>\u4e0e[48] [122]\u4e2d\u4e00\u6837\uff0c\u79bb\u6563\u7ec6\u7c92\u5ea6\u8868\u793a\u5bf9\u4e8e\u97f5\u5f8b\u7279\u5f81\u4e5f\u5f88\u6709\u7528\uff0c\u53ef\u4ee5\u9650\u5236\u83b7\u5f97\u7684\u8868\u793a\u7684\u6570\u91cf\u3002\u4e24\u9879\u7814\u7a76[48] [122]\u90fd\u4f7f\u7528VQ[149]\u5c06\u6bcf\u4e2a\u97f5\u5f8b\u5d4c\u5165\u6620\u5c04\u5230\u9884\u5b9a\u4e49\u7801\u672c\u4e2d\u6700\u8fd1\u7684\u79bb\u6563\u8868\u793a\u3002\u5728[48]\u4e2d\uff0c\u4f7f\u7528\u97f3\u9ad8\u9884\u6d4b\u5668\u6765\u9884\u6d4b\u4f7f\u7528\u6587\u672c\u7f16\u7801\u5668\u4f5c\u4e3a\u8f93\u5165\u7684\u5b57\u7b26\u5d4c\u5165\u7684\u5b57\u7b26\u7ea7\u8fde\u7eed\u97f3\u9ad8\u8868\u793a\u3002\u7136\u800c\uff0c\u5f20\u7b49\u4eba[122]\u4ece\u53c2\u8003\u7f16\u7801\u5668\u4ea7\u751f\u97f3\u8282\u7ea7\u97f5\u5f8b\u5d4c\u5165\uff0c\u8be5\u53c2\u8003\u7f16\u7801\u5668\u5c06\u53c2\u8003\u97f3\u9891\u7684F0\u3001\u5f3a\u5ea6\u548c\u6301\u7eed\u65f6\u95f4\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\u3002\u7136\u540e\uff0c\u5c06\u5f97\u5230\u7684\u97f5\u5f8b\u5d4c\u5165\u6620\u5c04\u5230\u4e00\u4e2a\u9884\u5b9a\u4e49\u7684\u7801\u672c\uff0c\u4ee5\u63d0\u53d6\u79bb\u6563\u7684\u97f5\u5f8b\u4ee3\u7801\u3002</p> <p>Resulting prosody codes in [48] represent the pitch and other suprasegmental information that can be adjusted via a specific bias value to generate speech with differ-ent pitch accents. The codes in [122], can be interpreted as representing some prosody features such as pitch and duration. The prosody variation at the syllable-level can be manually controlled by assigning each syllable the desired prosody code from the codebook. In [125], ToBI features, which involve a set of con-ventions used for transcribing and annotating speech prosody, are used. The applied ToBI features are four word-level tags: pitch accents, boundary tones, phrase accents, and break indices. The extracted ToBI tags are used as input to TTS model. Simultaneously, a ToBI pre-dictor is trained to predict these prosody tags based on grammatical and semantic information extracted from the input text using a self-supervised language model. The resulting model had the ability to control the stress,intonation, and pause of the generated speech to sound natural, utilizing only ToBI tags from the text-based predictor.</p> <p>\u5728[48]\u4e2d\uff0c\u5f97\u5230\u7684\u97f5\u5f8b\u4ee3\u7801\u8868\u793a\u97f3\u9ad8\u548c\u5176\u4ed6\u8d85\u97f3\u6bb5\u4fe1\u606f\uff0c\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u7684\u504f\u7f6e\u503c\u8fdb\u884c\u8c03\u6574\uff0c\u4ee5\u751f\u6210\u5177\u6709\u4e0d\u540c\u97f3\u9ad8\u91cd\u97f3\u7684\u8bed\u97f3\u3002\u5728[122]\u4e2d\uff0c\u4ee3\u7801\u53ef\u4ee5\u89e3\u91ca\u4e3a\u8868\u793a\u4e00\u4e9b\u97f5\u5f8b\u7279\u5f81\uff0c\u5982\u97f3\u9ad8\u548c\u6301\u7eed\u65f6\u95f4\u3002\u53ef\u4ee5\u901a\u8fc7\u5c06\u6bcf\u4e2a\u97f3\u8282\u5206\u914d\u4ece\u7801\u672c\u4e2d\u6240\u9700\u7684\u97f5\u5f8b\u4ee3\u7801\u6765\u624b\u52a8\u63a7\u5236\u97f3\u8282\u7ea7\u522b\u7684\u97f5\u5f8b\u53d8\u5316\u3002</p> <p>\u5728[125]\u4e2d\uff0c\u4f7f\u7528\u4e86ToBI\u7279\u5f81\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u8f6c\u5f55\u548c\u6ce8\u91ca\u8bed\u97f3\u97f5\u5f8b\u7684\u7ea6\u5b9a\u96c6\u3002\u5e94\u7528\u7684ToBI\u7279\u5f81\u662f\u56db\u4e2a\u5355\u8bcd\u7ea7\u522b\u7684\u6807\u7b7e\uff1a\u97f3\u9ad8\u91cd\u97f3\u3001\u8fb9\u754c\u97f3\u3001\u77ed\u8bed\u91cd\u97f3\u548c\u505c\u987f\u7d22\u5f15\u3002\u63d0\u53d6\u7684ToBI\u6807\u7b7e\u7528\u4f5cTTS\u6a21\u578b\u7684\u8f93\u5165\u3002\u540c\u65f6\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2aToBI\u9884\u6d4b\u5668\u6765\u9884\u6d4b\u8fd9\u4e9b\u97f5\u5f8b\u6807\u7b7e\uff0c\u57fa\u4e8e\u4f7f\u7528\u81ea\u76d1\u7763\u8bed\u8a00\u6a21\u578b\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\u3002</p> <p>\u6700\u7ec8\u6a21\u578b\u80fd\u591f\u4ec5\u4f7f\u7528\u6587\u672c\u9884\u6d4b\u5668\u4e2d\u7684ToBI\u6807\u7b7e\u6765\u63a7\u5236\u751f\u6210\u7684\u8bed\u97f3\u7684\u5f3a\u8c03\u3001\u8bed\u8c03\u548c\u505c\u987f\uff0c\u4f7f\u5176\u542c\u8d77\u6765\u81ea\u7136\u3002</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#533-modeling-prosody-strength","title":"5.3.3 Modeling prosody strength","text":"<p>This group of studies focus on regulating the strength of emotion or prosody. For instance, [61] utilizes the distance between emotion embeddings and the neutral emotion embeddings to identify scalar values for emotion intensity. It proposes a phoneme-level emotion embed-ding and a fine-grained emotion intensity. The emo-tion embedding is first obtained via a reference encoder. The emotion intensity is then generated by an intensity extractor that takes the emotion embedding as input. The intensity extractor produces intensity as a scalar value based on the distance between the emotion embedding and the centroid of a pre-defined cluster for neutral emo-tion embeddings. The resulting emotion intensity values are quantized into pseudo-labels that serve as the index for an intensity embedding table. Another method for learning emotion strength values in an unsupervised manner is by using ranking functions. Studies [27] [31] [33] [64] utilize a ranking function-based method named relative attributes [89] for this purpose. In[33], prosody is modeled at three levels: global-level rep-resentation by emotion embedding, utterance-level rep-resented by prosody embedding from a reference-based encoder, and the local-level represented by emotion strength. The study trains an emotion strength extractor at the syllable-level based on input speech utilizing the ranking function. Simultaneously, a predictor of emo-tion strength is trained based on features extracted from input text via BERT model. Besides changing emotion label and emotion reference audio, the model provides manual control of the emotion strength values in the syn-thesized speech. Alternatively, the reference encoder in [31] functions as a ranking function to learn a phoneme-level emotion strength (descriptor) sequence. The proposed ranking function [89] receives its input from fragments of target reference audio obtained via a forced alignment model to phoneme boundaries. The OpenSMILE [139] tool is then used to extract 384-dimensional emotion-related features from these reference speech fragments as input to the ranking function. Similarly, the proposed ranking function in [27] takes a set of acoustic features extracted from the input speech via OpenSMILE tool but at the utter-ance-level as input. The ranking function leverages the difference between neutral samples and samples associ-ated with each emotion class in the dataset. The training process is formulated as solving a max-margin optimiza-tion problem. The resulting emotion strength scalars can be manually adjusted or predicted based on text or refer-ence speech. In [64], both emotion class and emotion strength value are obtained via a joint emotion predictor based only on the input text. The input to the predictor is features extracted from input text via the Generative Pre-trained Transformer (GPT)-3 [88]. Emotion class and emotion strength are the two outputs of the predictor where the former is represented as a one-hot encoded vector and the latter is presented as a scalar value. Emotion labels and emotion strength values which are also obtained via[89], are used as ground truth for predictor training. Another ranking method is proposed in \"Cross-Speaker Emotion Transfer by Manipulating Speech Style Latents\" using the ranking support vector machine. The model generates style embedding and speaker embedding via two separate encoders. Both style and speaker embeddings at infer-ence time are represented by centroids of each single speaker and style embeddings. However, a linear SVM is trained with the model to provide the ability for style embedding adjustment. The proposed SVM model is trained to classify between neutral emotion and a specific emotion embedding, where the learned hyperplane is utilized to move(scale) the style vectors in a direction towards/opposite to the hyperplane. Another type of control that contributes to generat-ing speech with a better representation of local prosodic variation is introduced in [124]. The proposed model suggests an unsupervised approach to obtain word-level prominence and phrasal boundary strength features. For this purpose, continuous wavelet transform (CWT) [168]is utilized to extract continuous estimates of word promi-nence and boundary information from the audio signal. First, the three prosodic signals f0, energy, and duration are extracted and combined as input to the CWT. Then,the combined signal is decomposed via CWT into scales that represent prosodic hierarchy. Word and phrase-level prosody are then obtained by following ridges or valleys across certain scales. The continuous word prominence and boundary estimates are achieved via the integration of the resulting lines aligned with the textual informa-tion. With manually identified intervals, the continuous values of prominence and boundary strength are then discretized.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#534-prosody-clustering","title":"5.3.4 Prosody clustering","text":"<p>In this section, methods for selecting the appropri-ate prosody embedding for the referenced-based ETTS models are described. To begin with, clustering methods are utilized in [57] [58] to generate representative pros-ody embeddings for each emotion class when the GST-TTS model is trained with a labeled dataset. Initially,the resulting emotion embeddings are clustered in a 2d space. In [57], the centroid of each cluster is used as the weights of the GSTs to generate emotion embedding for each emotion class. In [58], the weight vector that repre-sents each emotion cluster is obtained by considering the inter and intra distances between emotion embedding clusters. Specifically, an algorithm is used for minimizing each embedding distance to the target emotion cluster and maximizing its distance to other emotion clusters. Similarly, clustering algorithms are applied in [112] [113] to achieve discrete prosody embeddings but for two specific prosody-related features. The two studies employ K-means algorithm to cluster F0 and duration features extracted for each phoneme. The centroids of the clus-ters are then used as discrete F0 and duration values/tokens for each phoneme. work [112] applies a balanced clustering method with duration features to overcome degradation in voice quality that appeared in [113] dur-ing duration control. Moreover, to keep phonetic and prosodic information separate during training, an atten-tion unit is introduced to map prosody tokens to decoder hidden states and generate prosody context vectors. The resulting discrete tokens for F0 and duration features provide a fine-grained level of control over prosody by changing the corresponding prosodic tokens for each phoneme. In [105], a cross-domain SER model with the GST-TTS model is proposed to obtain emotion embeddings for an unlabeled dataset. The cross-domain SER model is trained using two datasets including: 1) an SER data-set (source) labeled with emotions, and 2) a TTS data-set (target) that is not labeled. Simultaneously, the SER model trains an emotion classifier that generates soft labels for the unlabeled TTS dataset. These soft labels are then used to train an extended version of the baseline in[74] with an emotion predictor. In the training process,the weights of the style tokens layer are passed as input to the predictor, which employs the learned soft labels as ground truth values. At inference time, weights vectors for each emotion class are averaged to obtain the emo-tion class embedding. However, since the predicted labels for the TTS dataset are soft labels, and thus not entirely reliable, only the top K samples with the highest posterior probabilities are selected.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#54-speech-synthesis-for-unseen-speakers-and-unseen-styles","title":"5.4 Speech synthesis for unseen speakers and unseen styles","text":"<p>Building a speech synthesis model that supports mul-tiple speakers or styles can be achieved by training TTS model with a multi-speaker multi-style dataset. How-ever, generating speech for an unseen speaker or style is a challenging task for which several solutions have been proposed in the literature. A popular approach is to fine-tune the averaged TTS model with some samples from the unseen target speaker or style. The fine-tuning pro-cess may require a single sample from the unseen speaker or style (referred to as one-shot models) or a few samples(referred to as few-shot models). There are also models that do not require any fine-tuning steps, and these are known as zero-shot TTS models. For instance, the fine-tuning process proposed in [112]focused on sentences used in the process to ensure pho-netic coverage, meaning that each phoneme should appear at least once in these sentences. The proposed model requires about 5 minutes of recordings from the unseen target speaker to clone the voice and allow for manipulation of some voice features (such as F0 and duration) by the model at the phoneme-level. Another approach to address the problem of unseen data is to employ specific structures in the TTS model,as proposed in [52] [96] [97] [107]. As an example, in [107],a cycle consistency network is proposed with two Vari-ational Autoencoders (VAEs). The model incorporates two training paths: a paired path and an unpaired path. The unpaired path refers to training scenarios where the reference audio differs from the output (target) speech in terms of text, style, or speaker. Two separate style encod-ers are utilized in the model, with one dedicated to each path. This structure facilitates style transfer among intra-speaker, inter-speaker, and unseen speaker scenarios. In [52], the U-net structure proposed for the TTS model supports one-shot speech synthesis for unseen styles and speakers. The U-net structure is used between the style encoder and the mel decoder of the TTS model,with an opposite flow between them. Both the style encoder and decoder consist of multiple modules with the main building unit as ResCnn1D and instance nor-malization (IN) layers. The decoder receives phoneme embedding and produces the Mel-spectrogram as out-put. In parallel, the style encoder receives the reference audio and produces its linguistic content with guidance from the content (text) encoder. The style encoder mod-ules produce latent variables, i.e., mean, and standard deviation, for the hidden inputs in the IN layers. These latent variables are used to bias and scale the normal-ized hiddens of the corresponding module layers in the decoder. A separate encoder (reference encoder) has been used in [96] to extract speaker-related information besides the prosody encoder (extractor) that encodes prosody fea-tures into the prosody embedding. A prosody predictor is also trained to predict the prosody embedding based on the phoneme-embedding. While the instance nor-malization (IN) layer is utilized by the prosody extractor to remove global (speaker) information and to keep pros-ody-related information, the speaker encoder is designed with a special structure (Conv2D layers, residual blocks(GLU with fully connected layers), and a multi-head self-attention unit) for better extraction of speaker informa-tion. Moreover, instead of concatenation or summation with the decoder input, the speaker embedding is adap-tively affine transformed to the different FFT blocks of the decoder through a Speaker-Adaptive Linear Modu-lation (SALM) network that is inspired by Feature-wise Linear Modulation (FiLM) [141]. The speaker encoder and conditioning of decoder blocks with speaker embed-ding allow the model to generate natural speech for unseen speakers with only a single reference sample(zero-shot).The attention unit used in seq2seq TTS models aims at mapping the different length between text and audio pairs. However, it can get unstable when the input is not seen during training [97]. The STYLER model has addressed this issue by using a linear compression or expansion of the audio to match the text\u2019s length via a method named Mel Calibrator. With this simplification of the alignment process as a scaling method, the unseen data robustness issue is alleviated and all audio-related style factors become dependent only on the audio. Similarly, in [119], the Householder Normalizing Flow[169] is incorporated into the VAE-based baseline model[77]. The Householder normalizing flow applies a series of easily invertible affine transformations to align the VAE\u2019s latent vectors (style embeddings) with a full covari-ance Gaussian distribution. As a result, the correlation among the latent vectors is improved. Generally, this architecture enhances the disentanglement capability of the baseline model and enables it to generate embedding for unseen style with just a single (one-shot) utterance of around one second length. The Multi-SpectroGAN TTS model proposed in [98]is a multi-speaker model trained based on adversarial feedback. The model supports the generation of speech for unseen styles/speakers by introducing adversarial style combination (ASC) during the training process. Style combinations result from mixing/interpolating style embeddings from different source speakers. The model is then trained with adversarial feedback using mixed-style mel-spectrograms. Two mixing methods are employed:binary selection or manifold mix-up via linear combina-tion. This training strategy enables the model to generate more natural speech for unseen speakers.</p> <p>Lastly, recent TTS models based on in-context learning (NaturalSpeech2 (2022), VALL-E (2023), Voicebox) all share the capability to perform zero-shot speech synthesis, as explained in Section 4.4. In fact, the in-context training strategy underlies the ability of these models to synthesize speech given only a style prompt with the input text. Specifically, the synthesis process treats the provided prompt/reference as part of the desired output speech. Therefore, the model\u2019s goal is to predict the rest of this speech in the same style as the given part (prompt) and with the input text. In Table 5 we list papers addressing each challenge.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#6","title":"6.\u6570\u636e\u96c6\u4e0e\u5f00\u6e90\u4ee3\u7801","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#7","title":"7.\u8bc4\u4ef7\u6307\u6807","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#8","title":"8.\u8ba8\u8bba","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#9","title":"9.\u7ed3\u8bba","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#r","title":"R.\u53c2\u8003\u6587\u732e","text":"<ul> <li>019 Cross-Speaker Emotion Transfer by Manipulating Speech Style Latents <li>032 Controllable Emotion Transfer for End-to-End Speech Synthesis</li> <li>034 Cross-Speaker Emotion Disentangling and Transfer for End-to-End Speech Synthesis</li> <li>047 Multi-Speaker Emotional Speech Synthesis with Fine-Grained Prosody Modeling</li> <li>074 Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron</li> <li>090 Domain-Adversarial Training of Neural Networks</li> <li>097 STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text To Speech</li> <li>102 Joint and Adversarial Training with ASR for Expressive Speech Synthesis</li> <li>111 Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement</li>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]}]}