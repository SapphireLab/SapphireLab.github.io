{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Sapphire Lab!","text":""},{"location":"TTS/","title":"Text-to-Speech (TTS) Synthesis","text":""},{"location":"TTS/#_1","title":"\u6a21\u578b\u5217\u8868","text":""},{"location":"TTS/#vocoder","title":"\u58f0\u7801\u5668 Vocoder","text":""},{"location":"TTS/#acoustic-model","title":"\u58f0\u5b66\u6a21\u578b Acoustic Model","text":""},{"location":"TTS/#end-to-end-model","title":"\u7aef\u5230\u7aef\u6a21\u578b End-to-End Model","text":""},{"location":"TTS/#large-language-model-based-model","title":"\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b Large Language Model-Based Model","text":"<ul> <li>2023.01 VALL-E</li> <li>2024.04 RALL-E</li> </ul>"},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/","title":"Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech","text":"<p>Jaehyeon Kim Jungil Kong Juhee Son </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS com parable to ground truth.</p> <p>\u8fd1\u671f\u51e0\u9879\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u4f7f\u5f97\u5355\u9636\u6bb5\u8bad\u7ec3\u548c\u5e76\u884c\u91c7\u6837\u6210\u4e3a\u53ef\u80fd, \u4f46\u5b83\u4eec\u7684\u91c7\u6837\u8d28\u91cf\u4e0e\u4e24\u9636\u6bb5\u7cfb\u7edf\u7684\u8d28\u91cf\u76f8\u6bd4\u4ecd\u6709\u5dee\u8ddd. \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e76\u884c\u7684\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u65b9\u6cd5, \u5176\u751f\u6210\u7684\u97f3\u9891\u66f4\u81ea\u7136. \u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u6807\u51c6\u5316\u6d41\u548c\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u7684\u53d8\u5206\u63a8\u65ad, \u8fd9\u53ef\u4ee5\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u8868\u73b0\u529b. \u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7528\u4e8e\u4ece\u8f93\u5165\u6587\u672c\u751f\u6210\u591a\u79cd\u8282\u594f\u7684\u8bed\u97f3. \u901a\u8fc7\u5bf9\u9690\u53d8\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u8868\u8fbe\u81ea\u7136\u7684\u4e00\u5bf9\u591a\u5173\u7cfb, \u5373\u4e00\u4e2a\u6587\u672c\u8f93\u5165\u53ef\u4ee5\u88ab\u8bf4\u6210\u591a\u79cd\u97f3\u9ad8\u548c\u8282\u594f. \u5728 LJ Speech \u6570\u636e\u96c6\u7684\u4e3b\u89c2\u8bc4\u4ef7 (MOS) \u4e2d, \u6211\u4eec\u5bf9\u6bd4\u4e86\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u7684\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf, \u5e76\u8fbe\u5230\u4e86\u53ef\u6bd4\u7684MOS.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Text-to-speech (TTS) systems synthesize raw speech waveforms from given text through several components. With the rapid development of deep neural networks, TTS system pipelines have been simplified to two-stage generative modeling apart from text preprocessing such as text normalization and phonemization. The first stage is to produce intermediate speech representations such as mel-spectrograms (Tacotron2 (2017)) or linguistic features (Oord et al., 2016) from the preprocessed text (Although there is a text preprocessing step in TTS systems, We herein use preprocessed text interchangeably with the word \"text\".), and the second stage is to generate raw waveforms conditioned on the intermediate representations (Oord et al., 2016; WaveRNN (2018)). Models at each of the two-stage pipelines have been developed independently.</p> <p>\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u901a\u8fc7\u6570\u4e2a\u7ec4\u4ef6\u4ece\u7ed9\u5b9a\u7684\u6587\u672c\u751f\u6210\u539f\u59cb\u8bed\u97f3\u6ce2\u5f62. \u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8fc5\u901f\u53d1\u5c55, \u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u5df2\u7ecf\u7b80\u5316\u4e3a\u9664\u4e86\u6587\u672c\u9884\u5904\u7406 (\u6587\u672c\u89c4\u8303\u5316\u548c\u97f3\u7d20\u5316) \u4e4b\u5916\u7684\u4e24\u9636\u6bb5\u751f\u6210\u6a21\u578b. \u7b2c\u4e00\u9636\u6bb5\u662f\u751f\u6210\u4e2d\u95f4\u8bed\u97f3\u8868\u793a\u5982\u6885\u5c14\u9891\u8c31\u56fe\u6216\u6765\u81ea\u9884\u5904\u7406\u6587\u672c\u7684\u8bed\u8a00\u7279\u5f81, \u7b2c\u4e8c\u9636\u6bb5\u662f\u6839\u636e\u4e2d\u95f4\u8868\u793a\u751f\u6210\u539f\u59cb\u6ce2\u5f62. \u6bcf\u4e2a\u9636\u6bb5\u7684\u6a21\u578b\u90fd\u5355\u72ec\u5efa\u7acb.</p> <p>Neural network-based autoregressive TTS systems have shown the capability of synthesizing realistic speech (Tacotron2 (2017); Transformer-TTS (2018)), but their sequential generative process makes it difficult to fully utilize modern parallel processors. To overcome this limitation and improve synthesis speed, several non-autoregressive methods have been proposed. In the text-to-spectrogram generation step, extracting attention maps from pre-trained autoregressive teacher networks (FastSpeech (2019); ParaNet) is attempted to decrease the difficulty of learning alignments between text and spectrograms. More recently, likelihood-based methods further eliminate the dependency on external aligners by estimating or learning alignments that maximize the likelihood of target mel-spectrograms (AlignTTS (2020); Flow-TTS (2020); Glow-TTS (2020)). Meanwhile, Generative Adversarial Networks (GANs) have been explored in second stage models. GAN-based feed-forward networks with multiple discriminators, each distinguishing samples at different scales or periods, achieve high-quality raw waveform synthesis (MelGAN (2019); GAN-TTS (2019); HiFi-GAN (2020)).</p> <p>\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u5df2\u7ecf\u5c55\u793a\u4e86\u5408\u6210\u771f\u5b9e\u8bed\u97f3\u7684\u80fd\u529b, \u4f46\u5b83\u4eec\u7684\u987a\u5e8f\u751f\u6210\u8fc7\u7a0b\u4f7f\u5f97\u5b83\u4eec\u96be\u4ee5\u5145\u5206\u5229\u7528\u73b0\u4ee3\u5e76\u884c\u5904\u7406\u5668. \u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\u5e76\u63d0\u5347\u5408\u6210\u901f\u5ea6, \u5df2\u7ecf\u6709\u6570\u79cd\u975e\u81ea\u56de\u5f52\u65b9\u6cd5\u88ab\u63d0\u51fa. \u5728\u6587\u672c\u5230\u9891\u8c31\u56fe\u751f\u6210\u6b65\u9aa4\u4e2d, \u4ece\u9884\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u6559\u5e08\u7f51\u7edc\u4e2d\u63d0\u53d6\u6ce8\u610f\u529b\u7279\u5f81\u56fe\u8bd5\u56fe\u51cf\u5c11\u5b66\u4e60\u6587\u672c\u548c\u9891\u8c31\u56fe\u4e4b\u95f4\u5bf9\u9f50\u7684\u96be\u5ea6. \u8fd1\u671f, \u57fa\u4e8e\u4f3c\u7136\u7684\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6d88\u9664\u4e86\u5bf9\u5916\u90e8\u5bf9\u9f50\u5668\u7684\u4f9d\u8d56, \u901a\u8fc7\u6700\u5927\u5316\u76ee\u6807\u9891\u8c31\u56fe\u7684\u4f3c\u7136\u6765\u4f30\u8ba1\u6216\u5b66\u4e60\u5bf9\u9f50. \u540c\u65f6, \u751f\u6210\u5bf9\u6297\u7f51\u7edc\u88ab\u7528\u4e8e\u7b2c\u4e8c\u9636\u6a21\u578b. \u57fa\u4e8e GAN \u7684\u524d\u9988\u7f51\u7edc\u5177\u6709\u591a\u4e2a\u5224\u522b\u5668, \u6bcf\u4e2a\u5224\u522b\u4e0d\u540c\u5c3a\u5ea6\u6216\u5468\u671f\u7684\u6837\u672c, \u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u539f\u59cb\u6ce2\u5f62\u5408\u6210.</p> <p>Despite the progress of parallel TTS systems, two-stage pipelines remain problematic because they require sequential training or fine-tuning (Tacotron2 (2017); Wave-Tacotron (2020)) for high-quality production wherein latter stage models are trained with the generated samples of earlier stage models. In addition, their dependency on predefined intermediate features precludes applying learned hidden representations to obtain further improvements in performance. Recently, several works, i.e., FastSpeech 2s (2020) and EATS (2020), have proposed efficient end-to-end training methods such as training over short audio clips rather than entire waveforms, leveraging a mel-spectrogram decoder to aid text representation learning, and designing a specialized spectrogram loss to relax length-mismatch between target and generated speech.  However, despite potentially improving performance by utilizing the learned representations, their synthesis quality lags behind two-stage systems.</p> <p>\u5c3d\u7ba1\u5e76\u884c\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u53d6\u5f97\u4e86\u8fdb\u5c55, \u4f46\u4e24\u9636\u6bb5\u65b9\u6848\u4ecd\u7136\u5b58\u5728\u95ee\u9898. \u56e0\u4e3a\u5b83\u4eec\u8981\u6c42\u987a\u5e8f\u8bad\u7ec3\u6216\u5fae\u8c03\u4ee5\u83b7\u53d6\u9ad8\u8d28\u91cf\u7ed3\u679c, \u5373\u540e\u4e00\u9636\u6bb5\u7684\u6a21\u578b\u9700\u8981\u5728\u524d\u4e00\u9636\u6bb5\u6a21\u578b\u7684\u751f\u6210\u6837\u672c\u4e0a\u8fdb\u884c\u8bad\u7ec3. \u6b64\u5916, \u4ed6\u4eec\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u4e2d\u95f4\u7279\u5f81\u9650\u5236\u4e86\u5c06\u5b66\u4e60\u5230\u7684\u9690\u85cf\u8868\u793a\u4ee5\u83b7\u5f97\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347. \u8fd1\u671f, \u6709\u51e0\u9879\u5de5\u4f5c\u5982 FastSpeech 2s (2020) \u548c EATS \u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5, \u5982\u8bad\u7ec3\u4e8e\u8f83\u77ed\u7684\u97f3\u9891\u7247\u6bb5\u800c\u4e0d\u662f\u5b8c\u6574\u6ce2\u5f62, \u5229\u7528\u6885\u5c14\u9891\u8c31\u56fe\u89e3\u7801\u5668\u8f85\u52a9\u6587\u672c\u8868\u793a\u5b66\u4e60, \u5e76\u8bbe\u8ba1\u4e13\u95e8\u7684\u9891\u8c31\u56fe\u635f\u5931\u4ee5\u677e\u5f1b\u76ee\u6807\u548c\u751f\u6210\u8bed\u97f3\u7684\u957f\u5ea6\u4e0d\u5339\u914d. \u7136\u800c, \u5c3d\u7ba1\u5229\u7528\u5b66\u4e60\u5230\u7684\u8868\u793a\u53ef\u4ee5\u63d0\u5347\u6027\u80fd, \u4f46\u5b83\u4eec\u7684\u5408\u6210\u8d28\u91cf\u4ecd\u7136\u843d\u540e\u4e8e\u4e24\u9636\u6bb5\u7cfb\u7edf.</p> <p>In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models.  Using a Variational AutoEncoder (VAE), we connect two modules of TTS systems through latent variables to enable efficient end-to-end learning.  To improve the expressive power of our method so that high-quality speech waveforms can be synthesized, we apply normalizing flows to our conditional prior distribution and adversarial training on the waveform domain.  In addition to generating fine-grained audio, it is important for TTS systems to express the one-to-many relationship in which text input can be spoken in multiple ways with different variations (e.g., pitch and duration).  To tackle the one-to-many problem, we also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method captures speech variations that cannot be represented by text. </p> <p>\u672c\u9879\u5de5\u4f5c\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e76\u884c\u7684\u7aef\u5230\u7aef\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u65b9\u6cd5, \u5176\u751f\u6210\u7684\u97f3\u9891\u66f4\u81ea\u7136. \u6211\u4eec\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAE) \u5c06\u4e24\u4e2a\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u7684\u6a21\u5757\u8fde\u63a5\u8d77\u6765, \u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u5b66\u4e60. \u4e3a\u4e86\u63d0\u9ad8\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u8868\u73b0\u529b, \u6211\u4eec\u5c06\u6807\u51c6\u5316\u6d41\u5e94\u7528\u5230\u6761\u4ef6\u5148\u9a8c\u5206\u5e03, \u5e76\u5728\u6ce2\u5f62\u57df\u4e0a\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3. \u9664\u4e86\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u97f3\u9891\u4e4b\u5916, \u5bf9\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u6765\u8bf4, \u8868\u8fbe\u4e00\u5bf9\u591a\u5173\u7cfb\u662f\u81f3\u5173\u91cd\u8981\u7684, \u5373\u8f93\u5165\u6587\u672c\u53ef\u4ee5\u4ee5\u4e0d\u540c\u7684\u97f3\u9ad8\u548c\u8282\u594f\u88ab\u8bf4\u51fa\u6765. \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898, \u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u7528\u4e8e\u4ece\u8f93\u5165\u6587\u672c\u751f\u6210\u591a\u79cd\u8282\u594f\u7684\u8bed\u97f3. \u901a\u8fc7\u5bf9\u9690\u53d8\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u6355\u6349\u5230\u4e0d\u80fd\u7528\u6587\u672c\u8868\u793a\u7684\u8bed\u97f3\u53d8\u4f53.</p> <p>Our method obtains more natural sounding speech and higher sampling efficiency than the best publicly available TTS system, Glow-TTS (2020) with HiFi-GAN (2020).  We make both our demo page and source-code publicly available.</p> <p>\u6211\u4eec\u7684\u65b9\u6cd5\u83b7\u5f97\u66f4\u81ea\u7136\u7684\u8bed\u97f3\u548c\u66f4\u9ad8\u7684\u91c7\u6837\u6548\u7387.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#2related-works","title":"2.Related Works\u00b7\u76f8\u5173\u5de5\u4f5c","text":"<p>\u6ce8: \u539f\u6587\u7b2c\u4e94\u8282 Section 5 in Original.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#21end-to-end-text-to-speech","title":"2.1.End-to-End Text-to-Speech\u00b7\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3","text":"<p>Currently, neural TTS models with a two-stage pipeline can synthesize human-like speech (WaveNet (2016); DeepVoice3; Tacotron2 (2017)). However, they typically require vocoders trained or fine-tuned with first stage model output, which causes training and deployment inefficiency. They are also unable to reap the potential benefits of an end-to-end approach that can use learned hidden representations rather than predefined intermediate features.</p> <p>\u5f53\u524d\u4e24\u9636\u6bb5\u7684\u795e\u7ecf\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u53ef\u4ee5\u5408\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8bed\u97f3. \u7136\u800c\u4ed6\u4eec\u540c\u6837\u8981\u6c42\u4f7f\u7528\u7b2c\u4e00\u9636\u6bb5\u6a21\u578b\u7684\u8f93\u51fa\u7528\u4e8e\u8bad\u7ec3\u6216\u5fae\u8c03\u58f0\u7801\u5668, \u8fd9\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u548c\u90e8\u7f72\u6548\u7387\u4f4e\u4e0b. \u4ed6\u4eec\u8fd8\u65e0\u6cd5\u5229\u7528\u7aef\u5230\u7aef\u65b9\u6cd5\u7684\u6f5c\u5728\u4f18\u52bf, \u5373\u53ef\u4ee5\u5229\u7528\u5b66\u4e60\u5230\u7684\u9690\u85cf\u8868\u793a\u800c\u975e\u9884\u5b9a\u4e49\u7684\u4e2d\u95f4\u7279\u5f81.</p> <p>Recently, single-stage end-to-end TTS models have been proposed to tackle the more challenging task of generating raw waveforms, which contain richer information (e.g., high-frequency response and phase) than mel-spectrograms, directly from text. FastSpeech 2s (2020) is an extension of FastSpeech 2 that enables end-to-end parallel generation by adopting adversarial training and an auxiliary mel-spectrogram decoder that helps learn text representations. However, to resolve the one-to-many problem, FastSpeech 2s must extract phoneme duration, pitch, and energy from speech used as input conditions in training. EATS (2020) employs adversarial training as well and a differentiable alignment scheme. To resolve the length mismatch problem between generated and target speech, EATS adopts soft dynamic time warping loss that is calculated by dynamic programming. Wave-Tacotron (2020) combines normalizing flows with Tacotron 2 for an end-to-end structure but remains autoregressive. The audio quality of all the aforementioned end-to-end TTS models is less than that of two-stage models.</p> <p>\u6700\u8fd1\u5355\u9636\u6bb5\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u88ab\u63d0\u51fa\u7528\u4e8e\u89e3\u51b3\u751f\u6210\u539f\u59cb\u6ce2\u5f62\u7684\u66f4\u5177\u6311\u6218\u6027\u4efb\u52a1, \u5b83\u5305\u542b\u6bd4\u6885\u5c14\u9891\u8c31\u56fe\u66f4\u591a\u7684\u4fe1\u606f (\u5982\u9ad8\u9891\u54cd\u5e94\u548c\u76f8\u4f4d), \u76f4\u63a5\u4ece\u6587\u672c\u751f\u6210. FastSpeech 2s \u662f FastSpeech 2 \u7684\u6269\u5c55, \u5b83\u901a\u8fc7\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u548c\u4e00\u4e2a\u8f85\u52a9\u7684\u6885\u5c14\u9891\u8c31\u56fe\u89e3\u7801\u5668\u6765\u5b9e\u73b0\u7aef\u5230\u7aef\u5e76\u884c\u751f\u6210. \u7136\u800c, \u4e3a\u4e86\u89e3\u51b3\u4e00\u5bf9\u591a\u95ee\u9898, FastSpeech 2s \u5fc5\u987b\u4ece\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u8bed\u97f3\u4e2d\u63d0\u53d6\u97f3\u7d20\u65f6\u957f, \u97f3\u9ad8\u548c\u80fd\u91cf. EATS \u4e5f\u662f\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u548c\u4e0d\u540c\u7684\u5bf9\u9f50\u65b9\u6848. \u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u548c\u76ee\u6807\u8bed\u97f3\u4e4b\u95f4\u7684\u957f\u5ea6\u4e0d\u5339\u914d\u95ee\u9898, EATS \u91c7\u7528\u8f6f\u52a8\u6001\u65f6\u95f4\u7a97\u635f\u5931, \u5b83\u662f\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u8ba1\u7b97\u7684. Wave Tacotron \u662f\u5c06\u6807\u51c6\u5316\u6d41\u4e0e Tacotron 2 \u7ed3\u5408\u7528\u4e8e\u7aef\u5230\u7aef\u7ed3\u6784, \u4f46\u4ecd\u7136\u662f\u81ea\u56de\u5f52\u7684. \u6240\u6709\u8fd9\u4e9b\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u7684\u8bed\u97f3\u8d28\u91cf\u90fd\u4e0d\u5982\u4e24\u9636\u6bb5\u6a21\u578b.</p> <p>Unlike the aforementioned end-to-end models, by utilizing a conditional VAE, our model </p> <ol> <li>learns to synthesize raw waveforms directly from text without requiring additional input conditions,</li> <li>uses a dynamic programming method, MAS, to search the optimal alignment rather than to calculate loss, </li> <li>generates samples in parallel, </li> <li>outperforms the best publicly available two-stage models.</li> </ol> <p>\u548c\u524d\u9762\u63d0\u53ca\u7684\u7aef\u5230\u7aef\u6a21\u578b\u4e0d\u540c, \u901a\u8fc7\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668, \u6211\u4eec\u7684\u6a21\u578b: 1. \u76f4\u63a5\u4ece\u6587\u672c\u751f\u6210\u539f\u59cb\u6ce2\u5f62\u800c\u4e0d\u9700\u8981\u989d\u5916\u7684\u8f93\u5165\u6761\u4ef6, 2. \u4f7f\u7528\u52a8\u6001\u89c4\u5212\u65b9\u6cd5 MAS \u641c\u7d22\u6700\u4f18\u5bf9\u9f50\u800c\u4e0d\u662f\u8ba1\u7b97\u635f\u5931, 3. \u4ee5\u5e76\u884c\u7684\u65b9\u5f0f\u751f\u6210\u6837\u672c, 4. \u6027\u80fd\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u516c\u5f00\u53ef\u7528\u7684\u4e24\u9636\u6bb5\u6a21\u578b.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#22variational-autoencoders","title":"2.2.Variational Autoencoders\u00b7\u53d8\u5206\u81ea\u7f16\u7801\u5668","text":"<p>VAEs are one of the most widely used likelihood-based deep generative models. We adopt a conditional VAE to a TTS system. A conditional VAE is a conditional generative model where the observed conditions modulate the prior distribution of latent variables used to generate outputs. In speech synthesis, GMVAE-Tacotron (2018) and Learning latent representations for style control and transfer in end-to-end speech synthesis combine Tacotron 2 and VAEs to learn speech style and prosody. BVAE-TTS (2020) generates mel-spectrograms in parallel based on a bidirectional VAE (IAF (2016)). Unlike the previous works that applied VAEs to first stage models, we adopt a VAE to a parallel end-to-end TTS system. Variational inference with normalizing flows, Variational Lossy Autoencoder and Latent normalizing flows for discrete sequences improve VAE performance by enhancing the expressive power of prior and posterior distribution with normalizing flows. To improve the representation power of the prior distribution, we add normalizing flows to our conditional prior network, leading to the generation of more realistic samples.</p> <p>\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAEs) \u662f\u6700\u5e38\u7528\u7684\u57fa\u4e8e\u4f3c\u7136\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e4b\u4e00. \u6211\u4eec\u5c06\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf. \u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u662f\u4e00\u79cd\u6761\u4ef6\u751f\u6210\u6a21\u578b, \u5176\u4e2d\u89c2\u5bdf\u5230\u7684\u6761\u4ef6\u4f1a\u5f71\u54cd\u7528\u4e8e\u751f\u6210\u8f93\u51fa\u7684\u9690\u53d8\u91cf\u7684\u5148\u9a8c\u5206\u5e03. \u5728\u8bed\u97f3\u5408\u6210\u4e2d, Hsu \u548c Zhang \u5c06 Tacotron2 \u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7ed3\u5408\u4ee5\u5b66\u4e60\u8bed\u97f3\u98ce\u683c\u548c\u97f5\u5f8b. BVAE-TTS \u57fa\u4e8e\u53cc\u5411\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ee5\u5e76\u884c\u751f\u6210\u6885\u5c14\u9891\u8c31. \u548c\u8fd9\u4e9b\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u7b2c\u4e00\u9636\u6bb5\u6a21\u578b\u7684\u5de5\u4f5c\u4e0d\u540c, \u6211\u4eec\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u5e76\u884c\u7aef\u5230\u7aef\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf. Rezende, Chen, Ziegler \u901a\u8fc7\u4f7f\u7528\u6807\u51c6\u5316\u6d41\u589e\u5f3a\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\u8868\u73b0\u6027\u4ee5\u63d0\u9ad8\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6027\u80fd. \u4e3a\u4e86\u63d0\u5347\u5148\u9a8c\u5206\u5e03\u7684\u8868\u793a\u80fd\u529b, \u6211\u4eec\u5728\u6761\u4ef6\u5148\u9a8c\u7f51\u7edc\u4e2d\u52a0\u5165\u4e86\u6807\u51c6\u5316\u6d41, \u4ece\u800c\u80fd\u591f\u751f\u6210\u66f4\u771f\u5b9e\u7684\u6837\u672c.</p> <p>Similar to our work, Flowseq proposed a conditional VAE with normalizing flows in a conditional prior network for non-autoregressive neural machine translation, FlowSeq. However, the fact that our model can explicitly align a latent sequence with the source sequence differs from FlowSeq, which needs to learn implicit alignment through attention mechanisms. Our model removes the burden of transforming the latent sequence into standard normal random variables by matching the latent sequence with the time-aligned source sequence via MAS, which allows for simpler architecture of normalizing flows.</p> <p>\u548c\u6211\u4eec\u7684\u5de5\u4f5c\u7c7b\u4f3c, Ma \u7b49\u4eba\u63d0\u51fa\u4e86 FlowSeq \u4e00\u4e2a\u5e26\u6709\u6807\u51c6\u5316\u6d41\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668, \u7528\u4e8e\u975e\u81ea\u56de\u5f52\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1. \u7136\u800c, \u7531\u4e8e\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u663e\u5f0f\u5730\u5bf9\u9f50\u9690\u53d8\u91cf\u5e8f\u5217\u548c\u6e90\u5e8f\u5217, \u4e0e FlowSeq \u4e0d\u540c, \u5b83\u9700\u8981\u901a\u8fc7\u6ce8\u610f\u673a\u5236\u5b66\u4e60\u9690\u5f0f\u5bf9\u9f50. \u6211\u4eec\u7684\u6a21\u578b\u901a\u8fc7 MAS \u5339\u914d\u9690\u53d8\u91cf\u5e8f\u5217\u548c\u65f6\u95f4\u5bf9\u9f50\u7684\u6e90\u5e8f\u5217, \u4ece\u800c\u6d88\u9664\u4e86\u5c06\u9690\u53d8\u91cf\u5e8f\u5217\u8f6c\u6362\u4e3a\u6807\u51c6\u6b63\u6001\u968f\u673a\u53d8\u91cf\u7684\u8d1f\u62c5, \u8fd9\u4f7f\u5f97\u6807\u51c6\u5316\u6d41\u7684\u67b6\u6784\u66f4\u7b80\u5355.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#23duration-prediction-in-non-autoregressive-text-to-speech","title":"2.3.Duration Prediction in Non-Autoregressive Text-to-Speech\u00b7\u975e\u81ea\u56de\u5f52\u6587\u672c\u8f6c\u8bed\u97f3\u4e2d\u7684\u65f6\u957f\u9884\u6d4b","text":"<p>Autoregressive TTS models (VoiceLoop (2017.07); Tacotron2 (2017); Flowtron (2021)) generate diverse speech with different rhythms through their autoregressive structure and several tricks including maintaining dropout probability during inference and priming (Generating sequences with recurrent neural networks). Parallel TTS models (FastSpeech (2019); ParaNet (2020); Glow-TTS (2020); FastSpeech 2s (2020); BVAE-TTS (2020)), on the other hand, have been relied on deterministic duration prediction. It is because parallel models have to predict target phoneme duration or the total length of target speech in one feed-forward path, which makes it hard to capture the correlated joint distribution of speech rhythms. In this work, we suggest a flow-based stochastic duration predictor that learns the joint distribution of the estimated phoneme duration, resulting in the generation of diverse speech rhythms in parallel.</p> <p>\u81ea\u56de\u5f52\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u901a\u8fc7\u4ed6\u4eec\u7684\u81ea\u56de\u5f52\u7ed3\u6784\u548c\u4e00\u4e9b\u6280\u5de7\u5305\u62ec\u5728\u63a8\u7406\u7ef4\u6301\u968f\u673a\u5931\u6d3b\u7387\u7b49\u7b49\u751f\u6210\u5177\u6709\u4e0d\u540c\u8282\u594f\u7684\u591a\u6837\u8bed\u97f3. \u5e76\u884c\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u5219\u4f9d\u8d56\u4e8e\u786e\u5b9a\u6027\u7684\u65f6\u957f\u9884\u6d4b, \u8fd9\u662f\u56e0\u4e3a\u5e76\u884c\u6a21\u578b\u5fc5\u987b\u5728\u4e00\u4e2a\u524d\u9988\u8def\u5f84\u4e2d\u9884\u6d4b\u76ee\u6807\u97f3\u7d20\u65f6\u957f\u6216\u76ee\u6807\u8bed\u97f3\u7684\u603b\u957f\u5ea6, \u8fd9\u4f7f\u5f97\u96be\u4ee5\u6355\u6349\u8bed\u97f3\u8282\u594f\u7684\u76f8\u5173\u8054\u5408\u5206\u5e03. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6d41\u7684\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u5b83\u5b66\u4e60\u4f30\u8ba1\u7684\u97f3\u7d20\u65f6\u957f\u7684\u8054\u5408\u5206\u5e03, \u4ece\u800c\u5728\u5e76\u884c\u751f\u6210\u4e2d\u751f\u6210\u5177\u6709\u4e0d\u540c\u8282\u594f\u7684\u591a\u6837\u8bed\u97f3.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#3method","title":"3.Method","text":"<p>In this section, we explain our proposed method and the architecture of it. The proposed method is mostly described in the first three subsections: a conditional VAE formulation; alignment estimation derived from variational inference; adversarial training for improving synthesis quality. The overall architecture is described at the end of this section. Fig.01a and Fig.01b show the training and inference procedures of our method, respectively. From now on, we will refer to our method as Variational Inference with adversarial learning for end-to-end Text-to-Speech (VITS).</p> <p></p> <p>\u672c\u8282\u5c06\u89e3\u91ca\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ca\u5176\u67b6\u6784. \u5728\u524d\u4e09\u4e2a\u5c0f\u8282\u4e2d, \u6211\u4eec\u5c06\u4ecb\u7ecd\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u4e3b\u8981\u5185\u5bb9: \u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5f62\u5f0f; \u7531\u53d8\u5206\u63a8\u65ad\u5f97\u5230\u7684\u5bf9\u9f50\u4f30\u8ba1; \u5bf9\u6297\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u5408\u6210\u8d28\u91cf. \u6700\u540e\u4e00\u5c0f\u8282\u5c06\u4ecb\u7ecd\u6574\u4f53\u67b6\u6784. \u56fe 01a \u548c\u56fe 01b \u5206\u522b\u5c55\u793a\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b. \u4e4b\u540e, \u6211\u4eec\u5c06\u4f7f\u7528 VITS \u6765\u6307\u4ee3\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#31variational-inference","title":"3.1.Variational Inference\u00b7\u53d8\u5206\u63a8\u65ad","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#311overview","title":"3.1.1.Overview\u00b7\u6982\u89c8","text":"<p>VITS can be expressed as a conditional VAE with the objective of maximizing the variational lower bound, also called the evidence lower bound (ELBO), of the intractable marginal log-likelihood of data $\\log p_{\\theta}(x|c)$:</p> <p>VITS \u53ef\u4ee5\u8868\u793a\u4e3a\u4e00\u4e2a\u6761\u4ef6\u81ea\u7f16\u7801\u5668, \u5176\u76ee\u6807\u662f\u6700\u5927\u5316\u96be\u4ee5\u5904\u7406\u7684\u6570\u636e\u7684\u8fb9\u9645\u5206\u5e03\u5bf9\u6570\u4f3c\u7136 $\\log p_{\\theta}(x|c)$ \u7684\u53d8\u5206\u4e0b\u754c, \u4e5f\u79f0\u4e3a\u8bc1\u636e\u4e0b\u754c (Evidence Lower BOund, ELBO).</p> <p>$$   \\log p_{\\theta}(x|c) \\geq \\mathbb{E}{q{\\phi}(z|x)}\\left[\\log p_{\\theta}(x|z)-\\log\\dfrac{q_{\\phi}(z|x)}{p_{\\theta}(z|c)}\\right] \\tag{1} $$</p> <p>where $p_{\\theta}(z|c)$ denotes a prior distribution of the latent variables $z$ given condition $c$,  $p_{\\theta}(x|z)$ is the likelihood function of a data point $x$, and $q_{\\phi}(z|x)$ is an approximate posterior distribution.</p> <ul> <li>$p_{\\theta}(z|c)$: \u7ed9\u5b9a\u6761\u4ef6 $c$ \u7684\u9690\u53d8\u91cf $z$ \u7684\u5148\u9a8c\u5206\u5e03.</li> <li>$p_{\\theta}(x|z)$: \u6570\u636e\u70b9 $x$ \u7684\u4f3c\u7136\u51fd\u6570.</li> <li>$q_{\\phi}(z|x)$: \u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03.</li> </ul> <p>The training loss is then the negative ELBO, which can be viewed as the sum of reconstruction loss $-\\log p_{\\theta}(x|z)$ and KL-divergence $\\log q_{\\phi}(z|x) -\\log | p_{\\theta}(z|c)$, where $z\\sim q_{\\phi}(z|x)$.</p> <p>\u8bad\u7ec3\u635f\u5931\u4e3a\u8d1f\u7684 ELBO, \u5b83\u53ef\u4ee5\u88ab\u89c6\u4e3a\u91cd\u6784\u635f\u5931 $-\\log p_{\\theta}(x|z)$ \u548c KL \u6563\u5ea6 $\\log q_{\\phi}(z|x) -\\log | p_{\\theta}(z|c)$ \u7684\u603b\u548c, \u5176\u4e2d $z\\sim q_{\\phi}(z|x)$.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#312reconstruction-loss","title":"3.1.2.Reconstruction Loss\u00b7\u91cd\u6784\u635f\u5931","text":"<p>As a target data point in the reconstruction loss, we use a mel-spectrogram instead of a raw waveform, denoted by $x_{mel}$.  We upsample the latent variables $z$ to the waveform domain $\\hat{y}$ through a decoder and transform $\\hat{y}$ to the mel-spectrogram domain $\\hat{x}_{mel}$.  Then the $L_1$ loss between the predicted and target mel-spectrogram is used as the reconstruction loss:</p> <p>\u91cd\u6784\u635f\u5931\u4e2d\u4f7f\u7528\u7684\u76ee\u6807\u6570\u636e\u70b9\u662f\u6885\u5c14\u9891\u8c31\u800c\u4e0d\u662f\u539f\u59cb\u6ce2\u5f62, \u8bb0\u4e3a $x_{mel}$. \u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u89e3\u7801\u5668\u5c06\u9690\u53d8\u91cf $z$ \u4e0a\u91c7\u6837\u5230\u6ce2\u5f62\u57df $\\hat{y}$, \u5e76\u5c06 $\\hat{y}$ \u8f6c\u6362\u5230\u6885\u5c14\u9891\u8c31\u57df $\\hat{x}_{mel}$. \u7136\u540e, \u6211\u4eec\u4f7f\u7528\u9884\u6d4b\u7684\u548c\u76ee\u6807\u7684\u6885\u5c14\u9891\u8c31\u4e4b\u95f4\u7684 $L_1$ \u635f\u5931\u4f5c\u4e3a\u91cd\u6784\u635f\u5931:</p> <p>$$   L_{recon}= |x_{mel}\u2212\\hat{x}_{mel}|_1 \\tag{2} $$</p> <p>This can be viewed as maximum likelihood estimation assuming a Laplace distribution for the data distribution and ignoring constant terms. </p> <p>\u8fd9\u53ef\u4ee5\u89c6\u4e3a\u5047\u8bbe\u6570\u636e\u5206\u5e03\u4e3a Laplace \u5206\u5e03, \u5e76\u5ffd\u7565\u5e38\u6570\u9879\u540e\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1.</p> <p>We define the reconstruction loss in the mel-spectrogram domain to improve the perceptual quality by using a mel-scale that approximates the response of the human auditory system.  Note that the mel-spectrogram estimation from a raw waveform does not require trainable parameters as it only uses STFT and linear projection onto the mel-scale.  Furthermore, the estimation is only employed during training, not inference.  In practice, we do not upsample the whole latent variables $z$ but use partial sequences as an input for the decoder, which is the windowed generator training used for efficient end-to-end training (FastSpeech 2s (2020); EATS (2020)).</p> <p>\u6211\u4eec\u5728\u6885\u5c14\u9891\u8c31\u57df\u4e2d\u5b9a\u4e49\u4e86\u91cd\u6784\u635f\u5931, \u4ee5\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf, \u56e0\u4e3a\u5b83\u4f7f\u7528\u4e00\u4e2a\u8fd1\u4f3c\u4eba\u7c7b\u542c\u89c9\u7cfb\u7edf\u54cd\u5e94\u7684\u6885\u5c14\u523b\u5ea6. \u6ce8\u610f, \u4ece\u539f\u59cb\u6ce2\u5f62\u4f30\u8ba1\u6885\u5c14\u9891\u8c31\u4e0d\u9700\u8981\u53ef\u8bad\u7ec3\u53c2\u6570, \u56e0\u4e3a\u5b83\u53ea\u4f7f\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\u548c\u7ebf\u6027\u6295\u5f71\u5230\u6885\u5c14\u523b\u5ea6. \u6b64\u5916, \u4f30\u8ba1\u4ec5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528, \u800c\u63a8\u7406\u65f6\u4e0d\u4f7f\u7528. \u5b9e\u9645\u4e0a, \u6211\u4eec\u4e0d\u4e0a\u91c7\u6837\u6574\u4e2a\u9690\u53d8\u91cf $z$, \u800c\u662f\u4f7f\u7528\u90e8\u5206\u5e8f\u5217\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165, \u8fd9\u4e0e\u7528\u4e8e\u9ad8\u6548\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u7a97\u53e3\u751f\u6210\u5668\u8bad\u7ec3\u76f8\u540c.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#313kl-divergencekl","title":"3.1.3.KL-Divergence\u00b7KL \u6563\u5ea6","text":"<p>The input condition of the prior encoder $c$ is composed of phonemes $c_{text}$ extracted from text and an alignment $A$ between phonemes and latent variables. </p> <p>\u5148\u9a8c\u7f16\u7801\u5668\u7684\u8f93\u5165\u6761\u4ef6 $c$ \u7531\u6587\u672c\u4e2d\u63d0\u53d6\u7684\u97f3\u7d20 $c_{text}$, \u97f3\u7d20\u548c\u9690\u53d8\u91cf\u4e4b\u95f4\u7684\u5bf9\u9f50 $A$ \u7ec4\u6210.</p> <p>The alignment is a hard monotonic attention matrix with $|c_{text}|\\times|z|$ dimensions representing how long each input phoneme expands to be time-aligned with the target speech. Because there are no ground truth labels for the alignment, we must estimate the alignment at each training iteration, which we will discuss in Section 2.2.1. </p> <p>\u8fd9\u4e2a\u5bf9\u9f50\u662f\u4e00\u4e2a\u7ef4\u5ea6\u4e3a $|c_{text}|\\times|z|$ \u786c\u6027\u5355\u8c03\u6ce8\u610f\u529b\u77e9\u9635, \u8868\u793a\u6bcf\u4e2a\u8f93\u5165\u97f3\u7d20\u6269\u5c55\u5230\u4e0e\u76ee\u6807\u8bed\u97f3\u65f6\u95f4\u5bf9\u9f50\u7684\u65f6\u95f4\u957f\u5ea6. \u56e0\u4e3a\u5bf9\u9f50\u6ca1\u6709\u771f\u5b9e\u6807\u7b7e, \u6211\u4eec\u5fc5\u987b\u5728\u6bcf\u4e00\u6b21\u8bad\u7ec3\u8fed\u4ee3\u4e2d\u4f30\u8ba1\u5bf9\u9f50, \u6211\u4eec\u5c06\u5728\u7b2c 3.2.1 \u8282\u4e2d\u8ba8\u8bba. </p> <p>In our problem setting, we aim to provide more high-resolution information for the posterior encoder. We, therefore, use the linear-scale spectrogram of target speech $x_{lin}$ as input rather than the mel-spectrogram.  Note that the modified input does not violate the properties of variational inference. </p> <p>\u5728\u6211\u4eec\u7684\u95ee\u9898\u8bbe\u7f6e\u4e2d, \u6211\u4eec\u5e0c\u671b\u4e3a\u540e\u9a8c\u7f16\u7801\u5668\u63d0\u4f9b\u66f4\u591a\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f. \u56e0\u6b64, \u6211\u4eec\u4f7f\u7528\u76ee\u6807\u8bed\u97f3\u7684\u7ebf\u6027\u9891\u8c31 $x_{lin}$ \u4f5c\u4e3a\u8f93\u5165, \u800c\u4e0d\u662f\u6885\u5c14\u9891\u8c31. \u6ce8\u610f, \u6211\u4eec\u4fee\u6539\u7684\u8f93\u5165\u5e76\u4e0d\u8fdd\u53cd\u53d8\u5206\u63a8\u65ad\u7684\u6027\u8d28.</p> <p>The KL divergence is then:</p> <p>KL \u6563\u5ea6\u4e3a:</p> <p>$$   L_{kl} = \\log q_{\\phi}(z|x) -\\log p_{\\theta}(z|c_{text},A) \\tag{3} $$</p> <p>$$   z\\sim q_{\\phi}(z|x) =\\mathcal{N}(z;\\mu_{\\phi}(x_{lin}), \\sigma_{\\phi}(x_{lin})) $$</p> <p>The factorized normal distribution is used to parameterize our prior and posterior encoders. </p> <p>\u6211\u4eec\u4f7f\u7528\u56e0\u5b50\u6b63\u6001\u5206\u5e03\u6765\u53c2\u6570\u5316\u6211\u4eec\u7684\u5148\u9a8c\u548c\u540e\u9a8c\u7f16\u7801\u5668.</p> <p>We found that increasing the expressiveness of the prior distribution is important for generating realistic samples.  We, therefore, apply a normalizing flow $f_{\\theta}$ (Variational inference with normalizing flows), which allows an invertible transformation of a simple distribution into a more complex distribution following the rule of change-of-variables, on top of the factorized normal prior distribution:</p> <p>\u6211\u4eec\u53d1\u73b0\u589e\u52a0\u5148\u9a8c\u5206\u5e03\u7684\u8868\u73b0\u6027\u5bf9\u4e8e\u751f\u6210\u771f\u5b9e\u6837\u672c\u81f3\u5173\u91cd\u8981. \u56e0\u6b64, \u6211\u4eec\u5728\u56e0\u5b50\u6b63\u6001\u5148\u9a8c\u5206\u5e03\u4e0a\u5e94\u7528\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u6d41 $f_{\\theta}$, \u5b83\u80fd\u591f\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u5206\u5e03\u901a\u8fc7\u53d8\u91cf\u53d8\u6362\u89c4\u5219\u8f6c\u6362\u4e3a\u5230\u4e00\u4e2a\u66f4\u590d\u6742\u5206\u5e03\u7684\u53ef\u9006\u53d8\u6362:</p> <p>$$   p_{\\theta}(z|c) = \\mathcal{N}(f_{\\theta}(z);\\mu_{\\theta}(c),\\sigma_{\\theta}(c))|\\det\\dfrac{\\partial f_{\\theta}(z)}{\\partial z}| \\tag{4} $$</p> <p>\u5176\u4e2d $c=[c_{text}, A]$</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#32alignment-estimation","title":"3.2.Alignment Estimation\u00b7\u5bf9\u9f50\u4f30\u8ba1","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#321monotonic-alignment-search","title":"3.2.1.Monotonic Alignment Search\u00b7\u5355\u8c03\u5bf9\u9f50\u641c\u7d22","text":"<p>To estimate an alignment $A$ between input text and target speech, we adopt Monotonic Alignment Search (MAS), a method to search an alignment that maximizes the likelihood of data parameterized by a normalizing flow $f$:</p> <p>\u4e3a\u4e86\u4f30\u8ba1\u6587\u672c\u548c\u76ee\u6807\u8bed\u97f3\u4e4b\u95f4\u7684\u5bf9\u9f50 $A$, \u6211\u4eec\u91c7\u7528\u5355\u8c03\u5bf9\u9f50\u641c\u7d22 (Monotonic Alignment Search, MAS), \u8be5\u65b9\u6cd5\u641c\u7d22\u4e00\u4e2a\u5bf9\u9f50, \u4f7f\u5f97\u7531\u6807\u51c6\u5316\u6d41 $f$ \u53c2\u6570\u5316\u7684\u6570\u636e\u7684\u4f3c\u7136\u6700\u5927\u5316:</p> <p>$$ \\begin{aligned}   A &amp;= \\arg\\max_{\\hat{A}}\\log p_{\\theta}(x_{mel}|c_{text},\\hat{A}) \\   &amp;= \\arg\\max_{\\hat{A}}\\log\\mathcal{N}(f(x);\\mu(c_{text},\\hat{A}),\\sigma(c_{text},\\hat{A})) \\end{aligned}\\tag{5} $$</p> <p>where the candidate alignments are restricted to be monotonic and non-skipping following the fact that humans read text in order without skipping any words.</p> <p>\u5176\u4e2d\u5019\u9009\u5bf9\u9f50\u88ab\u9650\u5236\u4e3a\u5355\u8c03\u4e14\u4e0d\u8df3\u8fc7\u4efb\u4f55\u5355\u8bcd, \u56e0\u4e3a\u4eba\u7c7b\u9605\u8bfb\u6587\u672c\u662f\u6709\u5e8f\u7684\u4e14\u4e0d\u4f1a\u8df3\u8fc7\u4efb\u4f55\u5355\u8bcd.</p> <p>To find the optimal alignment, Glow-TTS (2020) use dynamic programming.  Applying MAS directly in our setting is difficult because our objective is the ELBO, not the exact log-likelihood.  We, therefore, redefine MAS to find an alignment that maximizes the ELBO, which reduces to finding an alignment that maximizes the log-likelihood of the latent variables $z$:</p> <p>\u4e3a\u4e86\u627e\u5230\u6700\u4f18\u5bf9\u9f50, Glow-TTS (2020) \u4f7f\u7528\u52a8\u6001\u89c4\u5212. \u5728\u6211\u4eec\u7684\u8bbe\u7f6e\u4e2d\u76f4\u63a5\u5e94\u7528 MAS \u56f0\u96be, \u56e0\u4e3a\u6211\u4eec\u7684\u76ee\u6807\u662f ELBO, \u800c\u4e0d\u662f\u7cbe\u786e\u7684\u5bf9\u6570\u4f3c\u7136. \u56e0\u6b64, \u6211\u4eec\u91cd\u65b0\u5b9a\u4e49 MAS, \u5bfb\u627e\u4e00\u4e2a\u5bf9\u9f50, \u4f7f\u5f97 ELBO \u6700\u5927\u5316, \u8fd9\u7b49\u4ef7\u4e8e\u5bfb\u627e\u4e00\u4e2a\u5bf9\u9f50, \u4f7f\u5f97\u9690\u53d8\u91cf $z$ \u7684\u5bf9\u6570\u4f3c\u7136\u6700\u5927\u5316:</p> <p>$$ \\begin{aligned}   &amp;\\arg\\max_{\\hat{A}}\\log p_{\\theta}(x_{mel}|z)-\\log\\frac{q_{\\phi}(z|x_{lin})}{p_{\\theta}(z|c_{text},\\hat{A})} \\   &amp;=\\arg\\max_{\\hat{A}}\\log p_{\\theta}(z|c_{text},\\hat{A}) \\   &amp;=\\arg\\max_{\\hat{A}}\\log \\mathcal{N}(f_\\theta(z);\\mu_\\theta(c_{text},\\hat{A}),\\sigma_\\theta(c_{text},\\hat{A}))  \\end{aligned} $$</p> <p>Due to the resemblance of Eq.05 to Eq.06, we can use the original MAS implementation without modification. Appendix A includes pseudocode for MAS. Although we search the alignment which maximizes the ELBO not the exact log-likelihood of data, we can use the MAS implementation of Glow-TTS as described in Section 2.2.1.</p> <p>\u7531\u4e8e\u65b9\u7a0b 05 \u548c\u65b9\u7a0b 06 \u7684\u76f8\u4f3c\u6027, \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u539f\u59cb\u7684 MAS \u5b9e\u73b0\u800c\u65e0\u9700\u4fee\u6539. \u9644\u5f55 A \u5305\u542b MAS \u7684\u4f2a\u4ee3\u7801. \u5c3d\u7ba1\u6211\u4eec\u641c\u7d22 ELBO \u6700\u5927\u7684\u5bf9\u9f50, \u800c\u4e0d\u662f\u6570\u636e\u7cbe\u786e\u7684\u5bf9\u6570\u4f3c\u7136, \u4f46\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 Glow-TTS \u7684 MAS \u5b9e\u73b0.</p> <pre><code>def monotonic_alignment_search(value):\n    \"\"\"Returns the most likely alignment for the given log-likelihood matrix.\n    Args:\n        value: the log-likelihood matrix. Its (i, j)-th entry contains\n        the log-likelihood of the j-th latent variable\n        for the given i-th prior mean and variance:\n        .. math::\n            value_{i,j} = log N(f(z)_{j}; \\mu_{i}, \\sigma_{i})\n        (dtype=float, shape=[text_length, latent_variable_length])\n    Returns:\n        path: the most likely alignment.\n        (dtype=float, shape=[text_length, latent_variable_length])\n    \"\"\"\n    t_x, t_y = value.shape # [text_length, letent_variable_length]\n    path = zeros([t_x, t_y])\n    # A cache to store the log-likelihood for the most likely alignment so far.\n    Q = -INFINITY * ones([t_x, t_y])\n    for y in range(t_y):\n        for x in range(max(0, t_x + y - t_y), min(t_x, y + 1)):\n            if y == 0: # Base case. If y is 0, the possible x value is only 0.\n                Q[x, 0] = value[x, 0]\n            else:\n                if x == 0:\n                    v_prev = -INFINITY\n                else:\n                    v_prev = Q[x-1, y-1]\n                v_cur = Q[x, y-1]\n                Q[x, y] = value[x, y] + max(v_prev, v_cur)\n    # Backtrack from last observation.\n    index = t_x - 1\n    for y in range(t_y - 1, -1, -1):\n        path[index, y] = 1\n        if index != 0 and (index == y or Q[index, y-1] &lt; Q[index-1, y-1]):\n            index = index - 1\nreturn path\n</code></pre> <p>\u6e90\u4ee3\u7801\u91c7\u7528 Cython \u7f16\u5199: - \u5b9a\u4e49\u51fd\u6570 <code>maximum_path_each()</code>, \u63a5\u53d7\u4e00\u4e2a\u4e8c\u7ef4\u6570\u7ec4 <code>path</code>, \u4e8c\u7ef4\u6570\u7ec4 <code>value</code>, \u4e24\u4e2a\u6574\u6570 <code>t_x</code>, <code>t_y</code>, \u4ee5\u53ca\u4e00\u4e2a\u6d6e\u70b9\u6570 <code>max_neg_val</code>. \u7528\u4e8e\u8ba1\u7b97\u6bcf\u4e2a\u8def\u5f84\u7684\u6700\u5927\u503c. - \u5b9a\u4e49\u51fd\u6570 <code>maximum_path_c()</code>, \u63a5\u53d7\u4e09\u4e2a\u4e09\u7ef4\u6570\u7ec4 <code>paths</code>, <code>values</code> \u548c\u4e24\u4e2a\u4e00\u7ef4\u6570\u7ec4 <code>t_xs</code> \u548c <code>t_ys</code>, \u4ee5\u53ca\u4e00\u4e2a\u6d6e\u70b9\u6570 <code>max_neg_val</code>. \u7528\u4e8e\u8ba1\u7b97\u6240\u6709\u8def\u5f84\u7684\u6700\u5927\u503c.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#322duration-prediction-from-text","title":"3.2.2.Duration Prediction from Text\u00b7\u6587\u672c\u4e2d\u7684\u65f6\u957f\u9884\u6d4b","text":"<p>We can calculate the duration of each input token $d_i$ by summing all the columns in each row of the estimated alignment $\\sum_j A_{i,j}$.  The duration could be used to train a deterministic duration predictor, as proposed in previous work (HiFi-GAN (2020)), but it cannot express the way a person utters at different speaking rates each time.  To generate human-like rhythms of speech, we design a stochastic duration predictor so that its samples follow the duration distribution of given phonemes.  The stochastic duration predictor is a flow-based generative model that is typically trained via maximum likelihood estimation. The direct application of maximum likelihood estimation, however, is difficult because the duration of each input phoneme is  1. a discrete integer, which needs to be dequantized for using continuous normalizing flows, 2. a scalar, which prevents high-dimensional transformation due to invertibility. </p> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5bf9\u4f30\u8ba1\u7684\u5bf9\u9f50\u7684\u6bcf\u4e00\u884c\u4e2d\u6240\u6709\u5217\u6c42\u548c\u4ee5\u8ba1\u7b97\u6bcf\u4e2a\u8f93\u5165 Token $d_i$ \u7684\u65f6\u957f, \u5373 $\\sum_j A_{i,j}$. \u65f6\u957f\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3\u4e00\u4e2a\u786e\u5b9a\u6027\u7684\u65f6\u957f\u9884\u6d4b\u5668, \u5982 HiFi-GAN (2020) \u6240\u63d0\u51fa\u7684, \u4f46\u5b83\u4e0d\u80fd\u8868\u8fbe\u4eba\u7c7b\u5728\u4e0d\u540c\u8bed\u901f\u4e0b\u8bf4\u8bdd\u7684\u65b9\u5f0f. \u4e3a\u4e86\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u8bed\u8c03\u7684\u8bed\u97f3, \u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u4f7f\u5f97\u5176\u91c7\u6837\u7b26\u5408\u7ed9\u5b9a\u97f3\u7d20\u7684\u65f6\u957f\u5206\u5e03. \u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b, \u901a\u5e38\u901a\u8fc7\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u8fdb\u884c\u8bad\u7ec3. \u7136\u800c, \u76f4\u63a5\u5e94\u7528\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u662f\u56f0\u96be\u7684, \u56e0\u4e3a\u6bcf\u4e2a\u8f93\u5165\u97f3\u7d20\u7684\u65f6\u957f\u662f: 1. \u4e00\u4e2a\u79bb\u6563\u6574\u6570, \u8fd9\u9700\u8981\u5bf9\u5176\u53bb\u91cf\u5316\u4ee5\u4fbf\u4f7f\u7528\u8fde\u7eed\u6807\u51c6\u5316\u6d41, 2. \u4e00\u4e2a\u6807\u91cf, \u7531\u4e8e\u53ef\u9006\u6027\u800c\u963b\u788d\u4e86\u9ad8\u7ef4\u53d8\u6362.</p> <p>We apply variational dequantization (Flow++ (2019)) and variational data augmentation (VFlow (2020)) to solve these problems.  To be specific, we introduce two random variables $u$ and $\u03bd$, which have the same time resolution and dimension as that of the duration sequenced, for variational dequantization and variational data augmentation, respectively.  We restrict the support of $u$ to be $[0, 1)$ so that the difference $d\u2212u$ becomes a sequence of positive real numbers, and we concatenate $\u03bd$ and $d$ channel-wise to make a higher dimensional latent representation.  We sample the two variables through an approximate posterior distribution $q_{\\phi}(u, \u03bd|d, c_{text})$.  The resulting objective is a variational lower bound of the log-likelihood of the phoneme duration:</p> <p>\u6211\u4eec\u5e94\u7528\u53d8\u5206\u53bb\u91cf\u5316\u548c\u53d8\u5206\u6570\u636e\u589e\u5f3a\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898. \u5177\u4f53\u5730, \u6211\u4eec\u5f15\u5165\u4e24\u4e2a\u968f\u673a\u53d8\u91cf $u$ \u548c $v$, \u62e5\u6709\u548c\u65f6\u957f\u5e8f\u5217\u76f8\u540c\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7ef4\u5ea6, \u5206\u522b\u7528\u4e8e\u53d8\u5206\u53bb\u91cf\u5316\u548c\u53d8\u5206\u6570\u636e\u589e\u5f3a. \u6211\u4eec\u5c06 $u$ \u7684\u652f\u6301\u9650\u5236\u4e3a $[0, 1)$, \u8fd9\u6837 $d-u$ \u53d8\u6210\u4e00\u7cfb\u5217\u6b63\u5b9e\u6570, \u5e76\u5c06 $\u03bd$ \u548c $d$ \u9010\u901a\u9053\u62fc\u63a5\u4ee5\u751f\u6210\u66f4\u9ad8\u7ef4\u7684\u6f5c\u5728\u8868\u793a. \u6211\u4eec\u901a\u8fc7\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03 $q_{\\phi}(u, v|d, c_{text})$ \u91c7\u6837\u8fd9\u4e24\u4e2a\u53d8\u91cf. \u6700\u540e\u5f97\u5230\u7684\u76ee\u6807\u662f\u97f3\u7d20\u65f6\u957f\u7684\u5bf9\u6570\u4f3c\u7136\u7684\u53d8\u5206\u4e0b\u754c:</p> <p>$$   \\log p_{\\theta}(d|c_{text}) \\geq \\mathbb{E}{q{\\phi}(u, \u03bd|d, c_{text})} \\left[\\log\\dfrac{p_{\\theta}(d-u,v|c_{text})}{q_{\\phi}(u,v|d,c_{text})}\\right]\\tag{7} $$</p> <p>The training loss $L_{dur}$ is then the negative variational lower bound.  We apply the stop gradient operator (Neural discrete representation learning), which prevents back-propagating the gradient of inputs, to the input conditions so that the training of the duration predictor does not affect that of other modules.</p> <p>\u8bad\u7ec3\u635f\u5931 $L_{dur}$ \u5219\u662f\u8d1f\u53d8\u5206\u4e0b\u754c. \u6211\u4eec\u5e94\u7528\u5bf9\u8f93\u5165\u6761\u4ef6\u5e94\u7528\u505c\u6b62\u68af\u5ea6\u7b97\u5b50 (\u9632\u6b62\u68af\u5ea6\u53cd\u5411\u4f20\u64ad) \u4ee5\u4fbf\u8bad\u7ec3\u65f6\u957f\u9884\u6d4b\u5668\u4e0d\u5f71\u54cd\u5176\u4ed6\u6a21\u5757.</p> <p>The sampling procedure is relatively simple; the phoneme duration is sampled from random noise through the inverse transformation of the stochastic duration predictor, and then it is converted to integers.</p> <p>\u91c7\u6837\u8fc7\u7a0b\u76f8\u5bf9\u7b80\u5355; \u97f3\u7d20\u65f6\u957f\u901a\u8fc7\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7684\u9006\u53d8\u6362\u4ece\u968f\u673a\u566a\u58f0\u4e2d\u91c7\u6837, \u7136\u540e\u5c06\u5176\u8f6c\u6362\u4e3a\u6574\u6570.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#33adversarial-training","title":"3.3.Adversarial Training\u00b7\u5bf9\u6297\u8bad\u7ec3","text":"<p>To adopt adversarial training in our learning system, we add a discriminator $D$ that distinguishes between the output generated by the decoder $G$ and the ground truth waveform $y$. In this work, we use two types of loss successfully applied in speech synthesis; the least-squares loss function (LSGAN (2016)) for adversarial training, and the additional feature-matching loss (Autoencoding beyond pixels using a learned similarity metric) for training the generator:</p> <p>\u4e3a\u4e86\u5728\u6211\u4eec\u7684\u5b66\u4e60\u7cfb\u7edf\u4e2d\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3, \u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e2a\u5224\u522b\u5668 $D$ \u6765\u533a\u5206\u7531\u89e3\u7801\u5668 $G$ \u751f\u6210\u7684\u8f93\u51fa\u548c\u771f\u5b9e\u6ce2\u5f62 $y$. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u6210\u529f\u5730\u5e94\u7528\u4e86\u4e24\u79cd\u635f\u5931, \u5373\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u6700\u5c0f\u4e8c\u4e58\u635f\u5931\u548c\u989d\u5916\u7684\u7279\u5f81\u5339\u914d\u635f\u5931\u7528\u4e8e\u8bad\u7ec3\u751f\u6210\u5668:</p> <p>$$ \\begin{align}   L_{adv}(D) &amp;= \\mathbb{E}{(y,z)} [(D(y)-1)^2 + (D(G(z)))^2]\\tag{8}\\   L{adv}(G) &amp;= \\mathbb{E}{z} [(D(G(z)) - 1)^2]\\tag{9}\\   L{fm}(G) &amp;= \\mathbb{E}{(y,z)} \\left[\\sum{l=1}^T \\dfrac{1}{N_l}| D^l(y)-D^l(G(z))|_1\\right]\\tag{10} \\end{align} $$</p> <p>where $T$ denotes the total number of layers in the discriminator and $D^l$ outputs the feature map of the $l$-th layer of the discriminator with $N_l$ number of features.</p> <p>\u5176\u4e2d $T$ \u8868\u793a\u5224\u522b\u5668\u4e2d\u7684\u5c42\u6570, $D^l$ \u8f93\u51fa\u7b2c $l$ \u5c42\u5224\u522b\u5668\u7684\u7279\u5f81\u56fe, \u5176\u6709 $N_l$ \u4e2a\u7279\u5f81.</p> <p>Notably, the feature matching loss can be seen as reconstruction loss that is measured in the hidden layers of the discriminator suggested as an alternative to the element-wise reconstruction loss of VAEs (Autoencoding beyond pixels using a learned similarity metric).</p> <p>\u7279\u5f81\u5339\u914d\u635f\u5931\u53ef\u4ee5\u89c6\u4e3a\u91cd\u5efa\u635f\u5931, \u5b83\u5728\u5224\u522b\u5668\u7684\u9690\u85cf\u5c42\u4e0a\u8fdb\u884c\u5ea6\u91cf, \u88ab\u5efa\u8bae\u4f5c\u4e3a VAE \u7684\u9010\u5143\u7d20\u91cd\u5efa\u635f\u5931\u7684\u66ff\u4ee3.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#34final-loss","title":"3.4.Final Loss\u00b7\u6700\u7ec8\u635f\u5931","text":"<p>With the combination of VAE and GAN training, the total loss for training our conditional VAE can be expressed as follows:</p> <p>\u901a\u8fc7\u5bf9 VAE \u548c GAN \u8bad\u7ec3\u7684\u7ec4\u5408, \u6211\u4eec\u7684\u6761\u4ef6 VAE \u7684\u603b\u635f\u5931\u53ef\u4ee5\u8868\u793a\u5982\u4e0b:</p> <p>$$   L_{vae} = L_{recon} + L_{kl} + L_{dur} + L_{adv}(G) + L_{fm}(G) \\tag{11} $$</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#35model-architecture","title":"3.5.Model Architecture\u00b7\u6a21\u578b\u67b6\u6784","text":"<p>The overall architecture of the proposed model consists of a posterior encoder, prior encoder, decoder, discriminator, and stochastic duration predictor.  The posterior encoder and discriminator are only used for training, not for inference. Architectural details are available in Appendix B.</p> <p>\u6240\u63d0\u65b9\u6cd5\u7684\u6574\u4f53\u67b6\u6784\u7531\u540e\u9a8c\u7f16\u7801\u5668, \u5148\u9a8c\u7f16\u7801\u5668, \u89e3\u7801\u5668, \u5224\u522b\u5668\u548c\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7ec4\u6210. \u540e\u9a8c\u7f16\u7801\u5668\u548c\u5224\u522b\u5668\u4ec5\u7528\u4e8e\u8bad\u7ec3, \u800c\u4e0d\u7528\u4e8e\u63a8\u65ad.</p> <p>In this section, we mainly describe the newly added parts of VITS as we followed configurations of Glow-TTS and HiFi-GAN for several parts of our model:  we use the same transformer encoder and WaveNet residual blocks as those of Glow-TTS;  our decoder and the multi-period discriminator is the same as the generator and multi-period discriminator of HiFi-GAN, respectively, except that we use different input dimension for the decoder and append a sub-discriminator.</p> <p>\u672c\u8282\u4e3b\u8981\u63cf\u8ff0\u4e86\u6240\u63d0\u65b9\u6cd5\u4e2d\u65b0\u589e\u7684 VITS \u7684\u90e8\u5206, \u6211\u4eec\u9075\u5faa Glow-TTS \u548c HiFi-GAN \u7684\u914d\u7f6e, \u5e76\u5bf9\u6a21\u578b\u7684\u4e0d\u540c\u90e8\u5206\u8fdb\u884c\u4e86\u63cf\u8ff0: - \u6211\u4eec\u4f7f\u7528\u4e0e Glow-TTS \u76f8\u540c\u7684 Transformer \u7f16\u7801\u5668\u548c WaveNet \u6b8b\u5dee\u5757; - \u6211\u4eec\u4f7f\u7528\u4e0e HiFi-GAN \u76f8\u540c\u7684\u751f\u6210\u5668\u548c\u591a\u5468\u671f\u5224\u522b\u5668, \u9664\u4e86\u5bf9\u89e3\u7801\u5668\u4f7f\u7528\u4e0d\u540c\u7684\u8f93\u5165\u7ef4\u5ea6\u5e76\u6dfb\u52a0\u4e86\u4e00\u4e2a\u5b50\u5224\u522b\u5668.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#351posterior-encoder","title":"3.5.1.Posterior Encoder\u00b7\u540e\u9a8c\u7f16\u7801\u5668","text":"<p>For the posterior encoder, we use the non-causal WaveNet residual blocks used in WaveGlow (2018) and Glow-TTS (2020).  A WaveNet residual block consists of layers of dilated convolutions with a gated activation unit and skip connection.  The linear projection layer above the blocks produces the mean and variance of the normal posterior distribution.  For the multi-speaker case, we use global conditioning (Oord et al., 2016) in residual blocks to add speaker embedding.</p> <p>\u5bf9\u4e8e\u540e\u9a8c\u7f16\u7801\u5668, \u6211\u4eec\u91c7\u7528 WaveGlow \u548c Glow-TTS \u4e2d\u4f7f\u7528\u7684\u975e\u56e0\u679c\u7684 WaveNet \u6b8b\u5dee\u5757. WaveNet \u6b8b\u5dee\u5757\u7531\u5e26\u6709\u95e8\u63a7\u6fc0\u6d3b\u5355\u5143\u7684\u7a7a\u6d1e\u5377\u79ef\u5c42\u548c\u8df3\u8dc3\u8fde\u63a5\u7ec4\u6210. \u5757\u4e0a\u7684\u7ebf\u6027\u6620\u5c04\u5c42\u4ea7\u751f\u6b63\u6001\u540e\u9a8c\u5206\u5e03\u7684\u5747\u503c\u548c\u65b9\u5dee. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u60c5\u5f62, \u6211\u4eec\u5728\u6b8b\u5dee\u5757\u4e2d\u4f7f\u7528\u5168\u5c40\u6761\u4ef6\u5316\u4ee5\u6dfb\u52a0\u8bf4\u8bdd\u4eba\u5d4c\u5165.</p> <p>The posterior encoder, consisting of 16 WaveNet residual blocks, takes linear-scale log magnitude spectrograms and produce latent variables with 192 channels.</p> <p>\u540e\u9a8c\u7f16\u7801\u5668\u7531 16 \u4e2a WaveNet \u6b8b\u5dee\u5757\u7ec4\u6210, \u5b83\u63a5\u53d7\u7ebf\u6027\u7684\u5bf9\u6570\u5e45\u5ea6\u8c31\u56fe\u5e76\u751f\u6210\u5177\u6709 192 \u901a\u9053\u7684\u9690\u53d8\u91cf.</p> <pre><code>class PosteriorEncoder(nn.Module):\n    def __init__(self,\n        in_channels,\n        out_channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        gin_channels=0):\n        super().__init__()\n        self.in_channels     = in_channels\n        self.out_channels    = out_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size     = kernel_size\n        self.dilation_rate   = dilation_rate\n        self.n_layers        = n_layers\n        self.gin_channels    = gin_channels\n\n        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n        self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\n        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, x, x_lengths, g=None):\n        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n        x = self.pre(x) * x_mask\n        x = self.enc(x, x_mask, g=g)\n        stats = self.proj(x) * x_mask\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n        return z, m, logs, x_mask\n</code></pre> <p>\u9996\u5148\u5bf9\u8f93\u5165 x \u751f\u6210\u76f8\u5e94\u7684 x_mask \u4ee5\u53bb\u6389\u65e0\u6548\u90e8\u5206. \u4f7f\u7528\u4e00\u7ef4\u5377\u79ef\u5bf9\u8f93\u5165 x \u7684\u901a\u9053\u8fdb\u884c\u4fee\u6539. \u7136\u540e\u5c06\u4fee\u6539\u540e\u7684 x \u548c\u8bf4\u8bdd\u4eba\u5d4c\u5165\u4e00\u540c\u8f93\u5165\u5230 WaveNet \u4e2d. \u5bf9\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u7ebf\u6027\u6620\u5c04\u8c03\u6574\u901a\u9053\u6570\u4e3a\u9690\u85cf\u7ef4\u5ea6\u7684\u4e24\u500d, \u5207\u5206\u540e\u5f97\u5230\u5747\u503c\u548c\u5bf9\u6570\u6807\u51c6\u5dee. \u7136\u540e\u91c7\u6837\u9690\u53d8\u91cf z \u5e76\u8fd4\u56de.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#352prior-encoder","title":"3.5.2.Prior Encoder\u00b7\u5148\u9a8c\u7f16\u7801\u5668","text":"<p>The prior encoder consists of a text encoder that processes the input phonemes $c_{text}$ and a normalizing flow $f_{\\theta}$ that improves the flexibility of the prior distribution. The text encoder is a transformer encoder  that uses relative positional representation instead of absolute positional encoding. We can obtain the hidden representation $h_{text}$ from $c_{text}$ through the text encoder and a linear projection layer above the text encoder that produces the mean and variance used for constructing the prior distribution.</p> <p>\u5148\u9a8c\u7f16\u7801\u5668\u7531\u4e00\u4e2a\u7528\u4e8e\u5904\u7406\u8f93\u5165\u97f3\u7d20\u7684\u6587\u672c\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u5148\u9a8c\u5206\u5e03\u7075\u6d3b\u6027\u7684\u6807\u51c6\u5316\u6d41\u7ec4\u6210. \u6587\u672c\u7f16\u7801\u5668\u662f\u4e00\u4e2a Transformer \u7f16\u7801\u5668, \u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u8868\u793a\u800c\u4e0d\u662f\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801. \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u7f16\u7801\u5668\u548c\u5176\u4e0a\u7684\u7ebf\u6027\u6620\u5c04\u5c42\u6765\u83b7\u5f97\u9690\u85cf\u8868\u793a $h_{text}$ \u5e76\u7528\u4e8e\u6784\u9020\u5148\u9a8c\u5206\u5e03\u7684\u5747\u503c\u548c\u65b9\u5dee.</p> <pre><code>class TextEncoder(nn.Module):\n    def __init__(self,\n        n_vocab,\n        out_channels,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout):\n        super().__init__()\n        self.n_vocab = n_vocab\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n\n        self.emb = nn.Embedding(n_vocab, hidden_channels)\n        nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n\n        self.encoder = attentions.Encoder(\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size,\n        p_dropout)\n        self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\n    def forward(self, x, x_lengths):\n        x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\n        x = torch.transpose(x, 1, -1) # [b, h, t]\n        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n\n        x = self.encoder(x * x_mask, x_mask)\n        stats = self.proj(x) * x_mask\n\n        m, logs = torch.split(stats, self.out_channels, dim=1)\n        return x, m, logs, x_mask\n</code></pre> <p>\u6587\u672c\u7f16\u7801\u5668\u5bf9\u8f93\u5165 x \u8fdb\u884c\u5d4c\u5165\u67e5\u627e\u540e, \u4e58\u4ee5\u6839\u53f7\u4e0b\u9690\u85cf\u901a\u9053\u6570, \u7136\u540e\u8f6c\u7f6e. \u6784\u9020 mask \u5bf9\u8f93\u5165 x \u8fdb\u884c\u65e0\u6548\u4f4d\u7f6e\u63a9\u819c. \u8f93\u5165\u5230 Transformer \u7f16\u7801\u5668\u4e2d, \u901a\u8fc7\u4e00\u7ef4\u5377\u79ef\u5c42\u8c03\u6574\u901a\u9053\u6570\u4e3a\u9690\u85cf\u7ef4\u5ea6\u7684\u4e24\u500d, \u5207\u5206\u540e\u5f97\u5230\u5747\u503c\u548c\u5bf9\u6570\u6807\u51c6\u5dee.</p> <p>The normalizing flow is a stack of affine coupling layers (Density estimation using Real NVP) consisting of a stack of WaveNet residual blocks. For simplicity, we design the normalizing flow to be a volume-preserving transformation with the Jacobian determinant of one. For the multi-speaker setting, we add speaker embedding to the residual blocks in the normalizing flow through global conditioning.</p> <p>\u6807\u51c6\u5316\u6d41\u662f\u7531\u4eff\u5c04\u8026\u5408\u5c42\u5806\u53e0\u800c\u6210, \u6bcf\u4e2a\u4eff\u5c04\u8026\u5408\u5c42\u7531 WaveNet \u6b8b\u5dee\u5757\u5806\u53e0\u800c\u6210. \u4e3a\u4e86\u7b80\u5316, \u6211\u4eec\u8bbe\u8ba1\u6807\u51c6\u5316\u6d41\u4e3a\u5177\u6709\u5355\u4f4d\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u7684\u4f53\u79ef\u4fdd\u6301\u53d8\u6362. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u8bbe\u7f6e, \u6211\u4eec\u901a\u8fc7\u5168\u5c40\u6761\u4ef6\u5316\u5728\u6807\u51c6\u5316\u6d41\u7684\u6b8b\u5dee\u5757\u4e2d\u6dfb\u52a0\u8bf4\u8bdd\u4eba\u5d4c\u5165.</p> <p>The normalizing flow in the prior encoder is a stack of four affine coupling layers, each coupling layer consisting of four WaveNet residual blocks.  As we restrict the affine coupling layers to be volume-preserving transformations, the coupling layers do not produce scale parameters.</p> <p>\u5177\u4f53\u5730, \u5148\u9a8c\u7f16\u7801\u5668\u4e2d\u7684\u6807\u51c6\u5316\u6d41\u7531\u56db\u4e2a\u4eff\u5c04\u8026\u5408\u5c42\u7ec4\u6210, \u6bcf\u4e2a\u4eff\u5c04\u8026\u5408\u5c42\u7531\u56db\u4e2a WaveNet \u6b8b\u5dee\u5757\u7ec4\u6210. \u7531\u4e8e\u6211\u4eec\u9650\u5236\u4eff\u5c04\u8026\u5408\u5c42\u4e3a\u4f53\u79ef\u4fdd\u6301\u53d8\u6362, \u56e0\u6b64\u8026\u5408\u5c42\u4e0d\u4ea7\u751f\u5c3a\u5ea6\u53c2\u6570.</p> <pre><code>class ResidualCouplingBlock(nn.Module):\n    def __init__(self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        n_flows=4,\n        gin_channels=0):\n        super().__init__()\n        self.channels = channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.n_flows = n_flows\n        self.gin_channels = gin_channels\n\n        self.flows = nn.ModuleList()\n        for i in range(n_flows):\n            self.flows.append(modules.ResidualCouplingLayer(channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels, mean_only=True))\n            self.flows.append(modules.Flip())\n\n    def forward(self, x, x_mask, g=None, reverse=False):\n        if not reverse:\n            for flow in self.flows:\n                x, _ = flow(x, x_mask, g=g, reverse=reverse)\n        else:\n            for flow in reversed(self.flows):\n                x = flow(x, x_mask, g=g, reverse=reverse)\n        return x\n</code></pre>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#353decoder","title":"3.5.3.Decoder\u00b7\u89e3\u7801\u5668","text":"<p>The decoder is essentially the HiFi-GAN (2020) V1 generator. It is composed of a stack of transposed convolutions, each of which is followed by a multi-receptive field fusion module (MRF). The output of the MRF is the sum of the output of residual blocks that have different receptive field sizes. For the multi-speaker setting, we add a linear layer that transforms speaker embedding and add it to the input latent variables $z$.</p> <p>\u89e3\u7801\u5668\u4e3a HiFi-GAN V1 \u751f\u6210\u5668\u7684\u57fa\u672c\u7ed3\u6784. \u5b83\u7531\u8f6c\u7f6e\u5377\u79ef\u5c42\u548c\u591a\u611f\u53d7\u91ce\u878d\u5408\u6a21\u5757 (MRF) \u5806\u53e0\u800c\u6210. \u6bcf\u4e2a MRF \u7684\u8f93\u51fa\u662f\u4e0d\u540c\u611f\u53d7\u91ce\u5927\u5c0f\u7684\u6b8b\u5dee\u5757\u8f93\u51fa\u7684\u603b\u548c. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u8bbe\u7f6e, \u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e2a\u7ebf\u6027\u5c42, \u7528\u4e8e\u8f6c\u6362\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u5165\u9690\u53d8\u91cf $z$ \u4e2d.</p> <p>The input of our decoder is latent variables generated from the prior or posterior encoders, so the input channel size of the decoder is 192.  For the last convolutional layer of the decoder, we remove a bias parameter, as it causes unstable gradient scales during mixed precision training.</p> <p>\u89e3\u7801\u5668\u7684\u8f93\u5165\u662f\u4ece\u5148\u9a8c\u6216\u540e\u9a8c\u7f16\u7801\u5668\u751f\u6210\u7684\u56e0\u53d8\u91cf\u6240\u4ee5\u7f16\u7801\u5668\u7684\u8f93\u5165\u901a\u9053\u6570\u4e3a 192. \u5bf9\u4e8e\u89e3\u7801\u5668\u7684\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42, \u6211\u4eec\u79fb\u9664\u504f\u7f6e\u53c2\u6570, \u56e0\u4e3a\u5b83\u4f1a\u5bfc\u81f4\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u65f6\u7684\u4e0d\u7a33\u5b9a\u68af\u5ea6\u5c3a\u5ea6.</p> <pre><code>class Generator(torch.nn.Module):\n    def __init__(self, initial_channel, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):\n        super(Generator, self).__init__()\n        self.num_kernels = len(resblock_kernel_sizes)\n        self.num_upsamples = len(upsample_rates)\n        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\n        resblock = modules.ResBlock1 if resblock == '1' else modules.ResBlock2\n\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n            self.ups.append(weight_norm(\n                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n                                k, u, padding=(k-u)//2)))\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.ups)):\n            ch = upsample_initial_channel//(2**(i+1))\n            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n                self.resblocks.append(resblock(ch, k, d))\n\n        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n        self.ups.apply(init_weights)\n\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\n    def forward(self, x, g=None):\n        x = self.conv_pre(x)\n        if g is not None:\n          x = x + self.cond(g)\n\n        for i in range(self.num_upsamples):\n            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n            x = self.ups[i](x)\n            xs = None\n            for j in range(self.num_kernels):\n                if xs is None:\n                    xs = self.resblocks[i*self.num_kernels+j](x)\n                else:\n                    xs += self.resblocks[i*self.num_kernels+j](x)\n            x = xs / self.num_kernels\n        x = F.leaky_relu(x)\n        x = self.conv_post(x)\n        x = torch.tanh(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        print('Removing weight norm...')\n        for l in self.ups:\n            remove_weight_norm(l)\n        for l in self.resblocks:\n            l.remove_weight_norm()\n</code></pre>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#354discriminator","title":"3.5.4.Discriminator\u00b7\u5224\u522b\u5668","text":"<p>We follow the discriminator architecture of the multi-period discriminator proposed in HiFi-GAN (2020). The multi-period discriminator is a mixture of Markovian window-based sub-discriminators (MelGAN (2019)), each of which operates on different periodic patterns of input waveforms.</p> <p>\u6211\u4eec\u9075\u5faa HiFi-GAN \u63d0\u51fa\u7684\u591a\u5468\u671f\u5224\u522b\u5668\u7684\u67b6\u6784. \u591a\u5468\u671f\u5224\u522b\u5668\u662f\u4e00\u4e2a\u7531\u9a6c\u5c14\u53ef\u592b\u7a97\u53e3\u5b50\u5224\u522b\u5668\u7ec4\u6210\u7684\u6df7\u5408\u6a21\u578b, \u5b83\u4eec\u5206\u522b\u64cd\u4f5c\u4e8e\u8f93\u5165\u6ce2\u5f62\u7684\u4e0d\u540c\u5468\u671f\u6a21\u5f0f.</p> <p>For the discriminator, HiFi-GAN uses the multi-period discriminator containing five sub-discriminators with periods [2, 3, 5, 7, 11] and the multi-scale discriminator containing three sub-discriminators.  To improve training efficiency, we leave only the first sub-discriminator of the multi-scale discriminator that operates on raw waveforms and discard two sub-discriminators operating on average-pooled waveforms.  The resultant discriminator can be seen as the multi-period discriminator with periods [1, 2, 3, 5, 7, 11].</p> <p>\u5bf9\u4e8e\u5224\u522b\u5668, HiFi-GAN \u4f7f\u7528\u5305\u542b\u4e94\u4e2a\u5b50\u5224\u522b\u5668\u7684\u591a\u5468\u671f\u5224\u522b\u5668\u548c\u5305\u542b\u4e09\u4e2a\u5b50\u5224\u522b\u5668\u7684\u591a\u5c3a\u5ea6\u5224\u522b\u5668. \u4e3a\u4e86\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387, \u6211\u4eec\u4ec5\u4fdd\u7559\u591a\u5c3a\u5ea6\u5224\u522b\u5668\u7684\u7b2c\u4e00\u4e2a\u5b50\u5224\u522b\u5668, \u5e76\u4e22\u5f03\u4e24\u4e2a\u64cd\u4f5c\u4e8e\u5e73\u5747\u6c60\u5316\u6ce2\u5f62\u7684\u5b50\u5224\u522b\u5668. \u6700\u7ec8\u7684\u5224\u522b\u5668\u53ef\u4ee5\u770b\u4f5c\u662f\u5177\u6709\u5468\u671f [1, 2, 3, 5, 7, 11] \u7684\u591a\u5468\u671f\u5224\u522b\u5668.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#355stochastic-duration-predictor","title":"3.5.5.Stochastic Duration Predictor\u00b7\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668","text":"<p>The stochastic duration predictor estimates the distribution of phoneme duration from a conditional input $h_{text}$. For the efficient parameterization of the stochastic duration predictor, we stack residual blocks with dilated and depth-separable convolutional layers. We also apply Neural Spline Flows, which take the form of invertible nonlinear transformations by using monotonic rational-quadratic splines, to coupling layers. Neural spline flows improve transformation expressiveness with a similar number of parameters compared to commonly used affine coupling layers. For the multi-speaker setting, we add a linear layer that transforms speaker embedding and add it to the input $h_{text}$.</p> <p>\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u4ece\u6761\u4ef6\u8f93\u5165 $h_{text}$ \u4f30\u8ba1\u97f3\u7d20\u65f6\u957f\u7684\u5206\u5e03. \u4e3a\u4e86\u6709\u6548\u5730\u53c2\u6570\u5316\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u6211\u4eec\u5806\u53e0\u5177\u6709\u81a8\u80c0\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5c42\u7684\u6b8b\u5dee\u5757. \u6211\u4eec\u8fd8\u5e94\u7528\u4e86\u795e\u7ecf\u6837\u6761\u6d41, \u5b83\u662f\u4e00\u79cd\u4f7f\u7528\u5355\u8c03\u5206\u6bb5\u4e8c\u6b21\u6837\u6761\u7684\u975e\u7ebf\u6027\u53d8\u6362, \u7528\u4e8e\u8026\u5408\u5c42. \u795e\u7ecf\u6837\u6761\u6d41\u7528\u548c\u5e38\u7528\u7684\u4eff\u5c04\u8026\u5408\u5c42\u53c2\u6570\u6570\u91cf\u76f8\u5f53\u7684\u53c2\u6570\u6765\u63d0\u9ad8\u53d8\u6362\u7684\u8868\u8fbe\u80fd\u529b. \u5bf9\u4e8e\u591a\u8bf4\u8bdd\u4eba\u8bbe\u7f6e, \u6211\u4eec\u6dfb\u52a0\u4e86\u4e00\u4e2a\u7ebf\u6027\u5c42, \u7528\u4e8e\u8f6c\u6362\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u8f93\u5165 $h_{text}$.</p> <p>Fig.05a and Fig.05b show the training and inference procedures of the stochastic duration predictor, respectively.  The main building block of the stochastic duration predictor is the dilated and depth-wise separable convolutional (DDSConv) residual block as in Fig.05c.  Each convolutional layer in DDSConv blocks is followed by a layer normalization layer and GELU activation function.  We choose to use dilated and depth-wise separable convolutional layers for improving parameter efficiency while maintaining large receptive field size.</p> <p></p> <p>\u56fe 05a \u548c\u56fe 05b \u5206\u522b\u5c55\u793a\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b. \u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u7684\u4e3b\u8981\u6784\u5efa\u5757\u662f\u5177\u6709\u81a8\u80c0\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7684\u6b8b\u5dee\u5757 (DDSConv), \u5982\u56fe 05c \u6240\u793a. DDSConv \u4e2d\u7684\u6bcf\u4e2a\u5377\u79ef\u5c42\u540e\u8ddf\u968f\u4e00\u4e2a\u5c42\u5f52\u4e00\u5316\u5c42\u548c GELU \u6fc0\u6d3b\u51fd\u6570. \u6211\u4eec\u9009\u62e9\u4f7f\u7528\u81a8\u80c0\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5c42\u6765\u63d0\u9ad8\u53c2\u6570\u6548\u7387, \u540c\u65f6\u4fdd\u6301\u5927\u7684\u611f\u53d7\u91ce\u5c3a\u5bf8.</p> <p>The posterior encoder and normalizing flow module in the duration predictor are flow-based neural networks and have the similar architecture.  The difference is that the posterior encoder transforms a Gaussian noise sequence into two random variables $\u03bd$ and $u$ to express the approximate posterior distribution $q_{\\phi}(u, \u03bd|d, c_{text})$, and the normalizing flow module transforms $d\u2212u$ and $\u03bd$ into a Gaussian noise sequence to express the log-likelihood of the augmented and dequantized data $\\log p_{\\theta}(d \u2212 u, \u03bd|c_{text})$ as described in Section 2.2.2.</p> <p>\u65f6\u957f\u9884\u6d4b\u5668\u4e2d\u7684\u540e\u9a8c\u7f16\u7801\u5668\u548c\u6807\u51c6\u5316\u6d41\u6a21\u5757\u90fd\u662f\u57fa\u4e8e\u6d41\u7684\u795e\u7ecf\u7f51\u7edc, \u4e14\u5177\u6709\u76f8\u4f3c\u67b6\u6784. \u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u540e\u9a8c\u7f16\u7801\u5668\u5c06\u9ad8\u65af\u566a\u58f0\u5e8f\u5217\u8f6c\u6362\u4e3a\u4e24\u4e2a\u968f\u673a\u53d8\u91cf $\u03bd$ \u548c $u$, \u7528\u4e8e\u8868\u793a\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03 $q_{\\phi}(u, \u03bd|d, c_{text})$, \u800c\u6807\u51c6\u5316\u6d41\u6a21\u5757\u5c06 $d\u2212u$ \u548c $\u03bd$ \u8f6c\u6362\u4e3a\u9ad8\u65af\u566a\u58f0\u5e8f\u5217, \u7528\u4e8e\u8868\u793a\u589e\u5f3a\u548c\u53bb\u91cf\u5316\u6570\u636e\u7684\u5bf9\u6570\u4f3c\u7136 $\\log p_{\\theta}(d \u2212 u, \u03bd|c_{text})$, \u5982\u7b2c 2.2.2 \u8282\u6240\u8ff0.</p> <p>All input conditions are processed through condition encoders, each consisting of two 1x1 convolutional layers and a DDSConv residual block.  The posterior encoder and normalizing flow module have four coupling layers of neural spline flows.  Each coupling layer first processes input and input conditions through a DDSConv block and produces 29-channel parameters that are used to construct 10 rational-quadratic functions.  We set the hidden dimension of all coupling layers and condition encoders to 192.  Fig.06a and Fig.6b show the architecture of a condition encoder and a coupling layer used in the stochastic duration predictor.</p> <p></p> <p>\u6240\u6709\u7684\u8f93\u5165\u6761\u4ef6\u901a\u8fc7\u6761\u4ef6\u7f16\u7801\u5668\u5904\u7406, \u6bcf\u4e00\u4e2a\u7531\u4e24\u4e2a 1x1 \u5377\u79ef\u5c42\u548c DDSConv \u6b8b\u5dee\u5757\u7ec4\u6210. \u540e\u9a8c\u7f16\u7801\u5668\u548c\u6807\u51c6\u5316\u6d41\u6a21\u5757\u90fd\u6709\u56db\u4e2a\u8026\u5408\u5c42\u7684\u795e\u7ecf\u6837\u6761\u6d41. \u6bcf\u4e2a\u8026\u5408\u5c42\u9996\u5148\u901a\u8fc7 DDSConv \u5757\u5904\u7406\u8f93\u5165\u548c\u8f93\u5165\u6761\u4ef6, \u5e76\u4ea7\u751f 29 \u901a\u9053\u7684\u53c2\u6570, \u7528\u4e8e\u6784\u9020 10 \u4e2a\u5206\u6bb5\u4e8c\u6b21\u51fd\u6570. \u6211\u4eec\u5c06\u6240\u6709\u8026\u5408\u5c42\u548c\u6761\u4ef6\u7f16\u7801\u5668\u7684\u9690\u85cf\u7ef4\u5ea6\u8bbe\u7f6e\u4e3a 192. \u56fe 06a \u548c\u56fe 06b \u5c55\u793a\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u4e2d\u6761\u4ef6\u7f16\u7801\u5668\u548c\u8026\u5408\u5c42\u7684\u67b6\u6784.</p> <pre><code>class StochasticDurationPredictor(nn.Module):\n        def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\n        super().__init__()\n        filter_channels = in_channels # it needs to be removed from future version.\n        self.in_channels = in_channels\n        self.filter_channels = filter_channels\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.n_flows = n_flows\n        self.gin_channels = gin_channels\n\n        self.log_flow = modules.Log()\n        self.flows = nn.ModuleList()\n        self.flows.append(modules.ElementwiseAffine(2))\n        for i in range(n_flows):\n            self.flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n            self.flows.append(modules.Flip())\n\n        self.post_pre = nn.Conv1d(1, filter_channels, 1)\n        self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\n        self.post_convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n        self.post_flows = nn.ModuleList()\n        self.post_flows.append(modules.ElementwiseAffine(2))\n        for i in range(4):\n            self.post_flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n            self.post_flows.append(modules.Flip())\n\n        self.pre = nn.Conv1d(in_channels, filter_channels, 1)\n        self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\n        self.convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n        if gin_channels != 0:\n            self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\n\n        def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\n        x = torch.detach(x)\n        x = self.pre(x)\n        if g is not None:\n            g = torch.detach(g)\n            x = x + self.cond(g)\n        x = self.convs(x, x_mask)\n        x = self.proj(x) * x_mask\n\n        if not reverse:\n            flows = self.flows\n            assert w is not None\n\n            logdet_tot_q = 0 \n            h_w = self.post_pre(w)\n            h_w = self.post_convs(h_w, x_mask)\n            h_w = self.post_proj(h_w) * x_mask\n            e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n            z_q = e_q\n            for flow in self.post_flows:\n            z_q, logdet_q = flow(z_q, x_mask, g=(x + h_w))\n            logdet_tot_q += logdet_q\n            z_u, z1 = torch.split(z_q, [1, 1], 1) \n            u = torch.sigmoid(z_u) * x_mask\n            z0 = (w - u) * x_mask\n            logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1,2])\n            logq = torch.sum(-0.5 * (math.log(2*math.pi) + (e_q**2)) * x_mask, [1,2]) - logdet_tot_q\n\n            logdet_tot = 0\n            z0, logdet = self.log_flow(z0, x_mask)\n            logdet_tot += logdet\n            z = torch.cat([z0, z1], 1)\n            for flow in flows:\n            z, logdet = flow(z, x_mask, g=x, reverse=reverse)\n            logdet_tot = logdet_tot + logdet\n            nll = torch.sum(0.5 * (math.log(2*math.pi) + (z**2)) * x_mask, [1,2]) - logdet_tot\n            return nll + logq # [b]\n        else:\n            flows = list(reversed(self.flows))\n            flows = flows[:-2] + [flows[-1]] # remove a useless vflow\n            z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\n            for flow in flows:\n            z = flow(z, x_mask, g=x, reverse=reverse)\n            z0, z1 = torch.split(z, [1, 1], 1)\n            logw = z0\n            return logw\n</code></pre>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#4experiments","title":"4.Experiments\u00b7\u5b9e\u9a8c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#41datasets","title":"4.1.Datasets\u00b7\u6570\u636e\u96c6","text":"<p>We conducted experiments on two different datasets. We used the LJ Speech dataset for comparison with other publicly available models and the VCTK dataset to verify whether our model can learn and express diverse speech characteristics. The LJ Speech dataset consists of 13,100 short audio clips of a single speaker with a total length of approximately 24 hours. The audio format is 16-bit PCM with a sample rate of 22 kHz, and we used it without any manipulation. We randomly split the dataset into a training set (12,500 samples), validation set (100 samples), and test set (500 samples). The VCTK dataset consists of approximately 44,000 short audio clips uttered by 109 native English speakers with various accents. The total length of the audio clips is approximately 44 hours. The audio format is 16-bit PCM with a sample rate of 44 kHz. We reduced the sample rate to 22 kHz. We randomly split the dataset into a training set (43,470 samples), validation set (100 samples), and test set (500 samples).</p> <p>\u6211\u4eec\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c. \u6211\u4eec\u4f7f\u7528 LJ Speech \u6570\u636e\u96c6\u7528\u4e8e\u548c\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4, \u7136\u540e\u7528 VCTK \u6570\u636e\u96c6\u9a8c\u8bc1\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u5b66\u4e60\u548c\u8868\u8fbe\u591a\u6837\u5316\u7684\u8bed\u97f3\u7279\u5f81.</p> <p>LJ Speech \u6570\u636e\u96c6\u7531\u5355\u4e2a\u8bf4\u8bdd\u4eba\u7684 13,100 \u77ed\u97f3\u9891\u7247\u6bb5\u7ec4\u6210, \u603b\u65f6\u957f\u7ea6\u4e3a 24 \u5c0f\u65f6. \u97f3\u9891\u683c\u5f0f\u4e3a 16 \u4f4d PCM, \u91c7\u6837\u7387\u4e3a 22 kHz, \u6211\u4eec\u6ca1\u6709\u5bf9\u5176\u8fdb\u884c\u4efb\u4f55\u5904\u7406. \u6211\u4eec\u968f\u673a\u5c06\u6570\u636e\u96c6\u5206\u4e3a\u8bad\u7ec3\u96c6 (12,500 \u4e2a\u6837\u672c), \u9a8c\u8bc1\u96c6 (100 \u4e2a\u6837\u672c), \u548c\u6d4b\u8bd5\u96c6 (500 \u4e2a\u6837\u672c).</p> <p>VCTK \u6570\u636e\u96c6\u7531 109 \u540d\u82f1\u8bed\u6bcd\u8bed\u53d1\u8a00\u4eba\u53d1\u51fa\u7684\u7ea6 44,000 \u77ed\u97f3\u9891\u7247\u6bb5\u7ec4\u6210, \u603b\u65f6\u957f\u7ea6\u4e3a 44 \u5c0f\u65f6. \u97f3\u9891\u683c\u5f0f\u4e3a 16 \u4f4d PCM, \u91c7\u6837\u7387\u4e3a 44 kHz. \u6211\u4eec\u5c06\u91c7\u6837\u7387\u964d\u4f4e\u5230 22 kHz. \u6211\u4eec\u968f\u673a\u5c06\u6570\u636e\u96c6\u5206\u4e3a\u8bad\u7ec3\u96c6 (43,470 \u4e2a\u6837\u672c), \u9a8c\u8bc1\u96c6 (100 \u4e2a\u6837\u672c), \u548c\u6d4b\u8bd5\u96c6 (500 \u4e2a\u6837\u672c).</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#42preprocessing","title":"4.2.Preprocessing\u00b7\u9884\u5904\u7406","text":"<p>We use linear spectrograms which can be obtained from raw waveforms through the Short-time Fourier transform (STFT), as input of the posterior encoder. The FFT size, window size and hop size of the transform are set to 1024, 1024 and 256, respectively. We use 80 bands mel-scale spectrograms for reconstruction loss, which is obtained by applying a mel-filter bank to linear spectrograms. We use International Phonetic Alphabet (IPA) sequences as input to the prior encoder. We convert text sequences to IPA phoneme sequences using open-source software (Bernard, 2021), and the converted sequences are interspersed with a blank token following the implementation of Glow-TTS.</p> <p>\u6211\u4eec\u4f7f\u7528\u7ebf\u6027\u9891\u8c31\u56fe, \u5b83\u53ef\u4ee5\u4ece\u539f\u59cb\u6ce2\u5f62\u901a\u8fc7\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362 (STFT) \u83b7\u5f97. FFT \u5927\u5c0f, \u7a97\u53e3\u5927\u5c0f\u548c\u8df3\u8dc3\u5927\u5c0f\u5206\u522b\u8bbe\u7f6e\u4e3a 1024, 1024 \u548c 256. \u6211\u4eec\u4f7f\u7528 80 bands \u6885\u5c14\u9891\u8c31\u56fe\u7528\u4e8e\u8ba1\u7b97\u91cd\u6784\u635f\u5931, \u5b83\u662f\u901a\u8fc7\u5bf9\u7ebf\u6027\u9891\u8c31\u56fe\u5e94\u7528\u6885\u5c14\u6ee4\u6ce2\u5668\u83b7\u5f97\u7684. \u6211\u4eec\u4f7f\u7528 IPA \u5e8f\u5217\u4f5c\u4e3a\u5148\u9a8c\u7f16\u7801\u5668\u7684\u8f93\u5165. \u6211\u4eec\u4f7f\u7528\u5f00\u6e90\u8f6f\u4ef6\u5c06\u6587\u672c\u5e8f\u5217\u8f6c\u6362\u4e3a IPA \u97f3\u7d20\u5e8f\u5217, \u5e76\u9075\u5faa Glow-TTS \u7684\u5b9e\u73b0, \u5728\u97f3\u7d20\u5e8f\u5217\u4e4b\u95f4\u63d2\u5165\u7a7a\u767d Token.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#43training","title":"4.3.Training\u00b7\u8bad\u7ec3","text":"<p>The networks are trained using the AdamW optimizer with $\\beta_1= 0.8$, $\\beta_2=0.99$ and weight decay $\\lambda = 0.01$. The learning rate decay is scheduled by a 0.9991/8 factor in every epoch with an initial learning rate of $2\\times 10^{\u22124}$. Following previous work (FastSpeech 2s (2020); EATS (2020)), we adopt the windowed generator training, a method of generating only a part of raw waveforms to reduce the training time and memory usage during training. We randomly extract segments of latent representations with a window size of 32 to feed to the decoder instead of feeding entire latent representations and also extract the corresponding audio segments from the ground truth raw waveforms as training targets. We use mixed precision training on 4 NVIDIA V100 GPUs. The batch size is set to 64 per GPU and the model is trained up to 800k steps.</p> <p>\u7f51\u7edc\u91c7\u7528 AdamW \u4f18\u5316\u5668\u8fdb\u884c\u8bad\u7ec3, \u5176\u53c2\u6570\u4e3a $\\beta_1= 0.8$, $\\beta_2=0.99$ \u548c\u6743\u91cd\u8870\u51cf $\\lambda = 0.01$. \u5b66\u4e60\u7387\u8870\u51cf\u662f\u6bcf\u4e2a Epoch \u6309 0.9991/8 \u56e0\u5b50\u8fdb\u884c\u8c03\u5ea6, \u521d\u59cb\u5b66\u4e60\u7387\u4e3a $2\\times 10^{\u22124}$. \u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u7c7b\u4f3c, \u6211\u4eec\u91c7\u7528\u7a97\u53e3\u751f\u6210\u5668\u8bad\u7ec3, \u4e00\u79cd\u53ea\u751f\u6210\u90e8\u5206\u539f\u59cb\u6ce2\u5f62\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u5360\u7528. \u6211\u4eec\u968f\u673a\u63d0\u53d6 32 \u957f\u5ea6\u7684\u9690\u8868\u793a\u7247\u6bb5, \u5e76\u5c06\u5176\u8f93\u5165\u5230\u89e3\u7801\u5668, \u800c\u4e0d\u662f\u8f93\u5165\u6574\u4e2a\u9690\u8868\u793a, \u5e76\u4e14\u63d0\u53d6\u76f8\u5e94\u7684\u771f\u5b9e\u97f3\u9891\u7247\u6bb5\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807. \u6211\u4eec\u5728 4 \u5757 NVIDIA V100 GPU \u4e0a\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3. \u6bcf\u5757 GPU \u7684\u6279\u91cf\u5927\u5c0f\u8bbe\u7f6e\u4e3a 64, \u6a21\u578b\u8bad\u7ec3 800k \u6b65.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#44experiment-setup-for-comparison","title":"4.4.Experiment Setup for Comparison\u00b7\u5bf9\u6bd4\u5b9e\u9a8c\u8bbe\u7f6e","text":"<p>We compared our model with the best publicly available models. We used Tacotron 2, an autoregressive model, and Glow-TTS, a flow-based non-autoregressive model, as first stage models and HiFi-GAN as a second stage model. We used their public implementations and pre-trained weights. Since a two-stage TTS system can theoretically achieve higher synthesis quality through sequential training, we included the fine-tuned HiFi-GAN up to 100k steps with the predicted outputs from the first stage models. We empirically found that fine-tuning HiFi-GAN with the generated mel-spectrograms from Tacotron 2 under teacher-forcing mode, led to better quality for both Tacotron 2 and Glow-TTS than fine-tuning with the generated mel-spectrograms from Glow-TTS, so we appended the better fine-tuned HiFi-GAN to both Tacotron 2 and Glow-TTS. As each model has a degree of randomness during sampling, we fixed hyper-parameters that controls the randomness of each model throughout our experiments. The probability of dropout in the pre-net of Tactron 2 was set to 0.5. For Glow-TTS, the standard deviation of the prior distribution was set to 0.333. For VITS, the standard deviation of input noise of the stochastic duration predictor was set to 0.8 and we multiplied a scale factor of 0.667 to the standard deviation of the prior distribution.</p> <p>\u6211\u4eec\u5c06\u6a21\u578b\u548c\u6700\u4f73\u7684\u516c\u5f00\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83. \u6211\u4eec\u4f7f\u7528\u81ea\u56de\u5f52\u6a21\u578b Tacotron2, \u57fa\u4e8e\u6d41\u7684\u975e\u81ea\u56de\u5f52\u6a21\u578b Glow-TTS \u4f5c\u4e3a\u4e00\u9636\u6bb5\u6a21\u578b, HiFi-GAN \u4f5c\u4e3a\u4e8c\u9636\u6bb5\u6a21\u578b. \u6211\u4eec\u4f7f\u7528\u5b83\u4eec\u7684\u516c\u5f00\u5b9e\u73b0\u548c\u9884\u8bad\u7ec3\u6743\u91cd. \u7531\u4e8e\u4e24\u9636\u6bb5 TTS \u7cfb\u7edf\u7406\u8bba\u4e0a\u53ef\u4ee5\u901a\u8fc7\u4e32\u884c\u8bad\u7ec3\u5b9e\u73b0\u66f4\u9ad8\u7684\u5408\u6210\u8d28\u91cf, \u6211\u4eec\u5c06 HiFi-GAN \u4f7f\u7528 Tacotron2 \u548c Glow-TTS \u7684\u9884\u6d4b\u8f93\u51fa\u5fae\u8c03\u5230 100k \u6b65. \u6211\u4eec\u7ecf\u9a8c\u6027\u5730\u53d1\u73b0, \u4f7f\u7528 Tacotron2 \u751f\u6210\u7684\u6885\u5c14\u9891\u8c31\u56fe\u4f5c\u4e3a\u6559\u5e08\u5f3a\u5236\u6a21\u5f0f, \u5fae\u8c03 HiFi-GAN \u80fd\u4ea7\u751f\u66f4\u597d\u7684\u8d28\u91cf, \u56e0\u6b64\u6211\u4eec\u5c06\u66f4\u597d\u7684\u5fae\u8c03 HiFi-GAN \u8ffd\u52a0\u5230 Tacotron2 \u548c Glow-TTS. \u7531\u4e8e\u6bcf\u4e2a\u6a21\u578b\u5728\u91c7\u6837\u65f6\u90fd\u6709\u4e00\u5b9a\u7684\u968f\u673a\u6027, \u6211\u4eec\u5728\u5b9e\u9a8c\u4e2d\u56fa\u5b9a\u4e86\u6bcf\u4e2a\u6a21\u578b\u7684\u8d85\u53c2\u6570, \u4ee5\u63a7\u5236\u968f\u673a\u6027. Tacotron2 \u7684\u9884-\u7f51\u7edc\u7684\u4e22\u5f03\u6982\u7387\u8bbe\u7f6e\u4e3a 0.5. Glow-TTS \u7684\u5148\u9a8c\u5206\u5e03\u7684\u6807\u51c6\u5dee\u8bbe\u7f6e\u4e3a 0.333. VITS \u7684\u8f93\u5165\u566a\u58f0\u7684\u6807\u51c6\u5dee\u8bbe\u7f6e\u4e3a 0.8, \u5e76\u5c06\u5148\u9a8c\u5206\u5e03\u7684\u6807\u51c6\u5dee\u4e58\u4ee5 0.667.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#5results","title":"5.Results\u00b7\u7ed3\u679c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#51speech-synthesis-quality","title":"5.1.Speech Synthesis Quality\u00b7\u8bed\u97f3\u5408\u6210\u8d28\u91cf","text":"<p>We conducted crowd-sourced MOS tests to evaluate the quality. Raters listened to randomly selected audio samples, and rated their naturalness on a 5 point scale from 1 to 5. Raters were allowed to evaluate each audio sample once, and we normalized all the audio clips to avoid the effect of amplitude differences on the score. All of the quality assessments in this work were conducted in this manner. The evaluation results are shown in Tab.01.</p> <p>\u6211\u4eec\u8fdb\u884c\u4e86\u4f17\u5305 MOS \u6d4b\u8bd5, \u4ee5\u8bc4\u4f30\u8d28\u91cf. \u8bc4\u5206\u5458\u968f\u673a\u9009\u62e9\u4e86\u97f3\u9891\u6837\u672c, \u5e76\u5bf9\u5176\u81ea\u7136\u5ea6\u8fdb\u884c\u4e86 5 \u7ea7\u8bc4\u5206, 1 \u5230 5 \u4e4b\u95f4. \u8bc4\u5206\u5458\u4ec5\u5141\u8bb8\u5bf9\u6bcf\u4e2a\u97f3\u9891\u6837\u672c\u8fdb\u884c\u4e00\u6b21\u8bc4\u5206, \u6211\u4eec\u5bf9\u6240\u6709\u97f3\u9891\u7247\u6bb5\u8fdb\u884c\u4e86\u5f52\u4e00\u5316, \u4ee5\u907f\u514d\u632f\u5e45\u5dee\u5f02\u5bf9\u5f97\u5206\u7684\u5f71\u54cd. \u672c\u6587\u4e2d\u6240\u6709\u8d28\u91cf\u8bc4\u4f30\u90fd\u91c7\u7528\u4e86\u8fd9\u79cd\u65b9\u5f0f. \u8868 01 \u663e\u793a\u4e86\u8bc4\u4f30\u7ed3\u679c.</p> <p>Tab.01.Comparison of evaluated MOS with 95% confidence intervals on the LJ Speech dataset.</p> Model MOS (CI) Ground Truth 4.46 (\u00b10.06) Tacotron 2 + HiFi-GAN 3.77 (\u00b10.08) Tacotron 2 + HiFi-GAN (Fine-tuned) 4.25 (\u00b10.07) Glow-TTS + HiFi-GAN 4.14 (\u00b10.07) Glow-TTS + HiFi-GAN (Fine-tuned) 4.32 (\u00b10.07) VITS (DDP) 4.39 (\u00b10.06) VITS 4.43 (\u00b10.06) <p>VITS outperforms other TTS systems and achieves a similar MOS to that of ground truth. The VITS (DDP), which employs the same deterministic duration predictor architecture used in Glow-TTS rather than the stochastic duration predictor, scores the second-highest among TTS systems in the MOS evaluation. These results imply that 1) the stochastic duration predictor generates more realistic phoneme duration than the deterministic duration predictor and 2) our end-to-end training method is an effective way to make better samples than other TTS models even if maintaining the similar duration predictor architecture.  </p> <p>VITS \u4f18\u4e8e\u5176\u4ed6 TTS \u7cfb\u7edf, \u5e76\u8fbe\u5230\u548c\u771f\u5b9e\u503c\u76f8\u4f3c\u7684 MOS. VITS (DDP) \u4f7f\u7528\u548c Glow-TTS \u76f8\u540c\u7684\u786e\u5b9a\u6027\u65f6\u957f\u9884\u6d4b\u5668\u67b6\u6784\u800c\u4e0d\u662f\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u5728 MOS \u8bc4\u4f30\u4e2d\u6392\u540d\u7b2c\u4e8c. \u8fd9\u4e9b\u7ed3\u679c\u8868\u660e: 1. \u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u6bd4\u786e\u5b9a\u6027\u65f6\u957f\u9884\u6d4b\u5668\u751f\u6210\u66f4\u771f\u5b9e\u7684\u97f3\u7d20\u65f6\u957f, 2. \u6211\u4eec\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5, \u5373\u4f7f\u4fdd\u6301\u76f8\u4f3c\u7684\u65f6\u957f\u9884\u6d4b\u5668\u67b6\u6784, \u4e5f\u80fd\u4ea7\u751f\u66f4\u597d\u7684\u6837\u672c.</p> <p>We conducted an ablation study to demonstrate the effectiveness of our methods, including the normalized flow in the prior encoder and linear-scale spectrogram posterior input. All models in the ablation study were trained up to 300k steps. The results are shown in Tab.02.</p> <p>\u6211\u4eec\u8fdb\u884c\u4e86\u6d88\u878d\u5b9e\u9a8c\u4ee5\u8bf4\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027, \u5305\u62ec\u5148\u9a8c\u7f16\u7801\u5668\u4e2d\u7684\u5f52\u4e00\u5316\u6d41\u548c\u7ebf\u6027\u9891\u8c31\u56fe\u540e\u9a8c\u8f93\u5165. \u6d88\u878d\u5b9e\u9a8c\u4e2d\u6240\u6709\u6a21\u578b\u90fd\u8bad\u7ec3\u4e86 300k \u6b65. \u8868 02 \u663e\u793a\u4e86\u7ed3\u679c.</p> <p>Tab.02. MOS comparison in the ablation studies.</p> Model MOS (CI) Ground Truth 4.50 (\u00b10.06) Baseline 4.50 (\u00b10.06) without Normalizing Flow 2.98 (\u00b10.08) with Mel-spectrogram 4.31 (\u00b10.08) <p>Removing the normalizing flow in the prior encoder results in a 1.52 MOS decrease from the baseline, demonstrating that the prior distribution\u2019s flexibility significantly influences the synthesis quality. Replacing the linear-scale spectrogram for posterior input with the mel-spectrogram results in a quality degradation (-0.19 MOS), indicating that the high-resolution information is effective for VITS in improving the synthesis quality.</p> <p>\u79fb\u9664\u5148\u9a8c\u7f16\u7801\u5668\u4e2d\u7684\u6807\u51c6\u5316\u6d41\u4f1a\u964d\u4f4e 1.52 \u7684 MOS \u5f97\u5206, \u8bf4\u660e\u5148\u9a8c\u5206\u5e03\u7684\u7075\u6d3b\u6027\u5bf9\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd. \u7528\u6885\u5c14\u9891\u8c31\u56fe\u66ff\u6362\u7ebf\u6027\u9891\u8c31\u56fe\u540e\u9a8c\u8f93\u5165\u4f1a\u5bfc\u81f4\u8d28\u91cf\u964d\u4f4e (-0.19 MOS), \u8868\u660e\u9ad8\u5206\u8fa8\u7387\u4fe1\u606f\u5bf9\u4e8e VITS \u5728\u63d0\u5347\u8bed\u97f3\u5408\u6210\u8d28\u91cf\u6709\u4f5c\u7528.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#52generalization-to-multi-speaker-text-to-speech","title":"5.2.Generalization to Multi-Speaker Text-to-Speech\u00b7\u591a\u8bf4\u8bdd\u4eba\u6587\u672c\u8f6c\u8bed\u97f3","text":"<p>To verify that our model can learn and express diverse speech characteristics, we compared our model to Tacotron 2, Glow-TTS and HiFi-GAN, which showed the ability to extend to multi-speaker speech synthesis (Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis; Glow-TTS (2020); HiFi-GAN (2020)). We trained the models on the VCTK dataset. We added speaker embedding to our model as described in Section 2.5. For Tacotron 2, we broadcasted speaker embedding and concatenated it with the encoder output, and for Glow-TTS, we applied the global conditioning following the previous work. The evaluation method is the same as that described in Section 4.1. As shown in Tab.03, our model achieves a higher MOS than the other models.</p> <p>\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u548c\u8868\u793a\u591a\u6837\u7684\u8bed\u97f3\u7279\u6027, \u6211\u4eec\u5c06\u6a21\u578b\u4e0e Tacotron 2, Glow-TTS \u548c HiFi-GAN \u8fdb\u884c\u4e86\u6bd4\u8f83, \u5b83\u4eec\u90fd\u5177\u6709\u6269\u5c55\u5230\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u5408\u6210\u80fd\u529b. \u6211\u4eec\u5728 VCTK \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u6a21\u578b. \u6211\u4eec\u5728\u6a21\u578b\u4e2d\u52a0\u5165\u4e86\u8bf4\u8bdd\u4eba\u5d4c\u5165, \u5982\u7b2c 2.5 \u8282\u6240\u8ff0. \u5bf9\u4e8e Tacotron 2, \u6211\u4eec\u5e7f\u64ad\u4e86\u8bf4\u8bdd\u4eba\u5d4c\u5165\u5e76\u5c06\u5176\u4e0e\u7f16\u7801\u5668\u8f93\u51fa\u8fde\u63a5\u8d77\u6765, \u5bf9\u4e8e Glow-TTS, \u6211\u4eec\u9075\u5faa\u4e4b\u524d\u7684\u5de5\u4f5c, \u5e94\u7528\u5168\u5c40\u6761\u4ef6. \u8bc4\u4f30\u65b9\u6cd5\u4e0e\u7b2c 4.1 \u8282\u4e2d\u63cf\u8ff0\u7684\u76f8\u540c. \u5982\u8868 3 \u6240\u793a, \u6211\u4eec\u7684\u6a21\u578b\u8d85\u8fc7\u4e86\u5176\u4ed6\u6a21\u578b.</p> <p>Table 3.Comparison of evaluated MOS with 95% confidence intervals on the VCTK dataset.</p> Model MOS (CI) Ground Truth 4.38 (\u00b10.07) Tacotron 2 + HiFi-GAN 3.14 (\u00b10.09) Tacotron 2 + HiFi-GAN (Fine-tuned) 3.19 (\u00b10.09) Glow-TTS + HiFi-GAN 3.76 (\u00b10.07) Glow-TTS + HiFi-GAN (Fine-tuned) 3.82 (\u00b10.07) VITS 4.38 (\u00b10.06) <p>This demonstrates that our model learns and expresses various speech characteristics in an end-to-end manner.  </p> <p>\u8fd9\u8868\u660e, \u6211\u4eec\u7684\u6a21\u578b\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u5b66\u4e60\u548c\u8868\u793a\u5404\u79cd\u8bed\u97f3\u7279\u6027.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#53speech-variation","title":"5.3.Speech Variation\u00b7\u8bed\u97f3\u53d8\u5316","text":"<p>We verified how many different lengths of speech the stochastic duration predictor produces, and how many different speech characteristics the synthesized samples have. Similar to Flowtron (2020), all samples here were generated from a sentence \u201cHow much variation is there?\u201d. Fig.02a shows histograms of the lengths of 100 generated utterances from each model. While Glow-TTS generates only fixed-length utterances due to the deterministic duration predictor, samples from our model follow a similar length distribution to that of Tacotron 2. Fig.02b shows the lengths of 100 utterances generated with each of five speaker identities from our model in the multi-speaker setting, implying that the model learns the speaker-dependent phoneme duration. F0 contours of 10 utterances extracted with the YIN algorithm (a fundamental frequency estimator for speech and music) in Fig.03 shows that our model generates speech with diverse pitches and rhythms, and five samples generated with each of different speaker identities in Fig.03d demonstrates our model expresses very different lengths and pitches of speech for each speaker identity. Note that Glow-TTS could increase the diversity of pitch by increasing the standard deviation of the prior distribution, but on the contrary, it could lower the synthesis quality.</p> <p>\u6211\u4eec\u9a8c\u8bc1\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668\u751f\u6210\u7684\u8bed\u97f3\u957f\u5ea6\u6709\u591a\u5c11\u79cd, \u5408\u6210\u7684\u6837\u672c\u6709\u591a\u5c11\u79cd\u4e0d\u540c\u7684\u8bed\u97f3\u7279\u6027. \u4e0e Flowtron (2020) \u7c7b\u4f3c, \u8fd9\u91cc\u6240\u6709\u7684\u6837\u672c\u90fd\u6765\u81ea\u4e8e\u4e00\u53e5\u8bdd \"How much variation is there?\". \u56fe 2a \u663e\u793a\u4e86\u6765\u81ea\u6bcf\u4e2a\u6a21\u578b\u7684 100 \u4e2a\u5408\u6210\u53e5\u5b50\u7684\u957f\u5ea6\u76f4\u65b9\u56fe. \u867d\u7136 Glow-TTS \u7531\u4e8e\u786e\u5b9a\u6027\u65f6\u957f\u9884\u6d4b\u5668\u800c\u4ea7\u751f\u56fa\u5b9a\u957f\u5ea6\u7684\u53e5\u5b50, \u4f46\u6765\u81ea\u6211\u4eec\u7684\u6a21\u578b\u7684\u6837\u672c\u4e0e Tacotron 2 \u7684\u6837\u672c\u6709\u7740\u76f8\u4f3c\u7684\u957f\u5ea6\u5206\u5e03. \u56fe 2b \u663e\u793a\u4e86\u6765\u81ea\u6211\u4eec\u7684\u6a21\u578b\u7684 100 \u4e2a\u53e5\u5b50\u7684\u957f\u5ea6, \u8fd9\u8868\u660e\u6a21\u578b\u5b66\u4e60\u4e86\u8bf4\u8bdd\u4eba\u76f8\u5173\u7684\u97f3\u7d20\u65f6\u957f. \u56fe 3 \u663e\u793a\u4e86\u4f7f\u7528 YIN \u7b97\u6cd5\u63d0\u53d6\u7684 10 \u4e2a\u53e5\u5b50\u7684 F0 \u8f6e\u5ed3, \u8fd9\u8868\u660e\u6211\u4eec\u7684\u6a21\u578b\u751f\u6210\u4e86\u5177\u6709\u4e0d\u540c\u97f3\u9ad8\u548c\u97f5\u5f8b\u7684\u8bed\u97f3, \u56fe 3d \u663e\u793a\u4e86\u6765\u81ea\u4e94\u4e2a\u4e0d\u540c\u8bf4\u8bdd\u4eba\u6807\u8bc6\u7684 10 \u4e2a\u53e5\u5b50, \u8fd9\u8868\u660e\u6a21\u578b\u4e3a\u6bcf\u4e2a\u8bf4\u8bdd\u4eba\u6807\u8bc6\u751f\u6210\u4e86\u4e0d\u540c\u7684\u8bed\u97f3\u957f\u5ea6\u548c\u97f3\u9ad8. \u6ce8\u610f\u5230 Glow-TTS \u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u5148\u9a8c\u5206\u5e03\u7684\u6807\u51c6\u5dee\u6765\u589e\u52a0\u97f3\u9ad8\u7684\u591a\u6837\u6027, \u4f46\u76f8\u53cd, \u5b83\u4f1a\u964d\u4f4e\u5408\u6210\u8d28\u91cf.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#54synthesis-speed","title":"5.4.Synthesis Speed\u00b7\u5408\u6210\u901f\u5ea6","text":"<p>We compared the synthesis speed of our model with a parallel two-stage TTS system, Glow-TTS and HiFi-GAN. We measured the synchronized elapsed time over the entire process to generate raw waveforms from phoneme sequences with 100 sentences randomly selected from the test set of the LJ Speech dataset. We used a single NVIDIA V100 GPU with a batch size of 1. The results are shown in Table 4. Since our model does not require modules for generating predefined intermediate representations, its sampling efficiency and speed are greatly improved.</p> <p>\u6211\u4eec\u6bd4\u8f83\u4e86\u6211\u4eec\u7684\u6a21\u578b\u4e0e Glow-TTS \u548c HiFi-GAN \u7684\u5408\u6210\u901f\u5ea6. \u6211\u4eec\u6d4b\u91cf\u4e86\u4ece LJ Speech \u6d4b\u8bd5\u96c6\u4e2d\u968f\u673a\u9009\u62e9 100 \u4e2a\u53e5\u5b50\u7684\u97f3\u7d20\u5e8f\u5217\u751f\u6210\u539f\u59cb\u6ce2\u5f62\u6240\u9700\u7684\u65f6\u95f4. \u6211\u4eec\u4f7f\u7528\u5355\u4e2a NVIDIA V100 GPU, \u6279\u5927\u5c0f\u4e3a 1. \u7ed3\u679c\u5982\u8868 4 \u6240\u793a. \u7531\u4e8e\u6211\u4eec\u7684\u6a21\u578b\u4e0d\u9700\u8981\u751f\u6210\u9884\u5b9a\u4e49\u7684\u4e2d\u95f4\u8868\u793a, \u5176\u91c7\u6837\u6548\u7387\u548c\u901f\u5ea6\u5927\u5e45\u63d0\u9ad8.</p> <p>Table 4.Comparison of the synthesis speed.  Speed of n kHz means that the model can generate n\u00d71000 raw audio samples per second.  Real-time means the synthesis speed over real-time.</p> Model Speed (kHz) Real-time Glow-TTS + HiFi-GAN 606.05 \u00d727.48 VITS 1480.15 \u00d767.12 VITS (DDP) 2005.03 \u00d790.93","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2021.06_VITS/2021.06_VITS/#6conclusion","title":"6.Conclusion\u00b7\u7ed3\u8bba","text":"<p>In this work, we proposed a parallel TTS system, VITS, that can learn and generate in an end-to-end manner. We further introduced the stochastic duration predictor to express diverse rhythms of speech. The resulting system synthesizes natural sounding speech waveforms directly from text, without having to go through predefined intermediate speech representations. Our experimental results show that our method outperforms two-stage TTS systems and achieves close to human quality. We hope the proposed method will be used in many speech synthesis tasks, where two-stage TTS systems have been used, to achieve performance improvement and enjoy the simplified training procedure. We also want to point out that even though our method integrates two separated generative pipelines in TTS systems, there remains a problem of text preprocessing. Investigating self-supervised learning of language representations could be a possible direction for removing the text preprocessing step. We will release our source-code and pre-trained models to facilitate research in plenty of future directions.</p> <p>\u672c\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e76\u884c\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf, VITS, \u80fd\u591f\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u5b66\u4e60\u548c\u751f\u6210\u8bed\u97f3. \u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u968f\u673a\u65f6\u957f\u9884\u6d4b\u5668, \u4ee5\u751f\u6210\u5177\u6709\u591a\u6837\u97f5\u5f8b\u7684\u8bed\u97f3. \u8be5\u7cfb\u7edf\u80fd\u76f4\u63a5\u4ece\u6587\u672c\u5408\u6210\u81ea\u7136\u7684\u8bed\u97f3\u6ce2\u5f62, \u800c\u4e0d\u9700\u8981\u7ecf\u8fc7\u9884\u5148\u5b9a\u4e49\u7684\u4e2d\u95f4\u8bed\u97f3\u8868\u793a. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e, \u6211\u4eec\u7684\u6a21\u578b\u4f18\u4e8e\u4e24\u9636\u6bb5\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf, \u5e76\u8fbe\u5230\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u8d28\u91cf. \u6211\u4eec\u5e0c\u671b\u8be5\u65b9\u6cd5\u80fd\u591f\u7528\u4e8e\u8bb8\u591a\u4e4b\u524d\u5df2\u7ecf\u4f7f\u7528\u4e24\u9636\u6bb5\u6587\u672c\u8f6c\u8bed\u97f3\u7cfb\u7edf\u7684\u8bed\u97f3\u5408\u6210\u4efb\u52a1, \u4ee5\u63d0\u5347\u6027\u80fd\u5e76\u7b80\u5316\u8bad\u7ec3\u8fc7\u7a0b. \u6211\u4eec\u8fd8\u9700\u8981\u6307\u51fa\u5c3d\u7ba1\u6211\u4eec\u7684\u65b9\u6cd5\u5408\u5e76\u4e86\u4e24\u79cd\u5355\u72ec\u7684\u751f\u6210 pipelines, \u4f46\u4ecd\u7136\u5b58\u5728\u6587\u672c\u9884\u5904\u7406\u7684\u95ee\u9898. \u63a2\u7d22\u81ea\u76d1\u7763\u5b66\u4e60\u8bed\u8a00\u8868\u5f81\u53ef\u80fd\u662f\u6d88\u9664\u6587\u672c\u9884\u5904\u7406\u6b65\u9aa4\u7684\u4e00\u79cd\u53ef\u80fd\u65b9\u5411. \u6211\u4eec\u5c06\u4f1a\u53d1\u5e03\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b, \u4ee5\u4fc3\u8fdb\u7814\u7a76\u7684\u5e7f\u9614\u524d\u666f.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5bf9\u6297\u5b66\u4e60_Adversarial_Learning","\u53d8\u5206\u81ea\u7f16\u7801\u5668_VAE","\u6761\u4ef6\u5316_Conditional","\u5f00\u6e90_OpenSource","\u7aef\u5230\u7aef_End-to-End"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/","title":"VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers","text":"\u4f5c\u8005 \u673a\u6784 \u4f5c\u8005 \u673a\u6784 \u738b\u7a0b\u4e00 Microsoft \u9648\u4e09\u5143 Microsoft \u5434\u4fe3 Microsoft \u5f20\u81ea\u5f3a Microsoft \u5468\u9f99 Microsoft \u5218\u6811\u6770 Microsoft Zhuo Chen Microsoft Yanqing Liu Microsoft Huaming Wang Microsoft \u674e\u52b2\u5b87 Microsoft \u4f55\u78ca Microsoft \u8d75\u80dc Microsoft \u97e6\u798f\u5982 Microsoft","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#abstract","title":"Abstract","text":"<p>We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find that VALL-E could preserve the speaker\u2019s emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.</p> <p>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u8bed\u97f3\u5408\u6210\u7684\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5. \u5177\u4f53\u6765\u8bf4, \u6211\u4eec\u4f7f\u7528\u4ece\u73b0\u6210\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u5bfc\u51fa\u7684\u79bb\u6563\u7f16\u7801\u8bad\u7ec3\u4e86\u4e00\u4e2a\u795e\u7ecf\u7f16\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b (\u79f0\u4e3a VALL-E), \u5e76\u5c06 TTS \u89c6\u4e3a\u4e00\u4e2a\u6761\u4ef6\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1, \u800c\u4e0d\u662f\u50cf\u4ee5\u524d\u7684\u5de5\u4f5c\u90a3\u6837\u89c6\u4e3a\u8fde\u7eed\u4fe1\u53f7\u56de\u5f52\u4efb\u52a1. \u5728\u9884\u8bad\u7ec3\u9636\u6bb5, \u6211\u4eec\u5c06 TTS \u8bad\u7ec3\u6570\u636e\u6269\u5c55\u5230 60K \u5c0f\u65f6\u7684\u82f1\u8bed\u8bed\u97f3, \u8fd9\u6bd4\u73b0\u6709\u7cfb\u7edf\u5927\u6570\u767e\u500d. VALL-E \u5c55\u73b0\u51fa\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b, \u5e76\u53ef\u7528\u4e8e\u4f7f\u7528\u4ec5 3 \u79d2\u7684\u672a\u89c1\u8bf4\u8bdd\u8005\u7684\u8f93\u5165\u5f55\u97f3\u4f5c\u4e3a\u58f0\u5b66\u63d0\u793a\u6765\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u8bed\u97f3. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e VALL-E \u5728\u8bed\u97f3\u81ea\u7136\u5ea6\u548c\u8bf4\u8bdd\u8005\u76f8\u4f3c\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u6b21\u8bed\u8a00\u5408\u6210\u7cfb\u7edf. \u6b64\u5916, \u6211\u4eec\u53d1\u73b0 VALL-E \u53ef\u4ee5\u5728\u5408\u6210\u4e2d\u4fdd\u7559\u58f0\u5b66\u63d0\u793a\u4e2d\u7684\u8bf4\u8bdd\u8005\u7684\u60c5\u611f\u548c\u58f0\u5b66\u73af\u5883 \u8bf7\u8bbf\u95ee https://aka.ms/valle \u67e5\u770b\u672c\u9879\u5de5\u4f5c\u7684\u793a\u4f8b.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#1introduction","title":"1.Introduction","text":"<p>The last decade has yielded dramatic breakthroughs in speech synthesis through the development of neural networks and end-to-end modeling. Currently, cascaded text-to-speech (TTS) systems (Tacotron2 (2017), FastSpeech (2019), Transformer TTS (2018)) usually leverage a pipeline with an acoustic model and a vocoder using mel spectrograms as the intermediate representations. While advanced TTS systems can synthesize high-quality speech from single or multiple speakers (DelightfulTTS2 (2022), VITS (2021)), it still requires high-quality clean data from the recording studio. Large-scale data crawled from the Internet cannot meet the requirement, and always lead to performance degradation. Because the training data is relatively small, current TTS systems still suffer from poor generalization. Speaker similarity and speech naturalness decline dramatically for unseen speakers in the zero-shot scenario.</p> <p>To tackle the zero-shot TTS problem, existing work leverages speaker adaptation [Chen et al., 2019, Wang et al., 2020] and speaker encoding [Arik et al., 2018, YourTTS (2021)] methods, requiring additional fine-tuning, complex pre-designed features, or heavy structure engineering.</p> <p>Instead of designing a complex and specific network for this problem, the ultimate solution is to train a model with large and diverse data as much as possible, motivated by success in the field of text synthesis [Brown et al., 2020, Chowdhery et al., 2022]. Recent years have witnessed notable performance improvement for data increase in the text language model, from 16GB of uncompressed text [Devlin et al., 2019], to 160GB [Liu et al., 2019], to 570GB [Brown et al., 2020], and finally, around 1TB [Chowdhery et al., 2022]. Transferring this success to the field of speech synthesis, we introduce VALL-E, the first language model-based TTS framework leveraging large, diverse, and multi-speaker speech data.</p> <p></p> <p>As shown in Fig.01, to synthesize personalized speech (e.g., zero-shot TTS), VALL-E generates the corresponding acoustic tokens conditioned on the acoustic tokens of the 3-second enrolled recording and the phoneme prompt, which constrain the speaker and content information respectively. Finally, the generated acoustic tokens are used to synthesize the final waveform with the corresponding neural codec decoder [D\u00e9fossez et al., 2022]. The discrete acoustic tokens derived from an audio codec model enable us to treat TTS as conditional codec language modeling and advanced prompting-based large-model techniques (as in GPTs [Brown et al., 2020])can be leveraged for the TTS tasks. The acoustic tokens also allow us to generate diverse synthesized results in TTS by using different sampling strategies during inference.</p> <p>We train VALL-E with LibriLight [Kahn et al., 2020], a corpus consisting of 60K hours of English speech with over 7000 unique speakers. The original data is audio-only, so we employ a speech recognition model to generate the transcriptions. Compared to previous TTS training datasets, such as LibriTTS [Zen et al., 2019], our data contain more noisy speech and inaccurate transcriptions but provide diverse speakers and prosodies. We believe the proposed approach is robust to the noise and generalize well by leveraging large data. It is worth noting that existing TTS systems are always trained with dozens of hours of single-speaker data or hundreds of hours of multi-speaker data, which is over hundreds of times smaller than VALL-E. Tab.01 summarizes the innovation of VALL-E, a language model approach for TTS, using audio codec codes as intermediate representations, leveraging large and diverse data, leading to strong in-context learning capabilities.</p> Table 1 Current Systems VALL-E Intermediate Representation Mel Spectrogram Audio Codec Code Objective Function Continuous Signal Regression Language Model Training Data \u2264600 Hours 60K Hours In-Context Language \u00d7 \u221a <p>We evaluate VALL-E on LibriSpeech [Panayotov et al., 2015] and VCTK [Veaux et al., 2016]datasets, where all test speakers are unseen in the training corpus. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system (YourTTS (2021)) in terms of speech naturalness and speaker similarity, with +0.12 comparative mean option score (CMOS) and +0.93 similarity mean option score (SMOS) improvement on LibriSpeech. VALL-E also beats the baseline on VCTK with+0.11 SMOS and +0.23 CMOS improvements. It even achieves a +0.04 CMOS score against ground truth, showing the synthesized speech of unseen speakers is as natural as human recordings on VCTK. Moreover, the qualitative analysis shows that VALL-E is able to synthesize diverse outputs with the same text and target speaker, which could benefit pseudo-data creation for the speech recognition task. We also find that VALL-E could keep the acoustic environment (e.g., reverberation) and emotion (e.g. anger) of the acoustic prompt.</p> <p>In summary, we make the following contributions. - We propose VALL-E, the first TTS framework with strong in-context learning capabilities as GPT-3, which treats TTS as a language model task with audio codec codes as an intermediate representation to replace the traditional mel spectrogram. It has in-context learning capability and enables prompt-based approaches for zero-shot TTS, which does not require additional structure engineering, pre-designed acoustic features, and fine-tuning as in previous work. - We build a generalized TTS system in the speaker dimension by leveraging a huge amount of semi-supervised data, suggesting that simple scaling up semi-supervised data has been underestimated for TTS. - VALL-E is able to provide diverse outputs with the same input text and keep the acoustic environment and speaker\u2019s emotion of the acoustic prompt. - We verify that VALL-E synthesizes natural speech with high speaker similarity by prompt-ing in the zero-shot scenario. Evaluation results show that VALL-E significantly outperforms the state-of-the-art zero-shot TTS system on LibriSpeech and VCTK.</p> <p>We encourage the reader to listen to our samples on the demo page https://aka.ms/valle.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#2related-work","title":"2.Related Work","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#21zero-shot-tts","title":"2.1.Zero-Shot TTS","text":"<p>Current TTS methods can be categorized into cascaded and end-to-end methods. Cascaded TTS systems (Tacotron2 (2017), FastSpeech (2019), Transformer TTS (2018)) usually leverage a pipeline with an acoustic model and a vocoder using mel spectrograms as the intermediate representations. To tackle the drawbacks of the vocoder, end-to-end TTS models (VITS (2021), DelightfulTTS2 (2022)) are proposed to jointly optimize the acoustic model and vocoder. In real scenarios, it is highly desirable to customize a TTS system to an arbitrary voice with rare enrolled recordings. Therefore, there is growing interest in the zero-shot multi-speaker TTS techniques, and most of work is done in the context of cascaded TTS systems. As the pioneers, Arik et al.2018 proposes speaker adaptation and speaker encoding approaches. In the line of speaker adaptation, the following work [Chen et al., 2019, Wang et al., 2020, Chen et al., 2021] tries to improve the adaptation efficiency with less target speaker data and speaker-specific parameters. Huang et al.[2022] applies meta-learning on speaker adaptation, which only requires 5-shot to build a well-performed system. In parallel, speaker encoding-based methods achieved great progress in recent years. A speaker encoding based system contains a speaker encoder and a TTS component, where the speaker encoder could be pre-trained on the speaker verification task [Jia et al., 2018]. In Jia et al.[2018] and Arik et al.[2018], the experiments show that the model is able to generate high-quality outputs with 3 seconds enrolled recordings for in-domain speakers. To improve the quality of unseen speakers, advanced speaker embedding models [Cai et al., 2018] can be employed, but it is still undesirable according to Tan et al.[2021]. Another way is to design advanced but complex speaker encoder [Wu et al., 2022].Diffusion model based TTS [Popov et al., 2021, Kim et al., 2022] is also extended to zero-shot TTS [Kang et al., 2022] and achieved good results. Compared to previous work [FastSpeech (2019), Du et al.,2022], our work follows the line of cascaded TTS but first uses audio codec code as intermediate representations. It is the first one that has strong in-context learning capabilities as GPT-3, which does not require fine-tuning, pre-designed features, or a complex speaker encoder.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#22spoken-generative-pre-trained-models","title":"2.2.Spoken Generative Pre-Trained Models","text":"<p>Self-supervised learning is widely investigated in the field of speech understanding [Wav2Vec2.0 (2020), HuBERT (2021), Chen et al., 2022] and speech-to-speech generation [Lakhotia et al., 2021, AudioLM (2022)]. In the context of speech-to-speech generation, a hot topic is how to synthesize speech in a textless setting. GSLM [Lakhotia et al.,2021] proposes to synthesize speech based on HuBERT (2021) codes, and Polyak et al.[2021] improves the performance by combining HuBERT codes with codes of VQVAE and a speaker encoder. AudioLM (2022) follows a similar way but use audio codecs [Zeghidour et al.,2022] to synthesize speech, together with semantic codes. It should be noted that AudioLM is able to synthesize speech based on audio codecs without training an additional vocoder such as HifiGAN (2020). AudioLM is a speech-to-speech model, whereas VALL-E is a TTS model, so we can explicitly control the content in speech synthesis. Another direction is to apply pre-training to the neural TTS. Chung et al.[2018] pre-trains speech decoder in TTS through autoregressive mel-spectrogram prediction. In Ao et al.[2022], the authors propose a unified-modal encoder-decoder framework SpeechT5, which can leverage unlabeled speech and text data to pre-train all components of TTS model. Tjandra et al.[2019] quantizes unlabeled speech into discrete tokens by a VQVAE model [van den Oord et al., 2017], and train a model with the token-to-speech sequence. They demonstrate that the pre-trained model only requires a small amount of real data for fine-tuning. Bai et al.[2022] proposes mask and reconstruction on mel spectrogram and showing better performance on speech editing and synthesis. Previous TTS pre-training work leverages less than 1K hours of data, whereas VALL-E is pre-trained with 60K hours of data. Furthermore, VALL-E is the first to use audio codec codes as intermediate representations, and emerge in-context learning capability in zero-shot TTS.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#3background-speech-quantization","title":"3.Background: Speech Quantization","text":"<p>Since audio is typically stored as a sequence of 16-bit integer values, a generative model is required to output $2^{16}=65,536$ probabilities per timestep to synthesize the raw audio. In addition, the audio sample rate exceeding ten thousand leads to an extraordinarily long sequence length, making it more intractable for raw audio synthesis. To this end, speech quantization is required to compress integer values and sequence length.  $\\mu$-law transformation can quantize each timestep to 256 values and reconstruct high-quality raw audio. It is widely used in speech generative models, such as WaveNet [van den Oord et al., 2016], but the inference speed is still slow since the sequence length is not reduced. Recently, vector quantization is widely applied in self-supervised speech models for feature extraction, such as vq-wav2vec [Baevski et al., 2020a] and HuBERT (2021). The following work [Lakhotia et al., 2021, Du et al., 2022] shows the codes from self-supervised models can also reconstruct content, and the inference speed is faster than WaveNet. However, the speaker identity has been discarded and the reconstruction quality is low AudioLM (2022). AudioLM (2022) trains speech-to-speech language models on both k-means tokens from a self-supervised model and acoustic tokens from a neural codec model, leading to high-quality speech-to-speech generation.</p> <p>In this paper, we follow AudioLM (2022) to leverage neural codec models to represent speech in discrete tokens. To compress audio for network transmission, codec models are able to encode waveform into discrete acoustic codes and reconstruct high-quality waveform even if the speaker is unseen in training. Compared to traditional audio codec approaches, the neural-based codec is significantly better at low bitrates, and we believe the quantized tokens contain sufficient information about the speaker and recording conditions. Compared to other quantization methods,the audio codec shows the following advantages:  1. It contains abundant speaker information and acoustic information, which could maintain speaker identity in reconstruction compared to HuBERT (2021) codes. 2. There is an off-the-shelf codec decoder to convert discrete tokens into a waveform, without the additional efforts on vocoder training like VQ-based methods that operated on spectrum [Du et al., 2022]. 3. It could reduce the length of time steps for efficiency to address the problem in $\\mu$-law transformation [van den Oord et al., 2016].</p> <p>We adopt a pre-trained neural audio codec model, EnCodec [D\u00e9fossez et al., 2022], as our tokenizer. EnCodec is a convolutional encoder-decoder model, whose input and output are both 24 kHz audio across variable bitrates. The encoder produces embeddings at 75 Hz for input waveforms at 24 kHz,which is a 320-fold reduction in the sampling rate. Each embedding is modeled by a residual vector quantization (RVQ), in which we choose eight hierarchy quantizers with 1024 entries each as shown in Fig.02.</p> <p></p> <p>This configuration corresponds to EnCodec at 6K bitrates for 24 kHz audio reconstruction. In this setting, given a 10-second waveform, the discrete representation is a matrix with750 \u00d7 8entries, where 750 =24,000\u00d710/320 is the downsampled time step and 8 is the number of quantizers. It is fine to choose other bitrate settings. A larger bitrate corresponds to more quantizers and better reconstruction quality. For example, if we choose EnCodecc at 12K bitrates, there are 16 quantizers are needed and the 10-second waveform corresponds to a matrix with 750\u00d716 entries. With the discrete codes from all quantizers, the convolutional decoder of EnCodec generates real-valued embeddings and reconstructs the waveform at 24 kHz.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#4vall-e","title":"4.VALL-E","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#41problem-formulation-regarding-tts-as-conditional-codec-language-modeling","title":"4.1.Problem Formulation: Regarding TTS as Conditional Codec Language Modeling","text":"<p>Given a dataset $\\mathcal{D}={\\mathbf{x}i, \\mathbf{y}_i}$, where $\\mathbf{y}$ is an audio sample and $\\mathbf{x} = {x_0,x_1, \\cdots x_L}$ is its corresponding phoneme transcription, we use a pre-trained neural codec model to encode each audio sample into discrete acoustic codes, denoted as $\\text{Encodec}(\\mathbf{y}) = C^{T\\times 8}$, where $C$ represents the two-dimensional acoustic code matrix, and $T$ is the downsampled utterance length. The row vector of each acoustic code matrix $c{t,:}$ represents the eight codes for frametand the column vector of each acoustic code matrix $c_{:,j}$ represents the code sequence from the $j$-th codebook, where $j \\in {1,\\cdots 8}$. After quantization, the neural codec decoder is able to reconstruct the waveform, denoted as $\\text{Decodec}(C)\\approx\\hat{\\mathbf{y}}$.</p> <p>Zero-shot TTS requires the model to synthesize high-quality speech for unseen speakers. In this work, we regard zero-shot TTS as a conditional codec language modeling task. We train a neural language model to generate an acoustic code matrix $C$ conditioned on a phoneme sequence $\\mathbf{x}$ and an acoustic prompt matri $\\tilde{C}^{T'\\times 8}$ with the optimization objective of $\\max p(C|\\mathbf{x},\\tilde{C})$. Here, $\\tilde{C}$ is obtained by the same neural codec with an enrolled recording as the input. We expect the neural language model learns to extract the content and speaker information from the phoneme sequence and the acoustic prompt, respectively. During inference, given a phoneme sequence and a 3-second enrolled recording of the unseen speaker, the acoustic code matrix with corresponding content and speaker\u2019s voice is firstly estimated by the trained language model. Then the neural codec decoder synthesizes the high-quality speech.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#42training-conditional-codec-language-modeling","title":"4.2.Training: Conditional Codec Language Modeling","text":"<p>The neural speech codec model allows us to operate on discrete audio representations. Due to residual quantization in the neural codec model, the tokens have a hierarchical structure: tokens from previous quantizers recover acoustic properties like speaker identity, while the consecutive quantizers learn fine acoustic details. Each quantizer is trained to model the residual from the previous quantizers. Motivated by this, we design two conditional language models in a hierarchical manner.</p> <p>For the discrete tokens from the first quantizer $c_{:,1}$, we train an autoregressive (AR) decoder-only language model. It is conditioned on the phoneme sequencexand the acoustic prompt $\\tilde{C}{:,1}$, formulated as  $$   p(c{:,1}|\\mathbf{x}, \\tilde{C}{:,1}; \\theta{AR}) =\\prod_{t=0}^T p(c_{t,1}|c_{&lt;t,1},\\tilde{c}{:,1}, \\mathbf{x}; \\theta{AR}) \\tag{1} $$</p> <p>Since VALL-E is a decoder-only LM, the concatenation of $\\tilde{c}{:,1}$ and $c{:,1}$ is a whole sequence, and we do not distinguish them or insert a specific token in training. Only $c_{:,1}$ is predicted while the prefix $\\tilde{c}_{:,1}$ is given during inference.</p> <p>For the discrete tokens from the second to the last quantizers, $c_{:,j}\\in[2,8]$, we train a non-autoregressive (NAR) language model. Since the tokens can not access each other in a NAR manner, to constrain the speaker identity, the acoustic prompt matrix $\\tilde{C}$ is used as an acoustic prompt. Thus, the model is conditioned on the phoneme sequencex, the acoustic prompt $\\tilde{C}$ and the predicted acoustic tokens belong to the previous codebooks $C_{:,&lt;j}$: $$   p(C_{:,2:8}|\\mathbf{x},\\tilde{C};\\theta_{NAR})=\\prod_{j=2}^{8}p(c_{:,j}|C_{:,&lt;j},\\mathbf{x},\\tilde{C};\\theta_{NAR}) \\tag{2} $$</p> <p>The combination of the AR model and the NAR model provides a good trade-off between speech quality and inference speed. On the one hand, the rate of the generated speech should be consistent with the enrolled recording, and it is hard to train a length predictor for different speakers since their speaking speed may be very diverse. In this case, the AR model is a more natural choice with its flexibility for acoustic sequence length prediction. On the other hand, for the consecutive stages, as the number of output slots follows the sequence length of the first stage, NAR can reduce the time complexity from $\\mathcal{O}(T)$ to $\\mathcal{O}(1)$. Overall, the prediction of C can be modeled as: $$   p(C|\\mathbf{x},\\tilde{C};\\theta)=p(c_{:,1}|\\tilde{C}{:,1}, \\mathbf{X}; \\theta{AR}) \\prod_{j=2}^{8}p(c_{:,j}|c_{:,&lt;j},\\mathbf{x},\\tilde{C};\\theta_{NAR}) \\tag{3} $$</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#421autoregressive-codec-language-modeling","title":"4.2.1.Autoregressive Codec Language Modeling","text":"<p>The autoregressive language model generates the tokens from the first quantizer. It comprises a phoneme embedding $W_x$, an acoustic embedding $W_a$, a transformer decoder, and a prediction layer. In order to generate speech with specific content, we use the phoneme sequence as the phoneme prompt of the language model. Thus, the model input is the concatenation of $\\mathbf{x}$ and $\\mathbf{c}{:,1}$, and two special <code>&lt;EOS&gt;</code> tokens are appended after each of them. We compute sinuous position embedding separately for prompt and input tokens. For the causal transformer model, each tokenct,1can attend to $(\\mathbf{x}, c{\\leq t,1})$ as illustrated in the left part of Fig.03.</p> <p></p> <p>The model is optimized to maximize the probability of the next token in the first codebook. We share the parameters of the output projection layer with the parameters of the acoustic embedding $W_a$.</p> <p>In the AR model, we do not explicitly extract an audio clip as the prompt in training. The training process is pure casual language model training. In this way, any prefix sequence $c_{&lt;t,1}$ is treated as a prompt for the latter part of the sequence $c_{\\geq t,1}$. During inference, given an enrolled recording, we should concatenate the phoneme sequence of the enrolled recording and the phoneme sequence for synthesis together. Meanwhile, the acoustic token sequence of the enrolled recording is used as the prefix in AR decoding, as formulated in Eq.01. We will study the superiority of this setting in the experiment.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#422non-autoregressive-codec-language-modeling","title":"4.2.2.Non-Autoregressive Codec Language Modeling","text":"<p>When we obtain the first quantizer codes by the AR model, we employ a non-autoregressive (NAR) model to generate codes of the other seven quantizers. The NAR model has a similar architecture to the AR model, except that it contains eight separate acoustic embedding layers. In each training step, we randomly sample a training stage $i\\in [2, 8]$. The model is trained to maximize the acoustic tokens from the $i$-th quantizer codebook. The acoustic tokens from stage $1$ to stage $i\u22121$ are embedded and summed up as model input: $$ \\begin{align}e_{c_{t,j}}&amp;=W_a^j\\odot c_{t,j}\\tag{4}\\\\mathbf{e_{c_t}}&amp;=\\sum_{j=1}^{i-1}e_{c_t,j}\\tag{5}\\end{align} $$</p> <p>where $\\odot$ indicates index selection.</p> <p>The phoneme sequence is also regarded as the prompt of the language model. Besides, to clone the unique voice of the given speaker, we also use the acoustic tokens from the enrolled speech as the acoustic prompt. Specifically, we first tokenize the enrolled speech with the neural codec model as $\\tilde{C}^{T\\times 8}$. The embedded representations from all of the eight codebooks are summed up as the acoustic prompt $e_{\\tilde{c}t}=\\sum{j=1}^8 e_{\\tilde{c}{t,j}}$. To predict the acoustic tokens from thei-th codebook, the transformer input is the concatenation of $(\\mathbf{e}{\\mathbf{x}}, \\mathbf{e}{\\tilde{c}}, \\mathbf{e}{c_{:,&lt;i}})$. The positional embeddings are also computed separately for prompts and the acoustic sequence. The current stage $i$ is injected into the network with Adaptive Layer Normalization [Xu et al., 2019] operator, i.e., $\\text{AdaLN}(h, i) = a_i\\text{LayerNorm}(h) + b_i$, where $h$ is the intermediate activations, $a_i$ and $b_i$ are obtained from a linear projection of the stage embedding. Unlike AR, the NAR model allows each token to attend to all the input tokens in the self-attention layer. We also share the parameters of the acoustic embedding layer and the output prediction layer, which means the weights of thej-th prediction layer are the same as the $(j + 1)$-th acoustic embedding layer.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#43inference-in-context-learning-via-prompting","title":"4.3.Inference: In-Context Learning via Prompting","text":"<p>In-context learning is a surprising ability of the text-based language model, which is able to predict labels for unseen inputs without additional parameter updates. For TTS, if the model can synthesize high-quality speech for unseen speakers without fine-tuning, the model is believed to have in-context learning capability. However, the in-context learning capability of existing TTS systems is not strong,because they either require additional fine-tuning or degrade dramatically for unseen speakers.</p> <p>For language models, prompting is necessary to enable in-context learning in the zero-shot scenario. We design prompts and inference as follows. We first convert the text into a phoneme sequence and encode the enrolled recording into an acoustic matrix, forming the phoneme prompt and acoustic prompt. Both prompts are used in the AR and NAR models. For the AR model, we use sampling-based decoding conditioned on the prompts since we observe that beam search may lead the LM into an infinity loop. Furthermore, the sampling-based method could significantly increase the diversity of the output. For the NAR model, we use greedy decoding to choose the token with the highest probability. Finally, we use the neural codec decoder to generate the waveform conditioned on the eight code sequences.</p> <p>The acoustic prompt may or may not semantically relate to the speech to be synthesized, resulting in two cases: - VALL-E:  Our main interest is to generate given content for unseen speakers. The model is given a text sentence, a segment of enrolled speech, and its corresponding transcription. We prepend the transcription phoneme of the enrolled speech to the phoneme sequence of the given sentence as the phoneme prompt, and use the first layer acoustic token of the enrolled speech\u02dcc:,1as an acoustic prefix. With the phoneme prompt and the acoustic prefix, VALL-E generates the acoustic tokens for the given text cloning the voice of this speaker. - VALL-E-continual:  In this setting, we use the whole transcription and the first 3 seconds of the utterance as the phoneme and acoustic prompts respectively, and ask the model to generate the continuations. The inference process is the same as setting VALL-E, except that the enrolled speech and the generated speech are semantically continuous.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#5experiment","title":"5.Experiment","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#51experiment-setup","title":"5.1.Experiment Setup","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#52librispeech-evaluation","title":"5.2.LibriSpeech Evaluation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#53vctk-evaluation","title":"5.3.VCTK Evaluation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#54qualitative-analysis","title":"5.4.Qualitative Analysis","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.01_VALL-E/#6conclusion-limitations-future-work","title":"6.Conclusion, Limitations, Future Work","text":"<p>We introduced VALL-E, a language model approach for TTS with audio codec codes as intermediate representations.  We pre-train VALL-E with 60K hours of speech data, and show the in-context learning capability in zero-shot scenarios.  We achieve new state-of-the-art zero-shot TTS results on LibriSpeech and VCTK.  Furthermore, VALL-E could keep the acoustic environment and speaker\u2019s emotion in synthesis, and provide diverse outputs in different sampling-based decoding processes.</p> <p>Despite making significant progress, VALL-E still suffers from several issues.</p> <p>Synthesis robustness We observe that some words may be unclear, missed, or duplicated in speech synthesis.  It is mainly because the phoneme-to-acoustic language part is an autoregressive model, in which disordered attention alignments exist and no constraints to solving the issue.  The phenomenon is also observed in vanilla Transformer-based TTS, which was addressed by applying non-autoregressive models or modifying the attention mechanism in modeling.  In the future, we would like to leverage these techniques to solve the issue.</p> <p>Data coverage Even if we use 60K hours of data for training, it still cannot cover everyone\u2019s voice,especially accent speakers.  The worse result on VCTK than LibriSpeech also implies insufficient coverage of accent speakers. Moreover, the diversity of speaking styles is not enough, as LibriLight is an audiobook dataset, in which most utterances are in reading style.  In the future, we will further scale up the training data to improve the model performance across prosody, speaking style, and speaker similarity perspectives.  We believe the zero-shot TTS task could be almost solved through our approach with model and data scale-up.</p> <p>Model Structure Now, we use two models to predict codes of different quantizers.  A promising direction is to predict them with a large universal model.  Another interesting direction is using full NAR models to speed up model inference in the framework.</p> <p>Broader impacts Since VALL-E could synthesize speech that maintains speaker identity, it may carry potential risks in misuse of the model, such as spoofing voice identification or impersonating a specific speaker.  To mitigate such risks, it is possible to build a detection model to discriminate whether an audio clip was synthesized by VALL-E.  We will also put Microsoft AI Principles\u2217into practice when further developing the models.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u7f16\u89e3\u7801\u5668_Codec","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.03_VALL-E_X/","title":"Speak Foreign Languages with Your Own Voice:Cross-Lingual Neural Codec Language Modeling","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec","\u8de8\u8bed\u79cd_Cross-Lingual"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.03_VALL-E_X/#abstract","title":"Abstract","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec","\u8de8\u8bed\u79cd_Cross-Lingual"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.03_VALL-E_X/#1introduction","title":"1.Introduction","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec","\u8de8\u8bed\u79cd_Cross-Lingual"]},{"location":"TTS/Papers/2023.01_VALL-E/2023.03_VALL-E_X/#2related-works","title":"2.Related Works","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec","\u8de8\u8bed\u79cd_Cross-Lingual"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/","title":"ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering","text":"\u4f5c\u8005 \u673a\u6784 Yakun Song \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 Zhuo Chen Microsoft \u738b\u6653\u98de Microsoft \u9a6c\u5b50\u9633 \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 \u9648\u8c10 \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#abstract","title":"Abstract\u00b7\u6458\u8981","text":"<p>The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V1, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups2. Audio samples are available at https://ereboas.github.io/ELLAV/. </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#1introduction","title":"1.Introduction\u00b7\u5f15\u8a00","text":"<p>Recently, deep generative AI has achieved remarkable results in various tasks, leading to the emergence of many transformative real-world applications (Brown et al., 2020; Ramesh et al., 2022; Ho et al., 2020; Rombach et al., 2022; Borsos et al., 2023; Kim et al., 2021; Chiang et al., 2019). With the advancement of generative models, there have been rapid developments in the field of speech synthesis as well. In particular, zero-shot TTS technology has gained increasing attention because it can synthesize high-quality target voices without the need of specified speaker\u2019s training data. As a state-of-the-art generative model family, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song and Ermon, 2020) progressively add noise to the training data and then learn the reverse process to generate samples. By leveraging diffusion models and their variants (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song and Ermon, 2020; Song et al., 2021; Lipman et al., 2023), many works have successfully applied them to the audio domain (Popov et al., 2021; Huang et al., 2022, 2023; Shen et al., 2023). Another major class of generative models is language modeling based on Transformer (Vaswani et al., 2017a). Devlin et al. (2019); Raffel et al. (2020); Lewis et al. (2020) utilize encoder-only or encoder-decoder architectures to build masked language models so that they selectively focus on relevant segments and effectively model relationships in long sequences. However, masked language model often requires fine-tuning to adapt to specific tasks, which can be inconvenient for practical usage and deployment. On the other hand, AR language models use a decoder-only architecture to predict the next token in a sequence as the training objective, which has demonstrated extremely powerful few-shot and zero-shot capabilities in many generative tasks (Brown et al., 2020; Thoppilan et al., 2022; Chowdhery et al., 2023). In light of this, VALL-E (Wang et al., 2023a) and subsequent works (Kharitonov et al., 2023; Rubenstein et al., 2023; Wang et al., 2023b) have successfully employed decoder-only language model for zero-shot TTS. These approaches first quantize the speech signal into a series of discrete acoustic tokens. Subsequently, they employ an AR language model to predict coarse-grained acoustic tokens, eliminating the necessity for explicit duration predictors or speaker encoders. Once trained on a large-scale corpus, such as LibriLight (Kahn et al., 2020), these approaches are capable of synthesizing speech with competitive fidelity and naturalness in a zero-shot manner.</p> <p>While VALL-E and its variants have achieved numerous impressive milestones, they still possess certain limitations that impact practical deployment. For instance, existing methods (Wang et al., 2023a; Kharitonov et al., 2023) directly concatenate phoneme tokens and acoustic tokens as a whole sequence to train language models. In this way, the alignment between audio and phoneme sequences is completely learned through the self-attention in the transformer, making it potentially unstable as self-attention does not explicitly capture the monotonic alignment between audio and phoneme. Additionally, the decoder-only language model architecture can lead to potential attention degradation issues (Fu et al., 2023), where the alignment quality between the target audio sequence and the source phoneme sequence deteriorates as the generated sequence increases, resulting in inaccurate or low-quality speech outputs. Another limitation stems from the nature of AR language modeling. Specifically, given a sequence x, the standard AR language model factorizes the likelihood p(x)over the dimensions of x via the chain rule p(x) = QTt=0p(xt|x&lt;t). AR models predict the current tokens solely based on the historical tokens without users\u2019 control in the inference process, and sometimes generate semantic repetitions or incoherence in the generated output (Yang et al., 2019; Brown et al., 2020). In the TTS task, correspondingly, VALL-E cannot directly determine which segment of the output audio corresponds to which prompt phoneme, thus there is no trivial way to promptly detect and prevent issues occurring in the generation process. These drawbacks can manifest as meaningless phoneme repetitions, transpositions, omissions, or even catastrophic infinite silence, i.e., during the process of generation, the model anomalously outputs silence or noise tokens for an extended period of time without stopping. Specifically, Tab.01 demonstrates the word error rate (WER) and the probability of the infinite silence in VALL-E samples at different threshold top-p for nuclear sampling (Holtzman et al., 2019). The detailed experimental setup is described in Section 4. Notably, a shift in the decoding strategy of VALL-E from fully sampling-based to fully greedy-based leads to a marked decline in sample quality. It should be emphasized that while sampling-based stochastic decoding strategies have advantages in terms of synthesis diversity, deterministic decoding strategies (e.g., beam search and its variants) are more suitable for cases where there is less tolerance for synthesis errors and more emphasis on fluency and coherence (Ippolito et al., 2019).</p> <p>Faced with the pros and cons of the existing methods, we introduce ELLA-V, a simple but effective language model approach for zero-shot TTS. ELLA-V proposes a generalized AR (GAR) language model to generate the first layer of residual vector quantizer (RVQ) codes of a neural codec model. Then as with VALL-E, ELLA-V employs a non-autoregressive (NAR) language model to obtain codes of the other RVQs. Our core innovation lies in 3 fold: - Firstly, ELLA-V inserts phone tokens into the corresponding positions of the acoustic sequence. Unlike existing methods, Connecting phoneme tokens with their corresponding acoustic tokens can help the language model capture the alignment between phoneme and acoustic modalities in local dependencies. - Secondly, instead of maximizing the expected log-likelihood of the hybrid sequence under a conventional casual mask or a prefix mask like VALL-E and UniLM (Bao et al., 2020), ELLA-V computes loss only on acoustic tokens and special tokensEndOfPhone( EOP ) andEndOfSentence(EOS). This training objective not only reduces the redundant computation of cross-modal alignment in the output based on experimental results, but also provides a natural way to have fine-grained control in inference: the model predicts EOP , and then the user provides the next phone token. Meanwhile, ELLA-V\u2019s GAR model always maintains awareness of the phoneme it is currently synthesizing, allowing it to promptly detect and truncate any abnormal phoneme to avoid any possible infinite silence issue. - Thirdly, we further propose an improvement to the input sequence. We introduce local advance, which involves shifting the EOP token and the next-word phoneme token a few frames ahead. Intuitively, the pronunciation of a phoneme, especially its ending, is not only influenced by the context in history but also by the upcoming phonemes. By advancing these special tokens, the GAR model can better utilize local dependencies to predict the pronunciation of the current phoneme.</p> <p>Experimental results, using comparable model configurations and 960 hours of speech data from LibriSpeech (Panayotov et al., 2015) as a training set, demonstrate the superiority of ELLA-V. Compared to the state-of-the-art zero-shot TTS system VALL-E, ELLA-V significantly improves the accuracy of synthesized speech, and demonstrates comparable or superior speaker similarity and speech naturalness on a series of subjective and objective experiments. ELLA-V achieves a WER of 2.28% on the test-clean set of LibriSpeech. Notably, ELLA-V works well on a wide spectrum of decoding strategies \u2013 even greedy decoding, and still has a substantially better speech accuracy than the best of VALL-E. We further conducted ablation experiments to investigate the effects of our proposed modifications. The results indicate that the global advance in ELLA-V significantly improves the model\u2019s performance, while the local advance enhances the stability of the generated output.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#2related-work","title":"2.Related Work\u00b7\u76f8\u5173\u5de5\u4f5c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#language-modeling","title":"Language Modeling\u00b7\u8bed\u8a00\u5efa\u6a21","text":"<p>Recently, language models have garnered increasing interest in both the academic and industrial communities. Compared to models that are confined to specific tasks, language models have been proven to possess the capability to solve a wide array of tasks, shining across various domains such as text (Brown et al., 2020; Chowdhery et al., 2023; Rae et al., 2021; Yu et al., 2022), images (Alayrac et al., 2022; Tsimpoukelli et al., 2021), and videos (Yang et al., 2022; Wang et al., 2022). In the audio domain, AudioLM (Borsos et al., 2023) trains language models on discretized audio tokens, achieving speech synthesis tasks through hierarchical prediction of these tokens. AudioGen (Kreuk et al., 2023) employs an auto-encoding approach to extract discrete encodings of raw audio, and trains a language model conditioned on textual features for controlled audio generation. LM-VC (Wang et al., 2023d) employs three language models\u2014a masked prefix language model, an external LM, and a prefix LM\u2014to achieve zero-shot voice conversion.Kakouros et al. (2023) investigates the role of word surprisal, extracted from language models, in influencing the prosody of speech synthesized by TTS systems. For zero-shot TTS, Wang et al. (2023a) approaches TTS as a conditional language modeling task rather than a continuous signal regression. By employing discrete audio codes obtained from pre-trained neural codec, it trains a discrete audio language model, achieving improved naturalness in speech and preservation of speaker characteristics. VALL-E-X (Zhang et al., 2023) extends VALL-E by utilizing source language speech and target language text as prompts when predicting the acoustic marker sequence of the target language speech. This approach supports high-quality zero-shot cross-lingual voice synthesis. These methods require only a single utterance of an unknown speaker as a prompt to generate high-quality, specified speech.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#speech-synthesis","title":"Speech Synthesis\u00b7\u8bed\u97f3\u5408\u6210","text":"<p>Speech synthesis has long been a significant topic in the fields of artificial intelligence, natural language processing, and speech processing. Early methods were based on Statistical Parametric Speech Synthesis (SPSS) (Zen et al., 2009), typically involving complex components such as text analysis models, acoustic models, and vocoders (e.g., hidden Markov model(HMM) (Yoshimura et al., 1999) based). While cost-effective in terms of data, the generated speech of SPSS still exhibited noticeable differences from natural human speech. With the advancement of modern neural networks, some work initially replaced HMMs with recurrent neural networks (RNNs) but still followed the SPSS paradigm (Fan et al., 2014; Zen and Sak, 2015; Valentini-Botinhao et al., 2016). Later, end-to-end neural TTS models were introduced, which synthesize Mel spectrograms and employ a vocoder (Oord et al., 2017; Prenger et al., 2019) for speech synthesis (Wang et al., 2017; Ar\u0131k et al., 2017; Ren et al., 2019). Some methods, utilizing techniques such as VAE (Hsu et al., 2019; Lee et al., 2022), flow (Miao et al., 2020; Kim et al., 2020), diffusion (Jeong et al., 2021; Kim et al., 2022; Popov et al., 2021), and others (Wu and Shi, 2022), have achieved promising performance in end-to-end speech synthesis.On the other hand, models like VALL-E (Wang et al., 2023a) and AudioLM (Borsos et al., 2023) utilize autoregressive Transformers to model discrete audio tokens, achieving great in-context learning performance.When it comes to zero-shot speech synthesis, autoregressive Transformer-based models can predict and generate audio without the need for an additional duration model, which strikes a favorable balance between efficiency and performance, and has been garnering increasing attention.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#3method","title":"3.Method\u00b7\u65b9\u6cd5","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#31overview","title":"3.1.Overview\u00b7\u6982\u89c8","text":"<p>Fig.01 demonstrates the overall architecture of ELLA-V. ELLA-V primarily follows a two-stage framework similar to VALL-E, considering zero-shot TTS as a conditional codec language modeling task. ELLA-V maps input text prompts and speech prompts into a unified vocabulary space with a text encoder and a neural codec, respectively. Different from VALL-E, an additional sequence order rearranging step is performed to the text-audio token sequence, after which, ELLA-V utilizes a decoder-only language model to learn to perform conditional generation on the hybrid sequences of phoneme and audio tokens. Detailed information about the language model will be presented in Section 3.2. To obtain discrete audio representations, we employ a pre-trained neural audio codec model, EnCodec (D\u00e9fossez et al., 2023), following VALL-E (Wang et al., 2023a). EnCodec transforms 24 kHz raw waveforms into 75 Hz discrete tokens usingLRVQ layers. The discrete acoustic tokens have a hierarchical structure, where the first layer quantizer contains semantic information and coarse-grained acoustic contours, while subsequent L \u2212 1quantizers learn fine-grained acoustic details. In our experiments, we use the same settings as VALL-E, withL = 8. For each quantizer, we set the codebook size to 1024. In this setting, each second of the waveform is represented by75 \u00d7 8 discrete tokens from RVQ.</p> <p>To obtain phoneme sequences, we apply the Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) to the input audio and text transcriptions. Notably, MFA not only serves as a text tokenizer but also extracts alignment relationships between phonemes and the corresponding speech. The forced alignment information is essential for ELLA-V to change sequence order.In Section 3.2, we will provide a detailed explanation of how this information is used to construct the target sequence.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#32training-codec-language-model","title":"3.2.Training: Codec Language Model\u00b7\u8bad\u7ec3\uff1a\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b","text":"<p>ELLA-V employs a Generalized Autoregressive Codec language model for the prediction of the first quantization layer in the EnCodec, which corresponds to capturing semantic information and coarse-grained acoustic profiles. Subsequently, a non-autoregressive language model is utilized to generate codes for the subsequent quantization layers, aimed at reconstructing fine-grained acoustic details. Specifically, given a speech corpus D = {xi, yi}, wherexrepresents an audio sample, andyis its text transcription. We utilize the EnCodec to extract the discrete representation ofx, formulated as  whereCrepresents the two-dimensional acoustic code matrix, andTis the downsampled utterance length. We employ MFA to obtain the phoneme sequenceP1:ncorresponding to the transcriptiony, while also extracting forced alignment information between the audio x and the transcription y:  wherenis the number of phonemes of the audio samplex, andlidenotes the length of thei-th phoneme of the discrete audio sequence. MFA treats silence also as a kind of phoneme, so that the original audio sequence is partitioned intonconsecutive intervals corresponding tonphonemes. Specifically, let\u27e8Ci\u27e9li\u00d78represent the audio sequence corresponding to the i-th phoneme:</p> <p>After quantization, we utilize the EnCodec decoder to reconstruct the audio waveform from the discrete acoustic sequenceC, formulated as \u02c6x \u2248 DeCodec(C). For the zero-shot TTS task, the optimization objective is max p(C|P, \u02c6C), where\u02c6Cis the acoustic prompt of the unseen speaker. We use language modeling to generate acoustic tokens for the unseen speaker, by learning on the mixed sequence composed of phonemes and codec codes, consistent with previous works (Wang et al., 2023a; Rubenstein et al., 2023). Unlike existing approaches, ELLA-V does not concatenate phoneme tokens and acoustic tokens directly to form the target sequence for training the language model. Instead, ELLA-V interleaves phoneme and acoustic tokens in order to make it easier for language models to learn the alignment between audio and text. Specifically, we insert each phoneme tokenPi(except the silence phoneme) into the corresponding position of the audio sequence, so that each phoneme\u2019s audio\u27e8Ci\u27e9 is sandwiched between Pi and EOP tokens. We also prepend the phoneme sequence to the beginning of the mixed sequence, which is referred to as global advance. In Section 3.4, we further propose a variant sequence order with higher generation stability, named local advance, which moves the non-acoustic tokens of the sequence several frames forward.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#321","title":"3.2.1.","text":"<p>Generalized Autoregressive Codec (GAR) Codec Language Model\u00b7\u901a\u7528\u81ea\u56de\u5f52\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b</p> <p>As shown in Fig.02, ELLA-V first constructs a hybrid sequenceH:,1of acoustic and phoneme tokens, structured as:</p> <p>It is worth noting that the MFA (Montreal Forced Aligner) treats silence as a distinct phoneme, whereas our phoneme sequencePexclusively comprises phonemes other than silence. To clarify, we retain the acoustic component associated with silence but do not sandwich it with an EOP and a specific silence phoneme, nor do we use a silence phoneme in the global advance part. We design a GAR language model to learn the continuation task on the aforementioned hybrid sequence, to generate the discrete acoustic code sequenceC:,1. The GAR model consists of multiple Transformer decoder layers (Vaswani et al., 2017b). After training, it can generate discrete audio codes for a specified text prompt and acoustic prompt. GAR is also responsible for predicting EOP and EOSto indicate the conclusion of a phoneme and the entire sentence, respectively. The optimization of GAR is achieved by maximizing the likelihood of the acoustic partC:,1of the hybrid sequenceH:,1, as well as the special EOP andEOStokens. Under forward factorization, this process is formulated as:</p> <p>where H has a size ofTH\u00d7 8,{P}denotes the phoneme set,\ufffd\u02dcCi\ufffdis the concatenation of \u27e8Ci\u27e9 along with its broadcast trailing EOP and/or EOStokens,\u02dcCis then the concatenation of\u27e8Ci\u27e9, and \u03b8 GAR represents neural network parameters of GAR model.The factorization of the training objective naturally encapsulates the core intuition of the GAR model: GAR generates the audio sequence phoneme-by-phoneme. GAR produces maximum likelihood predictions for each phoneme token successively, indicating the end of generating a specified phoneme by predicting EOP . Through global advancement, GAR can directly infer the next phoneme to be generated without relying on network predictions. After the prediction for the last phoneme is completed, GAR stops the generation process by predictingEOS. The generated sequence by GAR is self-aligned, as it can instantly know the corresponding position of any generated acoustic token in relation to the phoneme prompt. During training, we apply a bidirectional mask to the phoneme sequence before the BOS in the hybrid sequence, while a unidirectional mask is used for the part after BOS . We frame the training as a next-token-prediction language modeling task on the hybrid sequence. However, it\u2019s important to note that the model does not predict phonemes (or BOS). In other words, as shown in Fig.02, we only compute loss when the token to be predicted is not a phoneme (or BOS). During inference, whenever the model predicts an EOP for a phoneme, the next phoneme token is directly appended to the end of the sequence, which will be further discussed in Section 4.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#322non-autoregressive-nar-codec-language-model","title":"3.2.2.Non-Autoregressive (NAR) Codec Language Model\u00b7\u975e\u81ea\u56de\u5f52\u7f16\u89e3\u7801\u8bed\u8a00\u6a21\u578b","text":"<p>In the second stage, the NAR language model is employed to predict the codes from the second to the last quantization layers in parallel. The input-output sequence construction of the NAR model follows the same pattern as used in the GAR model discussed in Section 3.2.1. Specifically, the i-th columnH:,iof the hybrid sequence matrixHis structured as:</p> <p>And in practice if Pi represents the silence,C:,i will not be sandwiched by Pi and EOP . The NAR model takes the previously generated hybrid sequence of the previous j \u2212 1layers as input and predicts the codes of the j-th layer in parallel, formulated as:</p> <p>where {C:,j} denotes the acoustic token set of the j-th quantizer. In this formulation, The embeddings of tokens from the previous j \u2212 1quantizers are summed up to feed the NAR model to predict the j-th layer. Intuitively, both the GAR and NAR model of ELLA-V compute the loss on the acoustic tokens of the target sequence, and GAR additionally computes loss for EOP and EOS.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#33inference","title":"3.3.Inference\u00b7\u63a8\u7406","text":"<p>ELLA-V can use a short clip of speech from an unseen speaker as an acoustic prompt to synthesize speech for a specified text prompt. Fig.03 illustrates the inference process of the GAR model. While VALL-E may get stuck in an infinite loop during inference, resulting in the synthesis of either infinite silence or repetitive pronunciation (Wang et al., 2023a), ELLA-V is capable of generating EOP and promptly truncating abnormally long phonemes. Following an EOP , we can directly append the next phoneme token to the end of the generated sequence, ensuring the proper generation of speech without abnormal pauses or repetitions. For the GAR model, we employ a sampling-based decoding strategy, whereas for the NAR model, we use a greedy decoding approach to strike a balance between efficiency and performance.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#34local-advance","title":"3.4.Local Advance\u00b7\u5c40\u90e8\u8fdb\u6b65","text":"<p>One intuition is that the pronunciation of a phoneme is strongly related to the pronunciation of the phonemes just before and after it. However, due to the autoregressive nature of the GAR model, an acoustic token cannot attend to the following phoneme tokens, even though we can leverage the transformer\u2019s ability to model long-term dependencies through global advance to provide complete context for the acoustic token generation. To further harness the powerful capability of the transformer in modeling local dependencies, ELLA-V introduces an additional change in the sequence order based on Section 3.2. Specifically, we move the phoneme token and the EOP token ahead by a few frames, referred to as local advance.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#4experiment","title":"4.Experiment\u00b7\u5b9e\u9a8c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#41experi-mental-setup","title":"4.1.Experi-mental Setup\u00b7\u5b9e\u9a8c\u8bbe\u7f6e","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#data-tasks","title":"Data &amp; Tasks\u00b7\u6570\u636e\u4e0e\u4efb\u52a1","text":"<p>We trained ELLA-V using the Librispeech (Panayotov et al., 2015) 960h training dataset. We utilized Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) to obtain forced alignment information for the audio-transcription pairs. Sentences with unrecognized or unknown phones by MFA were excluded. The open-source 24kHz checkpoint3of EnCodec(D\u00e9fossez et al., 2023) was used as the codec to generate discrete acoustic tokens. The LibriSpeech training data was upsampled to 24 kHz before feeding it into EnCodec. In evaluating the model, two zero-shot TTS tasks were considered. For the zero-shot TTS continuation task, we adhered to methodologies established by previous works (Wang et al., 2023a; Le et al., 2023; Wang et al., 2023c), selecting examples ranging from 4 seconds to 10 seconds from the LibriSpeech test-clean dataset as our test set. In this task, we used the complete phoneme transcription as the text prompt and the first 3 seconds of the test audio sample as the acoustic prompt. The model was required to generate continuations. For the zero-shot TTS cross-speaker task, we designed a hard case set comprising 100 hard sentences, as outlined in the demo page . These sentences included challenging phonetic patterns, alliteration, and unusual (abnormal) combinations of words that might pose difficulties for a TTS system to generate natural-sounding speech. In this case, we randomly picked 3-second sentences from the LibriSpeech test-clean subset as the acoustic prompt. We then concatenated the transcription of this segment and the target phoneme sequence in the hard case set to form the text prompt. The model was tasked with cloning the voice of the speaker to say the specified target text in the hard case set.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#training-configuration","title":"Training Configuration\u00b7\u8bad\u7ec3\u914d\u7f6e","text":"<p>For both GAR and NAR models, we stacked 12 Transformer decoder layers with an embedding dimension of 1024, a hidden state dimension of 1024, and a feed-forward layer dimension of 4096. All models were trained in parallel using 8 NVIDIA Tesla V100 GPUs with a batch size of 16384 tokens for GAR and 12288 tokens for NAR per GPU, respectively, learning a total of 320k steps. We used the AdamW optimizer with \u03b21= 0.9,\u03b22= 0.999,\u03f5 = 10\u22129. We employed an inverse-sqrt learning rate scheduler with warm-up. For the first32000updates, we linearly increased the learning rate from10\u22127to a peak of 5 \u00d7 10\u22124. The weight decay was 0.01.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#baseline","title":"Baseline\u00b7\u57fa\u7ebf","text":"<p>In our research, we benchmarked the performance of zero-shot speech synthesis against VALL-E (Wang et al., 2023a). This system was originally trained on a substantial 60k hours of audio from the Librilight dataset (Kahn et al., 2020). To ensure a rigorous evaluation, we reproduced the VALL-E model and adapted it to train on the LibriSpeech 960h dataset. We also adjusted the model dimensions and the number of layers to match the parameter settings of ELLA-V and VALL-E. Both GAR (or AR) and NAR models of VALL-E and ELLA-V have 154.3M parameters. Moreover, to mitigate any potential bias introduced by the audio codec, we pre-processed the authentic speech samples using EnCodec\u2019s encoder and decoder. We also include the result for Encodec reconstructed speech for reference, denoted as Ground-Truth Encodec.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#evaluation-metrics","title":"Evaluation Metrics\u00b7\u8bc4\u4f30\u6307\u6807","text":"<p>We evaluated our system with several objective metrics. Speaker similarity (SPK) and WER served as our primary measures. SPK was assessed using the fine-tuned WavLMTDNN model4(Chen et al., 2022), scoring similarity on a scale of -1 to 1, with values above 0.86 indicate the same speaker identity (This value comes from the release model card page). The WER was determined by comparing the synthesized speech to the original text using the Conformer-Transducer model5(Gulati et al., 2020). In addition to these standard metrics, we introduced two novel measures: INF% and CUT%. INF% quantified the frequency of generating infinitely long audio, indicative of a failure in synthesis. It is used to measure the likelihood of the model falling into abnormal repetition (such as infinite silence). A higher INF% indicates poorer stability in the generated output of the model. In the practical implementation, INF% referred to the proportion of sentences for which generation was not stopped when the length of the generated audio reached twice the original, serving as a proxy for infinite generation. On the other hand, as discussed in the previous session, the design of ELLA-V enables the control of the duration for each phoneme during inference, thus avoiding the synthesis failure. In our experiments, we forcibly truncate the synthesis of phonemes with a length greater than 0.4 seconds. CUT% is used to measure the frequency of forced cuts of phonemes in synthesis by ELLA-V. For each objective metric, we reported average values over three experimental runs with different random seeds. For subjective analysis, we relied on the mean opinion score (MOS). 30 test samples were chosen for this purpose, with each sample being evaluated by at least 15 listeners for aspects like naturalness and speaker similarity. The comparative mean option score (CMOS) and the similarity mean option score (SMOS) were the key subjective metrics used. SMOS was rated on a 1 to 5 scale, in 0.5point increments, to gauge speaker similarity, while CMOS, ranging from -1 to 1, assessed the overall naturalness and quality of the synthesized speech against the baseline.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#42results","title":"4.2.Results\u00b7\u7ed3\u679c","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#zero-shot-tts-continuation-tasktts","title":"Zero-Shot TTS Continuation Task\u00b7\u96f6\u6837\u672cTTS\u7eed\u5199\u4efb\u52a1","text":"<p>We present the evaluation results in Tab.02, where a comparison between ELLA-V and VALL-E is shown. First, regarding speaker similarity, both subjective (SMOS) and objective (SPK) results indicate that ELLA-V and VALL-E performed similarly, which can be attributed to their shared backbone approach, combining (G)AR and NAR. Meanwhile, CMOS testing shows that ELLA-V achieved a +0.10 score, demonstrating a higher generation quality (i.e., naturalness) compared to VALL-E. Additionally, WERs calculated between the recognized text of synthesized audio and the ground-truth text show that ELLA-V is significantly better than VALL-E (2.28 versus 5.00). This underscores ELLA-V\u2019s enhanced capability in synthesizing higher-quality and more robust speech. Overall, ELLA-V substantially improved the synthesis accuracy and robustness of the language model-based TTS framework without affecting the naturalness and speaker similarity. This conclusion is not only corroborated by this easy continuation task, but also validated via the challenging synthesis sets in the subsequent section.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#zero-shot-tts-cross-speaker-task-on-hard-casestts","title":"Zero-Shot TTS Cross-Speaker Task on hard cases\u00b7\u96f6\u6837\u672cTTS\u8de8\u8bf4\u8bdd\u4efb\u52a1(\u56f0\u96be\u6848\u4f8b)","text":"<p>VALL-E utilized a traditional AR model that frequently resulted in alignment errors, including repetitions, transpositions, and omissions, particularly in more challenging synthesis cases (see Section 4.1 for details of the challenging synthesis set). Tab.03 presents the WER comparison of VALL-E and ELLA-V on the 100 particularly hard synthesis sentences. In contrast to VALL-E, ELLA-V demonstrates markedly lower WER, signifying its enhanced robustness. This substantial reduction in errors translates to more accurate and reliable voice synthesis applications, significantly improving user experience in real-world scenarios. Regarding VALL-E\u2019s tendency to fall into infinite silence, an intuitive explanation is that the silence patterns in the training data are relatively simple and many of them are repetitive. In this case, a traditional language model is prone to overfitting to these patterns. During testing, when the model encounters silence, it assigns a high probability to silence. This leads to issues such as beam search, which is based on maximum likelihood, getting stuck in a loop. However, ELLA-V does not face this problem.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#analysis-of-decoding-strategies","title":"Analysis of Decoding Strategies\u00b7\u89e3\u7801\u7b56\u7565\u5206\u6790","text":"<p>To demonstrate the stability of ELLA-V under different decoding strategies, we conducted an ablation study, testing the decoding performance with different top-p values for nuclear sampling, by varyingp \u2208 {1, 0.99, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0(greedy)}.The results are shown in Fig.05. We can observe that as top_p decreases, the accuracy of VALL-E\u2019s synthesized speech significantly decreases. At this point, VALL-E is more prone to generating a large number of overfit silence tokens, leading to a significant increase in INF%. And compared to VALL-E, the audio synthesized by ELLA-V is less sensitive to rate changes in the top_p sampling strategy, whose WER consistently outperforms VALL-E. When the local advance is set to 5 or 10 tokens, the generated audio exhibits significant stronger robustness. On the other hand, as shown in Fig.05 (right), as top_p decreases, VALL-E tends to get stuck in infinite loops of failed generation, while the generation of ELLA-V remains significantly stable. Moreover, ELLA-V can promptly handle (truncate) the synthesis of exceptional phonemes, resulting in significantly higher robustness.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#ablation-study","title":"Ablation Study\u00b7\u6d88\u878d\u5b9e\u9a8c","text":"<p>In this paragraph, we conduct ablation experiments. (1) To investigate the impact of global phoneme information on synthesized speech, we removed the global phoneme sequence at the beginning of the trained sequence (abbr. ELLA-V-noglobal). (2) To investigate whether it is necessary to provide the specific phoneme token before its corresponding acoustic tokens during both training and inference, rather than just using the EOP separator, we removed all phoneme tokens following BOS in the mixed sequence (abbr. ELLA-V-nophn). The experimental results are shown in Tab.04. It is observed that the accuracy of synthesized speech significantly deteriorated either when global phoneme tokens were not used or when local phoneme tokens were disabled within the hybrid sequence. It is also notable that even in the absence of global advance (i.e., in the ELLA-V-no global configuration), the SPK and WER of the synthesized audio were comparable to those of VALL-E. These findings indicate the importance of both local and global information in achieving more accurate synthesized audios, meanwhile, combining both of them potentially leads to further enhancements in accuracy.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.01_ELLA-V/2024.01_ELLA-V/#5conclusion","title":"5.Conclusion\u00b7\u7ed3\u8bba","text":"<p>In this paper, we introduce ELLA-V, a simple and efficient two-stage zero-shot TTS framework based on language modeling. By learning interleaved sequences of acoustic and text tokens, our proposed GAR model can provide fine-grained control over synthesized audio at the phoneme level and can better leverage local dependencies to predict the pronunciation of the current phoneme. Experimental results demonstrate that ELLA-V achieves higher accuracy and more stable results under different threshold top-p for nuclear sampling. We aspire for this work to advance research in enhancing the robustness of speech generation.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u96f6\u6837\u672c_Zero-Shot","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/","title":"PAVITS: Exploring Prosody-Aware VITS for End-to-End Emotional Voice Conversion","text":"","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#abstract","title":"Abstract","text":"<p>In this paper, we propose Prosody-Aware VITS (PAVITS) for emotional voice conversion (EVC), aiming to achieve two major objectives of EVC: high content naturalness and high emotional naturalness, which are crucial for meeting the demands of human perception. To improve the content naturalness of converted audio, we have developed an end-to-end EVC architecture inspired by the high audio quality of VITS. By seamlessly integrating an acoustic converter and vocoder, we effectively address the common issue of mismatch between emotional prosody training and run-time conversion that is prevalent in existing EVC models. To further enhance the emotional naturalness, we introduce an emotion descriptor to model the subtle prosody variations of different speech emotions. Additionally, we propose a prosody predictor, which predicts prosody features from text based on the provided emotion label. Notably, we introduce a prosody alignment loss to establish a connection between latent prosody features from two distinct modalities, ensuring effective training. Experimental results show that the performance of PAVITS is superior to the state-of-the-art EVC methods. Speech Samples are available at https://jeremychee4.github.io/pavits4EVC/.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#1introduction","title":"1.Introduction","text":"<p>Emotional voice conversion (EVC) endeavors to transform the state of a spoken utterance from one emotion to another, while preserving the linguistic content and speaker identity [1]. It brings the capability to facilitate emotional communication between individuals [2], enhancing the user experience in human-computer interaction [3], and even achieving a seamless integration of human presence within the virtual world [4].</p> <p>There are two distinct challenges in EVC: one is low content naturalness, and the other is that the converted audio lacks the richness of emotion compared to human voice [1]. Previous studies were focused on frame-based solutions, such as CycleGAN [5] and StarGAN [6,7]. However, due to the fixed-length nature and poor training stability, the naturalness of converted audio is quite low to apply in practice. To address this challenge, autoencoder-based [8,9] especially for sequence-to-sequence (seq2seq) [10,11] frameworks raise much interests for its variable-length speech generation. It achieves an acceptable naturalness through the joint training with Text-to-speech (TTS) [12], which is used to capture linguistic information and avoid mispronunciation as well as skipping-words. Since speech emotion is inherently supra-segmental [13], it is difficult to learn emotional representation from the spectrogram. To tackle this, various pretraining methods, such as leveraging speech emotion recognition (SER) model [14] and 2-stage training strategy [15], are introduced to extract emotional feature for EVC system.</p> <p>Despite these works have achieved great success in EVC, the converted audio still falls short in meeting human\u2019s perceptual needs, which implies that these two challenges still remain to be effectively addressed. Remarkably, current EVC models generally operate in a cascade manner, i.e., the acoustic converter and the vocoder [1, 5, 7, 8], resulting in a mismatch between emotional prosody training and run-time conversion, ultimately leading to a degradation in audio quality, which is vital to evaluate content naturalness and impacts the perceptual experience of emotional utterance. However, there is no EVC model that attempt to bridge this gap, let alone models that aim to capture prosody variations at a finer granularity. To handle the similar issue, multiple solutions have been explored in TTS, including FastSpeech2s (2020), EATS [17], VITS (2021) [19], etc., seeking to alleviate the mismatch between acoustic feature generation and waveform reconstruction by integrating these two stages together.</p> <p>In this paper, inspired by the high audio quality of VITS (2021), we propose Prosody-Aware VITS (PAVITS) for EVC, a novel end-to-end system with implicit prosody modeling to enhance content naturalness and emotional naturalness. To our best knowledge, PAVITS is the first EVC method in solving the mismatch between acoustic feature conversion and waveform reconstruction. Compared to original VITS (2021), our approach involves several key innovations. In order to improve content naturalness with speech quality, we build upon VITS (2021) to solve the two-stage mismatch in EVC, and apply multi-task learning since TTS can significantly reduce the mispronunciation. To enhance emotional naturalness, we introduce an emotion descriptor to capture prosody differences associated with different emotional states in speech. By utilizing Valence-Arousal-Dominance values as condition, emotional representation at utterance-level is learned. Latent code is further refined by a prosody integrator, which incorporates with speaker identity and linguistic content to model finer-grained prosody variations. Then frame-level prosody features are obtained from normalizing flow. We also introduce a prosody predictor that leverages emotion labels and phoneme-level text embedding to predict frame-level emotional prosody features. Finally, we devise a prosody alignment loss to connect two modalities, aligning prosody features obtained from audio and text, respectively.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#2proposed-method","title":"2.Proposed Method","text":"<p>As shown in Fig.01, inspired by VITS (2021), the proposed model is constructed based on conditional variational autoencoder (CVAE), consisting of four parts: a textual prosody prediction module, an acoustic prosody modeling module, an information alignment module, and an emotional speech synthesis module.</p> <p>The textual prosody prediction (TPP) module predicts the prior distribution $p(z_1|c_1)$ as:</p> <p>$$     z_1= TPP (c_1) \\sim p (z_1| c_1)\\tag{1}  $$</p> <p>where $c_1$ including text $t$ and emotion label $e$.</p> <p>The acoustic prosody modeling (APM) module disentangles emotional features with intricate prosody variation, speaker identity, and linguistic content from the source audio given emotion label, forming the posterior distribution $q(z_2|c_2)$ as: </p> <p>$$     z_2= APM (c_2) \\sim q (z_2|c_2)\\tag{2}  $$</p> <p>where $c_2$ including audio $y$ and emotion label $e$.</p> <p>The information alignment module facilitates the alignment of text and speech, as well as the alignment of textual and acoustic prosody representations. In emotional speech synthesis (ESS) module, the decoder reconstructs waveform $\\hat{y}$ according to latent representation $z$.</p> <p>$$     \\hat{y} = Decoder (z) \\sim p (y | z)\\tag{3} $$</p> <p>where $z$ comes from $z_1$ or $z_2$.</p> <p>While the proposed model can perform both EVC and emotional TTS after training, EVC will be the main focus of this paper. In the following, we will introduce the details of the four modules.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#21textual-prosody-prediction-module","title":"2.1.Textual Prosody Prediction Module","text":"<p>Given condition $c_1$ including text $t$ and emotion label $e$, the textual prosody prediction module provides the prior distribution $p(z_1|c_1)$ of CVAE. The text encoder takes phonemes as input and extracts linguistic information $h_{text}$ at first. Considering the extensive prosody variation associated with each phoneme, we employ a prosody predictor to extend the representation to frame-level and predict the prosody variation (a fine-grained prior normal distribution with mean $\\mu_{\\theta}$ and variance $\\sigma_{\\theta}$ generated by a normalizing flow $f_{\\theta}$) based on emotion label. </p> <p>$$     p(z_1|c_1) = \\mathcal{N}(f_{\\theta}(z_1); \\mu_{\\theta}(c_1);\\sigma_{\\theta}(c_1))\\left|\\det\\dfrac{\\partial f_{\\theta}(z_1)}{\\partial z}\\right|\\tag{4} $$</p> <p>Text Encoder: Since the training process is constrained by the volume of textual content within parallel datasets, we initially convert text or characters into a phoneme sequence as a preprocessing step to maximize the utility of the available data, resulting in improved compatibility with the acoustic prosody modeling module. Similar to VITS (2021), text encoder comprises multiple Feed-Forward Transformer (FFT) blocks with a linear projection layer for representing linguistic information.</p> <p>Prosody Predictor: Prosody predictor leverages phoneme-level linguistic information extracted by the text encoder to anticipate frame-level prosody variation given discrete emotion label. It has been observed that simply increasing the depth of stacked flow does not yield satisfactory emotional prosody variations, unlike the prosody predictor. Therefore, the inclusion of the prosody predictor guarantees a continuous enhancement in prosody modeling for both the TPP and APM modules. The prosody predictor comprises multiple one-dimensional convolution layers and a linear projection layer. Furthermore, we integrate predicted emotional prosody information with linguistic information as input for the duration predictor, which significantly benefits the modeling of emotional speech duration.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#22acoustic-prosody-modeling-module","title":"2.2.Acoustic Prosody Modeling Module","text":"<p>The acoustic prosody modeling module provides emotional features with fine-grained prosody variation based on dimensional emotion representation, i.e., Valence-Arousal-Dominance values.Speaker identity and speech content information are also disentangled from the source audio and then complete feature fusion through the prosody integrator as the posterior distribution $q (z_2|c_2)$. </p> <p>$$     q(z_2|c_2) = \\mathcal{N}(f_{\\theta}(z_2); \\mu_{\\theta}(c_2);\\sigma_{\\theta}(c_2))\\tag{5} $$</p> <p>Speaker encoder: Considering the APM module\u2019s increased focus on understanding emotional prosody more thoroughly compared to previous models, it\u2019s apparent that speaker characteristics could unintentionally be overlooked during conversion.Recognizing the critical role of fundamental frequency (F0) in speaker modeling [20], we augment the F0 predictor of VISinger (2021) by adding multiple one-dimensional convolutional layers and a linear layer to construct the speaker encoder, which tackles the issue effectively.</p> <p>Emotion descriptor: To enhance PAVITS\u2019s emotional naturalness, we employ a specific SER system rooted in Russell\u2019s circumplex theory [22] to predict dimensional emotion representation, encompassing Valence-Arousal-Dominance values as a conditional input. This input guides the capture of nuanced prosody variations, which ensures that while satisfying human perception of emotions at utterance-level, natural prosody variations are retained from segment-level down to frame-level, preserving intricate details. It consists of a SER module [23] and a linear projection layer.</p> <p>Prosody Integrator: The prosody integrator incorporates a combination of speaker identity attributes, emotional prosody characteristics, and intrinsic content properties extracted from the linear spectrogram. It is constructed using multiple convolution layers, WaveNet residual blocks, and a linear projection layer.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#23information-alignment-module","title":"2.3.Information Alignment Module","text":"<p>In VITS (2021), the existing alignment mechanism, which is called Monotonic Alignment Search (MAS), solely relies on textual and acoustic features from parallel datasets. Thus, it is insufficient in capturing emotional prosody nuances, hindering effective linkage between the TPP and APM modules. To overcome this limitation, we propose an additional prosody alignment loss function based on Kullback-Leibler divergence, to facilitate joint training for frame-level prosody modeling across the TPP and APM modules, with the goal of enhancing prosody information integration and synchronization within our model.</p> <p>$$     L_{psd} = D_{KL}(q(z_2|c+2)| p(z_1|c_1))\\tag{6} $$</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#24emotional-speech-synthesis-module","title":"2.4.Emotional Speech Synthesis Module","text":"<p>In the emotional speech synthesis module, the decoder generates a waveform based on latent $z$, employing adversarial learning to continuously enhance naturalness in both content and emotion. To improve the naturalness of content, $L_{recon-cls}$ minimizes the $L_1$ distance between predicted and target spectrograms, $L_{recon-fm}$ minimizes the $L_1$ distance between feature maps extracted from intermediate layers in each discriminator, aimed at enhancing training stability. Since the former predominantly influences the early-to-mid stage, while the latter assumes a more prominent role in mid-to-late stage, we introduce two coefficients to balance their contributions as follows.</p> <p>$$     L_{recon}= \\gamma L_{recon-cls}+ \\beta L_{recon-fm}(G)\\tag{7} $$</p> <p>To enhance the perception of emotions, $L_{emo-cls}$ represents the loss function for emotional classification, while $L_{emo-fm}$ denotes the loss associated with feature mapping for emotion discrimination.</p> <p>$$     L_{emo}= L_{emo-cls}+ \\beta L_{emo-fm}(G)\\tag{8} $$</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#25final-loss","title":"2.5.Final Loss","text":"<p>By combining CVAE with adversarial training, we formulate the overall loss function as follows:</p> <p>$$ \\begin{align}     L &amp;= L_{recon}+ L_{adv}(G) + L_{emo}+ L_{psd}+ L_{F0}+ L_{dur}\\tag{9}\\     L(D) &amp;= L_{adv}(D)\\tag{10} \\end{align} $$</p> <p>where $L_{adv}(G)$ and $L_{adv}(D)$ represent the adversarial loss for the Generator and Discriminator respectively, $L_{F0}$ minimizes the $L_2$ distance between the predicted F0 and corresponding ground truth, $L_{dur}$ minimizes the $L_2$ distance between the predicted duration and ground truth which is obtained through estimated alignment.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#26run-time-conversion","title":"2.6.Run-Time Conversion","text":"<p>At runtime, there are two converting methods: a fixed-length approach (Audio-$z_2$-Audio, named PAVITS-FL) and a variable-length approach (Audio-Text-$z_1$-Audio, named PAVITS-VL). The former uses APM module for latent $z$ prediction from audio, ensuring robustness as it remains unaffected by text encoding, but is constrained by a fixed spectrum length due to Dynamic Time Warping (DTW) limitations. The latter employs TPP module to predict latent $z$ from corresponding text obtained through automatic speech recognition (ASR) technique, which is not bound by duration modeling and offers greater naturalness. Finally, the ESS module\u2019s decoder takes latent $z$ (either $z_1$ or $z_2$) as input and synthesizes the converted waveform without a separate vocoder.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#3experiments","title":"3.Experiments","text":"","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#31datasets","title":"3.1.Datasets","text":"<p>We perform emotional conversion on a Mandarin corpus belonged to Emotional Speech Dataset (ESD) [24] from neutral to angry, happy, sad, and surprise, denoted as Neu-Ang, Neu-Hap, Neu-Sad, Neu-Sur respectively. For each emotion pair, we use 300 utterances for training, 30 utterances for evaluation, and 20 utterances for test. The total duration of training data is around 80 minutes (16 minutes per emotion category), which is absolutely small compared to others.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#32experimental-setup","title":"3.2.Experimental Setup","text":"<p>We train the following models for comparison. - CycleGAN [25] (baseline): CycleGAN-based EVC model with WORLD vocoder. - StarGAN [26] (baseline): StarGAN-based EVC model with WORLD vocoder. - Seq2seq-WA2 [15] (baseline): Seq2seq-based EVC model employing 2-stage training strategy with WaveRNN vocoder. - VITS (2021) (baseline):EVC model constructed by original VITS, operating independently in both fixed-length and variable-length, take the average as the result. - PAVITS-FL (proposed): the proposed model based on VITS, incorporates all the contributions outlined in the paper, but operate within a fixed-length framework. - PAVITS-VL (proposed): the proposed model based on VITS, incorporates all the contributions outlined in the paper, but operate within a variable-length framework leveraging ASR to obtain text from source audio.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#33results-discussion","title":"3.3.Results &amp; Discussion","text":"<p>Mel-cepstral distortion (MCD) was calculated for objective evaluation, as depicted in Tab.01.In terms of subjective evaluation, Mean Opinion Score (MOS) tests were conducted to appraise both the quality and naturalness of speech as shown in Tab.02. The naturalness score was derived by averaging the scores for content naturalness and emotional prosody naturalness, as rated by 24 participants, each of whom assessed a total of 148 utterances. We further report emotional similarity results between converted audio and human voice to gauge emotional naturalness as illustrated in Fig.02.</p> <p>Through the above-mentioned metrics, it is obvious that the proposed PAVITS achieves competitive performance on both objective and subjective evaluation. From the perspective of objective MCD and subjective MOS, both original VITS and our proposed PAVITS models always outperform other models with traditional vocoder or neural vocoder, which proves that the integration of neural acoustic converter and vocoder is suitable for EVC task to enhance speech quality and naturalness. It is worth noting that even in the case of the fixed-length PAVITS-FL model, there is a reduction of over 0.4 in MCD when compared to the variable-length seq2seq model and the original VITS model. Furthermore, there has been an enhancement of 0.6 and 0.2 in MOS, respectively. To some extent, it reflects how human tend to be influenced by audio quality when assessing model naturalness, especially when there are significant differences in quality being compared.</p> <p>As depicted in Fig.02, our proposed PAVITS-VL (variable-length) model aligns more closely with human perception in the converted audio, which attributed to the model\u2019s capacity for fine-grained granularity in modeling speech emotion, incorporating implicit prosody cues.To further show the effectiveness of our method, we visualize the spectrogram of testing clips, as exemplified in Fig.03. It is readily apparent that the spectrogram converted by PAVITS exhibits finer details in prosody variations within the pertinent frequency bands, while simultaneously preserving descriptive information for other frequency bands. Consequently, the audio generated by PAVITS possesses a prosody naturalness and emotional accuracy that closely approximates the ground truth spectrogram.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#34ablation-study","title":"3.4.Ablation Study","text":"<p>We further conduct an ablation study to validate different contributions.We remove prosody predictor, prosody alignment, and prosody integrator in turn and let the subjects evaluate quality and naturalness of converted audio. From Tab.03, we can see that all scores are degraded with the removal of different components. When remove prosody predictor, the speech quality does not undergo significant changes, as the original VITS primarily relies on textual features as input. However, a significant decrease in naturalness is observed, attributed to the loss of explicit emotion label for TPP module as a conditioning factor. This highlights the importance of aligning with APM module on the basis of information asymmetry, which reflects the ingenious design of prosody modeling structure. Note that the performance of PAVITS is worse than VITS after deleting prosody alignment, it might be attributed the fact that latent prosody representations are not constrained during training, which damages the original MAS mechanism present in VITS. To further show the contribution from the prosody integrator, we replace it with a simple concatenation. Both speech quality and naturalness show a slight decrease, indicating that utilizing prosody integrator for information fusion is quite effective for APM module.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.03_PAVITS/2024.03_PAVITS/#4conclusion","title":"4.Conclusion","text":"<p>In this paper, we propose Prosody-Aware VITS (PAVITS) for emotional voice conversion (EVC). By integrating acoustic prosody modeling (APM) module with textual prosody prediction (TPP) module through prosody alignment, the fine-grained emotional prosody features across various scales of emotional speech can be learned effectively. Experimental results on ESD corpus demonstrate the superiority of our proposed PAVITS for content naturalness and emotional naturalness, even when dealing with limited data scenarios. In the future, we will explore the controllable emotional prosody modeling to allow better interpretability of EVC.</p>","tags":["\u7aef\u5230\u7aef_End-to-End","\u58f0\u97f3\u514b\u9686_VC","\u60c5\u611f_Emotional","\u58f0\u97f3\u8f6c\u6362_VoiceCoversion"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/","title":"CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models","text":"<p>Xiang Li, Fan Bu, Ambuj Mehrish, Yingting Li, Jiale Han, Bo Cheng, Soujanya Poria</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#abstract","title":"Abstract","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#1introduction","title":"1.Introduction","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#2related-work","title":"2.Related Work","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#3background-consistency-models","title":"3.Background: Consistency Models","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#4cm-tts","title":"4.CM-TTS","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#5experiments","title":"5.Experiments","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_CM-TTS/2024.04_CM-TTS/#6results-and-discussion","title":"6.Results and Discussion","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5b9e\u65f6_RealTime","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/","title":"HyperTTS: Parameter Efficient Adaptation in Text-to-Speech Using Hypernetworks","text":"<p>Yingting Li, Rishabh Bhardwaj, Ambuj Mehrish, Bo Cheng, Soujanya Poria</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#abstract","title":"Abstract","text":"<p>Neural speech synthesis, or text-to-speech (TTS), aims to transform a signal from the text domain to the speech domain. While developing TTS architectures that train and test on the same set of speakers has seen significant improvements, out-of-domain speaker performance still faces enormous limitations. Domain adaptation on a new set of speakers can be achieved by fine-tuning the whole model for each new domain, thus making it parameter-inefficient. This problem can be solved by Adapters that provide a parameter-efficient alternative to domain adaptation. Although famous in NLP, speech synthesis has not seen much improvement from Adapters. In this work, we present HyperTTS, which comprises a small learnable network, \"hypernetwork\", that generates parameters of the Adapter blocks, allowing us to condition Adapters on speaker representations and making them dynamic. Extensive evaluations of two domain adaptation settings demonstrate its effectiveness in achieving state-of-the-art performance in the parameter-efficient regime. We also compare different variants of HyperTTS, comparing them with baselines in different studies.Promising results on the dynamic adaptation of adapter parameters using hypernetworks open up new avenues for domain-generic multi-speaker TTS systems. The audio samples and code are available at https://github.com/declare-lab/HyperTTS.</p> <p>\u795e\u7ecf\u8bed\u97f3\u5408\u6210\u6216\u6587\u672c\u8f6c\u8bed\u97f3\u7684\u76ee\u7684\u662f\u5c06\u6587\u672c\u57df\u7684\u4fe1\u53f7\u8f6c\u6362\u4e3a\u8bed\u97f3\u57df. \u867d\u7136\u5f00\u53d1\u5728\u76f8\u540c\u7684\u8bf4\u8bdd\u4eba\u96c6\u5408\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7684\u8bed\u97f3\u5408\u6210\u67b6\u6784\u5df2\u7ecf\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb, \u4f46\u5bf9\u4e8e\u57df\u5916\u8bf4\u8bdd\u4eba\u7684\u6027\u80fd\u4ecd\u7136\u9762\u4e34\u7740\u5de8\u5927\u7684\u9650\u5236. \u8fd9\u4e00\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7 Adapter, \u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u9886\u57df\u81ea\u9002\u5e94\u6765\u89e3\u51b3. \u5c3d\u7ba1\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5f88\u6709\u540d, \u4f46\u8bed\u97f3\u5408\u6210\u8fd8\u6ca1\u6709\u4ece Adapter \u4e2d\u83b7\u5f97\u592a\u591a\u7684\u6539\u8fdb. \u5728\u672c\u6587\u4e2d, \u6211\u4eec\u63d0\u51fa\u4e86 HyperTTS, \u5b83\u7531\u4e00\u4e2a\u5c0f\u578b\u53ef\u5b66\u4e60\u7684\u7f51\u7edc, \"\u8d85\u7f51\u7edc\", \u751f\u6210 Adapter \u5757\u7684\u53c2\u6570, \u5141\u8bb8\u6211\u4eec\u6839\u636e\u8bf4\u8bdd\u4eba\u8868\u793a\u6765\u6761\u4ef6\u5316\u9002\u914d\u5668, \u5e76\u4f7f\u5176\u52a8\u6001. \u5728\u4e24\u4e2a\u9886\u57df\u81ea\u9002\u5e94\u8bbe\u7f6e\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d, \u6211\u4eec\u8bc1\u660e\u4e86 HyperTTS \u5728\u53c2\u6570\u9ad8\u6548\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd. \u6211\u4eec\u8fd8\u6bd4\u8f83\u4e86 HyperTTS \u7684\u4e0d\u540c\u53d8\u4f53, \u4e0e\u4e0d\u540c\u7814\u7a76\u4e2d\u7684\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83. \u901a\u8fc7\u4f7f\u7528\u8d85\u7f51\u7edc\u6765\u52a8\u6001\u9002\u914d\u5668\u53c2\u6570\u7684\u53ef\u884c\u6027, \u6211\u4eec\u5f00\u8f9f\u4e86\u65b0\u7684\u591a\u8bf4\u8bdd\u4eba TTS \u7cfb\u7edf\u7684\u9886\u57df\u901a\u7528\u9053\u8def.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#1introduction","title":"1.Introduction","text":"<p>Neural text-to-speech (TTS) synthesis has trans formed our interactions with digital content by converting text into natural-sounding speech.Cur rent TTS systems are often limited to predefined speaker styles or specific sets of speaker IDs (Ren et al., 2019a), reducing their utility in multi-speaker environments with unseen speakers. To make TTS scalable and economical, parameter-efficient adaptation of such systems to new speakers is an important, but highly challenging problem (Li et al., 2023b). Zero-shot and few-shot speaker adaptation techniques (Shen et al., 2023; Li et al., 2023a; Casanova et al., 2021; Cooper et al., 2020; Casanova et al., 2022; Shen et al., 2023) have gained prominence in the domain of TTS, aiming at accommodating new speakers and styles with limited speaker-specific data. While these methods excel in scenarios with constrained data, it\u2019s important to note that when sufficient data is available, fine-tuning the model offers distinct advantages. Fine-tuning allows for highly personalized and tailored speech synthesis, precise control over the alignment of synthesized speech with the speaker\u2019s characteristics, and the production of higher-quality, more natural-sounding speech. In this paper, we assume sufficient availability of data from the adaptation domain.When adapting a multi-speaker TTS model (backbone model) to a target domain, the traditional approach involves complete fine-tuning of the entire back bone (Figure 1-Fine-tuning).However, this approach is resource-intensive, requiring separate copies of model parameters for each new target domain. To make the adaptation scalable, recent research has introduced parameter-efficient do main adaptation methods using Adapters, as seen in NLP (Houlsby et al., 2019) and speech (Li et al., 2023b). Adapters incorporate small blocks of learn able dense layers into each block of the backbone model, with the aim of learning additional parameters while keeping the main model parameters fixed (Figure 1-AdapterTTS). Despite the advantages demonstrated by adapters in various NLP tasks, their direct application in adapting a TTS backbone to a target domain has shown limited improvements (Li et al., 2023b) Since learning a generic TTS system that works well across different speaker styles is a more difficult problem than learning one network per speaker (Ren et al., 2019a, 2021), we hypothesize the same is the case with adapters. Forcing a static set of adapter parameters to perform well across multiple speakers of the adaptation domain can be challenging and potentially infeasible due to under parameterization (Mehrish et al., 2023a; Biadsy et al., 2022). In this paper, we present HyperTTS, a pioneer ing approach for the parameter-efficient adaptation of TTS models to new speakers.This method conditions adapters on speaker embeddings, expanding the learnable parameter space through a \"hypernetwork\". The main highlights of HyperTTS are:. 1. Dynamic Adapters: Instead of keeping the adapters static, for each speaker in the adaptation domain, HyperTTS learns speaker adaptive adapters.Adapter conditioning on speaker representations is observed to unlock adapter capabilities and make them performant which was a challenge with static adapters (Li et al., 2023b). 2. Parameter Sampling: A large set of speak ers makes it infeasible to keep the space of adapter parameters discrete. To facilitate this, we employ parameter sampling from a continuous distribution defined by a learnable hyper network. 3. ParameterEfficiency: Compared to parameter-expensive fine-tuning, it achieves competitive results with less than1% of the backbone parameters, making it highly practical and resource-friendly for scalable applications.</p> <p>We perform a comprehensive set of experiments to showcase HyperTTS\u2019s effectiveness (see Figure 1) compared to traditional methods like static bottleneck adapters (AdapterTTS) and full model fine-tuning (TTS-FT). Our experiments cover datasets from diverse environmental conditions, such as LibriTTS and VCTK, representing various accents from different regions. Results highlight HyperTTS\u2019s parameter-efficient performance advantages over the baselines across both objective and subjective metrics.Notably, HyperTTS can even surpass fine-tuning in performance with only a 20% increase in parameters (Table 6-HyperTTS<sub>e/v/d</sub>). A key strength of HyperTTS lies in its remarkable parameter efficiency: it achieves results within 1 point of fine-tuning while using less than 1% of the parameter count in the backbone. This practical and resource-friendly approach enables real-world applications.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#2related-work","title":"2.Related Work","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#text-to-speech-models","title":"Text-to-Speech Models","text":"<p>The rise of deep learning has transformed TTS technology, with neural network-based architectures like Tacotron (2017); Tacotron2 (2017), FastSpeech2 (2020), and Transformer-TTS (2018) leading the way. These models represent significant progress in TTS, leveraging deep learning techniques.</p> <p>Autoregressive TTS models (Tacotron (2017); Flowtron (2020); FastSpeech; FastSpeech2 (2020); Glow-TTS (2020); FastPitch (2020)), while effective, face limitations in maintaining alignment in long utterances and exhibit slower training and inference speeds with longer sequences.</p> <p>In contrast, non-autoregressive (parallel) models separate phoneme duration estimation from decoding, reducing latency and enhancing training efficiency. These models typically rely on external aligners or pre-trained autoregressive models for phoneme duration. To achieve training efficiency and support end-to-end TTS, this paper focuses on a non-autoregressive TTS model with an alignment framework based on the RAD-TTS (2022) alignment learning objective.</p> <p>Recently, several speech models have been compared to GPT in natural language processing, with a focus on in-context learning for speech. Notably, VALL-E (2023) and SPEAR-TTS (2023) leverage emerging codecs to learn discrete speech tokens and employ a vocoder-like decodec to convert these tokens into waveforms. Meanwhile, Voicebox (2023), inspired by flow-matching and aligned with the Fastspeech (2019) framework, utilizes continuous features like Mel spectrogram and HiFi-GAN (2020).</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#speaker-adaptation-in-tts","title":"Speaker Adaptation in TTS","text":"<p>Speaker adaptation is a crucial aspect of TTS systems, aiming to personalize the synthesized speech by modify ing the voice characteristics to match those of a specific target speaker. Over the years, various techniques and approaches have been proposed to address the challenges associated with speaker adaptation in TTS (Jia et al., 2018; Chen et al.; Min et al., 2021; Hsieh et al., 2022; Gabry\u00b4s et al.2022). Furthermore, several studies have focused on exploring parameter-efficient methods for adapt ing TTS to new sets of speakers, addressing the need for effective adaptation in diverse speaker scenarios. These approaches aim to accommodate a wide range of linguistic variations (Pamisetty et al., 2023; Do et al., 2022), including diverse ac cents (Yang et al., 2023), speakers (Luo et al., 2021; Miao et al., 2021; Mehrish et al., 2023a), and low-resource scenarios introduced by the tar get domain (Azizah and Jatmiko, 2022; Mehrish et al., 2023a; Lux and Vu, 2022), while maintain ing the number of trainable parameters. HYPER TTS primarily focuses on contributing in the line of parameter-efficient domain adaptation of the back bone TTS model to a target set of speakers.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#dynamic-parameters","title":"Dynamic Parameters","text":"<p>Parameter generation, although not popular in speech, has been used in various forms in other domains, such as Klein et al. (2015); Riegler et al. (2015) in NLP and Ha et al. (2017) in computer vision. Specific to adapters, Bhardwaj et al. (2022); Chen et al. (2020) make prompt tokens dynamic by conditioning their val ues on input text using a parameter prompt generator network, (\u00dcst\u00fcn et al., 2022; Mahabadi et al., 2021) used hypernetworks for generating adapter down and up-projection weights. Shared hypernetworks obviate the need to maintain a separate set of parameters for each task (or new setting) and generate weights for each block of the backbone network (Mahabadi et al., 2021). To the best of our knowledge, this is the first work that studies the utility of a parameter generator in the domain of speech (Mehrish et al., 2023b).</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#3methodology","title":"3.Methodology","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#31encoder","title":"3.1.Encoder","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#32variance-adaptor","title":"3.2.Variance Adaptor","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#duration-predictor","title":"Duration Predictor","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#pitch-predictor","title":"Pitch Predictor","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#energy-predictor","title":"Energy Predictor","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#33mel-decoder-and-postnet","title":"3.3.Mel-Decoder and Postnet","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#34hypernetwork","title":"3.4.Hypernetwork","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#implementation","title":"Implementation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#4experiments","title":"4.Experiments","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#41baseline-models","title":"4.1.Baseline Models","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#tts-0","title":"TTS-0","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#reference-and-reference-voc","title":"Reference and Reference (Voc.)","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#tts-ft-full-fine-tuning","title":"TTS-FT (Full Fine-Tuning)","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#adaptertts","title":"AdapterTTS","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#hypertts","title":"HyperTTS","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#42datasets","title":"4.2.Datasets","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#43model-configuration","title":"4.3.Model Configuration","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#backbone-model-pre-training","title":"Backbone Model Pre-training","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#44evaluation-metrics","title":"4.4.Evaluation Metrics","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#objective-metrics","title":"Objective Metrics","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#subjective-metrics","title":"Subjective Metrics","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#5results-discussions","title":"5.Results &amp; Discussions","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#51subjective-evaluation","title":"5.1.Subjective Evaluation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#52impact-of-parameter-efficiency","title":"5.2.Impact of Parameter Efficiency","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#53output-of-hypernetwork","title":"5.3.Output of Hypernetwork","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#54other-discussions","title":"5.4.Other Discussions","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#layernorms-standard-conditional","title":"Layernorms (Standard &amp; Conditional)","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#low-rank-adaptation","title":"Low-Rank Adaptation","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_HyperTTS/2024.04_HyperTTS/#6conclusion","title":"6.Conclusion","text":"<p>In this paper, we present HyperTTS, an approach that enhances the effectiveness of adapters by conditioning them on speaker embeddings. Utilizing a \"hypernetwork\" to customize adapter block weights for the TTS backbone network, we significantly expand the adapter parameter space. This dynamic method replaces the conventional static adapter parameter set, enabling input-conditioned parameter sampling. Additionally, the hypernetwork\u2019s continuous parameter space theoretically allows the generation of adapter parameters for numerous speakers without increasing hypernetwork parameters. This makes HyperTTS an excellent choice for multi-speaker TTS adaptation, surpassing traditional adapter limitations.</p> <p>Limitations  While hypernetworks exhibit promising enhancements in both adaptation domains, there are training challenges to address. Time and resource constraints may have led to potential underfitting, negatively impacting performance. Additionally, hypernetworks tend to overfit the backbone model on the adaptation domain, warranting further research to enhance their generalizability. Notably, the relatively higher number of parameters in hypernetworks poses potential inefficiency for low-resource training.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u8d85\u7f51\u7edc_HyperNetwork","\u5f00\u6e90_OpenSource"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/","title":"LLaMA-VITS: Enhancing TTS Synthesis with Semantic Awareness","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#abstract","title":"Abstract","text":"<p>Recent advancements in Natural Language Processing (NLP) have seen Large-scale Language Models (LLMs) excel at producing high-quality text for various purposes. Notably, in Text-To-Speech (TTS) systems, the integration of BERT for semantic token generation has underscored the importance of semantic content in producing coherent speech outputs. Despite this, the specific utility of LLMs in enhancing TTS synthesis remains considerably limited. This research introduces an innovative approach, ==LLaMA-VITS==, which enhances TTS synthesis by enriching the semantic content of text using LLM. ==LLaMA-VITS== integrates semantic embeddings from LLaMA2 (2023) with the VITS model, a leading end-to-end TTS framework. By leveraging LLaMA2 (2023) for the primary speech synthesis process, our experiments demonstrate that ==LLaMA-VITS== matches the naturalness of the original VITS (ORI-VITS) and those incorporate BERT (BERT-VITS), on the LJSpeech dataset, a substantial collection of neutral, clear speech. Moreover, our method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem dataset, a curated selection of emotionally consistent speech from the EmoV_DB dataset, highlighting its potential to generate emotive speech.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#1introduction","title":"1.Introduction","text":"<p>Text-to-Speech (TTS) synthesis is a technology that transforms written text into its spoken equivalent, thereby enhancing content accessibility. This technology finds application in the production of audiobooks (Chen et al., 2022) and virtual assistants (Wu et al., 2023). However, traditional TTS models, which primarily focus on the acoustic features, often fall short in comprehending the semantic and emotional information embedded within the text. With the significant advancements in Natural Language Processing (NLP) technologies, particularly through Language Models (LMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018; Brown et al., 2020), which have demonstrated formidable capabilities in understanding and generating natural language, researchers have proposed various BERT-based TTS models (Mukherjee et al., 2022; Abbas et al., 2022; Li et al., 2023; Guo et al., 2022) to improve the expressiveness of synthesized speech. Nonetheless, the effectiveness and flexibility of BERT-based TTS models in diverse applications are limited due to the smaller parameter size of BERT models and the necessity for designing specific fine-tuning tasks to enhance their capabilities. On the other hand, Large-scale Language Models (LLMs), such as LLaMA2 (2023), not only require decreasing computational re sources and achieve higher levels of text generation but also possess excellent zero-shot learning capabilities. Moreover, they can achieve improvements comparable to fine-tuning by adjusting only a minimal number of parameters through prompt tuning (Liu et al., 2022; Tu et al., 2022). However, the potential of these LLMs for TTS tasks has not been fully explored. In light of this context, we introduce ==LLaMA-VITS==, a model that leverages semantic representations extracted from LLaMA2 (2023) on top of a state-of-the-art TTS model, VITS (2021), enabling the generated speech to retain acoustic information while understanding and expressing semantics and emotions. Through comprehensive objective and subjective evaluations, ==LLaMA-VITS== has been verified to surpass TTS baselines without semantic input or those integrated with BERT. The main contributions encapsulate:  - We propose LLaMA-VITS model that utilizes the semantic understanding and expression capabilities of LLaMA2 (2023), offering equal or superior acoustic performance compared to baseline models, along with a significantly enhanced ability to understand and express semantics and emotions. - Through empirical analysis, we demonstrate that global tokens in ==LLaMA-VITS== provide more significant improvements than sequential to kens, contrasting with observations in BERT-based TTS models. - We quantitatively verified our findings using both subjective and objective metrics. Our code, models, audio demos, and the filtered single female speaker emotional dataset EmoV_DB_bea_sem are available at https://github.com/xincanfeng/vitsgpt.git.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#2related-works","title":"2.Related Works","text":"<p>TTS technology has significantly advanced in learning acoustic features through structural evolution. However, comprehending and conveying semantics remain challenging. Since BERT-like LMs have demonstrated profound capabilities in understanding semantics through extensive pre-training on vast text corpora, some studies have integrated BERT-like LMs with TTS technology to enhance synthesized speech. Nonetheless, research on incorporating GPT-like LMs within TTS technology is notably scarce.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#21text-to-speech-models","title":"2.1.Text-to-Speech Models","text":"<p>TTS task aims to generate natural, fluent, and easily comprehensible speech. Traditional TTS systems, e.g., a Statistical Parametric Speech Synthesis (SPSS) system (Taylor, 2009), usually comprise multiple distinct components. These include a frontend module that converts text into linguistic features (such as duration and pitch), an acoustic model that maps these linguistic features to acoustic features, and a vocoder responsible for generating speech waveforms from the acoustic features. Over the past decades, the complexity of traditional models has been notable, attributed to their reliance on manually engineered features and the intricate communication between modules.</p> <p>Transitioning from Hidden Markov Models (HMM) based models (Black et al., 2007), through Deep Neural Networks (DNN) models (Zen et al., 2013), to Generative Adversarial Networks (GAN) based models (Saito et al., 2017), there has been a no table enhancement in voice quality, yet the architectural complexity remains significant.</p> <p>The advent of end-to-end TTS models marks a significant milestone, increasingly reducing the distinction between synthesized speech and human voice. End-to-end models are capable of trans forming raw text directly into final speech output, which not only streamlines the structural complexity of TTS systems and facilitates easier deployment but also significantly reduces the dependency on manual feature engineering, simplifying the training process. Moreover, they notably enhance the naturalness and intelligibility of the speech, thereby be coming the predominant architecture in TTS models. For instance, Char2Wav (2017) introduces an attentive encoder-decoder frame work for direct speech synthesis from text input. Tacotron (2017) undertakes training from the ground up and directly predicts linear spectrograms. Furthermore, the speech produced by Tacotron2 (2017) closely mirrors the natural human voice.</p> <p>In the realm of end-to-end TTS models, many have adopted a non-autoregressive architecture. This architecture enables parallel data processing, where the model\u2019s output generation does not depend on the output of the previous time step, thereby enhancing processing speed. It also circumvents the error accumulation issue inherent in traditional autoregressive models, which significantly boosts TTS performance. FastSpeech (2019) and its variants exemplify this trend. FastSpeech (2019) employs a transformer-based architecture to generate mel-spectrograms in parallel. Building on FastSpeech (2019), FastPitch (2020) predicts pitch contours during inference, enabling the production of more expressive and high quality speech. FastSpeech2 (2020) further incorporates explicit duration prediction and introduces pitch and energy as conditional inputs.</p> <p>Previous non-autoregressive approaches typically involve distinct training phases for acoustic models and vocoders. VITS (2021) introduces a more natural-sounding output compared to these two-stage systems through its one-stage parallel end-to-end architecture. Innovatively, VITS (2021) incorporates variational inference combined with normalizing flows and employs an adversarial training methodology. Due to VITS (2021)\u2019s exemplary performance across multiple benchmarks, we select it as the foundational TTS model for our system.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#22-fine-tuning-bert-like-lms-for-tts","title":"2.2. Fine-tuning BERT-like LMs for TTS","text":"<p>While TTS models have increasingly advanced in replicating acoustic features, insufficient training data can hinder the model\u2019s ability to learn the semantic nuances of the same input across different contexts, thus limiting its expressiveness. Consequently, researchers have turned to leveraging the transfer learning capabilities of BERT-like LMs. Ultimately, TTS systems that incorporate pre-trained and fine-tuned BERT-like LMs have achieved better understandings of semantics and enhanced generated speech, marking a significant advancement.</p> <p>Hayashi et al. (2019) utilized a pre-trained BERT model as an auxiliary input to enhance a Tacotron2 based TTS system, resulting in improved speech naturalness. Similarly, Yang et al. (2019) applied a pre-trained BERT model to achieve enhanced front end accuracy. Kenter et al. (2020) demonstrated that integrating a BERT model, pre-trained on extensive unlabeled data and fine-tuned for speech, into an RNN-based TTS system enhances prosody. Kenter et al. (2020) specifically suggest updating the BERT\u2019s parameters during the training of their RNN-based speech synthesis model, emphasizing the critical role of fine-tuning the BERT component for optimal outcomes. As prompt tuning draws wide attention in guiding text or image generation, PromptTTS (2022) takes a prompt representation with both style and content descriptions from a BERT model as input to generate speech with precise style control and high speech quality.</p> <p>In particular, Mukherjee et al. (2022) utilized a pre-trained BERT model to develop a text emotion classification model, employing the final hidden states of the initial <code>[CLS]</code> token as a comprehensive representation of the text. Researchers such as Kenter et al. (2020); Li et al. (2021); Abbas et al. (2022) have applied word-level BERT to capture the semantic and syntactic structure of sentences, thereby aiding TTS synthesis. Li et al. (2023) introduced a phoneme-level BERT, designed with a preliminary task of predicting corresponding graphemes in addition to regular masked phoneme predictions, to enhance the naturalness of speech synthesized from out-of-distribution (OOD) texts.</p> <p>However, despite BERT\u2019s acknowledged capacity to provide detailed word importance, syn tactic and semantic insights, and general knowledge (Hayashi et al., 2019; Kenter et al., 2020), its effectiveness is constrained by the particularities of fine-tuning approaches. Furthermore, BERT\u2019s inherent non-generative nature might limit its ability to account for information outside the immediate sentence context.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#23integrating-gpt-like-lms-for-tts","title":"2.3.Integrating GPT-like LMs for TTS","text":"<p>Considering semantic understanding and expression capabilities, BERT is primarily utilized for com prehension tasks. In comparison, GPT excels not only in understanding text but also in generating natural and coherent text. Moreover, with the larger model parameters, GPT is particularly adept at zero-shot or few-shot learning, enabling its direct application to various tasks with little to no need for fine-tuning or structural modifications.</p> <p>However, research on leveraging GPT-like models to aid TTS systems is very limited. Stephenson et al. (2021) explores the potential of improving speech synthesis naturalness by text input lookahead with GPT prediction. Such an approach potentially restricts TTS applications, as altering the input is often undesirable. Furthermore, the findings were not verified by human subjective evaluation. Saito et al. (2023) suggest employing ChatGPT to aid in empathetic dialogue speech synthesis by extracting the context of conversations. They particularly instruct ChatGPT to produce three key words that encapsulate the intention, emotion, and speaking Style of speech observed in the dialogue history. These keywords are subsequently utilized to train a speech synthesis model. However, due to the inaccessibility of ChatGPT to the public, the re searchers resort to processing ChatGPT\u2019s outputs with BERT to extract embeddings. This approach essentially positions ChatGPT as an alternative to manual annotation, yet it does not delve into investigating ChatGPT\u2019s internal representations and their potential impact on speech-related tasks.</p> <p>In our study, we selected LLaMA2 (2023), a GPT-like LM, for integration into our TTS system, motivated by its technological advancements and potential for di verse applications. LLaMA2 (2023) stands out as one of the largest publicly accessible LMs, rivaling proprietary models such as GPT3.5 (OpenAI et al., 2024) and PaLM (540B) (Chowdhery et al., 2022), and sur passes other open-source alternatives like MPT and Falcon (Almazrouei et al., 2023) in benchmark evaluations. Additionally, the novel architecture of LLaMA2 (2023) not only ensures enhanced security but also facilitates the extension of various down stream tasks (Touvron et al., 2023).</p> <p>Related research that employs LLaMA2 (2023) in speech and other multimodal tasks (Radhakrishnan et al., 2023; Zhang et al., 2023), coupled with the ongoing efforts to reduce computing costs associated with LLaMA2, underscores the model\u2019s significant research interest and its promising prospects in multimodal applications.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#3methodology","title":"3.Methodology","text":"<p>We propose leveraging semantic embeddings de rived from a GPT-like LM to improve TTS synthesis. In our work, LLaMA2 (2023) is employed as the GPT-like model, as elaborated in Sec.2.3, and VITS (2021) is utilized as the TTS model for generating audio from phoneme embeddings, as detailed in Sec.2.1. In essence, we extract semantic embeddings $E_{s}$ from the final hidden layer of LLaMA2 (2023) and integrate them with the original acoustic text embeddings $E_{a}$ of VITS (2021), forming enhanced text embeddings $E_{as}$ for speech synthesis. Specifically, either a global token or a sequence of tokens is used to encapsulate the semantic attributes of an input sentence for varying objectives. The distinctions between these two token types are further explicated in Sec.3.1.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#31semantic-embeddings-derived-from-llama2","title":"3.1.Semantic Embeddings Derived from LLaMA2","text":"<p>For each input sentence $s$, we extract information from the final hidden layer before the output of LLaMA2 (2023). Different strategies are employed to cre ate various tokens that serve as the semantic em bedding for the sentence.</p> <p>Let $E_{s}$ denote the semantic embedding of sentence $s$, and $H_{LLaMA}^F(s)$ represent the output of the LLaMA2 (2023) model for sentence $s$ at the final hidden layer $F$. Therefore, $E_{s}$ can be expressed as: </p> <p>$$     E_{s}=  H_{LLaMA}^F(s)\\tag{1} $$ </p> <p>Here, $H_{LLaMA}^F(s)$ is a vector that encapsulates the semantic representation of sentence $s$ after pro cessing through all layers of the LLaMA2 (2023), culminating in the final layer.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#formulation-for-global-tokens","title":"Formulation for Global Tokens","text":"<p>We explored five types of global tokens to represent the over arching semantic features of an input sentence, namely <code>[AVE]</code>, <code>[PCA]</code>, <code>[LAST]</code>, <code>[EIS_Word]</code>, and <code>[EIS_Sentence]</code>, with each strategy employing a single token.</p> <p>In the <code>[AVE]</code> strategy, the semantic token is de rived by calculating the average of all tokens\u2019 out put vectors for sentence $s$, formulated as: </p> <p>$$     E_{s}^{AVE}= \\dfrac{1}{n}\\sum_{i=1}^n H_{LLaMA}^F(s,i)\\tag{2}  $$</p> <p>Here, $E_{s}^{AVE}$ denotes the semantic token obtained using the <code>[AVE]</code> strategy, and $H_{LLaMA}^F(s,i)$ represents the output of the $i$ th token of sentence $s$ at the final hidden layer $F$ of LLaMA2, with $s$ comprising $n$ tokens.</p> <p>For the <code>[PCA]</code> strategy, we apply Principal Component Analysis to the output vectors of sentence sto extract principal components and rescale the mean of the PCA results according to the original data\u2019s value range. This rescaling ensures that the PCA-processed data maintains a scale consistent with the original data, preserving the relative importance of semantic information numerically. Formulated as: </p> <p>$$     E_{s}^{PCA}= \\text{PCArescale}(H_{LLaMA}^F(s)) \\tag{3} $$</p> <p>In the <code>[LAST]</code> strategy, the semantic token is obtained by selecting the last token from the output vector of sentence s, as shown in the formula: </p> <p>$$     E_{s}^{LAST}= H_{LLaMA}^F(s, n)\\tag{4}  $$</p> <p>where $H_{LLaMA}^F(s, n)$ refers to the representation of the last token of sentence $s$ after processing through all layers of LLaMA2 (2023) at the final layer.</p> <p>In the <code>[EIS_Word]</code> and <code>[EIS_Sentence]</code> strategies, unlike the above approaches that utilize the sentence itself for representation, we derive the semantic representation of sentence $s$ based on LLaMA2 (2023)\u2019s comprehension $u$. Adapted from Saito et al. (2023)\u2019s practice, we employ prompts as illustrated in 2a and 2b, respectively, to obtain LLaMA2 (2023)\u2019s understanding of sentence $s$ in terms of Emotion, Intention, and speaking Style, denoted as $u$, and calculate the average of this understanding\u2019s representation to serve as the semantic embedding.</p> <p>In the <code>[EIS_Word]</code> strategy, LLaMA2 (2023) is prompted to describe Emotion, Intention, and speaking Style with three separate words, resulting in the following formula for the final semantic token: </p> <p>$$     E_{s}^{\\text{EISWord}} = \\dfrac{1}{m} [\\sum_{i} H_{LLaMA}^{F}(u_E, i) + \\sum_j H_{LLaMA}^{F}(u_I, j) +\\sum_k H_{LLaMA}^{F}(u_S, k)] \\tag{5}  $$</p> <p>where $u_E$, $u_I$, $u_S$ are the representations of LLaMA2 (2023)\u2019s output expressing the sentence\u2019s Emotion, Intention, and speaking Style at the final hid den layer, respectively, with $i$, $j$, $k$ indicating the tokens of each output word, and m being the total number of these tokens.</p> <p>In the <code>[EIS_Sentence]</code> strategy, LLaMA2 (2023) is guided to describe its understanding of the input sentence\u2019s Emotion, Intention, and speaking Style with an easy-to-understand sentence, leading to the fol lowing formula for the final semantic token: </p> <p>$$     E_s^{\\text{EISSentence}} = \\dfrac{1}{m}\\sum_{i=1}^m H_{LLaMA}^{F}(u_{EIS}, i)\\tag{6}  $$</p> <p>where $u_{EIS}$ is the representation of LLaMA2 (2023)\u2019s output expressing the understanding of the original sentence at the final hidden layer, and $m$ is the total number of tokens in this sentence representation.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#formulation-for-sequential-tokens","title":"Formulation for Sequential Tokens","text":"<p>In the implementation of sequential tokens strategies, we focus on utilizing the complete set of tokens from the input sentence to represent its semantic in formation. Unlike the global token approaches, sequential tokens strategies encompass representations based on either text or phonemes, aiming to better align with the TTS model\u2019s potential em phasis on acoustic features. The mathematical representations for these two strategies are as follows:  Under the <code>[TEX]</code> strategy, we directly employ all tokens from the textual form of sentence $s$ to represent its semantic information. If the output of sentence $s$ at the final hidden layer $F$ of LLaMA2 (2023) consists of $n$ tokens, then the semantic token $T_{s}^{TEX}$ is represented as a sequence: </p> <p>$$     E_s^{TEX}= {H_{LLaMA}^F(s,1), H_{LLaMA}^F(s, 2),\\cdots, H_{LLaMA}^F(s, n)} \\tag{7}  $$</p> <p>In the <code>[PHO]</code> strategy, we consider the complete set of tokens from the phonemic form. Here, $s_{pho}$ denotes the phonemic representation of sentence $s$. If the output of $s_{pho}$ at the final hidden layerF of LLaMA2 (2023) comprises $m$ tokens, then the semantic token $T_{s}^{PHO}$ is represented as a sequence: </p> <p>$$     E_{s}^{PHO}={H_{LLaMA}^F(s_{pho}, 1),H_{LLaMA}^F(s_{pho}, 2),\\cdots,H_{LLaMA}^F(s_{pho}, m)}\\tag{8}  $$</p> <p>In both strategies, $H_{LLaMA}^F(s, i)$ and $H_{LLaMA}^F(s_{pho}, i)$ respectively represent the outputs of the $i$ th token of sentence $s$ in its textual and phonemic forms at the final hidden layer $F$ of LLaMA2 (2023). This representation allows the TTS model to leverage the complete semantic information of a sentence, whether based on text or phonemes.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#32fusing-semantic-embeddings-with-acoustic-embeddings","title":"3.2.Fusing Semantic Embeddings with Acoustic Embeddings","text":"<p>To align the dimensions of semantic embedding extracted from LLaMA2 (2023), denoted as $E_{s}$, with the acoustic embeddings from VITS (2021), denoted as $E_{a}$, we employ a linear projection. The original dimension of $E_{s}$, $d_{LLaMA}$, is projected to match the dimension of VITS (2021) acoustic embedding, $d_{VITS}$, using a linear transformation matrix $W$ of dimensions $d_{VITS}\\times d_{LLaMA}$. The projected semantic embedding, $E_s'$, is calculated as follows: </p> <p>$$     E_s'= W \\cdot E_{s} \\tag{9}  $$ </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#fusing-global-embedding-with-acoustic-embedding","title":"Fusing Global Embedding with Acoustic Embedding","text":"<p>To obtain an embedding $E_{as}$ that integrates both semantic and acoustic information, for global tokens, we simply add the dimensionally unified global embedding to VITS (2021)\u2019s acoustic em bedding, as shown in the equation: </p> <p>$$     E_{as} = E_{a} + E_s\u2032\\tag{10} $$ </p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#fusing-sequential-embeddings-to-enhance-text-embeddings","title":"Fusing Sequential Embeddings to Enhance Text Embeddings","text":"<p>We utilize the Scaled Dot Product Attention mechanism to merge sequential embeddings with VITS (2021)\u2019s original acoustic embedding to gain enhanced embedding $E_{as}$ , which can be described by the following mathematical formulas:  First, calculate the attention scores $A$:</p> <p>$$     A = \\dfrac{q\\cdot k^{\\mathsf{T}}}{\\gamma} \\tag{11} $$</p> <p>where $q$ is the acoustic embedding $E_{a}$ in VITS (2021) with dimensions $[b, t, d]$;  $k$ and $v$ denotes the semantic embedding $E_s'$ from LLaMA2 (2023), also with dimensions $[b, t, d]$;  $b$ is the batch size,tis the sequence length, and $d$ is the embedding dimension; $\\gamma$ is temperature for scaling. $k^{\\mathsf{T}}$ denotes the transpose of $k$, transforming $k$ from $[b, t, d]$ to $[b, d, t]$ for matrix multiplication. The resulting $A$ has dimensions $[b, t, t]$.</p> <p>If a source mask or target mask is present, a masking operation is applied, setting the attention scores at masked positions to a very low value (e.g.,\u22126e4) to nearly eliminate their weight contribution in the subsequent softmax step.</p> <p>Next, apply the softmax function and dropout to the attention scores, obtaining the final attention weights $W_{attn}$: </p> <p>$$     W_{attn}= \\text{Dropout}(\\text{Softmax}(A))\\tag{12}  $$</p> <p>Finally, the output $E_{as}$ is calculated by weighting $v$ with the attention weights: </p> <p>$$     E_{as} = W_{attn}\\cdot v $$ </p> <p>The output $E_{as}$ , viewed as text embedding fused with semantic information, has dimensions $[b, t, d]$ that match those of $q$.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#4experiments","title":"4.Experiments","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#41experimental-settings","title":"4.1.Experimental Settings","text":"<p>We propose ==LLaMA-VITS== which uses semantic to kens derived from LLaMA2 (2023) to enhance acoustic embedding in VITS (2021) for better TTS performance. To show the effectiveness of our method, we experimented with two baseline models. In the ORI-VITS baseline, we use the original VITS (2021) without external semantic information. In the BERT-VITS baseline, we extract various semantic tokens according to for mer research introduced in Section \u00a7 2.2. Specifically, we use the <code>[CLS]</code> token of BERT as the global token. To form the baseline of the sequential token in BERT, we use all the tokens in the sentence trained by text or phoneme, named <code>[BERT_TEX]</code> and <code>[BERT_PHO]</code>, respectively. In our proposed ==LLaMA-VITS==, we derive global token <code>[AVE]</code>, <code>[LAST]</code>, <code>[PCA]</code>, <code>[EIS_Word]</code>, and <code>[EIS_Sentence]</code>, and sequential tokens <code>[TEX]</code> and <code>[PHO]</code> from LLaMA2 (2023), cor responding to those in BERT-VITS. We use LLaMA2 (2023) (13b) to generate semantic embeddings of dimension 5120. <code>[CLS]</code> and <code>[BERT_TEX]</code> tokens are extracted from BERT-base-uncased model which has a parameter size of 110M that generates token embedding of 768 dimensions. <code>[BERT_PHO]</code> token is extracted from BERT-x-phone-base model whose parameter size is 88M to generate token embedding of 768 dimensions.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#global-token-extraction","title":"Global Token Extraction","text":"<p>In our proposed ==LLaMA-VITS==, global strategy <code>[LAST]</code> only uses the last to ken in the final hidden layer of LLaMA2 (2023) for each sentence. <code>[AVE]</code> uses the average of all tokens for each sentence. <code>[PCA]</code> uses the concatenation of all tokens whose dimension was reduced by Principal Component Analysis (PCA). <code>[EIS_Word]</code> and <code>[EIS_Sentence]</code> use the average of tokens for an answer, which is formed in three words or a sentence by prompts shown in Fig.02, to describe the Emotion, Intention, and speaking Style of the transcript. In BERT-VITS baseline, global strategy <code>[CLS]</code> only uses the first token from the BERT-base-uncased model for each input sentence.</p> <p></p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#sequential-token-extraction","title":"Sequential Token Extraction","text":"<p>In our proposed ==LLaMA-VITS==, sequential strategy <code>[TEX]</code> concatenates the sequence of tokens in a sentence generated by LLaMA2 (2023) using text input. <code>[PHO]</code> concatenates the sequence of tokens of a sentence generated by LLaMA2 (2023) using phonemic input. In the baseline BERT-VITS, sequential strategy <code>[BERT_TEX]</code> concatenates all the tokens in a sentence extracted from BERT-base-uncased model. <code>[BERT_PHO]</code> concatenates all the tokens in a sentence extracted from BERT-x-phone-base model.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#datasets","title":"Datasets","text":"<p>We utilized full LJSpeech, 1-hour LJSpeech, and EmoV_DB_bea_sem dataset for experimental verification. LJSpeech4comprises 24 hours recorded of English speech by sin gle female speaker, where we evaluate how the embeddings extracted from LLaMA2 (2023) can help improve the speech naturalness.Besides full LJSpeech dataset, we also randomly filtered 1 hour LJSpeech which contains only 1-hour records as an ablation study to show how dataset size influences. EmoV_DB5(Adigwe et al., 2018) is a database of emotional speech that contains data for male and female actors in English and French. EmoV_DB covers 5 emotion classes, amused, an gry, disgusted, neutral, and sleepy. To factor out the effect of different speakers, we filtered the original EmoV_DB dataset into the speech of a specific female English speaker, bea. Then we use LLaMA2 (2023) to predict the emotion label of the transcript cho sen from the above 5 emotion classes, and select the audio samples which has the same predicted emotion. The filtered dataset contains 22.8-min records for training. We named the filtered dataset EmoV_DB_bea_sem and investigated how the semantic embeddings from LLaMA2 (2023) behave in naturalness and expressiveness on it. Please refer to Appendix A 12 for more dataset statistics.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#implementation-hyper-parameters-training","title":"Implementation, Hyper-parameters, Training","text":"<p>Our ==LLaMA-VITS== system was built on the VITS (2021) framework using its original implementation, augmented with semantic embeddings de rived from LLaMA2 (2023) (Touvron et al., 2023) using its original implementation7. For training LJSpeech, we use the public configs in the original implementation of VITS (2021). For EmoV_DB_bea_sem, we use the same config as LJSpeech but changed batch size from 64 to 16 since this dataset is much smaller. Besides implementing our proposed ==LLaMA-VITS==, we extracted corresponding semantic tokens <code>[CLS]</code>, <code>[BERT_TEX]</code> from BERT-uncased-base model and <code>[BERT_PHO]</code> from BERT pre-trained on phoneme for comparison. In comparing the experimental results, we choose 100k-step results on both full LJSpeech and 1-hour LJSpeech datasets since they are rather large. On EmoV_DB_bea_sem, we used the pre-trained checkpoint of LJSpeech on 100k step and compare the fine-tuning results on EmoV_DB_bea_sem at 150k-step since it is rather small.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Both subjective and objective metrics are implemented for a comprehensive evaluation. In subjective evaluation, we con duct Emotion Similarity Mean Opinion Score (ES MOS) (Zhu et al., 2023) experiments to evaluate emotion similarity for EmoV_DB_bea_sem.In the subjective evaluation, we compared <code>[AVE]</code>, <code>[TEX]</code> and <code>[PHO]</code> strategies in our ==LLaMA-VITS== with the corresponding token <code>[CLS]</code>, <code>[BERT_TEX]</code> and <code>[BERT_PHO]</code> extracted from different BERT models and the baseline ORI-VITS who does not con tain semantic tokens, with the ground truth samples GT.</p> <p>In evaluating ESMOS, we randomly chose 5 samples from the total 51 test samples proportionally divided by us and received 100 test results from different speakers on Amazon Mechanical Turk. The result significance level is thus 500. Each participant is asked to give a score on emotion similarity compared with ground truth in a 5-scale: Excellent Match 5, Good Match 4, Fair Match 3, Poor Match 2, Bad Match 1.</p> <p>In objective evaluation,we utilize UTokyo-SaruLab Mean Opinion Score (UTMOS) (Saeki et al., 2022), Mel-Cepstral Distortion (MCD), and speech recognition performance measured by Character Error Rate (CER) and Word Error Rate (WER). UTMOS is a MOS prediction network using speech samples from previous Blizzard Challenges and Voice Conversion Challenges, which has reached the best performance in VoiceMOS Challenge 2022. We evaluate objective intelligibility by using Whisper-large (Radford et al., 2022). For calculating UTMOS, we use the implementation in SpeechMOS. For calculating MCD and ASR, we use the evaluation implementation of ESPnet (Hayashi et al., 2020, 2021).</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#5experiment-results","title":"5.Experiment Results","text":"<p>We evaluated our proposed ==LLaMA-VITS== along with baselines ORI-VITS and BERT-VITS models on three distinct datasets: the full LJSpeech, the 1 hour LJSpeech, and EmoV_DB_bea_sem. The experimental outcomes provide a comprehensive understanding of the model performance and the impact of semantic tokens selection. A summary of these results is articulated below and can be referenced in Table 1.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#51results-on-full-ljspeech","title":"5.1.Results on full LJSpeech","text":"<p>The ORI-VITS baseline, achieving a UTMOS of 4.19 \u00b1 0.05, an MCD of7.32 \u00b1 0.61, a CER of6.2, and a WER of 16.5. Enhancements were observed with the BERT-VITS baseline.Specifically, BERT-VITS with <code>[BERT_TEX]</code> semantic tokens demonstrated superior performance in UTMOS (4.22\u00b10.05) and MCD (7.27 \u00b1 0.61), indicating improved speech quality and reduced mel-cepstral distortion. Additionally, a reduced CER of5.9and WER of15.9were noted, highlighting enhanced automatic speech recognition accuracy. Our proposed ==LLaMA-VITS==, integrating various global and sequential semantic tokens, displayed competitive performance.The <code>[PCA]</code> strategy stood out, achieving an MCD of7.23 \u00b1 0.61, indicating optimal mel-cepstral distortion.The <code>[EIS_Sentence]</code>, <code>[AVE]</code>, and <code>[LAST]</code> tokens yielded a top-tier UTMOS of4.21\u00b10.04/0.05, underscoring their effectiveness in enhancing perceived speech quality.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#52results-on-1-hour-ljspeech","title":"5.2.Results on 1-hour LJSpeech","text":"<p>In the more challenging 1-hour LJSpeech dataset, all models experienced a slight performance de crease, an expected outcome given the reduced training data size. BERT-VITS baseline with <code>[CLS]</code> tokens exhibited notable MCD performance (7.39 \u00b1 0.62), while the <code>[BERT_PHO]</code> excelled in UTMOS (4.05 \u00b1 0.07), reflecting enhanced speech naturalness and reduced mel-cepstral distortion. ==LLaMA-VITS== with <code>[AVE]</code> tokens achieved the high est UTMOS (4.10 \u00b1 0.07), while <code>[EIS_Sentence]</code> tokens resulted in the most favorable MCD (7.36 \u00b1 0.59), illustrating the model\u2019s versatility and efficacy in different token configurations.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#53results-on-emov_db_bea_sem","title":"5.3.Results on EmoV_DB_bea_sem","text":"<p>On this even more challenging dataset, a small improvement observed in BERT-VITS only exists in the <code>[BERT_TEX]</code> with a CER of 4.4. While our proposed ==LLaMA-VITS== displayed no table enhancements. The <code>[TEX]</code> strategy achieves an ESMOS of3.22 \u00b1 0.07, indicating much more emotiveness. The <code>[LAST]</code> yielded the best performance on CER of4.3and WER of17.4, other strategies also perform better than or comparable to BERT-VITS, underscoring its effectiveness in enhancing perceived speech expressiveness.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#54analysis","title":"5.4.Analysis","text":"<p>Speaking of the strengths of different tokens, BERT based tokens generally contribute to improving MCD and ASR scores, indicating the enriched semantic understanding translated to speech qual ity. Tokens of ==LLaMA-VITS== exhibited a balanced performance across all metrics, with specific to ken configurations excelling in particular aspects. For instance, <code>[PCA]</code> token emerged as a strong contender in reducing MCD, <code>[AVE]</code> enhanced the UTMOS scores, <code>[TEX]</code> had superior performance to improve ESMOS score. In individual comparisons, ==LLaMA-VITS==\u2019s five global tokens generally outperformed BERT-VITS on the UTMOS metric for naturalness. In the ESMOS metric for emotional expression, ==LLaMA-VITS==\u2019s two sequential tokens also generally sur passed BERT-VITS, particularly the <code>[TEX]</code> token. Therefore, we can infer that GPT-like LMs may have greater potential for TTS tasks than BERT like models. Further, our results reflect different patterns of gains from GPT-like and BERT-like models in TTS tasks. For instance, in the UTMOS naturalness metric, ==LLaMA-VITS==\u2019s global tokens often outperformed sequential tokens, which is the opposite for BERT-VITS; in the ESMOS emotion metric, ==LLaMA-VITS==\u2019s sequential token <code>[TEX]</code> significantly outperformed other tokens, while for BERT-VITS, global tokens performed better. Overall, ==LLaMA-VITS== showed a different pattern in UTMOS compared to BERT-VITS, and superior performance in ESMOS. These results highlight the potential for further exploration of semantic to ken types and fusion methods to achieve more significant enhancements in speech synthesis, particularly in scenarios constrained by limited and complex training data.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#6discussion","title":"6.Discussion","text":"<p>In this section, we discuss factors influencing current outcomes.Based on this discussion, we also point out the directions for future work in Appendix 13.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#61gpt-like-vs-bert-like","title":"6.1.GPT-like vs BERT-like","text":"<p>Initial observations from our experiments indicate that, even without any fine-tuning of LLaMA2 (2023), ==LLaMA-VITS== significantly outperforms both BERT-VITS and ORI-VITS in terms of emotional expressive ness. This finding opens up avenues for future research into emotive TTS tasks. Furthermore, a comparison between BERT-VITS and ==LLaMA-VITS== highlights their distinct performance traits. BERT-VITS, leveraging deep con textual embeddings, provides profound semantic insights yet encounters challenges in customization and adaptability across a range of TTS tasks. Conversely, ==LLaMA-VITS== can provide a more versa tile and adaptable approach, with its array of token types demonstrating particular advantages across various evaluation metrics.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#62semantic-token-strategy","title":"6.2.Semantic Token Strategy","text":"<p>The varying effectiveness of distinct semantic to kens underscores the importance of careful selection and integration tailored to the particular goals of TTS systems. Optimizing the type of token and method of fusion can be instrumental in enhancing aspects such as speech naturalness, emotional expressiveness, Mel Cepstral Distortion (MCD), or Automatic Speech Recognition (ASR) performance.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#7conclusion","title":"7.Conclusion","text":"<p>In summary, this study exemplifies a significant stride towards optimized TTS synthesis by integrating semantic tokens, leveraging the strengths of ==LLaMA-VITS==. Our findings, validated by comprehensive experiments on the LJSpeech and EmoV_DB_bea_sem datasets, underscore the pivotal role of semantic embeddings in enhancing speech quality, naturalness, and emotiveness. The adaptability and efficacy of ==LLaMA-VITS==, especially, open new vistas for customized and context sensitive TTS applications.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_LLaMA-VITS/2024.04_LLaMA-VITS/#8limitations","title":"8.Limitations","text":"<p>Compared with our baseline which uses different BERT models, we only tested our method using LLaMA2 (2023). As Kenter et al. (2020) indicate for their BERT-based TTS model, small BERT models work better than big ones, but the parameter size of our proposed GPT-based TTS influence is yet stud ied by our research. Although BERT-based TTS models are normally finetuned on speech tasks to provide more explicit acoustic information for TTS, we didn\u2019t try designing prompts to generate acoustic features and only studied how general semantic information can help. Our experiments were conducted only on clean datasets with limited size, and the effect on more complex datasets is to be further explored. The integration of LLaMA2 (2023)\u2019s embeddings introduces additional computational costs, potentially limiting real-time applications.</p> <p>\u76f8\u6bd4\u4e8e\u57fa\u7ebf\u6a21\u578b\u4f7f\u7528\u4e86\u4e0d\u540c\u7684 BERT \u6a21\u578b, \u6211\u4eec\u7684\u65b9\u6cd5\u4ec5\u4f7f\u7528\u4e86 LLaMA2.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","\u5927\u8bed\u8a00\u6a21\u578b_LLM"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/","title":"2024.04 RALL E","text":"<p>@import \"../../style.less\"</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#rall-e-robust-codec-language-modeling-with-chain-of-thought-prompting-for-text-to-speech-synthesis","title":"RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis","text":"\u4f5c\u8005 \u673a\u6784 \u4f5c\u8005 \u673a\u6784 \u8f9b\u5fb7\u6cf0 Microsoft\u4e1c\u4eac\u5927\u5b66 \u8c2d\u65ed Microsoft \u6c88\u9534 Microsoft\u6d59\u6c5f\u5927\u5b66 \u741a\u6cfd\u8c26 Microsoft\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66 \u6768\u4e1c\u8d85 \u9999\u6e2f\u4e2d\u6587\u5927\u5b66 \u738b\u8fdc\u7a0b \u9999\u6e2f\u4e2d\u6587\u5927\u5b66(\u6df1\u5733) \u9ad8\u9053 \u614e\u4e4b\u4ecb \u4e1c\u4eac\u5927\u5b66 \u733f\u6e21 \u6d0b \u4e1c\u4eac\u5927\u5b66 \u5218\u6811\u6770 Microsoft \u674e\u52b2\u5b87 Microsoft \u8d75\u80dc Microsoft","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#abstract","title":"Abstract","text":"<p>We present ==RALL-E==, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind ==RALL-E== is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, ==RALL-E== first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, ==RALL-E== utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E (2023), ==RALL-E== significantly improves the WER of zero-shot TTS from 6.3% (without reranking) and 2.1% (with reranking) to 2.8% and 1.0%, respectively. Furthermore, we demonstrate that ==RALL-E== correctly synthesizes sentences that are hard for VALL-E (2023) and reduces the error rate from 68% to 4%.</p> <p>\u6211\u4eec\u63d0\u51fa\u4e86 ==RALL-E==, \u4e00\u79cd\u9c81\u68d2\u7684\u7528\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u7684\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5. \u4e4b\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b (Large Language Models, LLMs) \u7684\u5de5\u4f5c\u5728\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272, \u4f46\u540c\u65f6\u4e5f\u5b58\u5728\u4e00\u4e9b\u4e0d\u7a33\u5b9a\u6027, \u5982\u4e0d\u7a33\u5b9a\u97f5\u5f8b (\u5947\u602a\u7684\u97f3\u9ad8\u548c\u8282\u594f/\u65f6\u957f) \u548c\u9ad8\u8bcd\u9519\u8bef\u7387 (Word Error Error, WER), \u8fd9\u4e9b\u90fd\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u56de\u5f52\u9884\u6d4b\u65b9\u5f0f\u6709\u5173. ==RALL-E== \u7684\u6838\u5fc3\u601d\u60f3\u662f\u601d\u7ef4\u94fe (Chain-of-Thought, CoT) \u63d0\u793a, \u5b83\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u7684\u6b65\u9aa4, \u4ee5\u63d0\u5347\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u7684\u9c81\u68d2\u6027. \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u601d\u60f3, ==RALL-E== \u9996\u5148\u9884\u6d4b\u8f93\u5165\u6587\u672c\u7684\u97f5\u5f8b\u7279\u5f81 (\u97f3\u9ad8\u548c\u65f6\u957f), \u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a\u4e2d\u95f4\u6761\u4ef6\u7528\u4e8e\u9884\u6d4b\u601d\u7ef4\u94fe\u5f62\u5f0f\u7684\u8bed\u97f3 Tokens. \u5176\u6b21, ==RALL-E== \u5229\u7528\u9884\u6d4b\u7684\u65f6\u957f\u63d0\u793a\u6765\u5f15\u5bfc Transformer \u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u6743\u91cd\u8ba1\u7b97, \u4ee5\u5f3a\u5236\u6a21\u578b\u5728\u9884\u6d4b\u8bed\u97f3 Token \u65f6\u4e13\u6ce8\u4e8e\u76f8\u5e94\u7684\u97f3\u7d20\u548c\u97f5\u5f8b\u7279\u5f81. \u7efc\u5408\u7684\u5ba2\u89c2\u8bc4\u4f30\u548c\u4e3b\u89c2\u8bc4\u4f30\u8bf4\u660e, \u76f8\u6bd4 VALL-E (2023), ==RALL-E== \u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6587\u672c\u8f6c\u8bed\u97f3\u7684\u8bcd\u9519\u8bef\u7387\u8868\u73b0, \u4ece 6.3% (\u4e0d\u4f7f\u7528 reranking) \u548c 2.1% (\u4f7f\u7528 reranking) \u5206\u522b\u4e0b\u964d\u5230 2.8% \u548c 1.0%. \u6b64\u5916, \u6211\u4eec\u8fd8\u5c55\u793a\u4e86 ==RALL-E== \u80fd\u591f\u6b63\u786e\u5408\u6210 VALL-E (2023) \u96be\u4ee5\u5408\u6210\u7684\u53e5\u5b50, \u5e76\u5c06\u9519\u8bef\u7387\u4ece 68% \u4e0b\u964d\u5230 4%.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#1introduction","title":"1.Introduction","text":"<p>Large language models (LLMs) have demonstrated great progress in natural language generation [1, 20]. With a sufficient model size LLMs emerge powerful in-context learning abilities that can handle unseen tasks with a text instruction (usually called prompt) in a zero-shot or few-shot manner [31]. Moreover, the simple yet effective next-token prediction task of LLMs makes it easy to apply LLMs on other fields, such as vision [5] and speech synthesis [29], as long as the data can be converted to discrete speech tokens. This work focuses on the language modeling of text-to-speech (TTS) synthesis. Recent work [14,29] have shown that TTS can be modeled by a decoder-only language model by using a neural codec [4,34] to convert continuous waveforms into discrete tokens. These methods, typically leverage tens of thousands of hours of speech data, emerge in-context learning ability that can clone a speaker\u2019s voice by providing a short audio prompt to the language model, and thus have impressive performance on zero-shot TTS. However, due to the sequential generation property of language models, such codec language models suffer from poor robustness. Although the AutoRegressive (AR) prediction style of language models enables the model to generate speech with diverse prosody patterns, they can also cause bad cases with unnatural prosody. Moreover, since there is no strict alignment between text and speech, the models can omit or repeat words in the input text. This is quite different from TTS methods based on Non-AutoRegressive (NAR) generative models [12,16,25], which predicts all tokens at the same time, thus have high robustness but relatively low diversity. As suggested by previous work [12,33], LLM-based TTS have a higher Word Error Rate (WER) than NAR TTS even if they have similar performance on other metrics. To alleviate this problem, a simple but effective method is to sample the same input text multiple times and select the best one [14,33]. However, such a reranking method further increases the inference time. In this paper, we present ==RALL-E== (the abbreviation of Robust VALL-E), a method to improve the robustness of LLM-based TTS. The core idea of ==RALL-E== is inspired from the Chain-of-Thought (CoT) prompting [32]. In CoT prompting, the LLM is instructed to generate an intermediate result that is used as a condition for the prediction of the final result. The CoT prompting breaks a complex task into several simpler steps, so that can improve the robustness of LLMs, especially on hard tasks like arithmetic [32]. To adapt CoT prompting to LLM-based TTS, ==RALL-E== predicts prosody tokens (pitch and duration) before predicting speech tokens to stabilize the prosody. Given an input sentence, ==RALL-E== first predicts phoneme-level pitch and duration of the input, then predicts speech tokens conditioning on both the input phonemes and the predicted prosody tokens. Furthermore, ==RALL-E== utilizes the predicted duration to mask irrelevant phonemes and prosody tokens when computing self-attention weights, so that the codec language model is enforced to concentrate on tokens around the phoneme and prosody token the speech token corresponds to. We use VALL-E (2023) [29], a recent powerful LLM-based TTS method, as the baseline, and conduct experiments on a large dataset with 44K hours speech data. Results of comprehensive objective and subjective evaluations demonstrate that RALL significantly improves the robustness of LLM-based TTS by reducing the WER on the LibriSpeech [18] test-clean set from6.3%(w/o reranking) and2.1%(with reranking) to2.8%and 1.0%, respectively. Furthermore, we evaluate the performance of ==RALL-E== on 50 particularly hard sentences. As demonstrated in Tab.01, compared to VALL-E (2023), ==RALL-E== significantly reduces WER from68%to4%by eliminating almost all types of error, which demonstrates the superior robustness of ==RALL-E== (see Section 4.4 for more details). The contributions of this work are summarized as follows:  - We present ==RALL-E==, a robust codec language modeling method with chain-of-thought prompting for TTS. ==RALL-E== improves the robustness of LLM-based TTS by (1) incorporating prosody tokens as chain-of-thought prompting to stabilize the generation of speech tokens and (2) using duration-guided masking to enhance the alignment between phoneme and speech tokens. - We conduct comprehensive objective and subjective evaluations. Experimental results demonstrate that ==RALL-E== obtains significantly better robustness than the baseline VALL-E (2023) and two previous works. - We further evaluate ==RALL-E== on sentences that are particularly hard to synthesize for LLM-based TTS. The results demonstrate that ==RALL-E== correctly synthesizes hard sentences and reduces the error rate from68%to4%compared to VALL-E (2023), which closely approaches the performance of non-autoregressive TTS. Audio samples can be found at https://ralle-demo.github.io/RALL-E.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#2related-work","title":"2.Related Work","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#llm-based-tts","title":"LLM-Based TTS","text":"<p>Inspired by the success of LLMs [1,20], several recent works adopt language models to model TTS (SPEAR-TTS (2023), VALL-E (2023), UniAudio (2023)) and begin to use decoder-only architecture based on Transformer (2017). In such models, text and speech tokens are concatenated together and fed to a single transformer. The whole model is trained on a next-token prediction task like a language model. The LLM-based TTS systems are typically trained on tens of thousands of hours of speech data and have hundreds of millions of parameters, hence can leverage the emergent abilities of LLMs like in-context learning [31] to enable zero-shot TTS (VALL-E (2023)). Besides, recent works (AudioPalM (2023); VioLA (2023); UniAudio (2023)) have shown the decoder-only architecture can be used to learn multiple tasks, as the input and output are processed jointly by a language model, and the model can be signaled to generate results for different tasks by inputting pre-defined special tokens. ==RALL-E== focuses on the robustness problem of LLM-based TTS.</p> <p>\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u83b7\u5f97\u7075\u611f, \u8fd1\u671f\u7684\u4e00\u4e9b\u5de5\u4f5c\u5c06\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u5efa\u6a21\u8bed\u97f3\u5408\u6210\u5e76\u5f00\u59cb\u4f7f\u7528\u4ec5\u6709 Transformer \u89e3\u7801\u5668\u7684\u67b6\u6784. \u8fd9\u4e9b\u6a21\u578b\u5c06\u6587\u672c\u548c\u8bed\u97f3\u6807\u8bb0\u4e32\u8054\u8d77\u6765, \u8f93\u5165\u5230\u5355\u4e2a Transformer \u4e2d. \u6574\u4e2a\u6a21\u578b\u5728\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3, \u5982\u8bed\u8a00\u6a21\u578b\u4e00\u6837. \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u901a\u5e38\u662f\u5728\u6570\u5343\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\u4e0a\u8bad\u7ec3\u7684, \u5177\u6709\u6570\u767e\u4e07\u4e2a\u53c2\u6570, \u56e0\u6b64\u53ef\u4ee5\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u80fd\u529b, \u5982\u4e0a\u4e0b\u6587\u5b66\u4e60, \u5b9e\u73b0\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210. \u9664\u6b64\u4e4b\u5916, \u8fd1\u671f\u7684\u5de5\u4f5c [22,30,33] \u8868\u660e, \u4ec5\u6709\u89e3\u7801\u5668\u67b6\u6784\u7684\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u591a\u4e2a\u4efb\u52a1, \u56e0\u4e3a\u8f93\u5165\u548c\u8f93\u51fa\u662f\u7531\u8bed\u8a00\u6a21\u578b\u4e00\u8d77\u5904\u7406\u7684, \u5e76\u4e14\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u9884\u5b9a\u4e49\u7684\u7279\u6b8a\u6807\u8bb0\u4fe1\u53f7\u751f\u6210\u4e0d\u540c\u4efb\u52a1\u7684\u7ed3\u679c. ==RALL-E== \u7740\u91cd\u4e8e\u8bed\u8a00\u6a21\u578b\u5efa\u6a21\u8bed\u97f3\u5408\u6210\u7684\u5065\u58ee\u6027\u95ee\u9898.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#robust-autoregressive-tts","title":"Robust Autoregressive TTS","text":"<p>The robustness of AR TTS is a popular topic in the literature. For encoder-decoder AR TTS, several previous works enforce the attention weights to be monotonic [2,9, 36] that can effectively improve the robustness. In addition, Shen et al.[24] proposed a non-attentive Tacotron, in which the attention module was replaced by a duration predictor to determine the alignment path before decoding. For decoder-only TTS, a key difference is that the attention weights are computed on text and context at the same time, hence the whole attention weights should not be monotonic. Song et al. proposed ELLA-V (2024) that interleaves the speech tokens with phonemes by inserting a phoneme token and a special End Of Phone(EOP) token at the beginning and end of the speech tokens corresponding to the phoneme, respectively. While the inserted phoneme and the EOP token indicate the duration of each phoneme, such an implicit way entangles the prediction of speech tokens and duration together. ==RALL-E== disentangles the predictions of duration and speech tokens by predicting the duration of all phonemes before the speech tokens, hence has higher controllability over the generation process. Du et al. proposed VALL-T (2024) that uses an unsupervised transducer loss [7] to implicitly model the duration of phonemes. Compared to RALL-E, although VALL-T doesn\u2019t rely on external alignment tools during training, its training time is considerably decelerated since the transducer loss requires the model to perform a forward process for every phoneme. Besides, like ELLA-V (2024), VALL-T (2024) also entangles the predictions of duration and speech tokens, thus has weaker controllability than RALL-E.</p> <p>\u81ea\u56de\u5f52\u8bed\u97f3\u5408\u6210\u7684\u5065\u58ee\u6027\u662f\u4e00\u4e2a\u70ed\u95e8\u8bdd\u9898. \u5bf9\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u81ea\u56de\u5f52\u8bed\u97f3\u5408\u6210, \u4e4b\u524d\u7684\u5de5\u4f5c\u5df2\u7ecf\u63d0\u51fa\u4e86\u5f3a\u5316\u6ce8\u610f\u529b\u6743\u91cd\u4e3a\u5355\u8c03\u7684\u7b56\u7565 [2,9, 36], \u80fd\u591f\u6709\u6548\u63d0\u5347\u8bed\u97f3\u5408\u6210\u7684\u5065\u58ee\u6027. \u6b64\u5916, Shen \u7b49\u4eba\u63d0\u51fa\u4e86\u4e00\u4e2a\u975e\u6ce8\u610f\u529b\u7684 Tacotron, \u5176\u4e2d\u66ff\u6362\u4e86\u6ce8\u610f\u529b\u6a21\u5757, \u7528\u4e00\u4e2a\u65f6\u957f\u9884\u6d4b\u5668\u5728\u89e3\u7801\u524d\u786e\u5b9a\u5bf9\u9f50\u8def\u5f84.</p> <p>\u5bf9\u4e8e\u4ec5\u6709\u89e3\u7801\u5668\u7684\u8bed\u97f3\u5408\u6210, \u5173\u952e\u533a\u522b\u662f\u6ce8\u610f\u529b\u6743\u91cd\u662f\u540c\u65f6\u8ba1\u7b97\u6587\u672c\u548c\u4e0a\u4e0b\u6587\u7684, \u56e0\u6b64\u6ce8\u610f\u529b\u6743\u91cd\u4e0d\u5e94\u8be5\u662f\u5355\u8c03\u7684. Song \u7b49\u4eba\u63d0\u51fa ELLA-V (2024), \u5176\u4e2d\u63d2\u5165\u4e86\u4e00\u4e2a\u97f3\u7d20\u6807\u8bb0\u548c\u4e00\u4e2a\u7279\u6b8a\u7684\u7ed3\u675f\u97f3\u7d20\u6807\u8bb0, \u524d\u8005\u5bf9\u5e94\u4e8e\u97f3\u7d20\u7684\u65f6\u957f, \u540e\u8005\u5bf9\u5e94\u4e8e\u97f3\u7d20\u7684\u7ed3\u675f. \u8fd9\u79cd\u9690\u5f0f\u7684\u65b9\u5f0f\u5c06\u8bed\u97f3\u6807\u8bb0\u548c\u65f6\u957f\u9884\u6d4b\u8054\u7cfb\u5728\u4e00\u8d77, \u5bfc\u81f4\u6a21\u578b\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u63a7\u5236\u529b\u8f83\u5f31. ==RALL-E== \u901a\u8fc7\u9884\u6d4b\u6240\u6709\u97f3\u7d20\u7684\u65f6\u957f, \u89e3\u8026\u4e86\u65f6\u957f\u548c\u8bed\u97f3\u6807\u8bb0\u7684\u9884\u6d4b, \u56e0\u6b64\u53ef\u4ee5\u66f4\u597d\u5730\u63a7\u5236\u751f\u6210\u8fc7\u7a0b. Du \u7b49\u4eba\u63d0\u51fa VALL-T (2024), \u4f7f\u7528\u65e0\u76d1\u7763\u7684\u8f6c\u6362\u5668\u635f\u5931 [7] \u6a21\u578b\u97f3\u7d20\u7684\u65f6\u957f. \u4e0e ==RALL-E== \u76f8\u6bd4, VALL-T \u867d\u7136\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u4f9d\u8d56\u5916\u90e8\u5bf9\u9f50\u5de5\u5177, \u4f46\u8bad\u7ec3\u65f6\u95f4\u663e\u8457\u51cf\u5c11, \u56e0\u4e3a\u8f6c\u6362\u5668\u635f\u5931\u9700\u8981\u6a21\u578b\u5bf9\u6bcf\u4e2a\u97f3\u7d20\u8fdb\u884c\u4e00\u6b21\u524d\u5411\u8ba1\u7b97. \u6b64\u5916, \u4e0e ELLA-V (2024) \u4e00\u6837, VALL-T (2024) \u4e5f\u5c06\u65f6\u957f\u548c\u8bed\u97f3\u6807\u8bb0\u8054\u7cfb\u5728\u4e00\u8d77, \u56e0\u6b64\u63a7\u5236\u529b\u8f83\u5f31.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#3rall-e","title":"3.RALL-E","text":"<p>The overview of ==RALL-E== is illustrated in Fig.01.</p> <p></p> <p>The core idea of ==RALL-E== is CoT prompting that generates intermediate results to assist and stabilize the generation of speech tokens and improve the robustness of LLM-based TTS. To accomplish this idea, we first propose to predict two kinds of phoneme-level prosody tokens: pitch and duration before predicting the speech tokens. The distributions of the prosody tokens are modeled together with speech tokens by a single Transformer so that they can influence the duration and pitch of the predicted speech tokens. To further utilize the predicted duration to guide the generation and improve the robustness, we propose duration-guided masking to enhance the alignment between speech tokens, phonemes, and prosody tokens learned by the language model. At each decoding step of the speech tokens, ==RALL-E== masks phonemes and prosody tokens that are irrelevant to the synthesis of the current speech token based on the duration information. In the following sections, we first briefly introduce VALL-E (2023) since we apply the proposed method to it in the experiments. We then formulate and introduce ==RALL-E== in detail. It should be stressed that, though we use VALL-E (2023) to implement ==RALL-E==, the proposed method can be applied in any decoder-only AR TTS model.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#31preliminary-vall-e","title":"3.1.Preliminary: VALL-E","text":"<p>We inherit most symbols from the original paper of VALL-E (2023) for ease of reading. Readers are recommended to refer to the original paper for more details.</p> <p>Generally, VALL-E (2023) is a decoder-only LLM-based TTS system that uses two Transformers [28] to predict speech tokens from the text. The speech tokens here are extracted from EnCodec (2022), a neural audio codec based on residual vector quantization (RVQ) [34] that can convert continuous speech signal into discrete tokens. After predicting the discrete tokens, the waveforms can be reconstructed by feeding the tokens into the decoder of EnCodec. An RVQ typically contains $N$ quantization layers ($N = 8$ in VALL-E (2023)), hence at each time step the encoded speech has $N$ tokens.  Formally, given speech $\\mathbf{y}$ and its transcription $\\mathbf{x}$, the discrete speech token matrix $\\mathbf{C}$ encoded by the codec has a shape of $T \\times N$, where $T$ is the total time step. In addition tox, to clone a speaker\u2019s voice and utilize the in-context learning ability of LLMs, VALL-E (2023) receives a short prompt $\\tilde{\\mathbf{C}}^{T'\\times N}$ as input before predicting $\\mathbf{C}$. Hence, VALL-E (2023) models and maximizes the following distribution:</p> <p>$$   \\mathbb{P}(\\mathbf{C}|\\mathbf{x},\\tilde{\\mathbf{C}}).\\tag{1} $$</p> <p>VALL-E (2023) predicts speech tokens hierarchically where the speech tokens of the 1st layer of RVQ are first predicted by an AR Transformer, and the tokens of the rest layers are predicted by a NAR Transformer. This is because RVQ uses a residual quantization method, i.e. higher layers encode the information that is not encoded by the lower layers, hence tokens of the1st layer contain most information of the waveforms, and the information encoded by the rest layers gradually decreases. The AR Transformer takes the phoneme sequence $\\mathbf{x}$, and speech tokens of the1st layer of the prompt $\\tilde{c}{:,1}$ as input to predict the target speech tokens of the1st layer $\\mathbf{c}{:,1}$ sequentially, i.e. maximizes the following distribution:</p> <p>$$   \\mathbb{P}(\\mathbf{c}{:,1}|\\mathbf{x},\\tilde{\\mathbf{c}}{:,1};\\theta_{AR})=\\prod_{t=1}^T\\mathbb{P}(\\mathbf{c}{t,1}|\\mathbf{x},\\mathbf{c}{&lt;t,1},\\tilde{\\mathbf{c}}{:,1};\\theta{AR}),\\tag{2} $$</p> <p>where $\\theta_{\\text{AR}}$ is the trainable parameters of the AR Transformer. The NAR Transformer predicts all target speech tokens $\\mathbf{c}{:,j}$ of the $j$-th layer at the same time with the phoneme sequence $\\mathbf{x}$, the prompt $\\tilde{\\mathbf{C}}$, and target speech tokens $\\mathbf{c}{:,&lt;j}$ of all layers less than $j$ as the conditions, i.e. maximizes the following distribution:</p> <p>$$   \\mathbb{P}(\\mathbf{c}{:,2:N}|\\mathbf{x},\\tilde{\\mathbf{C}};\\theta{NAR})=\\prod_{j=2}^N\\mathbb{P}(\\mathbf{c}{:,j}|\\mathbf{x},\\mathbf{c}{:,&lt;j},\\tilde{\\mathbf{C}};\\theta_{NAR}),\\tag{3} $$</p> <p>where $\\theta_{\\text{NAR}}$ is the trainable parameters of the NAR Transformer. By combining Eq.2 and Eq.3, VALL-E (2023) breaks Eq.1 into the following form:</p> <p>$$   \\mathbb{P}(\\mathbf{C}|\\mathbf{x},\\tilde{\\mathbf{C}})=\\mathbb{P}(\\mathbf{c}{:,1}|\\mathbf{x},\\tilde{\\mathbf{c}}{:,1};\\theta_{AR})\\mathbb{P}(\\mathbf{c}{:,2:N}|\\mathbf{x},\\tilde{\\mathbf{C}};\\theta{NAR}).\\tag{4} $$</p> <p>It is noteworthy that in practice the two Transformers have the same architecture but have different attention masks during computation. Specifically, both the two Transformers use a bidirectional mask for the phoneme sequence $\\mathbf{x}$, which means every phoneme $x_i$ can attend to all other phonemes $x_{\\neq i}$. However, for the speech tokens, the AR Transformers uses a unidirectional mask so that $\\mathbf{c}{t,1}$ can only attend to previous tokens $\\mathbf{c}{&lt;t,1}$, while the NAR Transformer still uses a bidirectional mask.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#32prosody-tokens-as-chain-of-thought-prompts","title":"3.2.Prosody Tokens as Chain-of-Thought Prompts","text":"<p>One of the problems of LLM-based TTS is that it directly generates speech from phonemes with no restriction on the prosody, e.g. pitch, duration, etc, which usually results in speech with unstable prosody. A similar problem is also observed in Wei et al.[32] where the authors find LLMs cannot directly answer a complex question like arithmetic and propose CoT prompting to solve this problem. The idea of CoT prompting is breaking a complex task into several simpler tasks so that LLMs can utilize the intermediate results to reach the final answers. As shown in Wei et al.[32], by CoT prompting the correct rate of LLMs on complex tasks can be significantly improved. This motivates us to adapt CoT prompting to LLM-based TTS by generating intermediate prosody tokens before generating speech tokens to alleviate the robustness problem of LLM-based TTS.</p> <p>\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7684\u95ee\u9898\u4e4b\u4e00\u662f\u5b83\u76f4\u63a5\u4ece\u97f3\u7d20\u751f\u6210\u8bed\u97f3, \u800c\u5bf9\u97f5\u5f8b\u6ca1\u6709\u9650\u5236, \u5982\u97f3\u9ad8, \u65f6\u957f\u7b49, \u8fd9\u901a\u5e38\u4f1a\u5bfc\u81f4\u751f\u6210\u97f5\u5f8b\u4e0d\u7a33\u5b9a\u7684\u8bed\u97f3. Wei \u7b49\u4eba\u7684\u7814\u7a76\u4e2d\u4e5f\u80fd\u89c2\u5bdf\u5230\u76f8\u4f3c\u95ee\u9898, \u4ed6\u4eec\u53d1\u73b0\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u4e0d\u80fd\u76f4\u63a5\u56de\u7b54\u590d\u6742\u7684\u95ee\u9898, \u5982\u7b97\u672f\u7b49, \u4ece\u800c\u63d0\u51fa\u4e86\u601d\u7ef4\u94fe\u63d0\u793a\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898. \u601d\u7ef4\u94fe\u63d0\u793a\u7684\u601d\u60f3\u662f\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u82e5\u5e72\u4e2a\u7b80\u5355\u4efb\u52a1\u4f7f\u5f97\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5229\u7528\u4e2d\u95f4\u7ed3\u679c\u6765\u83b7\u5f97\u6700\u7ec8\u7b54\u6848. \u5982 Wei \u7b49\u4eba\u6240\u5c55\u793a\u7684, \u901a\u8fc7\u601d\u7ef4\u94fe\u63d0\u793a, \u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6b63\u786e\u7387. \u8fd9\u6fc0\u52b1\u6211\u4eec\u5c06\u601d\u7ef4\u94fe\u63d0\u793a\u5e94\u7528\u4e8e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210, \u901a\u8fc7\u5728\u751f\u6210\u8bed\u97f3\u4e4b\u524d\u751f\u6210\u4e2d\u95f4\u97f5\u5f8b\u6807\u8bb0\u6765\u7f13\u89e3\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u7684\u5065\u58ee\u6027\u95ee\u9898.</p> <p>To incorporate pitch and duration in the AR Transformer of VALL-E (2023), we first get the alignment between phonemes and speech tokens and extract the pitch value for each speech token. We then compute phoneme-level pitch value based on the duration and linearly quantize it to $M_p$ buckets. We define a maximal duration value $M_d$, and all duration values that exceed $M_d$ will be truncated to the maximum. ==RALL-E== predicts the two prosody tokens before the speech tokens in a CoT style.</p> <p>\u4e3a\u4e86\u5728 VALL-E (2023) \u7684\u81ea\u56de\u5f52 Transformer \u4e2d\u6574\u5408\u97f3\u9ad8\u548c\u65f6\u957f, \u6211\u4eec\u9996\u5148\u83b7\u5f97\u97f3\u7d20\u548c\u8bed\u97f3 token \u4e4b\u95f4\u7684\u5bf9\u9f50, \u5e76\u63d0\u53d6\u6bcf\u4e2a\u8bed\u97f3 token \u7684\u97f3\u9ad8\u503c. \u7136\u540e\u6211\u4eec\u57fa\u4e8e\u65f6\u957f\u8ba1\u7b97\u97f3\u7d20\u7ea7\u522b\u7684\u97f3\u9ad8\u503c, \u5e76\u5c06\u5176\u7ebf\u6027\u91cf\u5316\u5230 $M_p$ \u4e2a\u6876\u4e2d. \u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u6700\u5927\u65f6\u957f\u503c $M_d$, \u8d85\u51fa\u8be5\u503c\u7684\u65f6\u957f\u503c\u5c06\u88ab\u622a\u65ad. ==RALL-E== \u4ee5 CoT \u98ce\u683c\u5728\u8bed\u97f3 tokens \u524d\u9884\u6d4b\u4e24\u4e2a\u97f5\u5f8b tokens.</p> <p>Formally, assume $p$, $d$ are the discrete pitch and duration sequences of the target speech tokens $\\mathbf{C}$, $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$ are the ones of the prompt $\\tilde{\\mathbf{C}}$, we model and maximize the following distribution:</p> <p>\u5f62\u5f0f\u4e0a, \u5047\u8bbe $p$, $d$ \u662f\u76ee\u6807\u8bed\u97f3 tokens $\\mathbf{C}$ \u7684\u79bb\u6563\u97f3\u9ad8\u548c\u65f6\u957f\u5e8f\u5217, $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$ \u662f\u63d0\u793a $\\tilde{\\mathbf{C}}$ \u7684\u97f3\u9ad8\u548c\u65f6\u957f\u5e8f\u5217, \u6211\u4eec\u53ef\u4ee5\u5efa\u6a21\u5e76\u6700\u5927\u5316\u4e0b\u5217\u5206\u5e03:</p> <p>$$   \\mathbb{P}(\\mathbf{p},\\mathbf{d}|\\mathbf{x},\\tilde{\\mathbf{p}},\\tilde{\\mathbf{d}};\\theta_{AR})=\\prod_{t=1}^L\\mathbb{P}(p_t,d_t|\\mathbf{x},\\mathbf{p}{&lt;t},\\mathbf{d}{&lt;t},\\tilde{\\mathbf{p}},\\tilde{\\mathbf{d}};\\theta_{AR}),\\tag{5} $$ </p> <p>where $L$ is the length of $\\mathbf{x}$. </p> <p>\u5176\u4e2d $L$ \u4e3a $\\mathbf{x}$ \u7684\u957f\u5ea6.</p> <p>In practice, the model predicts $p_t$ and $d_t$ with two separate heads, and their embeddings are summed up and fed to the model for the prediction of the next step. </p> <p>\u5b9e\u9645\u4e0a, \u6a21\u578b\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acb\u7684\u5934\u9884\u6d4b $p_t$ \u548c $d_t$, \u4ed6\u4eec\u7684\u5d4c\u5165\u5c06\u88ab\u6c42\u548c\u540e\u8f93\u5165\u6a21\u578b\u4ee5\u8fdb\u884c\u4e0b\u4e00\u6b65\u7684\u9884\u6d4b.</p> <p>==RALL-E== then predicts the speech tokens with $p$ and $d$ as a new condition, which makes Eq.2 becomes:</p> <p>==RALL-E== \u7136\u540e\u4ee5 $p$ \u548c $d$ \u4f5c\u4e3a\u65b0\u7684\u6761\u4ef6, \u4f7f\u5f97\u65b9\u7a0b 2 \u53d8\u4e3a:</p> <p>$$ \\mathbb{P}(\\mathbf{c}{:,1}|\\mathbf{x},\\tilde{\\mathbf{c}}{:,1},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta_{AR})=\\prod_{t=1}^T\\mathbb{P}(\\mathbf{c}{t,1}|\\mathbf{x},\\mathbf{c}{&lt;t,1},\\tilde{\\mathbf{c}}{:,1},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta{AR}).\\tag{6} $$</p> <p>The above two equations can be jointly optimized by the AR Transformer.  Although the proposed method adds additional $L$ decoding steps, since $L&lt;&lt;T$, it intuitively has little influence on the efficiency.</p> <p>\u4e0a\u9762\u7684\u4e24\u4e2a\u65b9\u7a0b\u53ef\u4ee5\u901a\u8fc7 AR Transformer \u8054\u5408\u4f18\u5316.  \u5c3d\u7ba1\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u589e\u52a0\u4e86 $L$ \u4e2a\u89e3\u7801\u6b65\u9aa4, \u4f46\u7531\u4e8e $L&lt;&lt;T$, \u5176\u5b9e\u9645\u5f71\u54cd\u5f88\u5c0f.</p> <p>For the NAR Transformer, we simply sum the embeddings of the phoneme, pitch, and duration together as the input. This makes Eq.3 becomes:</p> <p>\u5bf9\u4e8e\u975e\u81ea\u56de\u5f52 Transformer, \u6211\u4eec\u53ea\u9700\u5c06\u97f3\u7d20, \u97f3\u9ad8, \u65f6\u957f\u7684\u5d4c\u5165\u5411\u91cf\u76f8\u52a0\u4f5c\u4e3a\u8f93\u5165. \u8fd9\u4f7f\u5f97\u65b9\u7a0b 3 \u53d8\u4e3a: $$ \\begin{aligned}\\mathbb{P}(\\mathbf{c}{:,2:N}|\\mathbf{x},\\tilde{\\mathbf{C}},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta{NAR})&amp;=\\prod_{j=2}^N\\mathbb{P}(\\mathbf{c}{:,j}|\\mathbf{x},\\mathbf{c}{:,&lt;j},\\tilde{\\mathbf{C}},\\mathbf{p},\\tilde{\\mathbf{p}},\\mathbf{d},\\tilde{\\mathbf{d}};\\theta_{NAR}).\\end{aligned}\\tag{7} $$ </p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#33enhancing-alignment-with-duration-guided-masking","title":"3.3.Enhancing Alignment with Duration-Guided Masking","text":"<p>As the left side of Fig.02 illustrates, since the speech token attends to all phonemes in the AR Transformer of VALL-E (2023), the alignment between the phonemes and the speech tokens is implicitly modeled by the self-attention of VALL-E (2023). This can be imprecise and causes errors like word omissions or hallucinations. Though ==RALL-E== introduces prosody CoT prompting to guide and stabilize the generation, we still find the model can fail to align in the experiments. We thus propose duration guided masking to fully utilize the intermediate duration results and boost the robustness.</p> <p>\u5982\u56fe 2 \u5de6\u4fa7\u6240\u793a, \u7531\u4e8e VALL-E (2023) \u7684\u81ea\u56de\u5f52 Transformer \u4e2d\u7684\u8bed\u97f3 token \u80fd\u591f\u5173\u6ce8\u5230\u6240\u6709\u97f3\u7d20, \u56e0\u6b64\u97f3\u7d20\u548c\u8bed\u97f3 token \u7684\u5bf9\u9f50\u662f\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u9690\u5f0f\u5efa\u6a21\u7684. \u7136\u800c, \u8fd9\u79cd\u5bf9\u9f50\u53ef\u80fd\u4e0d\u7cbe\u786e, \u5bfc\u81f4\u8bf8\u5982\u8bcd\u6c47\u9057\u6f0f\u6216\u5e7b\u89c9\u7b49\u9519\u8bef. \u5c3d\u7ba1 ==RALL-E== \u63d0\u51fa\u4e86\u97f5\u5f8b CoT \u63d0\u793a\u6765\u5f15\u5bfc\u548c\u7a33\u5b9a\u751f\u6210, \u4f46\u6211\u4eec\u4ecd\u7136\u53d1\u73b0\u6a21\u578b\u5728\u5b9e\u9a8c\u4e2d\u53ef\u80fd\u65e0\u6cd5\u5bf9\u9f50. \u56e0\u6b64, \u6211\u4eec\u63d0\u51fa\u4e86\u65f6\u957f\u5bfc\u5411\u63a9\u7801 (duration-guided masking) \u6765\u5145\u5206\u5229\u7528\u4e2d\u95f4\u65f6\u957f\u7ed3\u679c\u5e76\u63d0\u5347\u9c81\u68d2\u6027.</p> <p>As the right side of Fig.02 illustrates, in the proposed duration-guided masking, the speech token is restricted to only attend on a phoneme (prosody token) window centered at the phoneme (prosody token) it corresponds to. We define the window size ask, thus each speech token can attend on $2k + 1$ phonemes and $2k + 1$ prosody tokens. All phonemes and prosody tokens at other positions will be masked out, hence their attention weights are always zero. When $k = 0$ the speech token strictly attends to the phoneme it corresponds to. If the alignment is perfect this should be enough. However, in the experiments, we found that the alignment results obtained by our alignment tool usually have errors. We thus loosen the restriction by also allowing the speech token to attend at the near phonemes of the corresponding phoneme. Another reason for this design is that the pronunciation of a phoneme is usually dependent on near phonemes. As one will see in Section.4.3 and Appendix.A, the experimental results verify the effectiveness of this design.</p> <p>\u5982\u56fe 2 \u53f3\u4fa7\u6240\u793a, \u5728\u6240\u63d0\u51fa\u7684\u65f6\u957f\u5bfc\u5411\u63a9\u7801\u4e2d, \u8bed\u97f3 token \u53ea\u80fd\u5173\u6ce8\u5230\u4e0e\u5176\u5bf9\u5e94\u7684\u97f3\u7d20 (\u97f5\u5f8b token) \u5904\u4e8e\u4e2d\u5fc3\u4f4d\u7f6e\u7684\u7a97\u53e3\u5185\u7684\u97f3\u7d20 (\u97f5\u5f8b token). \u6211\u4eec\u5b9a\u4e49\u7a97\u53e3\u5927\u5c0f\u4e3a $k$, \u56e0\u6b64\u6bcf\u4e2a\u8bed\u97f3 token \u53ef\u4ee5\u5173\u6ce8\u5230 $2k + 1$ \u4e2a\u97f3\u7d20\u548c $2k + 1$ \u4e2a\u97f5\u5f8b token. \u6240\u6709\u5176\u4ed6\u4f4d\u7f6e\u7684\u97f3\u7d20\u548c\u97f5\u5f8b token \u90fd\u5c06\u88ab\u63a9\u7801\u6389, \u56e0\u6b64\u5b83\u4eec\u7684\u6ce8\u610f\u529b\u6743\u91cd\u603b\u662f\u4e3a\u96f6. \u5f53 $k = 0$ \u65f6, \u8bed\u97f3 token \u4e25\u683c\u5173\u6ce8\u5230\u5b83\u5bf9\u5e94\u7684\u97f3\u7d20. \u7136\u800c, \u5728\u5b9e\u9a8c\u4e2d, \u6211\u4eec\u53d1\u73b0\u6240\u83b7\u5f97\u7684\u5bf9\u9f50\u7ed3\u679c\u7ecf\u5e38\u5b58\u5728\u9519\u8bef. \u56e0\u6b64, \u6211\u4eec\u901a\u8fc7\u5141\u8bb8\u8bed\u97f3 token \u5173\u6ce8\u5230\u4e0e\u5176\u5bf9\u5e94\u7684\u97f3\u7d20\u76f8\u90bb\u7684\u97f3\u7d20\u6765\u653e\u677e\u9650\u5236. \u53e6\u4e00\u4e2a\u539f\u56e0\u662f, \u4e00\u4e2a\u97f3\u7d20\u7684\u53d1\u97f3\u901a\u5e38\u4f9d\u8d56\u4e8e\u9644\u8fd1\u7684\u97f3\u7d20. \u5982 \u7b2c 4.3 \u8282 \u548c \u9644\u5f55 A \u6240\u793a, \u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8fd9\u79cd\u8bbe\u8ba1\u7684\u6709\u6548\u6027.</p> <p>For the NAR Transformer, we obtained almost no gain when applying the proposed masking strategy to it in our preliminary experiments. Thus we only apply the masking strategy on the AR Transformer.</p> <p>\u5bf9\u4e8e\u975e\u81ea\u56de\u5f52 Transformer, \u6211\u4eec\u5728\u521d\u6b65\u5b9e\u9a8c\u4e2d\u5e76\u672a\u83b7\u5f97\u6240\u63d0\u51fa\u7684\u63a9\u7801\u7b56\u7565\u7684\u660e\u663e\u6536\u76ca. \u56e0\u6b64, \u6211\u4eec\u53ea\u5728\u81ea\u56de\u5f52 Transformer \u4e0a\u5e94\u7528\u63a9\u7801\u7b56\u7565.</p> <p>The general inference procedure follows VALL-E (2023) with two differences. First, before sampling the speech tokens $\\mathbf{c}{:,1}$ the prosody tokens $\\mathbf{p}$ and $\\mathbf{d}$ are sampled conditioning on the phoneme sequence $\\mathbf{x}$ and acoustic prompt $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$. Second, although normal language models depend on a special token <code>&lt;eos&gt;</code> as the stop condition, since we know the total duration $D = \\sum^L{t=1}d_t$, we propose a duration guided inference method that forces the inference to stop at the $D$-th step. This method ensures no phoneme is omitted or repeated as it continues the inference if the <code>&lt;eos&gt;</code> token is predicted before the $D$-th step and stops at the right step as guided by the predicted duration..</p> <p>\u6574\u4f53\u63a8\u7406\u8fc7\u7a0b\u9075\u5faa VALL-E (2023) \u7684\u4e00\u822c\u8fc7\u7a0b, \u4f46\u6709\u4e24\u4e2a\u5dee\u522b. \u9996\u5148, \u5728\u91c7\u6837\u8bed\u97f3 tokens $\\mathbf{c}{:,1}$ \u4e4b\u524d, \u5148\u91c7\u6837\u97f5\u5f8b tokens $\\mathbf{p}$ \u548c $\\mathbf{d}$ \u4f5c\u4e3a\u6761\u4ef6, \u6761\u4ef6\u662f\u97f3\u7d20\u5e8f\u5217 $\\mathbf{x}$ \u548c\u58f0\u5b66\u63d0\u793a $\\tilde{\\mathbf{p}}$, $\\tilde{\\mathbf{d}}$. \u5176\u6b21, \u867d\u7136\u666e\u901a\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u4e8e\u7279\u6b8a\u7b26\u53f7 <code>&lt;eos&gt;</code> \u4f5c\u4e3a\u505c\u6b62\u6761\u4ef6, \u4f46\u7531\u4e8e\u6211\u4eec\u77e5\u9053\u603b\u65f6\u957f $D = \\sum^L{t=1}d_t$, \u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65f6\u957f\u5bfc\u5411\u63a8\u7406\u65b9\u6cd5, \u5f3a\u5236\u63a8\u7406\u505c\u6b62\u4e8e $D$-th \u6b65. \u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u786e\u4fdd\u4e0d\u4f1a\u9057\u6f0f\u6216\u91cd\u590d\u4efb\u4f55\u97f3\u7d20, \u56e0\u4e3a\u5982\u679c\u5728 $D$-th \u6b65\u9884\u6d4b\u5230 <code>&lt;eos&gt;</code> \u7b26\u53f7, \u63a8\u7406\u5c31\u4f1a\u7ee7\u7eed, \u76f4\u5230\u9884\u6d4b\u7684\u65f6\u957f\u8fbe\u5230\u8981\u6c42\u7684\u65f6\u957f.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#4experiments","title":"4.Experiments","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#41setup","title":"4.1.Setup","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#data","title":"Data","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#model-configuration","title":"Model Configuration","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#training-and-inference","title":"Training and Inference","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#baseline-methods","title":"Baseline Methods","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#objective-metrics","title":"Objective Metrics","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#word-error-rate-wer","title":"Word Error Rate (WER)","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#reranked-wer-wer-r","title":"Reranked WER (WER-R)","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#substitution-sub-deletion-del-insertion-ins","title":"Substitution (Sub), Deletion (Del), Insertion (Ins)","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#utmos","title":"UTMOS","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#speaker-similarity-sim","title":"Speaker Similarity (SIM)","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#subjective-metrics","title":"Subjective Metrics","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#42main-results","title":"4.2.Main Results","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#43ablation-study","title":"4.3.Ablation Study","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#44evaluation-on-hard-sentences","title":"4.4.Evaluation on Hard Sentences","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#5conclusion","title":"5.Conclusion","text":"<p>This paper presents ==RALL-E==, a robust codec language modeling method with CoT prompting for TTS. To address the robustness problem of LLM-based TTS, ==RALL-E== (1) incorporates prosody features (pitch and duration) in the LLM as a CoT prompting to assist and stabilize the generation of speech tokens, and (2) proposes duration-guided masking that enforces the model to attend on relevant phonemes (prosody features) corresponding to each speech token. We conduct comprehensive objective and subjective evaluations and demonstrate that ==RALL-E== can significantly improve the robustness of LLM-based TTS compared to the baseline VALL-E (2023) and two previous works. Furthermore, we show that ==RALL-E== can correctly synthesize sentences that are particularly hard to synthesize for VALL-E (2023) with a 4% error rate that even approaches the performance of non-autoregressive TTS.</p>","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#awindow-size-study","title":"A.Window Size Study","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/2024.04_RALL-E/2024.04_RALL-E/#btranscript-of-the-50-hard-sentences","title":"B.Transcript of the 50 hard sentences","text":"","tags":["\u601d\u7ef4\u94fe_CoT","\u5927\u8bed\u8a00\u6a21\u578b_LLM","\u8bed\u97f3\u5408\u6210_TTS","\u7f16\u89e3\u7801\u5668_Codec"]},{"location":"TTS/Papers/Authors/Jungil_Kong/","title":"Jungil Kong","text":""},{"location":"TTS/Papers/Authors/%E5%BC%A0%E8%87%AA%E5%BC%BA_%28Ziqiang_Zhang%29/","title":"\u5f20\u81ea\u5f3a (Ziqiang Zhang)","text":"<p>2020 \u4e2d\u56fd\u79d1\u6280\u5927\u5b66\u535a\u58eb\u5165\u5b66</p>"},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/","title":"DL Based Expressive Speech Synthesis","text":"<p>@import \"../style.less\"</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#deep-learning-based-expressive-speech-synthesis-a-systematic-review-of-approaches-challenges-and-resources","title":"Deep Learning-Based Expressive Speech Synthesis: A Systematic Review of Approaches,  Challenges, and Resources\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8868\u73b0\u6027\u8bed\u97f3\u5408\u6210: \u65b9\u6cd5, \u6311\u6218, \u8d44\u6e90\u7684\u7cfb\u7edf\u6027\u56de\u987e","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#a","title":"A.\u6458\u8981","text":"<p>Speech synthesis has made significant strides thanks to the transition from machine learning to deep learning models.  Contemporary text-to-speech (TTS) models possess the capability to generate speech of exceptionally high quality, closely mimicking human speech. Nevertheless, given the wide array of applications now employing TTS models, mere high-quality speech generation is no longer sufficient.  Present-day TTS models must also excel at producing expressive speech that can convey various speaking styles and emotions, akin to human speech. Consequently,researchers have concentrated their efforts on developing more efficient models for expressive speech synthesis in recent years. This paper presents a systematic review of the literature on expressive speech synthesis models published within the last 5 years, with a particular emphasis on approaches based on deep learning. We offer a comprehensive classification scheme for these models and provide concise descriptions of models falling into each category. Additionally, we summarize the principal challenges encountered in this research domain and outline the strategies employed to tackle these challenges as documented in the literature. In the Section 8, we pinpoint some research gaps in this field that necessitate further exploration. Our objective with this work is to give an all-encompassing overview of this hot research area to offer guidance to interested researchers and future endeavors in this field.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#1","title":"1.\u5f15\u8a00","text":"<p>Since the late 1950s, computer-based text-to-speech systems (TTS) have undergone significant advancements, culminating in the production of models that generate speech almost indistinguishable from that of a human. This progress has followed a path consisting of several stages, beginning with conventional methods named as concatenative synthesis and progressing to more advanced approaches known as Statistical Parametric Speech Synthesis (SPSS). Advanced approaches are mainly based on machine learning algorithms like hidden Markov models (HMMs) and gaussian mixture models (GMMs). Despite this progress, speech generated by these methods was still noticeably artificial. However, the emergence of deep learning (DL) as a new branch under machine learning (ML) in 2006 has led to significant improvements. Speech synthesis researchers,like many in other research fields, started incorporating deep neural networks (DNN) in their models. Initially,DNNs replaced HMMs and GMMs in SPSS models while the main structure still follows the primary framework of SPSS models as shown in Fig. 1. As discussed in \"Statistical Parametric Speech Synthesis Using Deep Neural Networks\", the deep learning-based models have overcome many limitations and problems associated with machine learning-based models.</p> <p>Researchers continue to aim for improved speech quality and more human-like speech despite past advancements. Additionally, they seek to simplify the framework of the text-to-speech models due to the intricate nature of the SPSS structure, which limits progress in this field to those with extensive linguistic knowledge and expertise. Deep learning advancements have brought about the simple encoder-decoder structure for TTS models as sequence-to-sequence (Seq2Seq) approaches. The pro-posed approaches have simplified the structure of conventional TTS with multiple components into training a single network that converts a set of input text characters/phonemes into a set of acoustic features (mel-spectrograms). A main concern in these advanced TTS models is the mapping process between the input and output sequences, which is a one-to-many problem, as the single input text can have multiple speech variations as output. In fact, there are two groups of recent TTS models, as shown in Fig.02. The first group generates mel-spectrograms in a sequential (autoregressive) manner using soft and automatic attention alignments between input and output sequences, such as the Tacotron model (Tacotron (2017) Tacotron2 (2017)). The second group utilizes hard alignments between the phonemes/characters and mel-spectro-grams, and thus its speech generation process is parallel(non-autoregressive), as in the FastSpeech model (FastSpeech (2019) FastSpeech2 (2020)).This improvement in the structure of the TTS model has encouraged rapid development in the field within the last few years, during which the proposed models produced speech that is nearly indistinguishable from human speech.</p> <p>Human speech is highly expressive and reflects various factors, such as the speaker\u2019s identity, emotion, and speaking style. In addition, there are many applications in which speech synthesis can be utilized, especially expressive speech synthesis. For instance, audiobooks and podcast applications that create audio versions of eBooks and podcasts, translation applications which provide real-time translation of foreign language text, dubbing applications that generate an alternative audio track for a video with different content, speaker, or language, and content creation applications which help produce audio versions of textual content, such as blogs and news articles. E-learning applications that allow for adding voice-over audio to e-learning courses, and conversational AI applications enable machines to communicate with users in a human-like manner, such as AI chatbots and virtual assistants.</p> <p>As spoken language is a crucial component in such applications, users must feel as if they are communicating with a real human rather than a machine. Therefore, the speech generated by these applications should convey appropriate emotion, intonation, stress, and speaking style to match the ongoing conversation or the content type and context of the text being read.</p> <p>As a result, there has been a recent attention towards building efficient expressive speech synthesis models as another step forward in achieving human-like speech. Therefore, many studies have been devoted to expressive speech synthesis (ETTS) as a hot research area, particularly over the last 5 years. In this work, we present the findings of our systematic literature review on ETTS field from the past 5 years. Firstly, we suggest a classification schema of deep learning-based ETTS models that are proposed during this period, based on structures,and learning methods followed in each study. A summary is then provided for each category in the classification schema and main papers related to this category. After that, we outline the main challenges in the ETTS area and solutions that have been proposed to solve them from literature. Finally, we conclude with a discussion of the implications of our work and a highlight of some gaps that require further research in this area.</p> <p>During our work on this review of expressive speech synthesis literature, we came across several review papers that focus on different stages of development in the speech synthesis field. The majority of these reviews concentrate on DL-based TTS approaches, while only a few papers cover recent TTS approaches in addition to early conventional ones. However, to the best of our knowledge, there are no review papers that cover the fast growth in the (expressive) speech synthesis area, especially in the last few years. Therefore, our main goal in this review is to provide an overview of research trends, techniques, and challenges in this area during this period. We hope that our work will offer researchers a comprehensive understanding of how and what has been accomplished in this field and the gaps that need to be filled as guidance for their future efforts.</p> <p>While we were writing this paper, we came across an interesting recent review paper \"An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era\" that is similar to our work. However, the review covers emotional speech synthesis (ESS) as a sub-field of voice transformation while our work is more comprehensive as a systematic literature review that discusses approaches,challenges, and resources. Furthermore, the taxonomy we provide for the reviewed approaches differs from the one given in as elaborated in the next section.</p> <p>The remaining sections of this paper are structured as follows: Section 2 provides an explanation of the methodology employed for conducting this review. Sections 3and 4 describe the different main and sub-categories of the proposed classification schema for DL-based expressive TTS models. Main challenges facing ETTS models and how they have been tackled in the literature are covered in Section 5. We then give a brief description of ETTS datasets and applied evaluation metrics in Sections 6 and 7, respectively. Finally, Section 8 concludes the paper.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#2","title":"2.\u65b9\u6cd5","text":"<p>The last few years have seen rapid growth in expressive and emotional speech synthesis approaches, resulting in a large number of papers and publications in this area. Here, we present the outcomes of a systematic literature review of the last 5 years\u2019 publications within this active research area. This section describes the methodology used to conduct the review, illustrated by Fig.03, which consists of three main stages: paper selection, paper exclusion, and paper classification.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#21","title":"2.1.\u8bba\u6587\u9009\u62e9","text":"<p>For our review, we used the Scopus database to retrieve papers as it encompasses most of the significant journals and conferences pertaining to the speech synthesis field. Our query criteria to find relevant papers on Scopus were twofold: (1) the paper title must include at least one of four words (emotion OR expressive OR prosod OR style) that denote expressive speech, and (2) the paper title, abstract, or keywords must comprise the terms \u201cspeech\u201d AND \u201csynthesis,\u201d in addition to at least one of the above-mentioned words for expressive speech. We considered all papers written in English and published in journals or conferences since 2018. The search query was conducted in January 2023, and it yielded 356papers. Scopus provides an Excel file containing all the primary information of the retrieved papers, which we used in the second stage of our review.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#22","title":"2.2.\u8bba\u6587\u6392\u9664","text":"<p>The exclusion of papers occurred in two phases. In the first phase, we screened the abstract text, while in the second phase, we screened the full text of the paper. Five main constraints were used to exclude papers, including (1) papers that were not related to the TTS field, (2)papers that were not DL-based models, (3) papers that did not focus on expressive or emotional TTS models, (4)papers that were too specific to non-English languages,and (5) papers that lacked details about the applied method. After screening the paper abstracts, we excluded180 papers, mostly based on the first exclusion criterion. During the second exclusion phase, in which we read the full text of each paper, we identified another 65 papers that met at least one of the five exclusion criteria. Consequently, 111 papers were included in the third stage of our review. Additionally, a group of recently published papers in this area ([17,18,19,20] InstructTTS (2023), [22,23, DiffProsody (2023), VoiceBox (2023)) was hand-picked and added to the final set of selected papers. While most of the reviewed papers trained their models on English data, a few other papers used data in other languages as listed in Table 1.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#23","title":"2.3.\u8bba\u6587\u5206\u7c7b","text":"<p>After summarizing the approach proposed for generating expressive speech in each selected paper, we categorized the papers based on the learning approach applied in each one. Accordingly, papers are divided into two main categories, including supervised and unsupervised approaches. Under the supervised category, where labeled data is utilized, we identified three subcategories based on how models are employed expressive speech synthesis. The three proposed subcategories are (1)labels as input features, (2) labels as separate layers or models, and (3) labels for emotion predictors/classifiers.</p> <p>Papers in the unsupervised approaches category are grouped into four different subcategories based on the main structure or method used for modeling expressivity in these papers. From our observation, most of the proposed methods in the last 5 years are based on three main early works in this field, namely, reference encoder [74], global style tokens[75], and latent features via variational autoencoders (VAE)[76] [77]. Specifically, proposed models in most of the papers under this category can be considered as an extension or enhancement of one of the three previously mentioned methods. Besides, we identify a fourth subcategory that includes the recent TTS models representing the new trend in the TTS area, which utilizes in-context learning. There is one factor common to all these four unsupervised models,which is that they are all based on using an audio reference/prompt. Additionally, we added a fifth subcategory (named other approaches) in which we include approaches outside the previous four main unsupervised approaches. Fig.04 illustrates the proposed classification schema for the DL-based expressive speech synthesis models.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#3","title":"3.\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5","text":"<p>Supervised approaches refer to models that are trained on datasets with emotion labels. Those labels guide model training, enabling it to learn accurate weights. Early deep learning-based expressive speech synthesis systems were primarily supervised models that utilized labeled speech exhibiting various emotions (such as sadness, happiness, and anger) or speaking styles (such as talk-show, newscaster, and call-center). Note that the term style has also been used to refer to a set of emotions or a mixture of emotions and speaking styles (ST-TTS, [68] [78] [79].  Generally, the structure of early conventional TTS models was built upon two primary networks: one for predicting duration and the other for predicting acoustic features. These acoustic features were then converted to speech using vocoders. Both networks receive linguistic features extracted from the input text.  In supervised ETTS approaches, speech labels (emotions and/or styles) are represented in the TTS model as either input features or as separate layers, models, or sets of neurons for each specific label. The following sections explain these three representations in detail then we provide a general summary of the supervised approaches reviewed in this work in Table 2.</p> <p>\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u662f\u6307\u5728\u5e26\u6709\u60c5\u611f\u6807\u7b7e\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u8fd9\u4e9b\u6807\u7b7e\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\u4ee5\u786e\u4fdd\u80fd\u591f\u5b66\u4e60\u5230\u51c6\u786e\u7684\u6743\u91cd\u3002 \u65e9\u671f\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u4e3b\u8981\u662f\u76d1\u7763\u6a21\u578b\uff0c\u4ed6\u4eec\u4f7f\u7528\u5e26\u6709\u5404\u79cd\u60c5\u611f \uff08\u5982\u60b2\u4f24\u3001\u5feb\u4e50\u548c\u6124\u6012\uff09\u6216\u8bf4\u8bdd\u98ce\u683c\uff08\u5982\u8131\u53e3\u79c0\u3001\u65b0\u95fb\u64ad\u97f3\u548c\u547c\u53eb\u4e2d\u5fc3\uff09\u6807\u7b7e\u7684\u8bed\u97f3\u3002\u6ce8\u610f\uff0c\u201c\u98ce\u683c\u201d\u8fd9\u4e00\u672f\u8bed\u540c\u6837\u7528\u4e8e\u6307\u4ee3\u4e00\u7ec4\u60c5\u611f\u6216\u60c5\u611f\u548c\u8bf4\u8bdd\u98ce\u683c\u7684\u6df7\u5408\u3002 \u901a\u5e38\u65e9\u671f\u4f20\u7edf TTS \u6a21\u578b\u7684\u7ed3\u6784\u662f\u5efa\u7acb\u5728\u4e24\u4e2a\u4e3b\u8981\u7f51\u7edc\u4e4b\u4e0a\uff1a\u4e00\u4e2a\u7528\u4e8e\u9884\u6d4b\u65f6\u957f\uff1b\u53e6\u4e00\u4e2a\u7528\u4e8e\u9884\u6d4b\u58f0\u5b66\u7279\u5f81\uff0c\u8fd9\u4e9b\u58f0\u5b66\u7279\u5f81\u4e4b\u540e\u901a\u8fc7\u58f0\u7801\u5668\u8f6c\u6362\u4e3a\u8bed\u97f3\u3002\u8fd9\u4e24\u4e2a\u7f51\u7edc\u90fd\u63a5\u6536\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u51fa\u7684\u7684\u8bed\u4e49\u7279\u5f81\u3002 \u5728\u76d1\u7763 ETTS \u65b9\u6cd5\u4e2d\uff0c\u8bed\u97f3\u6807\u7b7e\uff08\u60c5\u611f\u4e0e\u3001\u6216\u98ce\u683c\uff09\u5728 TTS \u6a21\u578b\u4e2d\u88ab\u8868\u793a\u4e3a\u8f93\u5165\u7279\u5f81\u6216\u4e3a\u6bcf\u4e2a\u7279\u5b9a\u6807\u7b7e\u7684\u5355\u72ec\u5c42\u3001\u6a21\u578b\u6216\u795e\u7ecf\u5143\u96c6\u5408\u3002\u4ee5\u4e0b\u5c0f\u8282\u5c06\u8be6\u7ec6\u89e3\u91ca\u8fd9\u4e09\u79cd\u8868\u793a\uff0c\u7136\u540e\u5728\u8868\u683c\u4e8c\u4e2d\u63d0\u4f9b\u672c\u6587\u56de\u987e\u7684\u76d1\u7763\u65b9\u6cd5\u7684\u6982\u8ff0\u3002</p> \u5f15\u7528\u5e8f\u53f7 \u7b97\u6cd5\u7b80\u79f0 \u8f93\u5165 \u60c5\u611f\u6807\u7b7e\u8868\u793a \u662f\u5426\u652f\u6301\u60c5\u7eea\u8f6c\u79fb TTS \u6a21\u578b 80 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801 DL-SPSS, HMM 65 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801/\u5355\u72ec\u5c42 \u221a DL-SPSS 66 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u611f\u77e5\u5411\u91cf/\u77e9\u9635 DL-SPSS 41 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801 DL-SPSS 42 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u4f9d\u8d56\u5c42 DL-SPSS 81 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801/\u795e\u7ecf\u5143\u96c6\u5408 \u221a DL-SPSS 43 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801/\u4f9d\u8d56\u5c42/\u72ec\u7acb\u6a21\u578b DL-SPSS 82 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801 \u221a DL-SPSS 83 \u97f3\u7d20\u5e8f\u5217+\u8bed\u8a00\u6a21\u578b\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u5d4c\u5165\u5411\u91cf Encoder-Dttention-Decoder 28, 78 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801/\u4f9d\u8d56\u5c42/\u72ec\u7acb\u6a21\u578b \u221a DL-SPSS 26 \u97f3\u7d20\u5e8f\u5217+\u6885\u5c14\u9891\u8c31+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801 GSTs \u6743\u91cd\u7684\u771f\u5b9e\u503c Tacotron2 27 \u97f3\u7d20\u5e8f\u5217+\u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u5d4c\u5165\u5411\u91cf Tacotron2 84 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u5d4c\u5165\u4e0e\u5176\u4ed6\u6570\u636e\u6807\u7b7e\u8054\u5408 DL-SPSS 85 \u8bed\u8a00\u7279\u5f81+\u97f5\u5f8b\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c DL-SPSS 86 \u97f3\u7d20\u5e8f\u5217+\u60c5\u611f\u6807\u7b7e \u5d4c\u5165\u5411\u91cf Transformer TTS 32, 36 \u5b57\u7b26\u5e8f\u5217+\u6885\u5c14\u9891\u8c31+\u60c5\u611f\u6807\u7b7e \u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c Tacotron2 69 \u8bed\u8a00\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u72ec\u70ed\u7f16\u7801/\u4f9d\u8d56\u5c42 \u221a DL-SPSS 34 \u97f3\u7d20\u5e8f\u5217+\u6885\u5c14\u9891\u8c31+\u60c5\u611f\u6807\u7b7e \u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c Tacotron2 64 \u5b57\u7b26\u5e8f\u5217+\u8bed\u8a00\u6a21\u578b\u7279\u5f81+\u60c5\u611f\u6807\u7b7e \u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c Tacotron2 39, 87 \u97f3\u7d20\u5e8f\u5217+\u6885\u5c14\u9891\u8c31+\u60c5\u611f\u6807\u7b7e \u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c Tacotron2","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#31-labels-as-input-features","title":"3.1 Labels as Input Features \u6807\u7b7e\u4f5c\u4e3a\u8f93\u5165\u7279\u5f81","text":"<p>The most straightforward method for representing emotion labels of annotated datasets as input to the TTS model is by using a one-hot vector. This approach entails using a vector with a size equivalent to the number of available labels. In this vector, a value of (1) is assigned to the index corresponding to the label ID, while all other values are set to (0). Many early ETTS models [43] [56] [65] [69] [78] [80] [82] [84] advocated for this direct representation of emotion labels in order to generate speech encompassing various emotions.</p> <p>\u7528\u4e8e\u8868\u793a\u5e26\u6709\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u7684\u60c5\u611f\u6807\u7b7e\u4f5c\u4e3a TTS \u6a21\u578b\u8f93\u5165\u7684\u6700\u76f4\u63a5\u65b9\u6cd5\u662f\u4f7f\u7528\u72ec\u70ed\u7f16\u7801\u3002\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u4f7f\u7528\u548c\u53ef\u7528\u6807\u7b7e\u6570\u91cf\u957f\u5ea6\u76f8\u540c\u7684\u5411\u91cf\u3002\u5728\u8fd9\u4e2a\u5411\u91cf\u5185\uff0c\u5c06\u503c\u4e3a 1 \u5206\u914d\u7ed9\u6807\u7b7e ID \u5bf9\u5e94\u7684\u7d22\u5f15\uff0c\u5176\u4ed6\u503c\u4e3a 0. \u8bb8\u591a\u65e9\u671f ETTS \u6a21\u578b\u63d0\u5021\u8fd9\u79cd\u76f4\u63a5\u8868\u793a\u60c5\u611f\u6807\u7b7e\u7684\u65b9\u6cd5\u4ee5\u751f\u6210\u5305\u542b\u5404\u79cd\u60c5\u7eea\u7684\u8bed\u97f3\u3002</p> <p>The one-hot emotion vector, also referred to as a style/emotion code in some studies [43] [78] [80] [82], is concatenated with the input linguistic features of the model.</p> <p>\u72ec\u70ed\u7f16\u7801\u5728\u67d0\u4e9b\u6587\u732e\u4e2d\u4e5f\u88ab\u79f0\u4e3a\u98ce\u683c/\u60c5\u611f\u7f16\u7801\uff0c\u548c\u6a21\u578b\u7684\u8f93\u5165\u8bed\u8a00\u7279\u5f81\u8fdb\u884c\u62fc\u63a5\u3002</p> <p>When dealing with large number of labels, the one-hot representation becomes both high-dimensional and sparse. Moreover, in other scenarios, merging label vectors with input features instead of concatenation can lead to length mismatch issues. </p> <p>\u5f53\u5904\u7406\u5927\u91cf\u6807\u7b7e\u65f6\uff0c\u72ec\u70ed\u7f16\u7801\u8868\u793a\u53d8\u5f97\u9ad8\u7ef4\u4e14\u7a00\u758f\u3002\u800c\u4e14\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u5c06\u6807\u7b7e\u5411\u91cf\u548c\u8f93\u5165\u7279\u5f81\u5408\u5e76\u800c\u4e0d\u662f\u62fc\u63a5\u4f1a\u5bfc\u81f4\u957f\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\u3002</p> <p>In both situations, the embedding layer offers a solution by creating a continuous representation for each label, known as embedding vectors. Unlike the one-hot vector, which is constrained in size based on the number of labels, an emotion embedding can have any dimension, regardless of the number of available labels.</p> <p>\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u5d4c\u5165\u5c42\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff1a\u901a\u8fc7\u7ed9\u6bcf\u4e2a\u6807\u7b7e\u521b\u5efa\u4e00\u4e2a\u8fde\u7eed\u7684\u8868\u793a\uff0c\u5373\u5d4c\u5165\u5411\u91cf\u3002\u548c\u72ec\u70ed\u7f16\u7801\u53d7\u5230\u6807\u7b7e\u6570\u91cf\u7684\u9650\u5236\u4e0d\u540c\uff0c\u60c5\u611f\u5d4c\u5165\u53ef\u4ee5\u6709\u4efb\u610f\u7684\u7ef4\u5ea6\uff0c\u548c\u53ef\u7528\u6807\u7b7e\u6570\u91cf\u65e0\u5173\u3002</p> <p>For instance, in [84], each sample in the training dataset has three separated labels including speaker, style(emotion), and cluster. In this context, the cluster value indicates the consistency in speech quality of a given speaker and style pair. If one-hot vector is used to represent each unique combined label of each sample, the resulting label vector will be high dimensional (which in this case is 67). Therefore, the three one-hot vectors representing the given three labels are combined and passed as input to an embedding layer to reduce its dimension (in this case 15). On a different note, [41] utilizes an embedding layer to expand concise binary one-hot label vectors to match with the dimensions of the input features to be added together as input to the TTS model.</p> <p>\u4f8b\u5982, \u5728\u6587\u732e [84] \u4e2d, \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6bcf\u4e2a\u6837\u672c\u6709\u4e09\u4e2a\u72ec\u7acb\u7684\u6807\u7b7e, \u5305\u62ec\u8bf4\u8bdd\u4eba, \u98ce\u683c (\u60c5\u611f) \u548c\u805a\u7c7b\u7c7b\u522b. \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u805a\u7c7b\u7c7b\u522b\u503c\u8868\u660e\u4e86\u540c\u65f6\u7ed9\u5b9a\u8bf4\u8bdd\u4eba\u548c\u98ce\u683c\u5728\u8bed\u97f3\u8d28\u91cf\u65b9\u9762\u7684\u4e00\u81f4\u6027. \u5982\u679c\u72ec\u70ed\u7f16\u7801\u7528\u4e8e\u8868\u793a\u6bcf\u4e00\u4e2a\u552f\u4e00\u62fc\u63a5\u7684\u6807\u7b7e, \u90a3\u4e48\u6807\u7b7e\u5411\u91cf\u4f1a\u53d8\u5f97\u975e\u5e38\u9ad8\u7ef4. \u56e0\u6b64\u8fd9\u4e09\u4e2a\u72ec\u70ed\u7f16\u7801\u5411\u91cf\u5206\u522b\u8868\u793a\u7ed9\u5b9a\u7684\u4e09\u4e2a\u6807\u7b7e\u7ed3\u5408\u5e76\u8f93\u5165\u5230\u5d4c\u5165\u5c42\u8fdb\u884c\u964d\u7ef4. \u800c\u6587\u732e [41] \u4f7f\u7528\u5d4c\u5165\u5c42\u5c06\u7b80\u6d01\u7684\u4e8c\u8fdb\u5236\u72ec\u70ed\u6807\u7b7e\u5411\u91cf\u6269\u5c55\u5230\u5339\u914d\u8f93\u5165\u7279\u5f81\u7684\u7ef4\u5ea6, \u4ee5\u4fbf\u76f8\u52a0\u4f5c\u4e3a TTS \u6a21\u578b\u7684\u8f93\u5165.</p> <p>To address the potential disparities between a talker\u2019s intent and a listener\u2019s perception when annotating emotional samples, in [66], a different methodology for representing labels is introduced. In the context of N emotion classes, each sample from the talker may be perceived by the listener as one of the N emotions. In response to this, the paper suggests the adoption of a singular vector termed the \u2019perception vector,\u2019 with N dimensions. This vector represents how samples from a specific emotion class are distributed among the N emotions, based on the listener\u2019s perception. Furthermore, in the context of multiple listeners, each emotion class can be represented as a confusion matrix that captures the diverse perceptions of samples belonging to that emotion class by multiple listeners.</p> <p>\u4e3a\u4e86\u89e3\u51b3\u5728\u6ce8\u91ca\u60c5\u611f\u6837\u672c\u65f6\u8bf4\u8bdd\u4eba\u610f\u56fe\u548c\u503e\u542c\u8005\u7684\u611f\u77e5\u4e4b\u95f4\u7684\u6f5c\u5728\u5dee\u5f02, \u6587\u732e [66] \u5f15\u5165\u4e86\u8868\u793a\u6807\u7b7e\u7684\u4e0d\u540c\u65b9\u6cd5. \u5728\u5177\u6709 N \u4e2a\u60c5\u7eea\u7c7b\u522b\u7684\u60c5\u51b5\u4e0b, \u6765\u81ea\u8bf4\u8bdd\u4eba\u7684\u6bcf\u4e2a\u6837\u672c\u53ef\u80fd\u88ab\u503e\u542c\u8005\u611f\u77e5\u4e3a\u8fd9 $N$ \u4e2a\u60c5\u7eea\u7684\u5176\u4e2d\u4e4b\u4e00. \u5bf9\u6b64, \u6587\u732e [66] \u5efa\u8bae\u91c7\u6837\u4e00\u4e2a\u540d\u4e3a\"\u611f\u77e5\u5411\u91cf\"\u7684\u5355\u4e2a N \u7ef4\u5411\u91cf. \u8fd9\u4e00\u5411\u91cf\u8868\u793a\u7279\u5b9a\u60c5\u7eea\u7c7b\u522b\u7684\u6837\u672c\u5982\u4f55\u6839\u636e\u503e\u542c\u8005\u7684\u611f\u77e5\u5728 N \u4e2a\u60c5\u7eea\u4e0a\u5206\u5e03. \u6b64\u5916, \u5728\u591a\u503e\u542c\u8005\u7684\u60c5\u51b5\u4e0b, \u6bcf\u4e2a\u60c5\u7eea\u7c7b\u53ef\u4ee5\u8868\u793a\u4e3a\u4e00\u4e2a\u6df7\u6dc6\u77e9\u9635, \u6355\u83b7\u7531\u591a\u4e2a\u503e\u542c\u8005\u63d0\u4f9b\u7684\u5c5e\u4e8e\u8be5\u60c5\u7eea\u7c7b\u522b\u7684\u6837\u672c\u7684\u591a\u6837\u6027\u611f\u77e5.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#32-labels-as-separate-layersmodels","title":"3.2 Labels as Separate Layers/Models \u6807\u7b7e\u4f5c\u4e3a\u5355\u72ec\u5c42/\u6a21\u578b","text":"<p>In this approach, to represent emotion or style labels in TTS models, each label is associated with either a separate instance of the DNN model, an emotion-specific layers, or a set of emotion-specific neurons within a layer. Initially, the model is trained using neutral data, which typically has larger size. Subsequently, in the first approach, multiple copies of the trained model are fine-tuned using emotion-specific data of small size [43] [78]. In the second approach, instead of creating an individual model for each emotion, only specific model layers (usually the uppermost or final layers) from the employed DNN model are assigned to each emotion [43] [65] [69] [78] as shown by Fig. 5. While shared layers are adjusted during training using neutral data, output layers corresponding to each emotion are modified exclusively when the model is trained with data from the respective emotion.</p> <p>\u5728\u8fd9\u79cd\u65b9\u6cd5\u4e2d, TTS \u6a21\u578b\u5185\u4e3a\u4e86\u8868\u793a\u60c5\u611f\u6216\u98ce\u683c\u6807\u7b7e, \u6bcf\u4e2a\u6807\u7b7e\u8981\u4e48\u548c DNN \u6a21\u578b\u7684\u5355\u72ec\u793a\u4f8b\u5373\u4e00\u4e2a\u7279\u5b9a\u60c5\u611f\u5c42, \u8981\u4e48\u548c\u4e00\u5c42\u5185\u7684\u7279\u5b9a\u60c5\u611f\u795e\u7ecf\u5143\u96c6\u5408\u76f8\u5173\u8054.  \u9996\u5148, \u6a21\u578b\u4f7f\u7528\u901a\u5e38\u5c3a\u5bf8\u8f83\u5927\u7684\u4e2d\u6027\u6570\u636e\u8fdb\u884c\u8bad\u7ec3.  \u7b2c\u4e00\u79cd\u65b9\u6cd5, \u5bf9\u591a\u4e2a\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u7684\u526f\u672c\u5206\u522b\u4f7f\u7528\u5c0f\u5c3a\u5bf8\u7684\u7279\u5b9a\u60c5\u611f\u6570\u636e\u8fdb\u884c\u5fae\u8c03;  \u7b2c\u4e8c\u79cd\u65b9\u6cd5, \u4e0d\u4e3a\u6bcf\u79cd\u60c5\u611f\u521b\u5efa\u5355\u72ec\u6a21\u578b, \u800c\u662f\u53ea\u5c06\u6240\u4f7f\u7528\u7684 DNN \u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u5c42 (\u901a\u5e38\u662f\u6700\u4e0a\u5c42/\u6700\u7ec8\u5c42) \u5206\u914d\u7ed9\u6bcf\u79cd\u60c5\u611f. \u5982\u56fe\u4e94\u6240\u793a. \u4f7f\u7528\u4e2d\u6027\u6570\u636e\u8bad\u7ec3\u65f6\u5171\u4eab\u5c42\u4f1a\u8fdb\u884c\u8c03\u6574, \u5bf9\u5e94\u6bcf\u79cd\u60c5\u611f\u7684\u8f93\u51fa\u5c42\u4ec5\u5728\u6a21\u578b\u4f7f\u7528\u76f8\u5e94\u60c5\u611f\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u662f\u8fdb\u884c\u4fee\u6539.</p> <p>Alternatively, when dealing with limited data for certain emotions/styles, the model can initially undergo training for emotions with large amount of data. Following this step, the weights of the shared layers within the model are fixed, and only the weights of the top layers are fine-tuned using the limited, emotion-specific data [42]. </p> <p>\u5f53\u5904\u7406\u67d0\u4e9b\u60c5\u611f\u6216\u98ce\u683c\u7684\u6709\u9650\u6570\u636e\u65f6, \u6a21\u578b\u53ef\u4ee5\u5148\u4e3a\u5177\u6709\u5927\u91cf\u6570\u636e\u7684\u60c5\u611f\u8fdb\u884c\u8bad\u7ec3. \u5b8c\u6210\u540e\u5c06\u5171\u4eab\u5c42\u7684\u6743\u91cd\u56fa\u5b9a, \u53ea\u6709\u6700\u9876\u5c42\u7684\u6743\u91cd\u4f7f\u7528\u6709\u9650\u7684, \u7279\u5b9a\u60c5\u611f\u7684\u6570\u636e\u8fdb\u884c\u5fae\u8c03. \u5982\u6587\u732e [42]. </p> <p>Another method for representing emotion labels involves allocating specific neurons from a layer within the DNN model for each emotion. In this approach, the hidden layers of the model could be expanded by introducing new neurons. Then, as outlined in [81], particular neurons from this expanded set are assigned to represent each distinct emotion. Importantly, the associated weights of these specific neuron subsets are adjusted solely during the processing of data relevant to the corresponding emotion. Furthermore, by substituting the subset of neurons dedicated to a particular emotional class with a different set, the model becomes capable of generating speech imbued with the desired emotional class. This capability holds true even for new speakers who only possess neutral data, and in this case, it is known as expression/emotion transplantation.</p> <p>\u5176\u4ed6\u8868\u793a\u60c5\u611f\u6807\u7b7e\u7684\u65b9\u6cd5\u662f\u5c06 DNN \u6a21\u578b\u5c42\u4e2d\u7279\u5b9a\u7684\u795e\u7ecf\u5143\u5206\u914d\u7ed9\u6bcf\u79cd\u60c5\u611f. \u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u795e\u7ecf\u5143\u6765\u6269\u5c55\u6a21\u578b\u7684\u9690\u85cf\u5c42. \u5982\u6587\u732e [81] \u4ece\u6269\u5c55\u7684\u795e\u7ecf\u5143\u96c6\u5408\u4e2d\u5206\u914d\u7279\u5b9a\u795e\u7ecf\u5143\u6765\u8868\u793a\u6bcf\u79cd\u4e0d\u540c\u7684\u60c5\u611f. \u91cd\u8981\u7684\u662f\u53ea\u6709\u5728\u5904\u7406\u548c\u76f8\u5e94\u60c5\u611f\u76f8\u5173\u7684\u6570\u636e\u65f6, \u8fd9\u4e9b\u7279\u5b9a\u795e\u7ecf\u5143\u5b50\u96c6\u7684\u5173\u8054\u6743\u91cd\u624d\u4f1a\u8fdb\u884c\u8c03\u6574. \u6b64\u5916, \u901a\u8fc7\u52a0\u5165\u4e13\u95e8\u7528\u4e8e\u67d0\u79cd\u7279\u5b9a\u60c5\u611f\u7c7b\u522b\u7684\u795e\u7ecf\u5143, \u6a21\u578b\u53ef\u4ee5\u751f\u6210\u5177\u6709\u6240\u9700\u60c5\u611f\u7c7b\u7684\u8bed\u97f3. \u8fd9\u79cd\u80fd\u529b\u5bf9\u4ec5\u6709\u4e2d\u6027\u6570\u636e\u7684\u65b0\u8bf4\u8bdd\u4eba\u4e5f\u6210\u7acb, \u8fd9\u79f0\u4e3a\u8868\u8fbe/\u60c5\u611f\u79fb\u690d.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#33-labels-for-emotion-predictorsclassifiers","title":"3.3 Labels for Emotion Predictors/Classifiers \u6807\u7b7e\u7528\u4e8e\u60c5\u611f\u9884\u6d4b\u5668/\u5206\u7c7b\u5668","text":"<p>Another common approach to utilize emotion labels is to use them directly or via emotion predictor or classifier to support the process of extracting emotion/prosody embedding. </p> <p>\u53e6\u4e00\u79cd\u5e38\u89c1\u7684\u4f7f\u7528\u60c5\u611f\u6807\u7b7e\u7684\u65b9\u6cd5\u662f\u76f4\u63a5\u4f7f\u7528\u5b83\u4eec\u6216\u8005\u901a\u8fc7\u60c5\u611f\u9884\u6d4b\u5668/\u5206\u7c7b\u5668\u4ee5\u652f\u6301\u63d0\u53d6\u60c5\u611f/\u97f5\u5f8b\u5d4c\u5165\u7684\u8fc7\u7a0b.</p> <p>For example, in \"End-to-End Emotional Speech Synthesis Using Style Tokens  and Semi-Supervised Training\" emotion labels represented as one-hot vectors are used as targets for the weight vectors of GSTs (explained in Section 4.3) where a cross entropy loss between the two vectors is added to the total loss function. Yoon et al. [64] proposes a joint emotion predictor based on the Generative Pre-trained Transformer (GPT)-3 [88]. The proposed predictor produces two outputs including emotion class and emotion strength based on features extracted from input text by (GPT)-3. A joint emotion encoder is then used to encode the predictor outputs into a joint emotion embedding. The joint emotion predictor is trained with the guidance of the emotion labels and emotion strength values obtained via a ranking support vector machine (RankSVM) [89].</p> <p>\u6587\u732e [026] \u4e2d\u60c5\u611f\u6807\u7b7e\u8868\u793a\u6210\u72ec\u70ed\u5411\u91cf, \u4f5c\u4e3a GSTs \u6743\u91cd\u5411\u91cf\u7684\u76ee\u6807\u503c, \u8fd9\u4e24\u4e2a\u5411\u91cf\u4e4b\u95f4\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u88ab\u6dfb\u52a0\u5230\u603b\u635f\u5931\u51fd\u6570\u4e2d; \u6587\u732e [64] \u63d0\u51fa\u4e86\u57fa\u4e8e GPT-3 \u7684\u8054\u5408\u60c5\u611f\u9884\u6d4b\u5668, \u8fd9\u4e2a\u9884\u6d4b\u5668\u57fa\u4e8e GPT-3 \u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u4ea7\u751f\u4e24\u4e2a\u8f93\u51fa\u5305\u62ec\u60c5\u611f\u7c7b\u522b\u548c\u60c5\u611f\u5f3a\u5ea6. \u8054\u5408\u60c5\u611f\u7f16\u7801\u5668\u5c06\u9884\u6d4b\u5668\u7684\u8f93\u51fa\u7f16\u7801\u4e3a\u4e00\u4e2a\u8054\u5408\u60c5\u611f\u5d4c\u5165. \u8054\u5408\u60c5\u611f\u9884\u6d4b\u5668\u5728\u901a\u8fc7 RankSVM \u83b7\u5f97\u7684\u60c5\u611f\u6807\u7b7e\u548c\u60c5\u611f\u5f3a\u5ea6\u503c\u5f97\u6307\u5bfc\u4e0b\u8fdb\u884c\u8bad\u7ec3.</p> <p>In [32], an emotion classifier is used to produce more discriminative emotion embeddings. Initially, the input Mel-spectrogram features from the reference-style audio and those predicted by the proposed TTS model are passed to two reference encoders (explained in Section 4.1) to generate reference embeddings. Both embeddings are then fed to two emotion classifiers, which consist of intermediate fully connected (FC) layers. The output of the second FC layer from both classifiers is considered as the emotion embedding. Apart from the loss of the classifiers, an additional loss function is established between the resulting emotion embeddings from the two classifiers. Similarly, an emotion classifier is also employed in [36] to reduce irrelevant information in the generated emotion embedding from an emotion encoder with reference speech (Mel-spectrogram) as input.</p> <p>\u6587\u732e [32] \u4f7f\u7528\u60c5\u611f\u5206\u7c7b\u5668\u7528\u4e8e\u4ea7\u751f\u66f4\u5177\u6709\u533a\u5206\u6027\u5f97\u60c5\u611f\u5d4c\u5165. \u9996\u5148\u4ece\u53c2\u8003\u98ce\u683c\u97f3\u9891\u7684\u8f93\u5165\u6885\u5c14\u9891\u8c31\u7279\u5f81\u548c TTS \u6a21\u578b\u7684\u76f8\u5e94\u9884\u6d4b\u4f20\u9012\u5230\u4e24\u4e2a\u53c2\u8003\u7f16\u7801\u5668\u4e2d\u4ee5\u751f\u6210\u53c2\u8003\u5d4c\u5165. \u4e24\u4e2a\u5d4c\u5165\u4e4b\u540e\u90fd\u8f93\u5165\u5230\u4e24\u4e2a\u60c5\u611f\u5206\u7c7b\u5668\u4e2d, \u7531\u4e2d\u95f4\u5168\u8fde\u63a5\u5c42\u7ec4\u6210. \u5206\u7c7b\u5668\u7684\u7b2c\u4e8c\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u88ab\u89c6\u4e3a\u60c5\u611f\u5d4c\u5165. \u9664\u4e86\u5206\u7c7b\u5668\u7684\u635f\u5931\u4e4b\u5916, \u8fd8\u5efa\u7acb\u4e86\u4e24\u4e2a\u5206\u7c7b\u5668\u4ea7\u751f\u7684\u60c5\u611f\u5d4c\u5165\u7ed3\u679c\u4e4b\u95f4\u7684\u9644\u52a0\u635f\u5931\u51fd\u6570.  \u6587\u732e [36] \u4f7f\u7528\u4e86\u60c5\u611f\u5206\u7c7b\u5668\u7528\u4e8e\u51cf\u5c11\u5e26\u6709\u53c2\u8003\u8bed\u97f3 (\u6885\u5c14\u9891\u8c31) \u7684\u60c5\u611f\u7f16\u7801\u5668\u751f\u6210\u7684\u60c5\u611f\u5d4c\u5165\u7684\u65e0\u5173\u4fe1\u606f.</p> <p>Several other studies [34] [36] [39] that support multiple speakers also suggest utilizing a speaker classifier in addition to the emotion classifier. This approach aims to improved the speaker embedding derived from speaker encoders. Moreover, these studies introduce an adversarial loss between the speaker encoder and the emotion classifier using a gradient reversal layer (GRL) [90].The purpose of this is to minimize the potential transfer of emotion-related information into the speaker embedding. The GRL technique involves updating the weights of the speaker encoder by utilizing the inverse of the gradient values obtained from the emotion classifier during the training process.</p> <p>\u6587\u732e [34] [36] [39] \u652f\u6301\u591a\u8bf4\u8bdd\u4eba\u7684\u7814\u7a76\u4e5f\u5efa\u8bae\u4f7f\u7528\u8bf4\u8bdd\u4eba\u5206\u7c7b\u5668\u4ee5\u53ca\u60c5\u611f\u5206\u7c7b\u5668. \u8fd9\u4e00\u65b9\u6cd5\u65e8\u5728\u6539\u5584\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u5bfc\u51fa\u7684\u8bf4\u8bdd\u4eba\u5d4c\u5165. \u6b64\u5916\u8fd9\u4e9b\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u4e00\u4e2a GRL \u5f15\u5165\u4e86\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u548c\u60c5\u611f\u5206\u7c7b\u5668\u7684\u5bf9\u6297\u635f\u5931. \u76ee\u7684\u662f\u6700\u5c0f\u5316\u60c5\u611f\u76f8\u5173\u7684\u4fe1\u606f\u8f6c\u79fb\u5230\u8bf4\u8bdd\u4eba\u5d4c\u5165\u4e2d. GRL \u6280\u672f\u6d89\u53ca\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4ece\u60c5\u611f\u5206\u7c7b\u5668\u83b7\u5f97\u7684\u68af\u5ea6\u503c\u7684\u9006\u6765\u66f4\u65b0\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u7684\u6743\u91cd.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#4","title":"4.\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5","text":"<p>Due to the limited availability and challenges associated with collecting or preparing labeled datasets of expressive speech, as discussed in Section 6, many researchers tend to resort to unsupervised approaches for generating expressive speech. Within these approaches, models are trained to extract speaking styles or emotions from expressive speech data through unsupervised methods. Unsupervised models typically utilize reference speech as an input to the TTS model, which extracts a style or prosody embedding which is then used to synthesize speech resembling the input style reference. In the literature, three primary structures emerge as baseline models for unsupervised ETTS models: including reference encoders, global style tokens, and variational autoencoders, which are explained in the following three sections. In addition, we identify the recent TTS models that utilize in-context learning as another group of unsupervised approaches. The last subcategory under the unsupervised approaches involves other individual approaches. We then provide a general summary of all the unsupervised approaches reviewed in this work in Table 3.</p> <p>\u7531\u4e8e\u8868\u8fbe\u6027\u8bed\u97f3\u7684\u6807\u6ce8\u6570\u636e\u96c6\u7684\u6536\u96c6\u6216\u51c6\u5907\u76f8\u5173\u7684\u6709\u9650\u53ef\u7528\u6027\u548c\u6311\u6218, \u8bb8\u591a\u7814\u7a76\u4eba\u5458\u503e\u5411\u4e8e\u91c7\u7528\u65e0\u76d1\u7763\u65b9\u6cd5\u7528\u4e8e\u751f\u6210\u8868\u8fbe\u6027\u8bed\u97f3. \u5728\u8fd9\u4e9b\u65b9\u6cd5\u4e2d, \u6a21\u578b\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u6cd5\u88ab\u8bad\u7ec3\u7528\u4e8e\u4ece\u8868\u8fbe\u6027\u8bed\u97f3\u6570\u636e\u4e2d\u63d0\u53d6\u8bf4\u8bdd\u98ce\u683c\u6216\u60c5\u611f. \u65e0\u76d1\u7763\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u53c2\u8003\u8bed\u97f3\u4f5c\u4e3a\u8f93\u5165\u4f20\u9012\u7ed9 TTS \u6a21\u578b, \u8be5\u6a21\u578b\u63d0\u53d6\u98ce\u683c\u6216\u97f5\u5f8b\u5d4c\u5165, \u4e4b\u540e\u7528\u4e8e\u5408\u6210\u7c7b\u4f3c\u4e8e\u8f93\u5165\u98ce\u683c\u53c2\u8003\u7684\u8bed\u97f3.  \u73b0\u6709\u7684\u6587\u732e\u4e2d\u51fa\u73b0\u4e86\u4e09\u4e2a\u4e3b\u8981\u7ed3\u6784\u4f5c\u4e3a\u65e0\u76d1\u7763 ETTS \u6a21\u578b\u7684\u57fa\u7ebf\u6a21\u578b: Reference Encoders, Global Style Tokens, VAEs. \u6b64\u5916\u6211\u4eec\u8ba4\u4e3a\u8fd1\u671f\u4f7f\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684 TTS \u6a21\u578b\u4e3a\u5176\u4ed6\u4e00\u7ec4\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5.\u6700\u540e\u4e00\u4e2a\u5b50\u7c7b\u522b\u8fd8\u6d89\u53ca\u5230\u5176\u4ed6\u4e2a\u522b\u65b9\u6cd5. \u6211\u4eec\u5728\u8868\u683c\u4e09\u79cd\u63d0\u4f9b\u4e86\u672c\u6587\u56de\u987e\u7684\u6240\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u4e00\u822c\u6027\u603b\u7ed3.</p> <p>|\u5e8f\u53f7|\u7ec4\u522b|TTS \u6a21\u578b|\u97f5\u5f8b\u7ea7\u522b|</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#41-direct-reference-encoding","title":"4.1 Direct Reference Encoding","text":"<p>The main approach, based on a reference or prosody encoder, can be traced back to an early Google paper[74]. The paper suggests using a reference encoder to produce a low-dimensional embedding for a given style reference audio, which is called a prosody embedding. This encoder takes spectrograms as input to represent the reference audio. The generated prosody embedding is then concatenated with the text embedding derived from the text encoder of a Seq2Seq TTS model such as Tacotron, Tacotron2. Figure 6 shows reference encoder integrated to the TTS model.</p> <p>\u57fa\u4e8e\u53c2\u8003\u6216\u97f5\u5f8b\u7f16\u7801\u5668\u7684\u4e3b\u8981\u65b9\u6cd5\u53ef\u4ee5\u56de\u6eaf\u5230 Google \u7684\u4e00\u7bc7\u8bba\u6587 [74] Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron. \u8be5\u6587\u732e\u5efa\u8bae\u4f7f\u7528\u53c2\u8003\u7f16\u7801\u5668\u4ee5\u4e3a\u7ed9\u5b9a\u98ce\u683c\u53c2\u8003\u97f3\u9891\u751f\u6210\u4f4e\u7ef4\u5d4c\u5165, \u79f0\u4e3a\u97f5\u5f8b\u5d4c\u5165. \u8fd9\u4e00\u7f16\u7801\u5668\u5c06\u9891\u8c31\u4f5c\u4e3a\u8f93\u5165\u6765\u8868\u793a\u53c2\u8003\u97f3\u9891. \u751f\u6210\u7684\u97f5\u5f8b\u5d4c\u5165\u4f1a\u4e0e Seq2Seq TTS \u6a21\u578b\u5982 Tacotron \u7684\u6587\u672c\u7f16\u7801\u5668\u5bfc\u51fa\u7684\u6587\u672c\u5d4c\u5165\u76f8\u62fc\u63a5. \u56fe\u516d\u5c55\u793a\u4e86\u53c2\u8003\u7f16\u7801\u5668\u96c6\u6210\u5230 TTS \u6a21\u578b.</p> <p>Various features have been employed in the literature as inputs for the reference encoder. For example, in the work [85], MFCC features extracted using the openSMILE toolkit [139] are fed into one of the encoders within its style extraction model, which is composed of a multi-modal dual recurrent encoder (MDRE). In another study [31], the reference encoder is proposed as a ranking function model, aimed at learning emotion strength at the phoneme level. This model leverages the OpenSMILE toolkit to extract 384-dimensional emotion-related features from segments of reference audio, derived using a forced alignment model for phoneme boundaries. Furthermore, in work [63], a word-level prosody embedding is generated. This is achieved by extracting phoneme-level F0 features from reference speech using the WORLD vocoder [140] and an internal aligner operating with the input text.</p> <p>\u6587\u732e\u4e2d\u5df2\u7ecf\u4f7f\u7528\u4e86\u5404\u79cd\u7279\u5f81\u4f5c\u4e3a\u53c2\u8003\u7f16\u7801\u5668\u7684\u8f93\u5165. - \u6587\u732e [85] \u4f7f\u7528 OpenSMILE \u5de5\u5177\u7bb1\u63d0\u53d6 MFCC \u7279\u5f81\u88ab\u8f93\u5165\u5230\u5176\u98ce\u683c\u63d0\u53d6\u6a21\u578b\u7684\u4e00\u4e2a\u7f16\u7801\u5668\u4e2d. \u8be5\u6a21\u578b\u6709\u4e00\u4e2a\u591a\u6a21\u6001\u5bf9\u5076\u5faa\u73af\u7f16\u7801\u5668\u7ec4\u6210. - \u6587\u732e [31] \u53c2\u8003\u7f16\u7801\u5668\u88ab\u4f5c\u4e3a\u4e00\u4e2a\u6392\u5e8f\u51fd\u6570\u6a21\u578b, \u65e8\u5728\u97f3\u7d20\u7ea7\u522b\u5b66\u4e60\u60c5\u611f\u5f3a\u5ea6. \u8fd9\u4e2a\u6a21\u578b\u5229\u7528 OpenSMILE \u5de5\u5177\u7bb1\u4ece\u53c2\u8003\u97f3\u9891\u7247\u6bb5\u4e2d\u63d0\u53d6\u548c\u60c5\u611f\u76f8\u5173\u7684 384 \u7ef4\u7279\u5f81, \u8fd9\u4e9b\u7247\u6bb5\u662f\u5bf9\u97f3\u7d20\u8fb9\u754c\u91c7\u7528\u5f3a\u5236\u5bf9\u9f50\u6a21\u578b\u83b7\u5f97\u7684. - \u6587\u732e [63] \u751f\u6210\u4e86\u57fa\u4e8e\u5355\u8bcd\u7ea7\u522b\u7684\u97f5\u5f8b\u5d4c\u5165. \u8fd9\u662f\u901a\u8fc7\u4f7f\u7528 WORLD \u58f0\u7801\u5668\u548c\u4e0e\u8f93\u5165\u95ee\u9898\u4e00\u8d77\u64cd\u4f5c\u7684\u5185\u7f6e\u5bf9\u9f50\u5668\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u63d0\u53d6\u97f3\u7d20\u7ea7\u522b\u7684 F0 \u7279\u5f81\u6765\u5b9e\u73b0\u7684.</p> <p>A prosody-aware module is proposed in [37] which extracts other prosody-related features. The prosody-aware module consists of an encoder, an extractor, and a predictor. The encoder receives the three phoneme-level features including logarithmic fundamental frequency(LF0), intensity, and duration from the extractor as input and generates the paragraph prosody embedding with the assistance of an attention unit. Simultaneously, the predictor is trained to predict these features at inference time based on the input text embedding only.</p> <p>\u6587\u732e [37] \u63d0\u51fa\u4e86\u97f5\u5f8b\u611f\u77e5\u6a21\u5757\u7528\u4e8e\u63d0\u53d6\u5176\u4ed6\u97f5\u5f8b\u76f8\u5173\u7279\u5f81. \u8be5\u6a21\u5757\u7531\u4e00\u4e2a\u7f16\u7801\u5668, \u4e00\u4e2a\u63d0\u53d6\u5668\u548c\u4e00\u4e2a\u9884\u6d4b\u5668\u7ec4\u6210. \u7f16\u7801\u5668\u63a5\u6536\u6765\u81ea\u63d0\u53d6\u5668\u7684\u4e09\u4e2a\u97f3\u7d20\u7ea7\u522b\u7279\u5f81\u5305\u62ec\u5bf9\u6570\u57fa\u9891 LF0, \u5f3a\u5ea6\u548c\u65f6\u957f\u4f5c\u4e3a\u8f93\u5165, \u5e76\u501f\u52a9\u6ce8\u610f\u529b\u5355\u5143\u751f\u6210\u6bb5\u843d\u97f5\u5f8b\u5d4c\u5165. \u540c\u65f6\u9884\u6d4b\u5668\u88ab\u8bad\u7ec3\u7528\u4e8e\u5728\u63a8\u7406\u65f6\u4ec5\u8f93\u5165\u6587\u672c\u5d4c\u5165\u6765\u9884\u6d4b\u8fd9\u4e9b\u7279\u5f81.</p> <p>In Daft-Exprt TTS model [118], the prosody encoder receives pitch, energy and spectrogram as input. The prosody encoder then uses FiLM conditioning layers[141] to carry out affine transformations to the intermediate features of specific layers in the TTS model. A slightly modified version of the FastSpeech2 model is utilized in this work where the phoneme encoder,prosody predictor and the decoder are the conditioned components. The prosody predictor is similar to the variance adaptor of FastSpeech2 but without the length regulator, and it estimates pitch, energy and duration at phoneme-level.</p> <p>\u5728 Draft-Exprt TTS \u6a21\u578b\u4e2d, \u97f5\u5f8b\u7f16\u7801\u5668\u63a5\u53d7\u97f3\u9ad8, \u80fd\u91cf\u548c\u9891\u8c31\u4f5c\u4e3a\u8f93\u5165. \u7136\u540e\u97f5\u5f8b\u7f16\u7801\u5668\u4f7f\u7528 FiLM \u6761\u4ef6\u5c42\u5bf9 TTS \u6a21\u578b\u7684\u7279\u5b9a\u5c42\u7684\u4e2d\u95f4\u7279\u5f81\u6267\u884c\u4eff\u5c04\u53d8\u6362. \u8fd9\u9879\u5de5\u4f5c\u4f7f\u7528\u4e86 FastSpeech2 \u7684\u7a0d\u5fae\u4fee\u6539\u7248\u672c, \u5176\u4e2d\u97f3\u7d20\u7f16\u7801\u5668, \u97f5\u5f8b\u9884\u6d4b\u5668\u548c\u89e3\u7801\u5668\u90fd\u662f\u6761\u4ef6\u5316\u7ec4\u4ef6. \u97f5\u5f8b\u9884\u6d4b\u5668\u548c FastSpeech2 \u7684\u65b9\u5dee\u9002\u914d\u5668\u76f8\u4f3c\u4f46\u6ca1\u6709\u957f\u5ea6\u8c03\u8282\u5668, \u4e14\u5176\u5728\u97f3\u7d20\u6c34\u5e73\u4f30\u8ba1\u97f3\u9ad8, \u80fd\u91cf\u548c\u65f6\u957f.</p> <p>A pre-trained Wav2Vec model [142] has also been utilized for extracting features from the reference waveform.</p> <p>\u6587\u732e [142] \u91c7\u7528\u9884\u8bad\u7ec3 Wav2Vec \u6a21\u578b\u7528\u4e8e\u4ece\u53c2\u8003\u6ce2\u5f62\u4e2d\u63d0\u53d6\u7279\u5f81. </p> <p>These features serve as input to the reference encoders of the proposed Emo-VITS model, which integrates an emotion network into the VITS model [143] to enhance expressive speech synthesis. In fact, the emotion network in the Emo-VITS model comprises two reference encoders. The resulting emotion embeddings from these encoders are then combined through a feature fusion module that employs an attention mechanism. Wav2vec2.0-derived features from the reference waveform in this work are particularly suitable for attention-based fusion and contribute to reducing the textual content within the resulting embeddings.</p> <p>Emo-VITS \u5c06\u8fd9\u4e9b\u7279\u5f81\u4f5c\u4e3a Emo-VITS \u7684\u53c2\u8003\u7f16\u7801\u5668\u7684\u8f93\u5165, \u5b83\u5c06\u60c5\u611f\u7f51\u7edc\u96c6\u6210\u5230 VITS \u6a21\u578b\u4e2d\u7528\u4e8e\u589e\u5f3a\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210. \u5b9e\u9645\u4e0a, \u60c5\u611f\u7f51\u7edc\u5305\u542b\u4e86\u4e24\u4e2a\u53c2\u8003\u7f16\u7801\u5668. \u8fd9\u4e9b\u7f16\u7801\u5668\u7684\u60c5\u611f\u5d4c\u5165\u8f93\u51fa\u4e4b\u540e\u901a\u8fc7\u4e00\u4e2a\u7279\u5f81\u878d\u5408\u6a21\u5757\u8fdb\u884c\u7ed3\u5408\u7136\u540e\u5e94\u7528\u6ce8\u610f\u529b\u673a\u5236. \u7531 Wave2Vec 2.0 \u4ece\u53c2\u8003\u97f3\u9891\u5bfc\u51fa\u7684\u7279\u5f81\u5c24\u5176\u9002\u5408\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u878d\u5408, \u4e14\u6709\u52a9\u4e8e\u5728\u7ed3\u679c\u5d4c\u5165\u4e2d\u51cf\u5c11\u6587\u672c\u5185\u5bb9.</p> <p>In contrast, [60] proposes a an image style transfer module to generate input for reference encoder. The concept of image style transfer involves altering the artistic style of an image from one domain to another while retaining the image\u2019s original content [144]. In specific research, the style reconstruction module from VGG-19[145], a deep neural network primarily used for image classification, is employed to extract style-related information from the Mel-spectrogram used as input image. Subsequently, the output of this module is fed into the reference encoder to generate the style embedding.</p> <p>\u6587\u732e [60] \u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u6a21\u5757\u7528\u4e8e\u751f\u6210\u53c2\u8003\u7f16\u7801\u5668\u7684\u8f93\u5165. \u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u7684\u6982\u5ff5\u6d89\u53ca\u5230\u56fe\u50cf\u7684\u827a\u672f\u98ce\u683c\u4ece\u4e00\u4e2a\u9886\u57df\u7684\u8f6c\u5316\u5230\u53e6\u4e00\u4e2a\u9886\u57df, \u540c\u65f6\u4fdd\u7559\u56fe\u50cf\u539f\u59cb\u5185\u5bb9. \u5177\u4f53\u800c\u8a00\u6765\u81ea VGG-19 \u7684\u98ce\u683c\u91cd\u6784\u6a21\u5757, \u4e00\u4e2a\u4e3b\u8981\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edc, \u5c06\u6885\u5c14\u9891\u8c31\u89c6\u4e3a\u8f93\u5165\u56fe\u50cf\u4ece\u4e2d\u63d0\u53d6\u548c\u98ce\u683c\u76f8\u5173\u7684\u4fe1\u606f. \u4e4b\u540e\u8fd9\u4e2a\u6a21\u5757\u7684\u8f93\u51fa\u4f20\u9012\u5230\u53c2\u8003\u7f16\u7801\u5668\u4e2d\u4ee5\u751f\u6210\u98ce\u683c\u5d4c\u5165.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#42-latent-features-via-variational-autoencoders-vae","title":"4.2 Latent Features via Variational Auto\u2011Encoders \u901a\u8fc7 VAE \u83b7\u53d6\u9690\u7279\u5f81","text":"<p>The goal of TTS models under this is to map input speech from the higher dimensional space to a well-organized and lower-dimensional latent space utilizing variational auto-encoders (VAEs) [146]. VAE is a generative model that is trained to learn the mapping between observed data x and continuous random vectors z in an unsupervised manner. In detail, VAEs learn a Gaussian distribution denoted as the latent space from which the latent vectors representing the given data x can be sampled. A typical variational autoencoder consists of two components. First, the encoder learns the parameters of the z vectors (latent distribution), namely the mean $\\mu(x)$ and variance $\\sigma^2(x)$, based on the input data x. Second,the decoder regenerates the input data x based on latent vectors z sampled from the distribution learned by the encoder. In addition to the reconstruction loss between the model input and the data, variational autoencoders are also trained to minimize a latent loss, which ensures that the latent space follows a Gaussian distribution.</p> <p>\u5728\u8fd9\u7c7b TTS \u6a21\u578b\u4e2d, \u76ee\u6807\u662f\u5229\u7528 VAE \u5c06\u6765\u81ea\u4e8e\u9ad8\u7ef4\u7a7a\u95f4\u7684\u8bed\u97f3\u6620\u5c04\u5230\u7ec4\u7ec7\u826f\u597d\u4e14\u7ef4\u5ea6\u8f83\u4f4e\u7684\u9690\u7a7a\u95f4. VAE \u662f\u4e00\u79cd\u751f\u6210\u6a21\u578b, \u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\u5b66\u4e60\u89c2\u5bdf\u6570\u636e $x$ \u548c\u8fde\u7eed\u968f\u673a\u5411\u91cf $z$ \u4e4b\u95f4\u7684\u6620\u5c04. \u5177\u4f53\u6765\u8bf4, VAE \u5b66\u4e60\u4e00\u4e2a\u8bb0\u4e3a\u9690\u7a7a\u95f4\u7684\u9ad8\u65af\u5206\u5e03, \u4ece\u4e2d\u53ef\u4ee5\u91c7\u6837\u5230\u80fd\u8868\u793a\u7ed9\u5b9a\u6570\u636e $x$ \u7684\u9690\u5411\u91cf. \u4e00\u4e2a\u5178\u578b\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7531\u4e24\u90e8\u5206\u7ec4\u6210: \u4e00\u662f\u7f16\u7801\u5668\u57fa\u4e8e\u8f93\u5165\u6570\u636e\u5b66\u4e60 $z$ \u5411\u91cf\u5373\u9690\u5206\u5e03\u7684\u53c2\u6570: \u5747\u503c\u548c\u65b9\u5dee, \u4e8c\u662f\u89e3\u7801\u5668\u57fa\u4e8e\u89e3\u7801\u5668\u5b66\u4e60\u5230\u7684\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u9690\u53d8\u91cf $z$ \u91cd\u65b0\u751f\u6210\u8f93\u5165\u6570\u636e. \u9664\u4e86\u6a21\u578b\u8f93\u51fa\u548c\u6570\u636e\u4e4b\u95f4\u7684\u91cd\u6784\u635f\u5931, \u53d8\u5206\u81ea\u7f16\u7801\u5668\u8fd8\u9700\u8981\u6700\u5c0f\u5316\u4e00\u4e2a\u6f5c\u5728\u635f\u5931, \u4f7f\u5f97\u9690\u7a7a\u95f4\u670d\u4ece\u9ad8\u65af\u5206\u5e03.</p> <p>Utilizing VAEs in expressive TTS models as shown by Fig. 7, allows for mapping the various speech styles within the given dataset to be encoded as latent vectors,often referred to as prosody vectors, within this latent space. During inference, these latent vectors can be sampled directly or with the guidance of reference audio from the VAE\u2019s latent space. Furthermore, the latent vectors offer the advantage of disentangling prosody features,meaning that some specific dimensions of these vectors independently represent single prosody features such as pitch variation or speaking rate. Disentangled prosody features allow for better prosody control via manipulating the latent vectors with different operations such as interpolation and scaling [77]. </p> <p>\u5728\u8868\u8fbe\u6027 TTS \u4e2d\u4f7f\u7528 VAEs \u5982\u56fe\u4e03\u6240\u793a, \u5141\u8bb8\u5c06\u7ed9\u5b9a\u6570\u636e\u96c6\u4e2d\u5404\u79cd\u8bed\u97f3\u98ce\u683c\u7f16\u7801\u4e3a\u9690\u53d8\u91cf, \u901a\u5e38\u79f0\u4e3a\u97f5\u5f8b\u5411\u91cf. \u5728\u63a8\u7406\u65f6\u8fd9\u4e9b\u9690\u53d8\u91cf\u53ef\u4ee5\u4ece\u9690\u7a7a\u95f4\u76f4\u63a5\u91c7\u6837\u6216\u5728\u53c2\u8003\u97f3\u9891\u7684\u6307\u5bfc\u4e0b\u91c7\u6837. \u6b64\u5916, \u9690\u53d8\u91cf\u8fd8\u63d0\u4f9b\u4e86\u5206\u79bb\u97f5\u5f8b\u7279\u5f81\u7684\u4f18\u52bf, \u610f\u5473\u7740\u8fd9\u4e9b\u5411\u91cf\u67d0\u4e9b\u7279\u5b9a\u7ef4\u5ea6\u72ec\u7acb\u5730\u8868\u793a\u5355\u4e2a\u97f5\u5f8b\u7279\u5f81, \u5982\u97f3\u9ad8\u53d8\u5316\u6216\u8bed\u901f. \u5206\u79bb\u7684\u97f5\u5f8b\u7279\u5f81\u901a\u8fc7\u67d0\u4e9b\u64cd\u4f5c\u4ee5\u8fdb\u884c\u66f4\u597d\u7684\u97f5\u5f8b\u63a7\u5236, \u5982\u63d2\u503c\u548c\u7f29\u653e.</p> <p>The two early papers, [76] [77], can be regarded as the baseline for latent feature-based approaches. The former study [76] introduces VAE within the VoiceLoop model [147], while the latter [77]incorporates VAE into Tacotron2 as an end-to-end TTS model for expressive speech synthesis.</p> <p>\u6587\u732e [76] [77] \u53ef\u4ee5\u89c6\u4e3a\u57fa\u4e8e\u9690\u7279\u5f81\u65b9\u6cd5\u7684\u57fa\u7ebf\u6a21\u578b. [76] \u5c06 VAE \u5f15\u5165 VoiceLoop \u6a21\u578b, [77] \u5c06 VAE \u96c6\u6210\u5230 Tacotron2 \u4e2d\u4f5c\u4e3a\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u7684\u7aef\u5230\u7aef TTS \u6a21\u578b.</p> <p>In the same direction of modeling the variation of the prosodic features in expressive speech, studies [109] [110] propose a hierarchical structure for the baseline variational autoencoder, known as Clockwork Hierarchical Variational AutoEncoder (CHiVE). Both the encoder and decoder in the CHiVE model have several layers to capture prosody at different levels based on the input text\u2019s hierarchical structure. Accordingly, linguistic features are also used alongside acoustic features as input to the model\u2019s encoder. The model\u2019s layers are dynamically clocked at specific rates: sentence, words, syllables, and phones. The encoder hierarchy goes from syllables to the sentence level, while the decoder hierarchy is in the reversed order.</p> <p>\u8868\u8fbe\u6027\u8bed\u97f3\u4e2d\u5efa\u6a21\u97f5\u5f8b\u7279\u5f81\u53d8\u5316\u7684\u65b9\u9762, \u6587\u732e [109], [110] \u4e3a\u57fa\u7ebf VAE \u63d0\u51fa\u4e86\u4e00\u4e2a\u5c42\u6b21\u7ed3\u6784, \u5373 CHiVE. \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u90fd\u6709\u6570\u5c42, \u57fa\u4e8e\u8f93\u5165\u6587\u672c\u7684\u5c42\u6b21\u7ed3\u6784\u5728\u4e0d\u540c\u7ea7\u522b\u6355\u83b7\u97f5\u5f8b. \u56e0\u6b64\u9664\u4e86\u58f0\u5b66\u7279\u5f81\u4e4b\u5916, \u8bed\u8a00\u7279\u5f81\u4e5f\u4f5c\u4e3a\u6a21\u578b\u7f16\u7801\u5668\u7684\u8f93\u5165. \u6a21\u578b\u7684\u5c42\u4ee5\u7279\u5b9a\u7684\u901f\u7387\u52a8\u6001\u8ba1\u65f6: \u53e5\u5b50, \u5355\u8bcd, \u97f3\u8282\u548c\u97f3\u7d20. \u7f16\u7801\u5668\u7684\u5c42\u6b21\u7ed3\u6784\u4ece\u97f3\u8282\u5230\u53e5\u5b50, \u800c\u89e3\u7801\u5668\u5219\u76f8\u53cd;</p> <p>The CHiVE-BERT model in [110], differs from the main model in [109] as it utilizes BERT [148] features for input text at the word-level. Since the features extracted by the BERT model incorporate both syntactic and semantic information from a large language model, CHiVE-BERT model is expected to have improved the prosody generation.</p> <p>ChiVE-BERT \u6a21\u578b\u4f7f\u7528 BERT \u7279\u5f81\u4f5c\u4e3a\u5355\u8bcd\u7ea7\u522b\u7684\u8f93\u5165\u6587\u672c. \u7531\u4e8e BERT \u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u5305\u542b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f, \u6240\u4ee5 CHiVE-BERT \u6a21\u578b\u9884\u8ba1\u5c06\u63d0\u9ad8\u97f5\u5f8b\u7684\u751f\u6210.</p> <p>Other studies DiffProsody [53] propose Vector-Quantized Variational Auto-Encoder (VQ-VAE) to achieve discretized latent prosody vectors. In vector quantization(VQ) [149], latent representations are mapped from the prosody latent space to a codebook of a limited number of prosody codes. Specifically, during training, the nearest neighbor lookup algorithm is applied to find the nearest codebook vector to the output of the reference encoder and used to condition TTS decoder. </p> <p>DiffProsody [53] \u63d0\u51fa\u77e2\u91cf\u91cf\u5316 VAE \u7528\u4e8e\u5b9e\u73b0\u79bb\u6563\u7684\u9690\u97f5\u5f8b\u5411\u91cf. \u5728 VQ \u4e2d, \u9690\u8868\u793a\u4ece\u97f5\u5f8b\u6f5c\u5728\u7a7a\u95f4\u6620\u5c04\u5230\u6709\u9650\u6570\u91cf\u7684\u97f5\u5f8b\u4ee3\u7801\u7684\u7801\u672c. \u7279\u522b\u5730\u5728\u8bad\u7ec3\u65f6, \u6700\u8fd1\u90bb\u67e5\u627e\u7b97\u6cd5\u5e94\u7528\u4e8e\u67e5\u627e\u53c2\u8003\u7f16\u7801\u5668\u8f93\u51fa\u6700\u8fd1\u7684\u7801\u672c\u5411\u91cf, \u5e76\u7528\u4e8e\u6761\u4ef6 TTS \u6a21\u578b. </p> <p>To further improve the quality of latent prosody vectors and consequently the expressiveness of the generated speech, DiffProsody proposes a diffusion-based VQ-VAE model.</p> <p>\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9690\u97f5\u5f8b\u5411\u91cf\u7684\u8d28\u91cf\u548c\u540e\u7eed\u751f\u6210\u8bed\u97f3\u7684\u8868\u8fbe\u6027, Diff-Prosody \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684 VQ-VAE \u6a21\u578b.</p> <p>In the proposed model a prosody generator that utilizes a denoising diffusion generative adversarial networks (DDGANs) [150] is trained to generate the prosody latent vectors based only on text and speaker information. At inference time, the prosody generator is used to produce prosody vectors based on input text and with no need for an audio reference which improves both quality and speed of speech synthesis.</p> <p>\u5728\u63d0\u51fa\u7684\u6a21\u578b\u4e2d, \u4e00\u4e2a\u97f5\u5f8b\u751f\u6210\u5668\u4f7f\u7528\u53bb\u566a\u6269\u6563\u5bf9\u6297\u751f\u6210\u6a21\u578b\u4ec5\u4f7f\u7528\u6587\u672c\u548c\u8bf4\u8bdd\u4eba\u4fe1\u606f\u751f\u6210\u97f5\u5f8b\u9690\u53d8\u91cf. \u5728\u63a8\u7406\u65f6, \u97f5\u5f8b\u751f\u6210\u5668\u7528\u4e8e\u57fa\u4e8e\u8f93\u5165\u6587\u672c\u7684\u97f5\u5f8b\u5411\u91cf\u800c\u65e0\u9700\u97f3\u9891\u53c2\u8003, \u4ece\u800c\u63d0\u5347\u8bed\u97f3\u5408\u6210\u7684\u8d28\u91cf\u548c\u901f\u5ea6.</p> <p>While most of the studies in this category follow the baseline model and use mel-spectrograms to represent the reference audio, other studies extract correlated prosody features as input to the VAE. For instance, frame-level F0, energy, and duration features are extracted from the reference speech as basic input for the hierarchical encoder of the CHiVE model [109]. These same features are also used as input for the VAE encoder in work [35], but at the phoneme level. In work [68], multi-resolution VAEs are employed, each with acoustic and linguistic input vectors. The acoustic feature vectors for each encoder include 70 mel-cepstral coefficients, log F0value, a voiced/unvoiced value, and 35 mel-cepstral analysis aperiodicity measures.</p> <p>\u6b64\u7c7b\u7684\u5927\u90e8\u5206\u7814\u7a76\u90fd\u9075\u5faa\u57fa\u7ebf\u6a21\u578b\u5e76\u4f7f\u7528\u6885\u5c14\u9891\u8c31\u8868\u793a\u53c2\u8003\u97f3\u9891, \u5176\u4ed6\u7814\u7a76\u63d0\u53d6\u76f8\u5173\u97f5\u5f8b\u7279\u5f81\u4f5c\u4e3a VAE \u7684\u8f93\u5165. \u4f8b\u5982 \u5e27\u7ea7\u522b\u7684 F0, \u80fd\u91cf, \u65f6\u957f\u7279\u5f81\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u63d0\u53d6, \u4f5c\u4e3a ChiVE \u6a21\u578b\u5c42\u6b21\u7f16\u7801\u5668\u7684\u8f93\u5165. \u8fd9\u4e9b\u76f8\u540c\u7279\u5f81\u540c\u6837\u5728\u6587\u732e [35] \u4e2d\u4f7f\u7528, \u4f46\u662f\u662f\u97f3\u7d20\u7ea7\u522b.</p> <p>\u6587\u732e [68] \u91c7\u7528\u4e86\u591a\u5206\u8fa8\u7387 VAEs, \u6bcf\u4e2a\u90fd\u6709\u58f0\u5b66\u548c\u8bed\u8a00\u8f93\u5165\u5411\u91cf. \u58f0\u5b66\u7279\u5f81\u5305\u62ec 70 \u4e2a\u6885\u5c14\u9891\u8c31\u7cfb\u6570, \u5bf9\u6570 F0 \u503c, \u6709\u58f0/\u65e0\u58f0\u503c\u548c 35 \u4e2a mel-cepstral \u5206\u6790\u975e\u5468\u671f\u5ea6\u91cf.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#43-global-style-tokens","title":"4.3 Global Style Tokens","text":"<p>The Global Style Tokens (GST) approach for expressive synthesis was first introduced in [75]. The paper proposes a framework to learn various speaking styles (referred to as style tokens) in an unsupervised manner within an end-to-end TTS model. The proposed approach can be seen as a soft clustering method that learns soft style clusters for expressive styles in an unlabeled dataset. In detail, GST, as shown by Fig. 8, extends the approach introduced in [74] by passing the resulting style embedding from the reference encoder to an attention unit,which functions as a similarity measure between the style embedding and a bank of randomly initialized tokens. During training, the model learns the style tokens and a set of weights, where each style embedding is generated via a weighted sum of the learned tokens. In fact, the obtained weights represent how each token contributes to the final style embedding. Therefore, each token will represent a single style or a single prosody-related feature, such as pitch, intensity, or speaking rate. At inference time, a reference audio can be passed to the model to generate its corresponding style embedding via a weighted sum of the style tokens. Alternatively, each individual style token can be used as a style embedding. In addition, GSTs offer an enhanced control over the speaking style through various operations. These include manual weight refinement, token scaling with different values, or the ability to condition different parts of the input text with distinct style tokens.</p> <p>\u7528\u4e8e\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u7684\u5168\u5c40\u98ce\u683c\u6807\u8bb0\u65b9\u6cd5\u5728\u6587\u732e [75] \u4e2d\u88ab\u9996\u6b21\u63d0\u51fa. \u8be5\u6587\u732e\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u7528\u4e8e\u7aef\u5230\u7aef TTS \u6a21\u578b\u4e2d\u4ee5\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u5f0f\u7528\u4e8e\u5b66\u4e60\u5404\u79cd\u8bf4\u8bdd\u98ce\u683c (\u79f0\u4e3a\u98ce\u683c\u6807\u8bb0). \u8be5\u65b9\u6cd5\u53ef\u4ee5\u89c6\u4e3a\u4e00\u79cd\u8f6f\u805a\u7c7b\u65b9\u6cd5\u7528\u4e8e\u65e0\u6807\u7b7e\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u8f6f\u98ce\u683c\u7c07. \u5177\u4f53\u5730 GST \u5982\u56fe\u516b\u6240\u793a, \u5c06\u6587\u732e [74] \u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6269\u5c55, \u5c06\u53c2\u8003\u7f16\u7801\u5668\u8f93\u51fa\u7684\u98ce\u683c\u5d4c\u5165\u4f20\u9012\u7ed9\u6ce8\u610f\u529b\u5355\u5143, \u4f5c\u4e3a\u98ce\u683c\u5d4c\u5165\u548c\u4e00\u7ec4\u968f\u673a\u521d\u59cb\u6807\u8bb0\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf. \u5728\u8bad\u7ec3\u65f6, \u6a21\u578b\u5b66\u4e60\u98ce\u683c\u6807\u8bb0\u548c\u4e00\u7ec4\u6743\u91cd, \u5176\u4e2d\u6bcf\u4e2a\u98ce\u683c\u5d4c\u5165\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u6807\u8bb0\u8fdb\u884c\u52a0\u6743\u548c\u83b7\u5f97. \u5b9e\u9645\u4e0a, \u83b7\u5f97\u7684\u6743\u91cd\u8868\u793a\u6bcf\u4e2a\u6807\u8bb0\u5bf9\u4e8e\u6700\u7ec8\u98ce\u683c\u5d4c\u5165\u7684\u8d21\u732e. \u56e0\u6b64\u6bcf\u4e2a\u6807\u8bb0\u5c06\u8868\u793a\u5355\u4e2a\u98ce\u683c\u6216\u5355\u4e2a\u97f5\u5f8b\u76f8\u5173\u7684\u7279\u5f81, \u4f8b\u5982\u97f3\u9ad8, \u5f3a\u5ea6\u6216\u8bed\u901f. \u5728\u63a8\u7406\u65f6, \u53c2\u8003\u97f3\u9891\u4f20\u9012\u7ed9\u6a21\u578b\u901a\u8fc7\u98ce\u683c\u6807\u8bb0\u7684\u52a0\u6743\u548c\u7528\u4e8e\u751f\u6210\u5bf9\u5e94\u98ce\u683c\u5d4c\u5165. \u6216\u8005\u6bcf\u4e2a\u5355\u72ec\u7684\u98ce\u683c\u6807\u8bb0\u4f5c\u4e3a\u98ce\u683c\u5d4c\u5165. \u6b64\u5916 GSTs \u63d0\u4f9b\u4e86\u901a\u8fc7\u5404\u79cd\u64cd\u4f5c\u589e\u5f3a\u5bf9\u8bf4\u8bdd\u98ce\u683c\u7684\u63a7\u5236, \u5305\u62ec\u624b\u52a8\u6743\u91cd\u7ec6\u5316, \u6807\u8bb0\u6309\u4e0d\u540c\u503c\u7f29\u653e\u6216\u4f7f\u7528\u4e0d\u540c\u98ce\u683c\u6807\u8bb0\u6761\u4ef6\u8bdd\u8f93\u5165\u6587\u672c\u7684\u4e0d\u540c\u90e8\u5206.</p> <p>The GST-TTS model can be further enhanced by modeling different levels of prosody to improve both expressiveness and control over the generated speech. For instance, [46] proposes a fine-grained GST-TTS model where word-level GSTs are generated to capture local style variations (WSVs) through a prosody extractor. The WSV extractor consists of a reference encoder and a style token layer, as described in [75], along with an attention unit to produce the word-level style token.</p> <p>In [133] a hierarchical structure of multi-layer GSTs with residuals is proposed. The model employs three GST layers, each with 10 tokens, resulting in a better interpretation of the tokens of each level. Upon tokens analysis, it was found that the first-layer tokens learned speaker representations, while the second-layer tokens captured various speaking style features such as pause position, duration, and stress. The third-layer tokens, however, were able to generate higher-quality samples with more distinct and interpretable styles. Similarly, in[50], a multi-scale GST extractor is proposed to extract speaking style at different levels. This extractor extracts style embeddings from the reference mel-spectrogram using three style encoders at global, sentence, and sub word levels, and combines their outputs to form the multi-scale style embedding.</p> <p>GST-TTS \u6a21\u578b\u80fd\u591f\u901a\u8fc7\u5efa\u6a21\u4e0d\u540c\u7ea7\u522b\u7684\u97f5\u5f8b\u6765\u63d0\u5347\u751f\u6210\u8bed\u97f3\u7684\u8868\u8fbe\u6027\u548c\u63a7\u5236.</p> <ul> <li>\u6587\u732e [46] \u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6 GST-TTS \u6a21\u578b, \u901a\u8fc7\u97f5\u5f8b\u63d0\u53d6\u5668\u751f\u6210\u4e86\u8bcd\u7ea7\u522b\u7684 GSTs \u4ee5\u6355\u83b7\u5c40\u90e8\u98ce\u683c\u53d8\u5316 (WSVs). WSV \u63d0\u53d6\u5668\u7531\u53c2\u8003\u7f16\u7801\u5668\u548c\u98ce\u683c\u6807\u8bb0\u5c42\u7ec4\u6210, \u5982\u6587\u732e [75] \u6240\u8ff0, \u4ee5\u53ca\u7528\u4e8e\u751f\u6210\u5355\u8bcd\u7ea7\u522b\u7684\u6ce8\u610f\u529b\u5355\u5143.</li> <li>\u6587\u732e [133] \u5177\u6709\u6b8b\u5dee\u7684\u591a\u5c42 GSTs \u7684\u5c42\u6b21\u7ed3\u6784, \u5e94\u7528\u4e09\u5c42 GST \u5c42, \u6bcf\u4e2a\u6709 10 \u4e2a\u6807\u8bb0, \u4ece\u800c\u5f97\u5230\u5404\u4e2a\u7ea7\u522b\u6807\u8bb0\u7684\u66f4\u4f73\u89e3\u91ca. \u6807\u8bb0\u5206\u6790\u53d1\u73b0\u7b2c\u4e00\u5c42\u6807\u8bb0\u5b66\u4e60\u5230\u8bf4\u8bdd\u4eba\u8868\u793a, \u7b2c\u4e8c\u5c42\u6355\u83b7\u4e86\u5404\u79cd\u8bf4\u8bdd\u98ce\u683c\u4f8b\u5982\u505c\u987f\u4f4d\u7f6e, \u65f6\u957f\u548c\u5f3a\u8c03. \u7b2c\u4e09\u5c42\u6807\u9898\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6837\u672c, \u5177\u6709\u66f4\u660e\u663e\u548c\u53ef\u89e3\u91ca\u7684\u98ce\u683c. </li> <li>\u7c7b\u4f3c\u5730 [50], \u4e00\u4e2a\u591a\u5c3a\u5ea6 GST \u63d0\u53d6\u5668\u7528\u4e8e\u63d0\u53d6\u4e0d\u540c\u7ea7\u522b\u7684\u8bf4\u8bdd\u98ce\u683c, \u8be5\u63d0\u53d6\u5668\u4f7f\u7528\u4e09\u79cd\u98ce\u683c\u7f16\u7801\u5668\u6309\u5168\u5c40, \u53e5\u5b50\u548c\u5355\u8bcd\u7ea7\u522b\u4ece\u53c2\u8003\u6885\u5c14\u9891\u8c31\u4e2d\u63d0\u53d6\u98ce\u683c\u5d4c\u5165, \u5e76\u5c06\u5b83\u4eec\u7684\u8f93\u51fa\u7ed3\u5408\u4ee5\u5f62\u6210\u591a\u5c3a\u5ea6\u7684\u98ce\u683c\u5d4c\u5165.</li> </ul> <p>With only a small portion of the training dataset labeled with emotions, \"End-to-End Emotional Speech Synthesis Using Style Tokens  and Semi-Supervised Training\" proposes a semi-supervised GST model for generating emotional speech. The model applies a cross-entropy loss between the one-hot vectors representing the emotion labels and the weights of GSTs,in addition to the GST-TTS reconstruction loss. The semi-GST model is trained on a dataset in which only 5%of the samples are labeled with emotion classes, while the rest of the dataset is unlabeled. After training, each style token represents a specific emotion class from the training dataset and can be used to generate speech in the corresponding emotion.</p> <p>\u5f53\u8bad\u7ec3\u96c6\u53ea\u6709\u4e00\u5c0f\u90e8\u5206\u5e26\u6709\u60c5\u611f\u6807\u7b7e\u65f6, \u6587\u732e [026] \u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7684 GST \u6a21\u578b\u7528\u4e8e\u751f\u6210\u60c5\u611f\u8bed\u97f3. \u8be5\u6a21\u578b\u5e94\u7528\u8868\u793a\u60c5\u611f\u6807\u7b7e\u7684\u72ec\u70ed\u7f16\u7801\u548c GSTs \u7684\u6743\u91cd\u4e4b\u95f4\u7684\u4ea4\u53c9\u71b5\u635f\u5931, \u4ee5\u53ca GST-TTS \u91cd\u6784\u635f\u5931. \u8fd9\u4e2a semi-GST \u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u53ea\u6709 5% \u7684\u6837\u672c\u5177\u6709\u60c5\u611f\u7c7b\u522b\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3, \u8bad\u7ec3\u540e\u6bcf\u4e2a\u98ce\u683c\u6807\u8bb0\u8868\u793a\u8bad\u7ec3\u96c6\u4e2d\u4e00\u4e2a\u7279\u5b9a\u7684\u60c5\u611f\u7c7b\u522b\u5e76\u4e14\u80fd\u7528\u4e8e\u5bf9\u5e94\u60c5\u611f\u751f\u6210\u8bed\u97f3.</p> <p>Furthermore, in [92], a speech emotion recognition(SER) model is proposed with the GST-TTS to generate emotional speech while acquiring only a small labeled dataset for training. The paper formulates the training process as reinforcement learning (RL). In this frame-work, the GST-TTS model is treated as the agent, and its parameters serve as the policy. The policy aims to predict the emotional acoustic features at each time step, where these features represent the actions. The pre-trained SER model then provides feedback on the predicted features through emotion recognition accuracy, which represents the reward. The policy gradient strategy is employed to perform backpropagation and optimize the TTS model to achieve the maximum reward.</p> <p>\u6587\u732e [92] \u4e00\u4e2a\u8bed\u97f3\u60c5\u611f\u8bc6\u522b SER \u6a21\u578b\u88ab\u63d0\u51fa\u548c GST-TTS \u6a21\u578b\u7ed3\u5408\u7528\u4e8e\u751f\u6210\u60c5\u611f\u8bed\u97f3, \u53ea\u9700\u8981\u5f88\u5c0f\u90e8\u5206\u5e26\u6807\u7b7e\u7684\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3. \u8be5\u6587\u732e\u5c06\u8bad\u7ec3\u8fc7\u7a0b\u5f62\u5f0f\u5316\u4e3a\u5f3a\u5316\u5b66\u4e60. \u5728\u6b64\u67b6\u6784\u4e0b, GST-TTS \u6a21\u578b\u89c6\u4e3a\u667a\u80fd\u4f53, \u5176\u53c2\u6570\u4f5c\u4e3a\u7b56\u7565. \u7b56\u7565\u65e8\u5728\u9884\u6d4b\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u60c5\u611f\u58f0\u5b66\u7279\u5f81, \u8fd9\u4e9b\u7279\u5f81\u8868\u793a\u52a8\u4f5c. \u9884\u8bad\u7ec3 SER \u6a21\u578b\u901a\u8fc7\u60c5\u611f\u8bc6\u522b\u7cbe\u5ea6\u63d0\u4f9b\u53cd\u9988, \u5373\u5956\u52b1. \u7b56\u7565\u68af\u5ea6\u7b56\u7565\u7528\u4e8e\u4f18\u5316 TTS \u6a21\u578b\u4ee5\u8fbe\u5230\u6700\u5927\u5956\u52b1.</p> <p>In contrast, the Mellotron model [114] introduces a unique structure for the GSTs, enabling Mellotron to generate speech in various styles, including singing styles, based on pitch and duration information extracted from the reference audio. This is achieved by obtaining a set of explicit and latent variables from the reference audio. Explicit variables (text, speaker, and F0contour) capture explicit audio information, while latent variables (style tokens and attention maps) capture the latent characteristics of speech that are hard to extract explicitly.</p> <p>\u6587\u732e [114] Mellotron \u5f15\u5165 GSTs \u72ec\u7279\u7ed3\u6784, \u4f7f\u5f97 Mellotron \u80fd\u591f\u57fa\u4e8e\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u63d0\u53d6\u7684\u97f3\u9ad8\u548c\u65f6\u957f\u4fe1\u606f\u751f\u6210\u5404\u79cd\u98ce\u683c\u7684\u8bed\u97f3, \u5305\u62ec\u6b4c\u5531\u98ce\u683c. \u8fd9\u901a\u8fc7\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u83b7\u53d6\u663e\u5f0f\u548c\u9690\u5f0f\u53d8\u91cf\u5b9e\u73b0. \u663e\u5f0f\u53d8\u91cf\u6355\u83b7\u663e\u5f0f\u97f3\u9891\u4fe1\u606f, \u9690\u5f0f\u53d8\u91cf\u6355\u83b7\u8bed\u97f3\u4e2d\u96be\u4ee5\u663e\u5f0f\u63d0\u53d6\u7684\u9690\u85cf\u7279\u5f81.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#44","title":"4.4.\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b9\u6cd5","text":"<p>These is a group of recent TTS models that are trained on a large amounts of data using in-context learning strategy. During in-context learning (also called prompt engineering), the model is trained to predict missing data based its context. In other words, the model is trained with a list of input-output pairs formed in a way that represents the in-context learning task. After training, the model should be able to predict the output based on a given input.</p> <p>\u8fd1\u671f\u6709\u4e00\u7ec4 TTS \u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u5728\u5927\u91cf\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3. \u5728\u4e0a\u4e0b\u6587\u5b66\u4e60 (\u6216\u63d0\u793a\u5de5\u7a0b) \u4e2d, \u6a21\u578b\u88ab\u8bad\u7ec3\u7528\u4e8e\u57fa\u4e8e\u4e0a\u4e0b\u6587\u9884\u6d4b\u7f3a\u5931\u6570\u636e. \u6362\u53e5\u8bdd\u8bf4\u6a21\u578b\u901a\u8fc7\u8868\u793a\u4e0a\u4e0b\u6587\u5b66\u4e60\u4efb\u52a1\u7684\u8f93\u5165\u8f93\u51fa\u5bf9\u5217\u8868\u8fdb\u884c\u8bad\u7ec3, \u8bad\u7ec3\u540e\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u7ed9\u5b9a\u8f93\u5165\u7684\u8f93\u51fa.</p> <p>For the TTS task, the provided style reference (referred to as prompt) is considered as part of the entire utterance to be synthesized. The TTS model training task is to generate the rest of this utterance following the style of the provided prompt as shown by Fig. 9. By employing this training strategy, recent TTS models such as VALL-E (2023), NaturalSpeech2 (2022), and Voicebox (2023) are capable of producing zero-shot speech synthesis using only a single acoustic prompt. Furthermore, these models demonstrate the ability to replicate speech style/emotion from a provided prompt (NaturalSpeech2 (2022), VALL-E (2023)) or reference (Voicebox (2023)) to the synthesized speech.</p> <p>\u5bf9\u4e8e TTS \u4efb\u52a1, \u63d0\u4f9b\u7684\u98ce\u683c\u53c2\u8003 (\u5373\u63d0\u793a) \u88ab\u8003\u8651\u4e3a\u8981\u5408\u6210\u7684\u6574\u4e2a\u53d1\u8a00\u7684\u4e00\u90e8\u5206. TTS \u6a21\u578b\u8bad\u7ec3\u4efb\u52a1\u5373\u9075\u5faa\u63d0\u793a\u7684\u98ce\u683c\u751f\u6210\u8fd9\u4e2a\u53d1\u8a00\u5269\u4e0b\u7684\u90e8\u5206. \u901a\u8fc7\u5e94\u7528\u8fd9\u79cd\u8bad\u7ec3\u7b56\u7565, \u8fd1\u671f TTS \u6a21\u578b\u4f8b\u5982 VALL-E, NaturalSpeech2 \u548c VoiceBox \u80fd\u591f\u4f7f\u7528\u5355\u4e2a\u58f0\u5b66\u63d0\u793a\u8fdb\u884c\u96f6\u6b21\u8bed\u97f3\u5408\u6210. \u6b64\u5916, \u8fd9\u4e9b\u6a21\u578b\u8bf4\u660e\u4e86\u4ece\u63d0\u4f9b\u7684\u63d0\u793a\u6216\u53c2\u8003\u590d\u5236\u8bed\u97f3\u98ce\u683c/\u60c5\u611f\u5230\u5408\u6210\u8bed\u97f3\u7684\u80fd\u529b.</p> <p>In VALL-E (2023), a language model is trained on tokens from Encodec (2022), and the input text is used to condi-tion the language model. Specifically, the Encodec model tokenizes audio frames into discrete latent vectors/codes,where each audio frame is encoded with eight codebooks. VALL-E employs two main models: the first one is an auto-regressive (AR) model that predicts the first code of each frame, and the second is non-auto-regressive (NAR)model that predicts the other seven codes of the frame.</p> <p>VALL-E \u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u5728 Encodec \u7684\u6807\u8bb0\u4e0a\u8bad\u7ec3, \u4e14\u8f93\u5165\u6587\u672c\u7528\u4e8e\u6761\u4ef6\u5316\u8bed\u8a00\u6a21\u578b. \u7279\u522b\u5730, Encodec \u6a21\u578b\u5c06\u97f3\u9891\u5e27\u79bb\u6563\u5316\u4e3a\u79bb\u6563\u7684\u9690\u5411\u91cf/\u4ee3\u7801, \u6bcf\u4e2a\u97f3\u9891\u5e27\u7531\u516b\u4e2a\u7801\u672c\u8fdb\u884c\u7f16\u7801. VALL-E \u5e94\u7528\u4e24\u4e2a\u4e3b\u8981\u6a21\u578b\u4e00\u4e2a\u662f\u81ea\u56de\u5f52\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u6bcf\u5e27\u7684\u7b2c\u4e00\u4e2a\u7f16\u7801, \u7b2c\u4e8c\u4e2a\u662f\u975e\u81ea\u56de\u5f52\u6a21\u578b\u7528\u4e8e\u9884\u6d4b\u5176\u4ed6\u4e03\u4e2a\u7f16\u7801.</p> <p>Instead of discrete tokens used in VALL-E, NaturalSpeech2 (2022) represents speech as latent vectors from a neural audio codec with residual vector quantizers. The latent vectors are then predicted via a diffusion model,conditioned on input text, pitch from a pitch predictor,and input speech prompt.</p> <p>\u548c VALL-E \u4e0d\u540c, NaturalSpeech 2 \u4f7f\u7528\u5177\u6709\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u7684\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5c06\u8bed\u97f3\u8868\u793a\u4e3a\u9690\u5411\u91cf. \u9690\u5411\u91cf\u4e4b\u540e\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9884\u6d4b, \u6839\u636e\u8f93\u5165\u95ee\u9898, \u97f3\u9ad8\u9884\u6d4b\u5668\u7684\u97f3\u9ad8\u548c\u8f93\u5165\u8bed\u97f3\u63d0\u793a\u8fdb\u884c\u6761\u4ef6\u5316.</p> <p>Another example of in-context training is Voicebox (2023) which is a versatile generative model for speech trained on a large amount of multilingual speech data. The model is trained on a text-guided speech infilling task, which gives it the flexibility to perform various speech tasks such as zero-shot TTS, noise removal, content editing,and diverse speech sampling. Voicebox is modeled as a non-autoregressive (NAR) flow-matching model with the ability to consider future context.</p> <p>\u53e6\u4e00\u4e2a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u8bad\u7ec3\u7684\u4f8b\u5b50\u662f Voicebox, \u662f\u4e00\u4e2a\u5728\u5927\u91cf\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u8bed\u97f3\u7684\u901a\u7528\u751f\u6210\u6a21\u578b. \u8be5\u6a21\u578b\u5728\u6587\u672c\u5f15\u5bfc\u7684\u8bed\u97f3\u586b\u5145\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3, \u8fd9\u4f7f\u5176\u80fd\u591f\u6267\u884c\u5404\u79cd\u8bed\u97f3\u4efb\u52a1, \u4f8b\u5982\u96f6\u6b21 TTS, \u53bb\u566a, \u5185\u5bb9\u7f16\u8f91\u548c\u591a\u6837\u5316\u7684\u8bed\u97f3\u91c7\u6837. Voicebox \u88ab\u5efa\u6a21\u4e3a\u4e00\u4e2a\u975e\u81ea\u56de\u5f52\u7684\u6d41\u5339\u914d\u6a21\u578b, \u80fd\u591f\u8003\u8651\u672a\u6765\u4e0a\u4e0b\u6587.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#45-other-approaches","title":"4.5 Other Approaches \u5176\u4ed6\u65b9\u6cd5","text":"<p>This category containes reviewed papers that propose individual techniques or methods which cannot be categorized under any of the previously mentioned unsupervised approaches. </p> <p>\u8fd9\u4e2a\u7c7b\u522b\u5305\u542b\u4e86\u4e00\u4e9b\u4e0d\u80fd\u5f52\u7c7b\u4e3a\u4e4b\u524d\u63d0\u5230\u7684\u4efb\u4f55\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u5355\u72ec\u6280\u672f\u548c\u65b9\u6cd5\u7684\u603b\u7ed3.</p> <p>For instance, in [121], a neural encoder is introduced to encode the residual error between the predictions of a trained average TTS model and the ground truth speech. The encoded error is then used as a style embedding that conditions the decoder of the TTS model to guide the synthesis process. </p> <p>\u6587\u732e [121] \u5f15\u5165\u795e\u7ecf\u7f16\u7801\u5668\u6765\u7f16\u7801\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u5e73\u5747 TTS \u6a21\u578b\u9884\u6d4b\u548c\u771f\u5b9e\u8bed\u97f3\u4e4b\u95f4\u7684\u6b8b\u5dee\u8bef\u5dee. \u7136\u540e\u7f16\u7801\u7684\u8bef\u5dee\u88ab\u7528\u4f5c\u98ce\u683c\u5d4c\u5165\u7528\u4e8e\u6761\u4ef6\u5316 TTS \u6a21\u578b\u7684\u89e3\u7801\u5668\u4ee5\u6307\u5bfc\u5408\u6210\u8fc7\u7a0b.</p> <p>Raitio and Seshadri [128] improves prosody modeling of FastSpeech2 model with an additional variance adaptor for utterance-wise prosody modeling. </p> <p>\u6587\u732e [128] \u901a\u8fc7\u4f7f\u7528\u989d\u5916\u7684\u65b9\u5dee\u9002\u914d\u5668\u7528\u4e8e\u8bed\u8c03\u97f5\u5f8b\u5efa\u6a21, \u4ee5\u63d0\u5347 FastSpeech2 \u6a21\u578b\u7684\u97f5\u5f8b\u5efa\u6a21.</p> <p>As context information is strongly related to speech expressivity, [45] proposes using multiple self-attention layers in Tacotron2 encoder to better capture the con-text information in the input text. The outputs of these layers in the encoder are combined through either direct aggregation (concatenation) or weighted aggregation using a multi-head attention layer. </p> <p>\u7531\u4e8e\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u8bed\u97f3\u8868\u8fbe\u6027\u5f3a\u76f8\u5173, \u6587\u732e [45] \u5728 Tacotron \u7f16\u7801\u5668\u4e2d\u4f7f\u7528\u591a\u4e2a\u81ea\u6ce8\u610f\u529b\u5c42\u7528\u4e8e\u66f4\u597d\u5730\u6355\u83b7\u8f93\u5165\u6587\u672c\u7684\u5185\u5bb9\u4fe1\u606f. \u8fd9\u4e9b\u5c42\u7684\u8f93\u51fa\u901a\u8fc7\u76f4\u63a5\u805a\u5408 (\u62fc\u63a5) \u6216\u52a0\u6743\u805a\u5408 (\u591a\u5934\u6ce8\u610f\u529b\u5c42) \u8fdb\u884c\u7ed3\u5408.</p> <p>Additionally, there are some papers that propose using only input text to obtain prosody-related representations/embeddings without any style references, and those are further discussed in Section 5.2.4.</p> <p>\u6b64\u5916, \u6709\u4e9b\u6587\u732e\u63d0\u51fa\u53ea\u4f7f\u7528\u8f93\u5165\u6587\u672c\u7528\u4e8e\u83b7\u5f97\u97f5\u5f8b\u76f8\u5173\u8868\u793a/\u5d4c\u5165, \u65e0\u9700\u98ce\u683c\u53c2\u8003, \u8fd9\u5728\u540e\u7eed\u7684 5.2.4 \u4e2d\u8fdb\u884c\u8ba8\u8bba.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#5","title":"5.\u8868\u73b0\u6027\u8bed\u97f3\u5408\u6210\u7684\u4e3b\u8981\u6311\u6218","text":"<p>In this section, we list and explain the most important challenges that face expressive TTS models and the main solutions that have been proposed in the literature to overcome these challenges. We then provide a summary of papers addressing each challenge in Table 5.</p> <p>\u672c\u8282\u5217\u51fa\u5e76\u5c55\u793a\u8868\u73b0\u6027\u8bed\u97f3\u5408\u6210\u6a21\u578b\u9762\u4e34\u7684\u6700\u91cd\u8981\u6311\u6218, \u4ee5\u53ca\u5728\u73b0\u6709\u6587\u732e\u4e2d\u63d0\u51fa\u7684\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u7684\u4e3b\u8981\u89e3\u51b3\u65b9\u6848. \u5728\u8868\u683c\u4e94\u79cd\u63d0\u4f9b\u4e86\u89e3\u51b3\u6bcf\u4e2a\u6311\u6218\u7684\u6587\u732e\u7684\u603b\u89c8.</p> \u53c2\u8003\u6587\u732e \u4fe1\u606f\u6cc4\u9732 \u7f3a\u5c11\u53c2\u8003\u97f3\u9891 \u97f5\u5f8b\u53ef\u63a7\u6027 \u672a\u77e5\u98ce\u683c/\u8bf4\u8bdd\u4eba \u221a \u221a \u221a \u221a \u221a \u221a \u221a 097 \u221a \u221a \u221a \u221a \u221a 102 \u221a \u221a \u221a 047 111 \u221a \u221a 019 \u221a \u221a \u221a \u221a","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#51-irrelevant-information-leakage","title":"5.1.\u65e0\u5173\u4fe1\u606f\u6cc4\u9732 (Irrelevant Information Leakage)","text":"<p>One main problem in unsupervised approaches that rely on having a style reference or a prompt, is the leakage of irrelevant information, like speaker or text related information, into the generated style or prosody embedding.  This irrelevant information within the speech style can lead to degradation in the quality of the synthesized speech.  As a result, many studies have investigated this problem, and several solutions have been proposed as outlined below.</p> <p>\u5728\u4f9d\u8d56\u98ce\u683c\u53c2\u8003\u6216\u63d0\u793a\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4e2d, \u4e00\u4e2a\u4e3b\u8981\u7684\u95ee\u9898\u662f\u65e0\u5173\u4fe1\u606f\u7684\u6cc4\u9732, \u5982\u8bf4\u8bdd\u4eba\u6216\u6587\u672c\u76f8\u5173\u7684\u4fe1\u606f\u8fdb\u5165\u5230\u751f\u6210\u7684\u98ce\u683c\u6216\u97f5\u5f8b\u5d4c\u5165. \u8fd9\u79cd\u8bed\u97f3\u98ce\u683c\u4e2d\u7684\u65e0\u5173\u4fe1\u606f\u53ef\u80fd\u5bfc\u81f4\u5408\u6210\u8bed\u97f3\u7684\u8d28\u91cf\u4e0b\u964d. \u56e0\u6b64\u8bb8\u591a\u6587\u732e\u7814\u7a76\u4e86\u8fd9\u4e00\u95ee\u9898\u5e76\u63d0\u51fa\u4e86\u4ee5\u4e0b\u51e0\u79cd\u65b9\u6848.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#511-adversarial-training","title":"5.1.1.\u5bf9\u6297\u6027\u8bad\u7ec3 (Adversarial Training)","text":"<p>Adversarial training is one of the widely used techniques to confront the information leakage problem. Typically, a classifier is trained to distinguish the type of unwanted information (such as speaker or content information) that is leaking from the prosody reference audio into the generated prosody embedding.  During the training process, the weights of the employed prosody encoder/extractor from the reference audio are modified with gradient inversion of the proposed classifier.  In other words, the classifier penalizes the prosody encoder/extractor for any undesired information in its output.  A Gradient Reversal Layer (GRL) is usually used to achieve the inversion of the classifier gradients.</p> <p>\u5bf9\u6297\u8bad\u7ec3\u662f\u5e7f\u6cdb\u7528\u4e8e\u5904\u7406\u4fe1\u606f\u6cc4\u9732\u7684\u4e00\u79cd\u6280\u672f. \u901a\u5e38, \u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668\u7528\u4e8e\u533a\u5206\u4ece\u97f5\u5f8b\u53c2\u8003\u97f3\u9891\u6cc4\u9732\u5230\u751f\u6210\u7684\u97f5\u5f8b\u5d4c\u5165\u4e2d\u7684\u4e0d\u9700\u8981\u7684\u4fe1\u606f\u7c7b\u578b (\u4f8b\u5982\u8bf4\u8bdd\u4eba\u6216\u5185\u5bb9\u4fe1\u606f). \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u53c2\u8003\u97f3\u9891\u4e2d\u4f7f\u7528\u7684\u97f5\u5f8b\u7f16\u7801\u5668\u6216\u63d0\u53d6\u5668\u7684\u6743\u91cd\u88ab\u5206\u7c7b\u5668\u7684\u68af\u5ea6\u53cd\u8f6c\u4fee\u6539. \u6362\u53e5\u8bdd\u8bf4, \u5206\u7c7b\u5668\u5728\u5b83\u7684\u8f93\u51fa\u4e2d\u5bf9\u4efb\u4f55\u4e0d\u9700\u8981\u7684\u4fe1\u606f\u60e9\u7f5a\u97f5\u5f8b\u7f16\u7801\u5668\u6216\u63d0\u53d6\u5668. \u901a\u5e38\u4f7f\u7528\u68af\u5ea6\u53cd\u8f6c\u5c42 (Gradient Reversal Layer, GRL) \u6765\u83b7\u5f97\u5206\u7c7b\u5668\u68af\u5ea6\u7684\u53cd\u8f6c.</p> <p>Several studies utilize adversarial training to prevent the flow of either speaker or content-related information from the given reference audio to the resulting prosody embedding.  For instance, the VAE-TTS model learns phoneme-level 3-dimensional prosody codes.  The VAE is conditioned on speaker and emotion embeddings, besides the tone sequence and mel-spectrogram from the reference audio.  Adversarial training using a Gradient Reversal Layer (GRL) is applied to disentangle speaker and tone from the resulting prosody codes. Similarly, adversarial training is introduced to the style encoder of the cross-speaker emotion transfer model to learn a speaker-independent style embedding, where the target speaker embedding is provided from a separate speaker encoder.</p> <p>\u6709\u51e0\u9879\u7814\u7a76\u5229\u7528\u5bf9\u6297\u8bad\u7ec3\u6765\u9632\u6b62\u8bf4\u8bdd\u4eba\u6216\u5185\u5bb9\u76f8\u5173\u4fe1\u606f\u4ece\u7ed9\u5b9a\u53c2\u8003\u97f3\u9891\u6d41\u52a8\u5230\u751f\u6210\u97f5\u5f8b\u5d4c\u5165. \u4f8b\u5982, VAE-TTS \u6a21\u578b\u5b66\u4e60\u97f3\u7d20\u7ea7\u522b\u7684\u4e09\u7ef4\u97f5\u5f8b\u7f16\u7801. \u7528\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u5d4c\u5165, \u53c2\u8003\u97f3\u9891\u7684\u8bed\u8c03\u5e8f\u5217\u548c\u6885\u5c14\u9891\u8c31\u6761\u4ef6\u5316 VAE. \u91c7\u7528\u68af\u5ea6\u53cd\u8f6c\u5c42\u7684\u5bf9\u6297\u5b66\u4e60\u7528\u4e8e\u89e3\u8026\u97f5\u5f8b\u7f16\u7801\u4e2d\u7684\u8bf4\u8bdd\u4eba\u548c\u8bed\u8c03. \u7c7b\u4f3c\u5730, \u8de8\u8bf4\u8bdd\u4eba\u60c5\u611f\u8f6c\u79fb\u6a21\u578b\u4e2d\u7684\u98ce\u683c\u7f16\u7801\u5668\u4e5f\u5f15\u5165\u4e86\u5bf9\u6297\u8bad\u7ec3\u7528\u4e8e\u5b66\u4e60\u8bf4\u8bdd\u4eba\u72ec\u7acb\u7684\u98ce\u683c\u5d4c\u5165, \u5176\u4e2d\u76ee\u6807\u8bf4\u8bdd\u4eba\u5d4c\u5165\u7531\u5355\u72ec\u7684\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u63d0\u4f9b.</p> <p>The STYLER model employs multiple style encoders to decompose the style reference into several components, including duration, pitch, speaker, energy, and noise.  Both channel-wise and frame-wise bottleneck layers are added to all the style encoders to eliminate content-related information from the resulting embeddings.  Furthermore, as noise is encoded individually by a separate encoder in the model, other encoders are constrained to exclude noise information by employing either domain adversarial training or residual decoding.</p> <p>\u5728 STYLE \u6a21\u578b\u4e2d\u4f7f\u7528\u4e86\u591a\u4e2a\u98ce\u683c\u7f16\u7801\u5668\u5c06\u98ce\u683c\u53c2\u8003\u5206\u89e3\u4e3a\u591a\u4e2a\u6210\u5206, \u5305\u62ec\u65f6\u957f, \u97f3\u9ad8, \u80fd\u91cf\u548c\u566a\u58f0. \u5728\u6240\u6709\u7684\u98ce\u683c\u7f16\u7801\u5668\u4e2d\u6dfb\u52a0\u901a\u9053\u7ea7\u548c\u5e27\u7ea7\u74f6\u9888\u5c42\u4ece\u5bfc\u51fa\u7684\u5d4c\u5165\u4e2d\u6392\u9664\u5185\u5bb9\u76f8\u5173\u7684\u4fe1\u606f. \u6b64\u5916, \u7531\u4e8e\u566a\u58f0\u5728\u6a21\u578b\u4e2d\u901a\u8fc7\u5355\u72ec\u7684\u7f16\u7801\u5668\u7f16\u7801, \u901a\u8fc7\u5e94\u7528\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u6216\u6b8b\u5dee\u89e3\u7801\u6765\u7ea6\u675f\u5176\u4ed6\u7f16\u7801\u5668\u4ee5\u6392\u9664\u566a\u58f0\u4fe1\u606f.</p> <p>In 111, prosody is modeled at the phone-level and utterance-level by two separate encoders.  The first encoder consists of two sub-encoders: a style encoder and a content encoder, besides two supporting classifiers.  The first classifier predicts phone identity based on the content embedding, while the other classifier makes the same prediction but based on the style embedding.  The content encoder is trained via collaborative training with the guidance of the first classifier, while adversarial training is used to train the style encoder, utilizing the second classifier.</p> <p>\u6587\u732e 111 \u4e2d\u97f5\u5f8b\u901a\u8fc7\u4e24\u4e2a\u5355\u72ec\u7684\u7f16\u7801\u5668\u5728\u97f3\u7d20\u7ea7\u522b\u548c\u8bed\u8c03\u7ea7\u522b\u8fdb\u884c\u5efa\u6a21. \u7b2c\u4e00\u4e2a\u7f16\u7801\u5668\u7531\u4e24\u4e2a\u5b50\u7f16\u7801\u5668\u7ec4\u6210: \u4e00\u4e2a\u98ce\u683c\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u5185\u5bb9\u7f16\u7801\u5668, \u4ee5\u53ca\u4e24\u4e2a\u652f\u6301\u5206\u7c7b\u5668. \u9996\u4e2a\u5206\u7c7b\u5668\u57fa\u4e8e\u5185\u5bb9\u5d4c\u5165\u9884\u6d4b\u97f3\u7d20\u6807\u8bc6, \u5176\u4ed6\u5206\u7c7b\u5668\u5219\u57fa\u4e8e\u98ce\u683c\u5d4c\u5165\u8fdb\u884c\u76f8\u540c\u7684\u9884\u6d4b. \u7ed3\u5408\u7b2c\u4e00\u4e2a\u5206\u7c7b\u5668\u7684\u6307\u5bfc\u91c7\u7528\u534f\u4f5c\u8bad\u7ec3\u6765\u8bad\u7ec3\u5185\u5bb9\u7f16\u7801\u5668, \u5229\u7528\u7b2c\u4e8c\u4e2a\u5206\u7c7b\u5668\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u6765\u8bad\u7ec3\u98ce\u683c\u7f16\u7801\u5668.</p> <p>On the other hand, 102 proposes adversarial training for the style reference by inverting the gradient of an Automatic Speech Recognition (ASR) model.  The proposed model introduces a shared layer between an ASR and a reference encoder-based model.  Specifically, a single BiLSTM layer from the listener module of a pre-trained ASR model serves as the prior layer to the reference encoder.  The process starts by passing the reference Mel-spectrogram to the shared layer to produce the shared embedding as input to both the reference encoder and the ASR model.  A Gradient Reversal Layer (GRL) is employed by the ASR model to reverse its gradient on the shared layer.  Accordingly, the reference encoder parameters are modified so that the ASR model fails to recognize the shared embedding, and thus content leakage to the style embedding from the reference encoder is reduced.</p> <p>\u53e6\u4e00\u65b9\u9762, \u6587\u732e 102 \u63d0\u51fa\u901a\u8fc7\u53cd\u8f6c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u68af\u5ea6\u5bf9\u98ce\u683c\u53c2\u8003\u8fdb\u884c\u5bf9\u6297\u6027\u8bad\u7ec3. \u8be5\u6a21\u578b\u5728 ASR \u548c\u57fa\u4e8e\u53c2\u8003\u7f16\u7801\u5668\u7684\u6a21\u578b\u4e4b\u95f4\u5f15\u5165\u4e86\u4e00\u4e2a\u5171\u4eab\u5c42. \u5177\u4f53\u5730, \u9884\u8bad\u7ec3 ASR \u6a21\u578b\u4e2d\u542c\u4f17\u6a21\u5757\u4e2d\u7684\u5355\u4e2a BiLSTM \u5c42\u4f5c\u4e3a\u53c2\u8003\u7f16\u7801\u5668\u7684\u524d\u4e00\u5c42. \u5c06\u53c2\u8003\u6885\u5c14\u9891\u8c31\u4f20\u9012\u5230\u5171\u4eab\u5c42\u7528\u4e8e\u751f\u6210\u5171\u4eab\u5d4c\u5165, \u4f5c\u4e3a\u53c2\u8003\u7f16\u7801\u5668\u548c ASR \u6a21\u578b\u7684\u8f93\u5165. ASR \u6a21\u578b\u901a\u8fc7\u4f7f\u7528\u68af\u5ea6\u53cd\u8f6c\u5c42\u6765\u53cd\u8f6c\u5171\u4eab\u5c42\u7684\u68af\u5ea6. \u56e0\u6b64, \u53c2\u8003\u7f16\u7801\u5668\u7684\u53c2\u6570\u88ab\u4fee\u6539\u4f7f\u5f97 ASR \u6a21\u578b\u65e0\u6cd5\u8bc6\u522b\u5171\u4eab\u5d4c\u5165, \u4ece\u800c\u51cf\u5c11\u4ece\u53c2\u8003\u7f16\u7801\u5668\u5230\u98ce\u683c\u5d4c\u5165\u5668\u7684\u5185\u5bb9\u6cc4\u9732.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#512-prosody-classifiers","title":"5.1.2.\u97f5\u5f8b\u5206\u7c7b\u5668 (Prosody Classifiers)","text":"<p>This is a supporting approach used by some studies to produce more discriminative prosody embeddings by passing them to a prosody classifier.  This method can be applied when the training data is labeled with emotion or style labels.  In the two consecutive studies  from the same research group, an auxiliary reference encoder is proposed and located after the decoder of the baseline TTS model.  The two reference encoders in the model are followed by emotion classifiers to further enhance the discriminative nature of their resulting embeddings. However, the emotion embedding that is passed to the TTS model is the output of an intermediate hidden layer of the classifiers.  In addition to the classification loss, an additional style loss is also applied between the two emotion embeddings from the two employed emotion classifiers.</p> <p>\u8fd9\u662f\u67d0\u4e9b\u7814\u7a76\u4e2d\u91c7\u7528\u7684\u652f\u6301\u65b9\u6cd5, \u901a\u8fc7\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9\u97f5\u5f8b\u5206\u7c7b\u5668\u7528\u4e8e\u4ea7\u751f\u66f4\u5177\u533a\u5206\u6027\u7684\u97f5\u5f8b\u5d4c\u5165. \u5f53\u8bad\u7ec3\u6570\u636e\u5e26\u6709\u60c5\u611f\u6216\u98ce\u683c\u6807\u7b7e\u65f6\u53ef\u4ee5\u91c7\u7528\u6b64\u65b9\u6cd5. \u5728\u540c\u4e00\u7814\u7a76\u5c0f\u7ec4\u7684\u4e24\u9879\u8fde\u7eed\u7814\u7a76\u4e2d, \u7ed9\u57fa\u7ebf TTS \u6a21\u578b\u7684\u89e3\u7801\u5668\u540e\u6dfb\u52a0\u4e00\u4e2a\u8f85\u52a9\u7684\u53c2\u8003\u7f16\u7801\u5668. \u6a21\u578b\u4e2d\u7684\u4e24\u4e2a\u53c2\u8003\u7f16\u7801\u5668\u540e\u8ddf\u7740\u60c5\u611f\u5206\u7c7b\u5668\u7528\u4e8e\u8fdb\u4e00\u6b65\u589e\u5f3a\u5176\u751f\u6210\u5d4c\u5165\u7684\u533a\u5206\u6027. \u7136\u800c\u4f20\u9012\u7ed9 TTS \u6a21\u578b\u7684\u60c5\u611f\u5d4c\u5165\u662f\u5206\u7c7b\u5668\u7684\u4e2d\u95f4\u9690\u85cf\u5c42\u7684\u8f93\u51fa. \u9664\u4e86\u5206\u7c7b\u5668\u635f\u5931\u5916, \u8fd8\u5e94\u7528\u4e86\u6765\u6e90\u4e8e\u4e24\u4e2a\u60c5\u611f\u5206\u7c7b\u5668\u7684\u60c5\u611f\u5d4c\u5165\u4e4b\u95f4\u7684\u9644\u52a0\u98ce\u683c\u635f\u5931.</p> <p>In [36], alongside the text encoder, two encoders are introduced to generate embeddings for speaker and emotion from a reference audio.  To further disentangle emotion, speaker, and text information, both speaker and emotion encoders are supported with a classifier to predict speaker and emotion labels, respectively.  Similarly, in paper [39], a model with two encoders and two classifiers is proposed to produce disentangled embeddings for speakers and emotions from a reference audio.  However, the paper claims that some emotional information is lost during the process of disentangling speaker identity from the emotion embedding.  As a result, an ASR model is introduced to compensate for the missing emotional information.  The emotion embedding is incorporated within a pre-trained ASR model through a Global Context (GC) block.  This block extracts global emotional features from the ASR model\u2019s intermediate features (AIF).  Subsequently, a prosody compensation encoder is utilized to generate emotion compensation information from the output of the AIF layer, which is then added to the emotion encoder output.</p> <p>\u5728\u6587\u732e [36] \u4e2d, \u9664\u4e86\u6587\u672c\u7f16\u7801\u5668\u4e4b\u5916, \u8fd8\u5f15\u5165\u4e86\u4e24\u4e2a\u7f16\u7801\u5668\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u751f\u6210\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u7684\u5d4c\u5165. \u4e3a\u4e86\u8fdb\u4e00\u6b65\u89e3\u8026\u60c5\u611f, \u8bf4\u8bdd\u4eba\u548c\u6587\u672c\u4fe1\u606f, \u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u7f16\u7801\u5668\u90fd\u7528\u4e00\u4e2a\u5206\u7c7b\u5668\u8fdb\u884c\u652f\u6301, \u5206\u522b\u7528\u4e8e\u9884\u6d4b\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u6807\u7b7e. \u7c7b\u4f3c\u5730, \u6587\u732e [39] \u4e2d, \u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u4e24\u4e2a\u7f16\u7801\u5668\u548c\u4e24\u4e2a\u5206\u7c7b\u5668\u7684\u6a21\u578b\u4ee5\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u4ea7\u751f\u89e3\u8026\u7684\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u5d4c\u5165. \u7136\u800c\u8be5\u6587\u732e\u58f0\u79f0\u518d\u4ece\u60c5\u611f\u5d4c\u5165\u4e2d\u89e3\u8026\u8bf4\u8bdd\u4eba\u6807\u8bc6\u65f6\u4e22\u5931\u4e86\u4e00\u4e9b\u60c5\u611f\u4fe1\u606f. \u56e0\u6b64\u5f15\u5165\u4e86\u4e00\u4e2a ASR \u6a21\u578b\u7528\u4e8e\u8865\u507f\u4e22\u5931\u7684\u60c5\u611f\u4fe1\u606f. \u60c5\u611f\u5d4c\u5165\u901a\u8fc7\u5168\u5c40\u6587\u672c\u5757\u88ab\u6574\u5408\u8fdb\u9884\u8bad\u7ec3 ASR \u6a21\u578b\u4e2d. \u8fd9\u4e2a\u5757\u4ece ASR \u6a21\u578b\u7684\u4e2d\u95f4\u7279\u5f81 (AIF) \u4e2d\u63d0\u53d6\u5168\u5c40\u60c5\u611f\u7279\u5f81. \u7136\u540e, \u4f7f\u7528\u97f5\u5f8b\u8865\u507f\u7f16\u7801\u5668\u4ece AIF \u5c42\u7684\u8f93\u51fa\u4e2d\u751f\u6210\u60c5\u611f\u8865\u507f\u4fe1\u606f, \u4e4b\u540e\u52a0\u5165\u5230\u60c5\u611f\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4e2d.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#513-information-bottleneck","title":"5.1.3.\u4fe1\u606f\u74f6\u9888 (Information Bottleneck)","text":"<p>The information bottleneck is a technique used to control information flow via a single layer/network.  It helps prevent information leakage as it projects input into a lower dimension so that there is not enough capacity to model additional information and only important information is passed through it.  In other words, the bottleneck can be seen as a down-sampling and up-sampling filter that restricts its output and generates a pure style embedding.  Several prosody-reference based approaches, as in [86] [93] [97] [101] [130], have employed this technique to prevent the flow of speaker or content-related information from the reference audio to the prosody embedding.</p> <p>\u4fe1\u606f\u74f6\u9888\u662f\u4e00\u79cd\u901a\u8fc7\u5355\u5c42\u6216\u7f51\u7edc\u63a7\u5236\u4fe1\u606f\u6d41\u7684\u6280\u672f\u3002 \u5b83\u6709\u52a9\u4e8e\u9632\u6b62\u4fe1\u606f\u6cc4\u9732\uff0c\u56e0\u4e3a\u5b83\u5c06\u8f93\u5165\u6295\u5c04\u5230\u8f83\u4f4e\u7ef4\u5ea6\u4ece\u800c\u6ca1\u6709\u8db3\u591f\u7684\u5bb9\u91cf\u6765\u5efa\u6a21\u989d\u5916\u4fe1\u606f\uff0c\u53ea\u6709\u91cd\u8981\u4fe1\u606f\u901a\u8fc7\u5b83\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0c\u4fe1\u606f\u74f6\u9888\u53ef\u4ee5\u89c6\u4e3a\u4e00\u4e2a\u4e0b\u91c7\u6837\u548c\u4e0a\u91c7\u6837\u6ee4\u6ce2\u5668\uff0c\u5b83\u9650\u5236\u4e86\u8f93\u51fa\u5e76\u751f\u6210\u7eaf\u98ce\u683c\u5d4c\u5165\u3002 \u6709\u51e0\u9879\u57fa\u4e8e\u97f5\u5f8b\u53c2\u8003\u7684\u7814\u7a76\u5df2\u7ecf\u5e94\u7528\u4e86\u8fd9\u4e00\u6280\u672f\u7528\u4e8e\u9632\u6b62\u8bf4\u8bdd\u4eba\u6216\u5185\u5bb9\u76f8\u5173\u7684\u4fe1\u606f\u4ece\u53c2\u8003\u97f3\u9891\u6d41\u5165\u97f5\u5f8b\u5d4c\u5165\u3002</p> <p>In [93], a bottleneck layer named sieve layer is introduced to the style encoder in GST-TTS to generate pure style embedding.  Similarly, in the multiple style encoders model STYLER [97], each encoder involves a channel-wise bottleneck block of two bidirectional-LSTM layers to eliminate content information from encoders\u2019 output.  Another example is the cross-speaker-style transfer Transformer-TTS model proposed in [86] with both speaker and style embeddings as input to the model encoder.  The speaker-style-combined output from the encoder is then passed to a prosody bottleneck sub-network, which produces a prosody embedding that involves only prosody-related features.  The proposed bottleneck sub-network consists of two CNN layers, a squeeze-and-excitation (SE) block [152], and a linear layer.  The encoder output is then concatenated with the resulting prosody embedding and used as input to the decoder.</p> <ul> <li>\u6587\u732e 093 \u5728 GST-TTS \u7684\u98ce\u683c\u7f16\u7801\u5668\u4e2d\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a\u7b5b\u5c42 (Sieve Layer) \u7684\u74f6\u9888\u5c42\u7528\u4e8e\u751f\u6210\u7eaf\u98ce\u683c\u5d4c\u5165.</li> <li>\u6587\u732e 097 \u63d0\u51fa\u7684\u591a\u98ce\u683c\u7f16\u7801\u5668\u6a21\u578b STYLER \u4e2d\uff0c\u6bcf\u4e2a\u7f16\u7801\u5668\u90fd\u5305\u542b\u4e24\u4e2a\u53cc\u5411 LSTM \u5c42\u901a\u9053\u7ea7\u522b\u74f6\u9888\u5757\u7528\u4e8e\u6d88\u9664\u7f16\u7801\u5668\u8f93\u51fa\u4e2d\u7684\u5185\u5bb9\u4fe1\u606f.</li> <li>\u6587\u732e 086 \u63d0\u51fa\u7684\u8de8\u8bf4\u8bdd\u4eba\u98ce\u683c\u8fc1\u79fb Transformer-TTS \u6a21\u578b\u4e2d, \u8bf4\u8bdd\u4eba\u548c\u98ce\u683c\u5d4c\u5165\u90fd\u4f5c\u4e3a\u6a21\u578b\u7f16\u7801\u5668\u7684\u8f93\u5165. \u7f16\u7801\u5668\u8f93\u51fa\u7684\u8bf4\u8bdd\u4eba-\u98ce\u683c\u7ec4\u5408\u88ab\u4f20\u9012\u5230\u97f5\u5f8b\u74f6\u9888\u81ea\u7f51\u7edc, \u5bfc\u51fa\u4ec5\u5305\u542b\u97f5\u5f8b\u76f8\u5173\u7279\u5f81\u7684\u97f5\u5f8b\u5d4c\u5165. \u4e4b\u540e\u5c06\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4e0e\u4ea7\u751f\u7684\u97f5\u5f8b\u5d4c\u5165\u8fdb\u884c\u62fc\u63a5\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165.</li> </ul> <p>The Copycat TTS model [130] is a prosody transfer model via VAE.  The model applies three techniques to disentangle the source speaker information from the prosody embedding.  One of these techniques is to use a temporal bottleneck encoder [153] within the reference encoder of the model.  The prosody embedding that is sampled from the latent space is passed to the bottleneck to reduce speaker identity-related information in the prosody embedding before it flows to the model decoder. Similarly, the model proposed in [101] produces a style embedding with less irrelevant style information by adding a variational information bottleneck (VIB) [154] layer to the reference encoder.  The idea behind this layer is to introduce a complexity constraint on mutual information(MI) between the reference encoder input and output so that it only flows out style-related information.</p> <ul> <li>\u6587\u732e 130 \u63d0\u51fa\u7684 Copycat TTS \u662f\u901a\u8fc7 VAE \u8fdb\u884c\u97f5\u5f8b\u8fc1\u79fb\u7684\u6a21\u578b. \u6a21\u578b\u5e94\u7528\u4e09\u79cd\u6280\u672f\u6765\u4ece\u97f5\u5f8b\u5d4c\u5165\u4e2d\u89e3\u8026\u6e90\u8bf4\u8bdd\u4eba\u4fe1\u606f. \u5176\u4e2d\u4e4b\u4e00\u662f\u5728\u6a21\u578b\u7684\u53c2\u8003\u7f16\u7801\u5668\u4e2d\u4f7f\u7528\u65f6\u5e8f\u74f6\u9888\u7f16\u7801\u5668 \u6587\u732e 153, \u5728\u4f20\u9012\u7ed9\u6a21\u578b\u89e3\u7801\u5668\u4e4b\u524d, \u4ece\u9690\u7a7a\u95f4\u4e2d\u91c7\u6837\u7684\u97f5\u5f8b\u5d4c\u5165\u88ab\u4f20\u9012\u5230\u74f6\u9888\u4ee5\u51cf\u5c11\u5176\u4e2d\u548c\u8bf4\u8bdd\u4eba\u8eab\u4efd\u76f8\u5173\u7684\u4fe1\u606f.</li> <li>\u6587\u732e 101 \u901a\u8fc7\u7ed9\u53c2\u8003\u7f16\u7801\u5668\u6dfb\u52a0\u53d8\u5206\u4fe1\u606f\u74f6\u9888 (Variational Information Bottleneck, VIB) \u5c42\u4ee5\u751f\u6210\u5177\u6709\u66f4\u5c11\u98ce\u683c\u65e0\u5173\u4fe1\u606f\u7684\u98ce\u683c\u5d4c\u5165. \u8be5\u5c42\u80cc\u540e\u7684\u601d\u60f3\u662f\u5728\u53c2\u8003\u7f16\u7801\u5668\u7684\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u5f15\u5165\u4e00\u4e2a\u590d\u6742\u5ea6\u7ea6\u675f\u4f7f\u5176\u53ea\u6d41\u51fa\u98ce\u683c\u76f8\u5173\u7684\u4fe1\u606f.</li> </ul>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#514-instance-normalization","title":"5.1.4.\u5b9e\u4f8b\u5f52\u4e00\u5316 (Instance Normalization)","text":"<p>Batch normalization (BN), first introduced in [155], is utilized in deep neural networks to accelerate the training process and increase its stability. Essentially, a batch normalization layer is added before each layer in deep neural networks to adjust the means and variances of the layer inputs, as illustrated by Eq.(1): $$   IN(x) = \\gamma \\left[\\dfrac{x-\\mu(x)}{\\sigma(x)}\\right]+\\beta, \\tag{1} $$</p> <p>where $\\gamma$, $\\beta$ are affine parameters learned from data and $\\mu$, $\\sigma$ are the mean and standard deviation which are calculated for each feature channel across the batch size. Instance normalization (IN) also follows equation (1); however, it calculates means and variances across spatial dimensions independently for each channel and each sample (instance).  In the field of computer vision, stylization approach is significantly improved by replacing (BN) layers with (IN) layers [156].  Consequently, researchers in the expressive speech field have started to apply IN to extract better prosody representations.  For example, an instance normalization (IN) layer is used at the reference encoder in [130], at the prosody extractor in [93], and at the style encoder in [96] to remove style/prosody irrelevant features (such as speaker identity features) and enhance the learned style/prosody embedding.</p> <p>\u6279\u91cf\u5f52\u4e00\u5316 (Batch Normalization, BN) \u7531\u6587\u732e 155 \u9996\u6b21\u63d0\u51fa, \u7528\u4e8e\u52a0\u901f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u5e76\u63d0\u9ad8\u5176\u7a33\u5b9a\u6027. \u672c\u8d28\u4e0a, \u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6bcf\u4e00\u5c42\u4e4b\u524d\u6dfb\u52a0\u6279\u91cf\u5f52\u4e00\u5316\u5c42\u7528\u4e8e\u8c03\u6574\u8f93\u5165\u7684\u5747\u503c\u548c\u65b9\u5dee, \u5982\u516c\u5f0f\u4e00\u6240\u793a. \u5176\u4e2d $\\gamma$ \u548c $\\beta$ \u662f\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u4eff\u5c04\u53c2\u6570, $\\mu$ \u548c $\\sigma$ \u662f\u5728\u6574\u4e2a\u6279\u91cf\u5927\u5c0f\u4e0a\u4e3a\u6bcf\u4e2a\u7279\u5f81\u901a\u9053\u8ba1\u7b97\u7684\u5747\u503c\u548c\u65b9\u5dee.</p> <p>\u5b9e\u4f8b\u5f52\u4e00\u5316 (Instance Normaliztion, IN) \u540c\u6837\u6ee1\u8db3\u516c\u5f0f\u4e00, \u7136\u800c\u5b83\u72ec\u7acb\u5730\u4e3a\u6bcf\u4e2a\u901a\u9053\u548c\u6bcf\u4e2a\u6837\u672c (\u53c8\u79f0\u5b9e\u4f8b) \u5728\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee. </p> <p>\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df, \u901a\u8fc7\u5c06\u6279\u91cf\u5f52\u4e00\u5316\u5c42\u66ff\u6362\u4e3a\u5b9e\u4f8b\u5f52\u4e00\u5316\u5c42, \u98ce\u683c\u5316\u65b9\u6cd5\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347. \u56e0\u6b64, \u8868\u8fbe\u6027\u8bed\u97f3\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u5c1d\u8bd5\u5e94\u7528\u5b9e\u4f8b\u5f52\u4e00\u5316\u7528\u4e8e\u63d0\u53d6\u66f4\u597d\u7684\u97f5\u5f8b\u8868\u793a. </p> <p>\u4f8b\u5982\u4ee5\u4e0b\u7814\u7a76\u90fd\u4f7f\u7528\u4e86\u5b9e\u4f8b\u5f52\u4e00\u5316\u5c42\u4ee5\u53bb\u9664\u98ce\u683c/\u97f5\u5f8b\u65e0\u5173\u7279\u5f81 (\u5982\u8bf4\u8bdd\u4eba\u8eab\u4efd\u7279\u5f81) \u5e76\u589e\u5f3a\u5b66\u4e60\u5230\u7684\u98ce\u683c/\u97f5\u5f8b\u5d4c\u5165.</p> <ul> <li>\u6587\u732e 130 \u7684\u53c2\u8003\u7f16\u7801\u5668;</li> <li>\u6587\u732e 093 \u7684\u97f5\u5f8b\u63d0\u53d6\u5668;</li> <li>\u6587\u732e 096 \u7684\u98ce\u683c\u7f16\u7801\u5668.</li> </ul>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#515-mutual-information-minimization","title":"5.1.5.\u4e92\u4fe1\u606f\u6700\u5c0f\u5316 (Mutual Information Minimization)","text":"<p>For a pair of random variables, mutual information (MI)is defined as the information obtained on one random variable by observing the other.  Specifically, if $X$ and $Y$ are two variables, then $MI(X; Y)$ shown by Venn diagram in Fig.10, can be seen as the KL-divergence between the joint distribution $(P_{XY})$ and the product of the marginals $(P_X, P_Y)$ as in equation (2). If the two random variables $X$ and $Y$ represent linguistic and style vectors, applying $MI$ minimization between these two vectors helps to produce style vectors with less information from the content vector. $$   MI(X;Y)=DL_{KL}(P_{(X,Y)}| P_X\\otimes P_Y),\\tag{2} $$</p> <p>For example, in [137], the Mutual Information Neural Estimation algorithm (MINE) [157] is employed to estimate the mutual information between the content and style vectors.  The algorithm uses a neural network that is trained to maximize the lower bound of the mutual information between the style and content vectors.  Simultaneously, the TTS model aims to minimize the reconstruction loss, making the overall problem a max-min problem.  Alternatively, in InstructTTS, the CLUB method [158], which computes an upper bound as the MI estimator, is used to prevent the leakage of speaker and content information into the style embedding.</p> <p>A new approach is proposed in [117] for MI estimation and minimization to reduce content/speaker information transfer to the style embedding in a VAE based approach.  Typically, the model needs to estimate MI between latent style embeddings and speaker/content embeddings.  To avoid the exponentially high statistical variance of the finite-sampling MI estimator, the paper suggests using a new algorithm for information divergence named R\u00e9nyi divergence.  Two variations from the R\u00e9nyi divergence family are proposed, including minimizing the Hellinger distance and minimizing the sum of R\u00e9nyi divergences.</p> <p>\u5bf9\u4e8e\u4e00\u5bf9\u968f\u673a\u53d8\u91cf, \u4e92\u4fe1\u606f (Mutual Information, MI) \u88ab\u5b9a\u4e49\u4e3a\u901a\u8fc7\u89c2\u5bdf\u53e6\u4e00\u4e2a\u968f\u673a\u53d8\u91cf\u83b7\u5f97\u7684\u5f53\u524d\u968f\u673a\u53d8\u91cf\u7684\u4fe1\u606f. \u5177\u4f53\u5730, \u82e5 $X$ \u548c $Y$ \u4e3a\u4e24\u4e2a\u53d8\u91cf, \u90a3\u4e48 $MI(X;Y)$ \u5982\u56fe\u5341\u663e\u793a\u7684\u97e6\u6069\u56fe\u6240\u793a, \u53ef\u4ee5\u89c6\u4e3a\u8054\u5408\u6982\u7387\u5206\u5e03 $P_{XY}$ \u548c\u8fb9\u9645\u6982\u7387\u5206\u5e03 $(P_X,P_Y)$ \u7684\u4e58\u79ef\u4e4b\u95f4\u7684 KL \u6563\u5ea6. \u82e5\u4e24\u4e2a\u968f\u673a\u53d8\u91cf $X$ \u548c $Y$ \u5206\u522b\u8868\u793a\u8bed\u8a00\u5411\u91cf\u548c\u98ce\u683c\u5411\u91cf, \u5bf9\u8fd9\u4e24\u4e2a\u5411\u91cf\u5e94\u7528\u4e92\u4fe1\u606f\u6700\u5c0f\u5316\u5c06\u6709\u52a9\u4e8e\u751f\u6210\u5177\u6709\u66f4\u5c11\u6765\u81ea\u5185\u5bb9\u5411\u91cf\u7684\u4fe1\u606f\u7684\u98ce\u683c\u5411\u91cf. $$     MI(X;Y)=DL_{KL}(P_{(X,Y)}| P_X\\otimes P_Y),\\tag{2} $$</p> <ul> <li>\u6587\u732e [137] \u5c06\u6587\u732e [157] \u7684\u4e92\u4fe1\u606f\u7f51\u7edc\u4f30\u8ba1\u7b97\u6cd5 (Mutual Information Neural Estimation, MINE) \u5e94\u7528\u4e8e\u4f30\u8ba1\u5185\u5bb9\u5411\u91cf\u548c\u98ce\u683c\u5411\u91cf\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f. \u8be5\u7b97\u6cd5\u4f7f\u7528\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc, \u88ab\u8bad\u7ec3\u4ee5\u6700\u5927\u5316\u98ce\u683c\u5411\u91cf\u548c\u5185\u5bb9\u5411\u91cf\u4e4b\u95f4\u4e92\u4fe1\u606f\u7684\u4e0b\u754c. \u540c\u65f6, TTS \u6a21\u578b\u6700\u5c0f\u5316\u91cd\u6784\u635f\u5931, \u4f7f\u5f97\u6574\u4f53\u53d8\u6210\u4e86\u6700\u5927-\u6700\u5c0f\u95ee\u9898.</li> <li>\u6587\u732e [021] \u5c06\u6587\u732e [158] \u4f7f\u7528 CLUB \u65b9\u6cd5\u8ba1\u7b97\u4e00\u4e2a\u4e0a\u754c\u4f5c\u4e3a MI \u4f30\u8ba1\u91cf, \u7528\u4e8e\u9632\u6b62\u8bf4\u8bdd\u4eba\u548c\u5185\u5bb9\u4fe1\u606f\u6cc4\u9732\u5230\u98ce\u683c\u5d4c\u5165\u4e2d.</li> <li>\u6587\u732e [117] \u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e92\u4fe1\u606f\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\u5e76\u5728\u57fa\u4e8e VAE \u65b9\u6cd5\u4e2d\u6700\u5c0f\u5316\u7528\u4e8e\u51cf\u5c11\u5185\u5bb9/\u8bf4\u8bdd\u4eba\u4fe1\u606f\u8f6c\u79fb\u5230\u98ce\u683c\u5d4c\u5165\u4e2d. \u901a\u5e38\u6a21\u578b\u9700\u8981\u4f30\u8ba1\u9690\u98ce\u683c\u5d4c\u5165\u548c\u8bf4\u8bdd\u4eba/\u5185\u5bb9\u5d4c\u5165\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f. \u4e3a\u4e86\u907f\u514d\u6709\u9650\u91c7\u6837\u4e92\u4fe1\u606f\u4f30\u8ba1\u5176\u7684\u6307\u6570\u9ad8\u7edf\u8ba1\u65b9\u5dee, \u4f5c\u8005\u5efa\u8bae\u4f7f\u7528\u4e00\u79cd\u65b0\u7b97\u6cd5\u7528\u4e8e\u4fe1\u606f\u6563\u5ea6, \u540d\u4e3a R\u00e9nyi \u6563\u5ea6. R\u00e9nyi \u6563\u5ea6\u5bfc\u51fa\u4e24\u79cd\u53d8\u4f53, \u5305\u62ec\u6700\u5c0f\u5316 Hellinger \u8ddd\u79bb\u548c\u6700\u5c0f\u5316 R\u00e9nyi \u6563\u5ea6\u4e4b\u548c.</li> </ul> <p>Fig. 10 Venn diagram of two random variables X and Y where P(X)and P(Y) represent their entropies, P(X|Y) is the conditional entropy of X given Y and P(Y|X) is the conditional entropy of Y given X, H(X,Y)is the joint entropy of X and Y and MI(X,Y) is their mutual information</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#516-wav2vec-features","title":"5.1.6.\u6ce2\u5f62\u8f6c\u5411\u91cf\u7279\u5f81 (Wav2Vec Features)","text":"<p>Wav2Vec [142] model converts speech waveform into context-dependent vectors/features. The model is trained via self-supervised or in-context training algorithms which are explained in Section 4.4. Features generated by wav2vec and similar models such as HuBERT [159] provide better representations of speech and its lexical and non-lexical information. Therefore, these models are utilized nowadays in different speech processing tasks such as speech recognition, synthesis, and downstream emotion detection.</p> <p>Wav2Vec \u6a21\u578b\u5c06\u8bed\u97f3\u6ce2\u5f62\u8f6c\u5316\u4e3a\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5411\u91cf\u6216\u7279\u5f81. \u8be5\u6a21\u578b\u901a\u8fc7\u81ea\u76d1\u7763\u6216\u4e0a\u4e0b\u6587\u8bad\u7ec3\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3. \u7531 Wav2Vec \u548c\u7c7b\u4f3c\u7684\u6a21\u578b\u5982 HuBERT \u751f\u6210\u7684\u7279\u5f81\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bed\u97f3\u53ca\u5176\u8bcd\u6c47\u548c\u975e\u8bcd\u6c47\u4fe1\u606f\u7684\u8868\u793a. \u56e0\u6b64\u8fd9\u4e9b\u6a21\u578b\u73b0\u5728\u88ab\u7528\u4e8e\u4e0d\u540c\u7684\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u4f8b\u5982\u8bed\u97f3\u8bc6\u522b, \u5408\u6210\u548c\u4e0b\u6e38\u60c5\u611f\u68c0\u6d4b.</p> <p>Some studies such as Emo-VITS [120] use Wav2vec 2.0 as a feature extractor to provide input to the reference encoder instead of spectrum features or raw audio waveform. Figure 11 illustrates the framework of the wav2vec technique and how it is utilized as a feature extractor with TTS models. The wav2vec model converts the continuous audio features into quantized finite set of discrete representations called tokens. This is done using a quantization module that maps the continuous feature vectors into a discrete set of tokens from a learned codebook. As those tokens are more abstract, they reduce the complexity of the features by retaining important features while filtering out all the irrelevant information. Because of that abstraction, it is harder to reconstruct audio from the wav2vec features, which means leakage of linguistic content into feature vectors is significantly lower compared to other features such as MFCCs.</p> <p>\u4e00\u4e9b\u7814\u7a76\u4f7f\u7528 Wav2Vec 2.0 \u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668, \u4e3a\u53c2\u8003\u7f16\u7801\u5668\u63d0\u4f9b\u8f93\u5165, \u800c\u4e0d\u662f\u539f\u59cb\u97f3\u9891\u6ce2\u5f62\u7684\u9891\u8c31\u7279\u5f81. \u56fe\u5341\u4e00\u5c55\u793a\u4e86 Wac2Vec \u6280\u672f\u53ca\u5176\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\u5982\u4f55\u4e0e TTS \u6a21\u578b\u76f8\u7ed3\u5408. Wav2Vec \u6a21\u578b\u5c06\u8fde\u7eed\u97f3\u9891\u7279\u5f81\u8f6c\u5316\u4e3a\u540d\u4e3a token \u7684\u79bb\u6563\u8868\u793a\u7684\u91cf\u5316\u6709\u9650\u96c6. \u8fd9\u901a\u8fc7\u4f7f\u7528\u91cf\u5316\u6a21\u5757\u5c06\u8fde\u7eed\u7279\u5f81\u5411\u91cf\u6620\u5c04\u5230\u53d6\u81ea\u5b66\u4e60\u597d\u7684\u7801\u672c\u7684\u79bb\u6563 token \u96c6\u5408. \u5f53\u8fd9\u4e9b token \u8d8a\u62bd\u8c61, \u5b83\u4eec\u901a\u8fc7\u4fdd\u7559\u91cd\u8981\u7279\u5f81\u5e76\u8fc7\u6ee4\u6389\u6240\u6709\u4e0d\u76f8\u5173\u7684\u4fe1\u606f\u6765\u51cf\u4f4e\u7279\u5f81\u7684\u590d\u6742\u5ea6. \u7531\u4e8e\u8fd9\u79cd\u62bd\u8c61\u6027, \u5f88\u96be\u4ece Wav2Vec \u7279\u5f81\u4e2d\u91cd\u6784\u97f3\u9891, \u8fd9\u610f\u5473\u7740\u4e0e\u5176\u4ed6\u7279\u5f81\u5982 MFCC \u76f8\u6bd4, \u8bed\u8a00\u5185\u5bb9\u6cc4\u6f0f\u5230\u7279\u5f81\u5411\u91cf\u5c06\u660e\u663e\u4e0b\u964d.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#517-orthogonality-loss","title":"5.1.7.\u6b63\u4ea4\u6027\u635f\u5931 (Orthogonality Loss)","text":"<p>Studies [34] [39] propose a model with two separate encoders to encode speaker and emotion information through speaker and emotion classification loss, along with gradient inversion of the emotion classification loss in the speaker encoder. Additionally, to disentangle the source speaker information from the emotion embedding, the emotion embedding is made orthogonal to the speaker embedding with an orthogonality loss shown in equation (3). An ablation study in [34] showed that applying an orthogonality constraint helped the encoders learn both speaker-irrelevant emotion embedding and emotion-irrelevant speaker embedding. $$     \\mathcal{L}{orth} = \\sum{i=1}^n |S_i-e_i|_F^2\\tag{3} $$</p> <p>where $|\\cdot|_F$ is the Frobenius norm, $e_i$ is the emotion embedding and $S_i$ is the speaker embedding.</p> <p>\u4e00\u4e9b\u7814\u7a76\u63d0\u51fa\u4e86\u5177\u6709\u4e24\u4e2a\u72ec\u7acb\u7f16\u7801\u5668\u7684\u6a21\u578b, \u5e76\u4e14\u901a\u8fc7\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u5206\u7c7b\u635f\u5931, \u4ee5\u53ca\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u4e2d\u7684\u60c5\u611f\u5206\u7c7b\u635f\u5931\u7684\u68af\u5ea6\u53cd\u8f6c\u6765\u7f16\u7801\u8bf4\u8bdd\u4eba\u548c\u60c5\u611f\u4fe1\u606f. \u6b64\u5916, \u4e3a\u4e86\u4ece\u60c5\u611f\u5d4c\u5165\u4e2d\u89e3\u8026\u6e90\u8bf4\u8bdd\u4eba\u7684\u4fe1\u606f, \u60c5\u611f\u5d4c\u5165\u901a\u8fc7\u516c\u5f0f\u4e09\u6240\u793a\u7684\u6b63\u4ea4\u6027\u635f\u5931\u4e0e\u8bf4\u8bdd\u4eba\u5d4c\u5165\u6b63\u4ea4.  $$     \\mathcal{L}{orth} = \\sum{i=1}^n |S_i-e_i|_F^2\\tag{3} $$</p> <p>\u5176\u4e2d $|\\cdot|_F$ \u662f F \u8303\u6570, $e_i$ \u662f\u60c5\u611f\u5d4c\u5165, $S_i$ \u662f\u8bf4\u8bdd\u4eba\u5d4c\u5165.</p> <p>\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u5e94\u7528\u6b63\u4ea4\u6027\u7ea6\u675f\u6709\u52a9\u4e8e\u7f16\u7801\u5668\u5b66\u4e60\u4e0e\u8bf4\u8bdd\u4eba\u65e0\u5173\u7684\u60c5\u611f\u5d4c\u5165\u548c\u4e0e\u60c5\u611f\u65e0\u5173\u7684\u8bf4\u8bdd\u4eba\u5d4c\u5165.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#52-inference-without-reference-audio","title":"5.2.\u65e0\u53c2\u8003\u97f3\u9891\u63a8\u7406 (Inference without Reference Audio)","text":"<p>A main drawback of the unsupervised approaches (Section 4) is that they require a reference audio for the desired prosody or style of the generated speech. However, prosody references are not always available for the desired speaker, style, or text. Besides, using prosody reference introduces the leakage problem as discussed in Section 5.1. As a result, different techniques have been proposed that enable unsupervised expressive speech synthesis without prosody references. Some techniques utilize the reference audio at training phase while at inference phase speech synthesis can be done with or without a reference audio. Other techniques depend on input text only to generate prosody embedding at both training and inference phases. In the following three sections, we will describe techniques for inference without reference audio applied with each of the three main unsupervised ETTS approaches. In Section 5.2.4, we will discuss some ETTS approaches that are based on text only. Then in Table 4, we summarize main approaches that are used to extract text-based features with related papers links.</p> <p>\u7b2c\u56db\u8282\u4e2d\u603b\u7ed3\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u4e00\u4e2a\u4e3b\u8981\u7f3a\u70b9\u662f, \u5b83\u4eec\u9700\u8981\u4e00\u4e2a\u53c2\u8003\u97f3\u9891\u7528\u4e8e\u6240\u751f\u6210\u8bed\u97f3\u7684\u671f\u671b\u97f5\u5f8b\u6216\u98ce\u683c. \u7136\u800c\u6240\u9700\u8bf4\u8bdd\u4eba/\u98ce\u683c/\u6587\u672c\u7684\u97f5\u5f8b\u53c2\u8003\u5e76\u4e0d\u603b\u662f\u80fd\u591f\u83b7\u5f97. \u6b64\u5916\u4f7f\u7528\u97f5\u5f8b\u53c2\u8003\u4f1a\u5f15\u5165\u7b2c5.1\u8282\u8ba8\u8bba\u7684\u6cc4\u9732\u95ee\u9898.  \u56e0\u6b64\u51fa\u73b0\u4e86\u5404\u79cd\u6280\u672f\u4f7f\u5f97\u65e0\u76d1\u7763\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u80fd\u591f\u65e0\u97f5\u5f8b\u53c2\u8003. \u4e00\u4e9b\u6280\u672f\u5728\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u53c2\u8003\u97f3\u9891, \u800c\u63a8\u7406\u9636\u6bb5\u8bed\u97f3\u5408\u6210\u5c31\u53ef\u4ee5\u7528\u6216\u4e0d\u540c\u53c2\u8003\u97f3\u9891. \u5176\u4ed6\u6280\u672f\u53ea\u4f9d\u8d56\u4e8e\u8f93\u5165\u6587\u672c\u7528\u4e8e\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u751f\u6210\u97f5\u5f8b\u5d4c\u5165. \u5728\u4ee5\u4e0b\u4e09\u5c0f\u8282\u4e2d\u5c06\u63cf\u8ff0\u65e0\u53c2\u8003\u97f3\u9891\u7684\u63a8\u7406\u6280\u672f, \u5e94\u7528\u4e8e\u4e09\u79cd\u4e3b\u8981\u7684\u65e0\u76d1\u7763\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u65b9\u6cd5\u4e2d. \u5728\u7b2c\u56db\u5c0f\u8282\u5c06\u8ba8\u8bba\u4e00\u4e9b\u4ec5\u4f9d\u8d56\u4e8e\u6587\u672c\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u65b9\u6cd5. \u8868\u683c\u56db\u603b\u7ed3\u4e86\u76f8\u5173\u6587\u732e\u4e2d\u7528\u4e8e\u63d0\u53d6\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u7684\u4e3b\u8981\u65b9\u6cd5.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#521-direct-reference-encoding-without-reference-audio","title":"5.2.1.\u65e0\u53c2\u8003\u97f3\u9891\u7684\u76f4\u63a5\u53c2\u8003\u7f16\u7801 Direct Reference Encoding without Reference Audio","text":"<p>In several studies, prosody predictors are trained jointly with the proposed reference encoder to bypass the requirement for reference audio at inference time. The prosody predictors are trained to predict either the prosody embeddings generated by reference encoders[50] [96] [111] [116], or the acoustic features used as input to reference encoders [37] [63]. As input to these prosody predictors, most studies utilize the phoneme embeddings[37] [63] [96] [111].</p> <p>\u5728\u4e00\u4e9b\u7814\u7a76\u4e2d, \u97f5\u5f8b\u9884\u6d4b\u5668\u4e0e\u6240\u63d0\u51fa\u7684\u53c2\u8003\u7f16\u7801\u5668\u8054\u5408\u8bad\u7ec3, \u4ee5\u7ed5\u8fc7\u63a8\u7406\u65f6\u5bf9\u53c2\u8003\u97f3\u9891\u7684\u8981\u6c42. \u97f5\u5f8b\u9884\u6d4b\u5668\u88ab\u8bad\u7ec3\u7528\u4e8e\u9884\u6d4b\u53c2\u8003\u7f16\u7801\u5668\u751f\u6210\u7684\u97f5\u5f8b\u5d4c\u5165\u6216\u7528\u4f5c\u53c2\u8003\u7f16\u7801\u5668\u8f93\u5165\u7684\u58f0\u5b66\u7279\u5f81. \u4f5c\u4e3a\u8fd9\u4e9b\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8f93\u5165, \u5927\u591a\u6570\u7814\u7a76\u4f7f\u7528\u97f3\u7d20\u5d4c\u5165.</p> <p>Alternatively, features extracted from input text can also be used as input for prosody predictors. In [50], the prosody predictor has a hierarchical structure that utilizes contextual information at both the sentence and paragraph levels to predict prosody embeddings. The input features for this predictor are in the form of 768-dimensional phrase embeddings extracted by the pre-trained language model XLNet [160]. Sentence embeddings are initially predicted from the input features using an attention network. Then a second attention network is used to predict the paragraph-level prosody embedding.</p> <p>\u6216\u8005\u4ece\u8f93\u5165\u6587\u672c\u63d0\u53d6\u7684\u7279\u5f81\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8f93\u5165. \u6587\u732e [050] \u4e2d\u97f5\u5f8b\u9884\u6d4b\u5176\u6709\u4e00\u4e2a\u5c42\u6b21\u7ed3\u6784, \u5229\u7528\u53e5\u5b50\u548c\u6bb5\u843d\u7ea7\u522b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u9884\u6d4b\u97f5\u5f8b\u5d4c\u5165. \u8fd9\u4e00\u9884\u6d4b\u5668\u7684\u8f93\u5165\u7279\u5f81\u662f\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b XLNet \u63d0\u53d6\u7684 768 \u7ef4\u7684\u77ed\u8bed\u5d4c\u5165\u5f62\u5f0f. \u9996\u5148\u901a\u8fc7\u6ce8\u610f\u529b\u7f51\u7edc\u4ece\u8f93\u5165\u7279\u5f81\u4e2d\u9884\u6d4b\u53e5\u5b50\u5d4c\u5165, \u7136\u540e\u7b2c\u4e8c\u4e2a\u6ce8\u610f\u529b\u7f51\u7edc\u7528\u4e8e\u9884\u6d4b\u6bb5\u843d\u7ea7\u522b\u7684\u97f5\u5f8b\u5d4c\u5165.</p> <p>Furthermore, in [33], emotion is modelled at three levels: global, utterance, and syllable (local). The model employs three prosody encoders, each with a predictor trained to predict the corresponding prosody embedding based on input text. The global-level predictor functions as an emotion classifier, where the output of its final soft-max layer serves as the global emotion embedding. The emotion label\u2019s embedding is used as the ground truth for this emotion classifier. Both the utterance and local prosody encoders receive level-aligned mel-spectrograms as input and produce utterance prosody embedding and local prosody strength embedding, respectively. Similarly, two prosody predictors are used to predict utterance and local-level embeddings based on the output from the text encoder of the TTS model.</p> <p>\u6587\u732e [033] \u4e2d\u60c5\u611f\u88ab\u5efa\u6a21\u5728\u4e09\u4e2a\u7ea7\u522b: \u5168\u5c40, \u8bed\u8c03\u548c\u97f3\u8282 (\u5c40\u90e8). \u6a21\u578b\u5e94\u7528\u4e09\u4e2a\u97f5\u5f8b\u7f16\u7801\u5668, \u6bcf\u4e2a\u7f16\u7801\u5668\u90fd\u6709\u4e00\u4e2a\u9884\u6d4b\u5668\u8bad\u7ec3\u6210\u57fa\u4e8e\u8f93\u5165\u6587\u672c\u9884\u6d4b\u5bf9\u5e94\u7684\u97f5\u5f8b\u5d4c\u5165. \u5168\u5c40\u9884\u6d4b\u5668\u4f5c\u4e3a\u60c5\u611f\u5206\u7c7b\u5668, \u5176\u6700\u7ec8\u7684 softmax \u5c42\u8f93\u51fa\u4f5c\u4e3a\u5168\u5c40\u60c5\u611f\u5d4c\u5165. \u60c5\u611f\u6807\u7b7e\u7684\u5d4c\u5165\u4f5c\u4e3a\u8fd9\u4e00\u60c5\u611f\u5206\u7c7b\u5668\u7684\u771f\u5b9e\u503c. \u8bed\u8c03\u548c\u5c40\u90e8\u97f5\u5f8b\u7f16\u7801\u5668\u90fd\u63a5\u6536\u7ea7\u522b\u5bf9\u9f50\u7684\u6885\u5c14\u9891\u8c31\u4f5c\u4e3a\u8f93\u5165, \u5e76\u5206\u522b\u751f\u6210\u8bed\u8c03\u97f5\u5f8b\u5d4c\u5165\u548c\u5c40\u90e8\u97f5\u5f8b\u5f3a\u5ea6\u5d4c\u5165. \u7c7b\u4f3c\u5730, \u4e24\u4e2a\u97f5\u5f8b\u9884\u6d4b\u5668\u57fa\u4e8e TTS \u6a21\u578b\u7684\u6587\u672c\u7f16\u7801\u5668\u7684\u8f93\u51fa\u9884\u6d4b\u8bed\u8c03\u548c\u5c40\u90e8\u7ea7\u522b\u5d4c\u5165.</p> <p>In contrast, the prosody predictor proposed in paper [44] learns multiple mixed Gaussian distributions model (GMM) for prosody representations. Therefore, the final outputs of the prosody predictor involve three parameters: mean, variance, and weight of multiple mixed Gaussian distributions from which prosody representations can be sampled at inference time. As input, the predictor receives two phoneme-level sequences including embeddings from the text encoder and embeddings from a pre-trained language model. Similar work is proposed in [95] where only phoneme embeddings are used as input to the prosody predictor. GMM in both studies is modeled via the mixture density network [161].</p> <p>\u6587\u732e [044] \u63d0\u51fa\u7684\u97f5\u5f8b\u9884\u6d4b\u5668\u5b66\u4e60\u591a\u91cd\u6df7\u5408\u9ad8\u65af\u5206\u5e03\u6a21\u578b\u7528\u4e8e\u97f5\u5f8b\u8868\u793a. \u56e0\u6b64\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u6700\u7ec8\u8f93\u51fa\u5305\u542b\u4e09\u4e2a\u53c2\u6570: \u5747\u503c, \u65b9\u5dee\u548c\u591a\u6df7\u5408\u9ad8\u65af\u5206\u5e03\u7684\u6743\u91cd. \u4ece\u8fd9\u4e9b\u5206\u5e03\u4e2d\u53ef\u4ee5\u5728\u63a8\u7406\u65f6\u91c7\u6837\u97f5\u5f8b\u8868\u793a. \u4f5c\u4e3a\u8f93\u5165, \u9884\u6d4b\u5668\u63a5\u6536\u4e24\u4e2a\u97f3\u7d20\u7ea7\u522b\u7684\u5e8f\u5217\u5305\u62ec\u6765\u81ea\u6587\u672c\u7f16\u7801\u5668\u7684\u5d4c\u5165\u548c\u6765\u81ea\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165.</p> <p>\u6587\u732e [095] \u63d0\u51fa\u7684\u76f8\u4f3c\u5de5\u4f5c, \u53ea\u4f7f\u7528\u97f3\u7d20\u5d4c\u5165\u4f5c\u4e3a\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8f93\u5165.</p> <p>\u4e24\u9879\u5de5\u4f5c\u7684 GMM \u90fd\u901a\u8fc7\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\u8fdb\u884c\u5efa\u6a21.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#522vae-vaebased-approaches-without-reference-audio","title":"5.2.2.\u65e0\u53c2\u8003\u8bed\u97f3\u7684VAE\u7c7b\u65b9\u6cd5 (VAE\u2011Based Approaches without Reference Audio)","text":"<p>Sampling from the latent space without reference audio results in less controllability of style. In addition, it can also introduce naturalness degradation and inappropriate contextual prosody with regard to the input text [68] [129].  Therefore, to avoid sampling the latent space without a reference, authors of [131] proposed utilizing the same prosody embedding of the most similar training sentence to input sentence at inference time. The selection process is based on measuring cosine similarity between sentences\u2019 linguistic features. Three methods are proposed for extracting sentence linguistic information including  (1) calculating the syntactic distance between words in the sentence using constituency trees [162],  (2) averaging the contextual word embeddings (CWE) for the words in the sentence using BERT, and  (3) combining the previous two methods.</p> <p>\u5728\u6ca1\u6709\u53c2\u8003\u97f3\u9891\u7684\u60c5\u51b5\u4e0b\u4ece\u9690\u7a7a\u95f4\u91c7\u6837\u4f1a\u5bfc\u81f4\u98ce\u683c\u7684\u53ef\u63a7\u6027\u964d\u4f4e. \u6b64\u5916\u5b83\u8fd8\u4f1a\u5f15\u5165\u81ea\u7136\u6027\u9000\u5316\u548c\u548c\u4e0d\u9002\u5408\u8f93\u5165\u6587\u672c\u7684\u8bed\u5883\u97f5\u5f8b. \u56e0\u6b64\u4e3a\u4e86\u907f\u514d\u5728\u6ca1\u6709\u53c2\u8003\u7684\u60c5\u51b5\u4e0b\u4ece\u9690\u7a7a\u95f4\u4e2d\u91c7\u6837, \u6587\u732e [131] \u63d0\u51fa\u63a8\u7406\u65f6\u7528\u548c\u8f93\u5165\u53e5\u5b50\u6700\u76f8\u4f3c\u7684\u8bad\u7ec3\u53e5\u5b50\u7684\u76f8\u540c\u97f5\u5f8b\u5d4c\u5165. \u9009\u62e9\u8fc7\u7a0b\u57fa\u4e8e\u53e5\u5b50\u8bed\u8a00\u7279\u5f81\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6. \u4e09\u79cd\u65b9\u6cd5\u7528\u4e8e\u63d0\u53d6\u53e5\u5b50\u8bed\u8a00\u4fe1\u606f: 1. \u4f7f\u7528\u53e5\u6cd5\u6811\u8ba1\u7b97\u53e5\u5b50\u4e2d\u5355\u8bcd\u7684\u53e5\u6cd5\u8ddd\u79bb; 2. \u4f7f\u7528 BERT \u8ba1\u7b97\u53e5\u5b50\u4e2d\u5355\u8bcd\u7684\u4e0a\u4e0b\u6587\u8bcd\u5d4c\u5165 (Contextual Word Embeddings, CWE) \u7684\u5e73\u5747\u503c; 3. \u7ed3\u5408\u524d\u4e24\u79cd\u65b9\u6cd5.</p> <p>Other studies approach the problem in alternative ways, seeking to enhance the sampling process either through refining the baseline model structure or by incorporating text-based components into the baseline. Regarding the improvement of the baseline structure, study [68] suggests the combination of multiple variational autoencoders to generate latent variables at three distinct levels: utterance-level, phrase-level, and word-level. Furthermore, they apply a conditional prior (CP) to learn the latent space distribution based on the input text embedding. To account for dependencies within the input text, they employ Autoregressive (AR) latent converters to transform latent variables from coarser to finer levels. An alternative approach is proposed in [126] by replacing the conventional VAE encoder with a residual encoder that leverages phoneme embedding and a set of learnable free parameters as inputs. With this modified structure, the model learns a latent distribution that represents various prosody styles for a specific sentence (i.e.,the input text), in addition to capturing potential global biases within the applied dataset (represented by the free parameters). At the same time, with this modification, the problem of speaker and content leakage into prosody embedding is addressed.</p> <p>\u5176\u4ed6\u7814\u7a76\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898, \u5bfb\u6c42\u901a\u8fc7\u6539\u8fdb\u57fa\u7ebf\u6a21\u578b\u7ed3\u6784\u6216\u5411\u57fa\u7ebf\u6a21\u578b\u4e2d\u6dfb\u52a0\u57fa\u4e8e\u6587\u672c\u7684\u7ec4\u4ef6\u6765\u589e\u5f3a\u91c7\u6837\u8fc7\u7a0b. \u5173\u4e8e\u57fa\u7ebf\u7ed3\u6784\u7684\u6539\u8fdb: - \u6587\u732e [068] \u5efa\u8bae\u7ec4\u5408\u591a\u4e2a\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u751f\u6210\u4e09\u4e2a\u4e0d\u540c\u7ea7\u522b\u7684\u9690\u53d8\u91cf: \u8bed\u8c03\u7ea7, \u77ed\u8bed\u7ea7\u548c\u5355\u8bcd\u7ea7. \u6b64\u5916\u5e94\u7528\u4e86\u6761\u4ef6\u5148\u9a8c\u57fa\u4e8e\u8f93\u5165\u6587\u672c\u5d4c\u5165\u5b66\u4e60\u9690\u7a7a\u95f4\u5206\u5e03. \u4e3a\u4e86\u8003\u8651\u8f93\u5165\u6587\u672c\u5185\u7684\u4f9d\u8d56\u6027, \u4ed6\u4eec\u5e94\u7528\u81ea\u56de\u5f52\u9690\u8f6c\u5316\u5668\u5c06\u9690\u53d8\u91cf\u4ece\u7c97\u7cd9\u7ea7\u522b\u8f6c\u5316\u4e3a\u7cbe\u7ec6\u7ea7\u522b. - \u6587\u732e [126] \u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5, \u901a\u8fc7\u5c06\u4f20\u7edf VAE \u66ff\u6362\u4e3a\u4e00\u4e2a\u5229\u7528\u97f3\u7d20\u5d4c\u5165\u548c\u4e00\u7ec4\u53ef\u5b66\u4e60\u81ea\u7531\u53c2\u6570\u4f5c\u4e3a\u8f93\u5165\u7684\u6b8b\u5dee\u7f16\u7801\u5668. \u91c7\u7528\u8fd9\u4e00\u7ed3\u6784, \u6a21\u578b\u4e3a\u4e00\u4e2a\u5177\u4f53\u53e5\u5b50\u5373\u8f93\u5165\u6587\u672c\u5b66\u4e60\u4e00\u4e2a\u9690\u5206\u5e03\u8868\u793a\u5404\u79cd\u97f5\u5f8b\u98ce\u683c, \u540c\u65f6\u6355\u6349\u5e94\u7528\u6570\u636e\u96c6\u7684\u6f5c\u5728\u5168\u5c40\u504f\u5dee (\u7531\u81ea\u7531\u53c2\u6570\u8868\u793a). \u540c\u65f6\u8bf4\u8bdd\u4eba\u548c\u5185\u5bb9\u6cc4\u9732\u5230\u97f5\u5f8b\u5d4c\u5165\u7684\u95ee\u9898\u4e5f\u5f97\u5230\u89e3\u51b3.</p> <p>Various studies propose training a predictor for the latent prosody vectors based on features extracted from the input text [35] [47]. The proposed model in [47] generates fine-grained prosody latent codes of three dimensions at phoneme-level. These prosody codes are then used to guide the training process of a prosody predictor that receives phoneme embeddings as input, in addition to emotion and speaker embeddings as sentence-level conditions. In [35], the predicted mean values of the latent space distribution are employed as prosody codes. Similarly, a prosody predictor is trained to predict these prosody codes using two text-based inputs, including sentence-level embeddings from a pre-trained BERT model and contextual information considering BERT embeddings of a few of surrounding k sentences given the current sentence.</p> <p>\u591a\u9879\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u8bad\u7ec3\u4e00\u4e2a\u9690\u97f5\u5f8b\u5411\u91cf\u7684\u9884\u6d4b\u5668. - \u6587\u732e [047] \u63d0\u51fa\u7684\u6a21\u578b\u5728\u97f3\u7d20\u7ea7\u522b\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u4e09\u7ef4\u97f5\u5f8b\u9690\u4ee3\u7801. \u8fd9\u4e9b\u97f5\u5f8b\u4ee3\u7801\u4e4b\u540e\u7528\u4e8e\u6307\u5bfc\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8bad\u7ec3. \u9884\u6d4b\u5668\u63a5\u6536\u97f3\u7d20\u5d4c\u5165\u4f5c\u4e3a\u8f93\u5165, \u6b64\u5916\u60c5\u611f\u548c\u8bf4\u8bdd\u4eba\u5d4c\u5165\u4f5c\u4e3a\u53e5\u5b50\u7ea7\u522b\u6761\u4ef6. - \u6587\u732e [035] \u4e2d\u9690\u7a7a\u95f4\u5206\u5e03\u7684\u9884\u6d4b\u5747\u503c\u4f5c\u4e3a\u97f5\u5f8b\u7f16\u7801. \u7c7b\u4f3c\u5730\u8bad\u7ec3\u4e00\u4e2a\u97f5\u5f8b\u9884\u6d4b\u5668\u4f7f\u7528\u4e24\u4e2a\u57fa\u4e8e\u6587\u672c\u7684\u8f93\u5165\u5305\u62ec\u6765\u81ea\u9884\u8bad\u7ec3 BERT \u6a21\u578b\u7684\u53e5\u5b50\u7ea7\u522b\u5d4c\u5165\u548c\u8003\u8651\u5f53\u524d\u53e5\u5b50\u5468\u56f4\u7684 k \u4e2a\u53e5\u5b50\u7684 BERT \u5d4c\u5165\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u7528\u4e8e\u9884\u6d4b\u8fd9\u4e9b\u97f5\u5f8b\u7f16\u7801.</p> <p>Alternatively, study [129] proposed training a sampler, i.e., Gaussian parameters, to sample the latent space using features extracted from the input text. Three different structures are investigated for the sampler based on the input features it receives. The applied text-based features include BERT representations of a sentence (semantic information), the parsing tree of the sentence (syntactic information) after it is fed to a graph attention network, and the concatenation of outputs from the previous two samplers.</p> <p>\u6587\u732e [129] \u63d0\u51fa\u8bad\u7ec3\u4e00\u4e2a\u91c7\u6837\u5668, \u5373\u9ad8\u65af\u53c2\u6570, \u4f7f\u7528\u8f93\u5165\u6587\u672c\u63d0\u53d6\u7684\u7279\u5f81\u5bf9\u9690\u7a7a\u95f4\u8fdb\u884c\u91c7\u6837. \u6839\u636e\u91c7\u6837\u5668\u63a5\u6536\u7684\u8f93\u5165\u7279\u5f81, \u7814\u7a76\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u7ed3\u6784. \u5e94\u7528\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u5305\u62ec\u53e5\u5b50\u7684 BERT \u8868\u793a (\u8bed\u4e49\u4fe1\u606f), \u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u8f93\u51fa\u7684\u53e5\u5b50\u7684\u89e3\u6790\u6811 (\u8bed\u6cd5\u4fe1\u606f) \u548c\u524d\u4e24\u4e2a\u91c7\u6837\u5668\u7684\u8f93\u51fa\u7684\u62fc\u63a5.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#523-gst-gstbased-approaches-without-reference-audio","title":"5.2.3.\u65e0\u53c2\u8003\u97f3\u9891\u7684 GST \u7c7b\u65b9\u6cd5 (GST\u2011Based Approaches without Reference Audio)","text":"<p>There are GST-TTS models that utilize text-based features from pre-trained language models such as BERT to guide expressive speech synthesis at inference time without a reference. In ST-TTS, the training dataset is labeled with short phrases that describe the style of the utterance and are known as style tags. A pre-trained Sentence BERT (SBERT) model is used to produce embeddings for each style tag as input to a style tag encoder. The style embedding from the GST-TTS model is used as ground truth for the style tag encoder. During inference, either a reference audio or a style tag can be used to generate speech.</p> <p>\u6709\u4e9b GST-TTS \u6a21\u578b\u4f7f\u7528\u6765\u81ea\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (\u5982 BERT) \u7684\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u7528\u4e8e\u6307\u5bfc\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u65e0\u53c2\u8003\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210. - ST-TTS \u4e2d\u8bad\u7ec3\u96c6\u7528\u63cf\u8ff0\u8bed\u8c03\u98ce\u683c\u7684\u77ed\u8bed\u8fdb\u884c\u6807\u6ce8, \u5373\u98ce\u683c\u6807\u7b7e. \u7528\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u53e5\u5b50 BERT \u6a21\u578b (SBERT) \u4e3a\u6bcf\u4e2a\u98ce\u683c\u6807\u7b7e\u751f\u6210\u5d4c\u5165\u4f5c\u4e3a\u98ce\u683c\u6807\u7b7e\u7f16\u7801\u5668\u7684\u8f93\u5165. \u6765\u81ea GST-TTS \u6a21\u578b\u7684\u98ce\u683c\u5d4c\u5165\u4f5c\u4e3a\u98ce\u683c\u6807\u7b7e\u7f16\u7801\u5668\u7684\u771f\u5b9e\u503c. \u5728\u63a8\u7406\u65f6, \u53ef\u4ee5\u4f7f\u7528\u53c2\u8003\u97f3\u9891\u6216\u98ce\u683c\u6807\u7b7e\u7528\u4e8e\u751f\u6210\u8bed\u97f3.</p> <p>Alternatively, pre-trained language models are used to extract features from the input text and train a prosody predictor to predict the style embedding based on these text-based features (Context-Aware Style Predictor, [46] [50] [73] [91] [94]. In [94], the baseline model [75] is extended with a prosody predictor module that extracts time-aggregated features from the output of the baseline text encoder. Two pathways are suggested for the targets of the predictor output: either using the weights of the GSTs or the final style embedding. Similarly, in [73], two prosody predictors are investigated, using different inputs from a pre-trained multi-language BERT model. While the first predictor utilizes BERT embeddings for the sub-word sequence of input text, the other predictor employs only the CLS token from the sentence-level information extracted by the BERT model. Both inputs provide rich information for the predictors to synthesize prosodic speech based solely on input text.</p> <p>\u6216\u8005\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u6765\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u7279\u5f81\u5e76\u8bad\u7ec3\u4e00\u4e2a\u97f5\u5f8b\u9884\u6d4b\u5668, \u57fa\u4e8e\u8fd9\u4e9b\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u6765\u9884\u6d4b\u98ce\u683c\u5d4c\u5165. - \u6587\u732e [094] \u4e2d\u5c06\u57fa\u7ebf\u6a21\u578b [075] \u7528\u97f5\u5f8b\u9884\u6d4b\u5668\u6a21\u5757\u8fdb\u884c\u6269\u5c55, \u7528\u4e8e\u4ece\u57fa\u7ebf\u6587\u672c\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4e2d\u63d0\u53d6\u65f6\u95f4\u805a\u5408\u7279\u5f81. \u5bf9\u4e8e\u9884\u6d4b\u5668\u8f93\u51fa\u7684\u76ee\u6807\u6709\u4e24\u4e2a\u5efa\u8bae: \u4f7f\u7528 GSTs \u7684\u6743\u91cd\u6216\u6700\u7ec8\u98ce\u683c\u5d4c\u5165. - \u6587\u732e [073] \u7814\u7a76\u4e86\u4e24\u4e2a\u97f5\u5f8b\u9884\u6d4b\u5668, \u4f7f\u7528\u6765\u81ea\u9884\u8bad\u7ec3\u591a\u8bed\u8a00 BERT \u6a21\u578b\u7684\u4e0d\u540c\u8f93\u5165. \u7b2c\u4e00\u4e2a\u9884\u6d4b\u5668\u4f7f\u7528\u8f93\u5165\u6587\u672c\u7684\u5b50\u8bcd\u5e8f\u5217\u7684 BERT \u5d4c\u5165, \u7b2c\u4e8c\u4e2a\u9884\u6d4b\u5668\u4ec5\u4f7f\u7528\u4ece BERT \u6a21\u578b\u63d0\u53d6\u7684\u53e5\u5b50\u7ea7\u522b\u4fe1\u606f\u4e2d\u7684 CLS token. \u8fd9\u4e24\u4e2a\u8f93\u5165\u90fd\u4e3a\u9884\u6d4b\u5668\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u4fe1\u606f\u4ece\u800c\u4ec5\u4f9d\u9760\u8f93\u5165\u6587\u672c\u5408\u6210\u97f5\u5f8b\u8bed\u97f3.</p> <p>The multi-scale GST-TTS proposed in [50] which employs three style encoders, also introduces three style predictors that employ hierarchical context encoders (HCE). The input to the first predictor is the BERT sub-word-level semantic embedding sequence. The attention units in the HCE, however, are used to aggregate the resulting context embedding sequence from lower-level as input to higher-level predictors. Additionally, the output of the higher-level predictor is used to condition the lower-level predictor. BERT embeddings are also used in [46] but at word level and are passed as input to the proposed prosody predictor. The style embedding which is generated via word-level GSTs is used to guide the prosody predictor during model training.</p> <ul> <li>\u6587\u732e [050] \u63d0\u51fa\u7684\u591a\u5c3a\u5ea6 GST-TTS \u91c7\u7528\u4e86\u4e09\u4e2a\u98ce\u683c\u7f16\u7801\u5668, \u4e5f\u5f15\u5165\u4e86\u4e09\u4e2a\u91c7\u7528\u5c42\u6b21\u4e0a\u4e0b\u6587\u7f16\u7801\u5668 (Hierarchical Context Encoders, HCE) \u7684\u98ce\u683c\u9884\u6d4b\u5668. \u7b2c\u4e00\u4e2a\u9884\u6d4b\u5668\u7684\u8f93\u5165\u662f BERT \u5b50\u8bcd\u7ea7\u522b\u8bed\u4e49\u5d4c\u5165\u5e8f\u5217. HCE \u4e2d\u7684\u6ce8\u610f\u529b\u5355\u5143\u7528\u4e8e\u805a\u5408\u7531\u4f4e\u5c42\u6b21\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u5e8f\u5217\u4f5c\u4e3a\u9ad8\u5c42\u6b21\u9884\u6d4b\u5668\u7684\u8f93\u5165. \u6b64\u5916\u9ad8\u5c42\u6b21\u9884\u6d4b\u5668\u7684\u8f93\u51fa\u7528\u4e8e\u6761\u4ef6\u5316\u4f4e\u5c42\u6b21\u9884\u6d4b\u5668. </li> <li>\u6587\u732e [044] \u540c\u6837\u4f7f\u7528 BERT \u5d4c\u5165\u4f46\u662f\u5728\u5355\u8bcd\u7ea7\u522b, \u5e76\u4e14\u4f5c\u4e3a\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8f93\u5165. \u901a\u8fc7\u8bcd\u7ea7 GSTs \u751f\u6210\u7684\u98ce\u683c\u5d4c\u5165\u5728\u6a21\u578b\u8bad\u7ec3\u65f6\u7528\u4e8e\u6307\u5bfc\u97f5\u5f8b\u9884\u6d4b\u5668.</li> </ul> <p>A Context-aware prosody predictor is proposed in \"Context-Aware Coherent Speaking Style Prediction with Hierarchical Transformers for Audiobook Speech Synthesis\" which considers both text-side context information and speech-side style information from preceding speech. This predictor comprises two hierarchical components: a sentence encoder and a fusion context encoder. The context-aware input to the predictor includes word-level embeddings from XLNet [160] for each word in the current sentence, as well as the N preceding and following sentences. The sentence encoder focuses on learning low-level word meanings within each sentence, while the fusion context encoder captures high-level contextual semantics between the sentences. Additionally, style embeddings from previous sentences are integrated into the fusion context encoder input to account for speech-side information.</p> <p>\u6587\u732e\u63d0\u51fa\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u97f5\u5f8b\u9884\u6d4b\u5668, \u8003\u8651\u6765\u6e90\u4e8e\u4e4b\u524d\u8bed\u97f3\u7684\u6587\u672c\u4fa7\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u8bed\u97f3\u4fa7\u7684\u98ce\u683c\u4fe1\u606f. \u8fd9\u4e00\u9884\u6d4b\u5668\u5305\u542b\u4e24\u4e2a\u5c42\u6b21\u7ec4\u4ef6: \u4e00\u4e2a\u53e5\u5b50\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u878d\u5408\u5185\u5bb9\u7f16\u7801\u5668. \u9884\u6d4b\u5668\u7684\u5185\u5bb9\u611f\u77e5\u8f93\u5165\u5305\u62ec XLNet \u5bf9\u5f53\u524d\u53e5\u5b50\u6bcf\u4e2a\u5355\u8bcd\u7684\u7684\u8bcd\u7ea7\u5d4c\u5165\u548c N \u4e2a\u524d\u540e\u53e5\u5b50. \u53e5\u5b50\u7f16\u7801\u5668\u91cd\u70b9\u5b66\u4e60\u53e5\u5b50\u5185\u4f4e\u5c42\u6b21\u5355\u8bcd\u542b\u4e49, \u800c\u878d\u5408\u5185\u5bb9\u7f16\u7801\u5668\u6355\u83b7\u53e5\u5b50\u95f4\u7684\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u8bed\u4e49. \u6b64\u5916\u524d\u9762\u53e5\u5b50\u7684\u98ce\u683c\u5d4c\u5165\u6574\u5408\u5230\u878d\u5408\u5185\u5bb9\u7f16\u7801\u5668\u7684\u8f93\u5165\u4ee5\u8003\u8651\u8bed\u97f3\u4fa7\u7684\u4fe1\u606f.</p> <p>In [91] Speech emotion recognition model (SER) is employed as a style descriptor to learn the implicit connection between style features and input text. Deep style features for both synthesized speech and reference speech are obtained from a small intermediate fully connected layer of a pre-trained SER model during training. The extracted style features are compared where an additional loss is introduced to the GST-TTS model loss. At inference time only text is used to synthesize expressive speech.</p> <p>\u6587\u732e [091] \u8bed\u97f3\u60c5\u611f\u8bc6\u522b (Speech Emotion Recognition, SER) \u6a21\u578b\u4f5c\u4e3a\u98ce\u683c\u63cf\u8ff0\u5668, \u5b66\u4e60\u98ce\u683c\u7279\u5f81\u548c\u8f93\u5165\u6587\u672c\u4e4b\u95f4\u7684\u9690\u5f0f\u8054\u7cfb. \u5408\u6210\u8bed\u97f3\u548c\u53c2\u8003\u8bed\u97f3\u7684\u6df1\u5ea6\u98ce\u683c\u7279\u5f81\u5728\u8bad\u7ec3\u65f6\u4ece\u9884\u8bad\u7ec3 SER \u6a21\u578b\u7684\u4e00\u4e2a\u5c0f\u7684\u4e2d\u95f4\u5168\u8fde\u63a5\u5c42\u4e2d\u83b7\u5f97. \u5f53\u7ed9 GST-TTS \u6a21\u578b\u635f\u5931\u5f15\u5165\u989d\u5916\u7684\u635f\u5931\u540e, \u5bf9\u63d0\u53d6\u7684\u98ce\u683c\u7279\u5f81\u8fdb\u884c\u4e86\u5bf9\u6bd4. \u5728\u63a8\u7406\u65f6\u4ec5\u8f93\u5165\u6587\u672c\u7528\u4e8e\u5408\u6210\u8868\u8fbe\u6027\u8bed\u97f3.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#524-etts-approaches-based-only-on-text","title":"5.2.4.\u4ec5\u4f9d\u8d56\u4e8e\u6587\u672c\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u65b9\u6cd5 (ETTS Approaches Based only on Text)","text":"<p>This category involves approaches that depend solely on input text to obtain prosody-related representations/embeddings during TTS model training. Several features related to speech prosody have been proposed by various studies for extraction from input text and subsequent transmission to a DNN-based module to generate prosody representations. For instance, the features extracted by the pre-trained language models can capture both semantic and syntactic relationships with the input text, making them effective representations for prosody. In [83], input text word-level embeddings are extracted by the Embeddings from Language Models (ELMo) model [163] and used to generate context-related embeddings via a context encoder. Similarly, in [29], BERT is employed to extract embeddings for utterance sentences and pass them to a specific context-encoder to aggregate these embeddings and form a final context vector.</p> <p>\u8fd9\u7c7b\u65b9\u6cd5\u5305\u542b\u5728\u8bad\u7ec3\u9636\u6bb5\u4ec5\u4f9d\u8d56\u8f93\u5165\u6587\u672c\u4ee5\u83b7\u53d6\u97f5\u5f8b\u76f8\u5173\u8868\u793a\u6216\u5d4c\u5165\u7684\u65b9\u6cd5. \u5404\u79cd\u7814\u7a76\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u51fa\u548c\u8bed\u97f3\u97f5\u5f8b\u76f8\u5173\u7684\u82e5\u5e72\u7279\u5f81, \u7136\u540e\u4f20\u8f93\u5230\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u5757\u4ee5\u751f\u6210\u97f5\u5f8b\u8868\u793a. \u4f8b\u5982\u7531\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u53ef\u4ee5\u6355\u83b7\u8f93\u5165\u6587\u672c\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u5173\u7cfb, \u4f7f\u4e4b\u6210\u4e3a\u97f5\u5f8b\u7684\u6709\u6548\u8868\u793a. - \u6587\u732e [083] \u901a\u8fc7 Embeddings from Language Models, ELMo \u63d0\u53d6\u8f93\u5165\u6587\u672c\u8bcd\u7ea7\u5d4c\u5165\u5e76\u901a\u8fc7\u5185\u5bb9\u7f16\u7801\u5668\u751f\u6210\u5185\u5bb9\u76f8\u5173\u7684\u5d4c\u5165. - \u6587\u732e [029] \u4f7f\u7528 BERT \u63d0\u53d6\u8bed\u8c03\u5e8f\u5217\u7684\u5d4c\u5165\u5e76\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9\u5177\u4f53\u6587\u672c\u7f16\u7801\u5668\u4ee5\u805a\u5408\u8fd9\u4e9b\u5d4c\u5165\u5e76\u5f62\u6210\u6700\u7ec8\u7684\u4e0a\u4e0b\u6587\u5411\u91cf.</p> <p>Other studies, such as [30] [40] [54], utilize graph representations of input text, which can also reflect semantic and syntactic information about the given text. In [30], the graphical representations of prosody boundaries in Chinese text are passed to a graph encoder based on Graph Neural Networks (GNN) to generate prosodic information for the input text. The prosody boundaries of the Chinese language can be manually annotated or predicted using a pre-trained model. In contrast, [54] combines BERT-extracted features for input text with its graph dependency tree to produce word-level prosody representations. Specifically, the input text is passed through both BERT and a dependency parsing model to extract the dependency tree for word-level BERT embedding. A Relational Gated Graph Network (RGGN) is used to convert this dependency tree into word-level semantic representations upon which the decoder of the TTS model is conditioned.</p> <p>\u5176\u4ed6\u7814\u7a76\u5229\u7528\u8f93\u5165\u6587\u672c\u7684\u56fe\u8868\u793a, \u540c\u6837\u53ef\u4ee5\u53cd\u6620\u7ed9\u5b9a\u6587\u672c\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u4fe1\u606f. - \u6587\u732e [030] \u5c06\u6c49\u8bed\u6587\u672c\u7684\u97f5\u5f8b\u8fb9\u754c\u7684\u56fe\u5f62\u8868\u793a\u8f6c\u9012\u7ed9\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u7f16\u7801\u5668\u4ee5\u751f\u6210\u8f93\u5165\u6587\u672c\u7684\u97f5\u5f8b\u4fe1\u606f. \u4e2d\u6587\u7684\u97f5\u5f8b\u8fb9\u754c\u53ef\u4ee5\u4eba\u5de5\u6807\u6ce8\u6216\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b.  - \u6587\u732e [054] \u5c06 BERT \u4ece\u8f93\u5165\u6587\u672c\u63d0\u53d6\u7684\u7279\u5f81\u548c\u5b83\u7684\u56fe\u4f9d\u8d56\u6811\u7ed3\u5408\u7528\u4e8e\u4ea7\u751f\u8bcd\u7ea7\u97f5\u5f8b\u8868\u793a. \u5177\u4f53\u5730, \u8f93\u5165\u6587\u672c\u4f20\u8f93\u5230 BERT \u548c\u4e00\u4e2a\u4f9d\u8d56\u89e3\u6790\u6a21\u578b\u7528\u4e8e\u63d0\u53d6\u8bcd\u7ea7 BERT \u5d4c\u5165\u7684\u4f9d\u8d56\u6811. \u4f7f\u7528\u76f8\u5173\u95e8\u63a7\u56fe\u7f51\u7edc (Relational Gated Graph Network, RGGN) \u5c06\u4f9d\u8d56\u6811\u8f6c\u5316\u4e3a\u8bcd\u7ea7\u8bed\u4e49\u8868\u793a, \u5e76\u4ee5\u6b64\u6761\u4ef6\u5316 TTS \u6a21\u578b\u7684\u89e3\u7801\u5668.</p> <p>Different text-based features have been extracted from input text to obtain prosody (style) embeddings in [40]. The paper utilizes an emotion lexicon to extract word-level emotion features, including VAD (valence, arousal, dominance) and BE5 (joy, anger, sadness, fear, disgust). Additionally, the [CLS] embedding by BERT for each utterance is also extracted. The obtained features are then passed to a style encoder to produce a style embedding.</p> <ul> <li>\u6587\u732e [40] \u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u4e0d\u540c\u7684\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u7528\u4e8e\u83b7\u5f97\u97f5\u5f8b/\u98ce\u683c\u5d4c\u5165. \u4f7f\u7528\u4e00\u4e2a\u60c5\u611f\u8bcd\u5178\u7528\u4e8e\u63d0\u53d6\u8bcd\u7ea7\u60c5\u611f\u7279\u5f81, \u5305\u62ec VAD (\u6548\u4ef7, \u5524\u9192, \u652f\u914d) \u548c BES (\u559c\u60a6, \u6124\u6012, \u60b2\u4f24, \u6050\u60e7, \u538c\u6076). \u6b64\u5916\u7531 BERT \u5bf9\u6bcf\u4e2a\u8bed\u8c03\u63d0\u53d6 [CLS] \u5d4c\u5165. \u83b7\u5f97\u7684\u7279\u5f81\u4e4b\u540e\u4f20\u9012\u7ed9\u98ce\u683c\u7f16\u7801\u5668\u4ee5\u4ea7\u751f\u98ce\u683c\u5d4c\u5165.</li> </ul> <p>Other models under this category train a prosody encoder/predictor jointly with an autoregressive TTS model such as Tacotron 2, to encode some prosody-related features utilizing text-based features. The trained encoder is then used at inference time to encode prosody-related features based on input text to the TTS model. The text-based input to these prosody encoders in most of the studies is the text\u2019s character/phoneme embeddings [20] [48] [71] [72] [103], while some studies use features extracted from the input text [64] [125]. For instance, [125] employs four ToBI (Tones and Break Indices) features as word-level prosody tags that are combined with the phoneme embedding as input to the TTS model. A ToBI predictor is jointly trained to predict four ToBI features based on grammatical and semantic information extracted from the input text using a self-supervised language representation model ELECTRA [164].</p> <p>\u5176\u4ed6\u6a21\u578b\u8bad\u7ec3\u4e00\u4e2a\u97f5\u5f8b\u7f16\u7801\u5668\u6216\u9884\u6d4b\u5668, \u548c\u81ea\u56de\u5f52 TTS \u6a21\u578b (\u5982 Tacotron 2) \u8054\u5408\u8bad\u7ec3, \u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u4ee5\u7f16\u7801\u4e00\u4e9b\u97f5\u5f8b\u76f8\u5173\u7684\u7279\u5f81. \u8bad\u7ec3\u7684\u7f16\u7801\u5668\u5728\u63a8\u7406\u65f6\u6839\u636e TTS \u6a21\u578b\u7684\u8f93\u5165\u6587\u672c\u7f16\u7801\u97f5\u5f8b\u76f8\u5173\u7684\u7279\u5f81. \u8fd9\u4e9b\u7528\u4e8e\u97f5\u5f8b\u7f16\u7801\u5668\u7684\u57fa\u4e8e\u6587\u672c\u7684\u8f93\u5165\u5728\u5927\u591a\u6570\u7814\u7a76\u4e2d\u91c7\u7528\u6587\u672c\u7684\u5b57\u7b26/\u97f3\u7d20\u5d4c\u5165, \u5176\u4ed6\u4e00\u4e9b\u7814\u7a76\u4f7f\u7528\u4ece\u8f93\u5165\u6587\u672c\u63d0\u53d6\u7684\u7279\u5f81. - \u6587\u732e [125] \u91c7\u7528\u56db\u4e2a ToBI \u7279\u5f81\u4f5c\u4e3a\u8bcd\u7ea7\u97f5\u5f8b\u53d8\u8fc1, \u548c\u97f3\u7d20\u5d4c\u5165\u7ed3\u5408\u4f5c\u4e3a TTS \u6a21\u578b\u7684\u8f93\u5165. ToBI \u9884\u6d4b\u5668\u8054\u5408\u8bad\u7ec3\u57fa\u4e8e\u8bed\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\u6765\u9884\u6d4b\u8fd9\u56db\u4e2a ToBI \u7279\u5f81, \u8fd9\u4e9b\u4fe1\u606f\u662f\u4f7f\u7528\u81ea\u76d1\u7763\u8bed\u8a00\u8868\u793a\u6a21\u578b ELECTRA \u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u51fa\u6765\u7684.</p> <p>In addition to the previously mentioned features, several other prosodic features are also proposed as the output of the prosody predictors in other studies. For example, the prosody predictor in [103] predicts a set of utterance-wise acoustic features, including log-pitch, log-pitch range, log-phone duration, log-energy, and spectral tilt. In [48], the proposed pitch predictor outputs a continuous pitch representation, which is converted into discrete values using Vector Quantization (VQ) [149]. Furthermore, studies [20] [71] propose predicting the three prosody-related features, i.e., F0, energy, and duration, either by a single acoustic features predictor (AFP)[71] or via three separated predictors [20]. </p> <p>\u9664\u4e86\u4e4b\u524d\u63d0\u5230\u7684\u7279\u5f81, \u5176\u4ed6\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u5176\u4ed6\u97f5\u5f8b\u7279\u5f81\u4f5c\u4e3a\u97f5\u5f8b\u9884\u6d4b\u5668\u7684\u8f93\u51fa.  - \u6587\u732e [103] \u7684\u97f5\u5f8b\u9884\u6d4b\u5668\u9884\u6d4b\u4e00\u7ec4\u8bed\u8c03\u58f0\u5b66\u7279\u5f81, \u5305\u62ec\u5bf9\u6570\u97f3\u9ad8, \u5bf9\u6570\u97f3\u9ad8\u8303\u56f4, \u5bf9\u6570\u97f3\u7d20\u65f6\u957f, \u5bf9\u6570\u80fd\u91cf\u548c\u9891\u8c31\u503e\u659c. - \u6587\u732e [048] \u7684\u97f3\u9ad8\u9884\u6d4b\u5668\u8f93\u51fa\u4e00\u4e2a\u8fde\u7eed\u7684\u97f3\u9ad8\u8868\u793a, \u4f7f\u7528\u77e2\u91cf\u91cf\u5316\u6280\u672f\u8f6c\u5316\u4e3a\u79bb\u6563\u503c. - \u6587\u732e [020] [071] \u901a\u8fc7\u5355\u4e2a\u58f0\u5b66\u7279\u5f81\u9884\u6d4b\u5668 (AFP) \u6216\u4e09\u4e2a\u5355\u72ec\u7684\u9884\u6d4b\u5668\u9884\u6d4b\u4e09\u4e2a\u97f5\u5f8b\u76f8\u5173\u7684\u7279\u5f81, \u5373 F0, \u80fd\u91cf\u548c\u65f6\u957f.</p> <p>Another type of emotion embedding is sentiment feature embedding, which is utilized to produce expressive speech by extracting sentiment information from the input text. This is demonstrated in work [135], where the Stanford Sentiment Parser is used to generate vector embeddings or sentiment probabilities based on the tree structure of the sentence. To synthesize expressive speech, different combinations of probabilities and vector embeddings (for individual words or word-context) are added to the linguistic features as inputs to the TTS model.</p> <p>\u53e6\u4e00\u79cd\u60c5\u611f\u5d4c\u5165\u662f\u60c5\u611f\u7279\u5f81\u5d4c\u5165, \u5b83\u7528\u4e8e\u901a\u8fc7\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u60c5\u611f\u4fe1\u606f\u6765\u4ea7\u751f\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8bed\u97f3. \u8fd9\u5728\u6587\u732e [135] \u4e2d\u5f97\u5230\u4e86\u8bc1\u660e, \u5176\u4e2d\u4f7f\u7528\u4e86\u65af\u5766\u798f\u60c5\u611f\u89e3\u6790\u5668\u6765\u6839\u636e\u53e5\u5b50\u7684\u6811\u7ed3\u6784\u751f\u6210\u5411\u91cf\u5d4c\u5165\u6216\u60c5\u611f\u6982\u7387. \u4e3a\u4e86\u5408\u6210\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8bed\u97f3, \u5c06\u4e0d\u540c\u7ec4\u5408\u7684\u6982\u7387\u548c\u5411\u91cf\u5d4c\u5165\uff08\u5bf9\u4e8e\u5355\u4e2a\u5355\u8bcd\u6216\u5355\u8bcd\u4e0a\u4e0b\u6587\uff09\u6dfb\u52a0\u5230\u4f5c\u4e3a TTS \u6a21\u578b\u8f93\u5165\u7684\u8bed\u8a00\u7279\u5f81\u4e2d.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#53-prosody-controllability","title":"5.3 Prosody Controllability \u97f5\u5f8b\u53ef\u63a7\u6027","text":"<p>Text-to-speech is a one-to-many mapping problem, i.e.,for one piece of text there could be many valid prosody patterns because of speaker-specific variations. Accord-ingly, providing a kind of controllability over prosody-related features in synthesized speech is essential for generating expressive speech with different variations. However, it\u2019s not always easy to mark-up prosody or even to define boundaries between prosody events, i.e., dura-tion boundaries can vary depending on segmentation,pitch contour prediction is error-prone, and prosody fea-tures may not always correlate well with what listeners perceive. Several studies in literature have addressed the control-lability issue in terms of selecting an emotion/style class or intensity level and adjusting prosody-related features at different speech levels. In this section, we discuss stud-ies considering prosody controllability.</p> <p>\u6587\u672c\u5230\u8bed\u97f3\u8f6c\u6362\u662f\u4e00\u4e2a\u4e00\u4e00\u5bf9\u5e94\u7684\u95ee\u9898\uff0c\u5373\u5bf9\u4e8e\u4e00\u6bb5\u6587\u672c\uff0c\u7531\u4e8e\u8bf4\u8bdd\u4eba\u7279\u5b9a\u7684\u53d8\u5316\uff0c\u53ef\u80fd\u5b58\u5728\u8bb8\u591a\u6709\u6548\u7684\u97f5\u5f8b\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u5728\u5408\u6210\u7684\u8bed\u97f3\u4e2d\u63d0\u4f9b\u5bf9\u97f5\u5f8b\u76f8\u5173\u7279\u5f81\u7684\u63a7\u5236\u5bf9\u4e8e\u751f\u6210\u5177\u6709\u4e0d\u540c\u53d8\u5316\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u81f3\u5173\u91cd\u8981\u3002 \u7136\u800c\uff0c\u6807\u8bb0\u97f5\u5f8b\u6216\u5b9a\u4e49\u97f5\u5f8b\u4e8b\u4ef6\u4e4b\u95f4\u7684\u754c\u9650\u5e76\u4e0d\u603b\u662f\u5bb9\u6613\u7684\uff0c\u5373\u6301\u7eed\u65f6\u95f4\u754c\u9650\u53ef\u80fd\u4f1a\u6839\u636e\u5206\u6bb5\u800c\u53d8\u5316\uff0c\u97f3\u9ad8\u8f6e\u5ed3\u9884\u6d4b\u5bb9\u6613\u51fa\u9519\uff0c\u97f5\u5f8b\u7279\u5f81\u5e76\u4e0d\u603b\u662f\u4e0e\u542c\u4f17\u611f\u77e5\u76f8\u5173\u3002 \u5728\u6587\u732e\u4e2d\uff0c\u6709\u51e0\u9879\u7814\u7a76\u4ece\u9009\u62e9\u60c5\u611f/\u98ce\u683c\u7c7b\u522b\u6216\u5f3a\u5ea6\u7ea7\u522b\u4ee5\u53ca\u5728\u4e0d\u540c\u7684\u8bed\u97f3\u7ea7\u522b\u8c03\u6574\u97f5\u5f8b\u76f8\u5173\u7279\u5f81\u7684\u89d2\u5ea6\u89e3\u51b3\u4e86\u53ef\u63a7\u6027\u95ee\u9898\u3002\u5728\u672c\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u8ba8\u8bba\u8003\u8651\u97f5\u5f8b\u53ef\u63a7\u6027\u7684\u7814\u7a76\u3002</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#531-modelingspecific-prosody-styles","title":"5.3.1 Modeling\u2011specific prosody styles","text":"<p>This group of studies provides individual representa-tions of expressive styles/emotions, enabling the control of prosody in synthesized speech by offering the ability to select from available representations or adjust their values. In some studies [55] [70] [116], style is modeled at a single speech/text level, while in other studies [68] [79] [133] a multi-level or hierarchical model of expressive styles is used to allow for a better capture of prosody var-iation in expressive speech. In single-level prosody modeling approaches, [55] is one of the early studies that extends a baseline with fine-grained control over the speaking style/prosody of syn-thesized speech. The proposed modification involves adding an embedding network with temporal structure to either the speech-side or text-side of the TTS model. Accordingly, the resulting prosody embedding is of vari-able length, and it is used to condition input to either encoder or decoder based on the position of the embed-ding network. Speech-side prosody embedding provides adjustment of prosody at frame-level, while text-side prosody embedding enables phoneme-level prosody control. Single-level prosody embeddings can be converted into discrete embeddings as in [70] [116]. Discrete pros-ody representations are easier to control and analyze and provide a better interpretation of prosodic styles.</p> <p>\u8fd9\u4e9b\u7814\u7a76\u63d0\u4f9b\u4e86\u8868\u8fbe\u98ce\u683c/\u60c5\u611f\u7684\u4e2a\u4f53\u8868\u793a\uff0c\u4f7f\u5f97\u5728\u5408\u6210\u8bed\u97f3\u4e2d\u63a7\u5236\u8bed\u8c03\u6210\u4e3a\u53ef\u80fd\uff0c\u901a\u8fc7\u63d0\u4f9b\u4ece\u53ef\u7528\u8868\u793a\u4e2d\u9009\u62e9\u6216\u8c03\u6574\u5176\u503c\u7684\u80fd\u529b\u3002\u5728\u67d0\u4e9b\u7814\u7a76\u4e2d\uff0c\u98ce\u683c\u5728\u5355\u4e2a\u8bed\u97f3/\u6587\u672c\u7ea7\u522b\u5efa\u6a21\uff0c\u800c\u5728\u5176\u4ed6\u7814\u7a76\u4e2d\uff0c\u4f7f\u7528\u591a\u7ea7\u6216\u5206\u5c42\u6a21\u578b\u6765\u66f4\u597d\u5730\u6355\u6349\u8868\u8fbe\u6027\u8bed\u97f3\u4e2d\u7684\u8bed\u8c03\u53d8\u5316\u3002</p> <p>\u5728\u5355\u7ea7\u8bed\u8c03\u5efa\u6a21\u65b9\u6cd5\u4e2d\uff0c[55]\u662f\u65e9\u671f\u7814\u7a76\u4e4b\u4e00\uff0c\u5b83\u6269\u5c55\u4e86\u57fa\u7ebf\uff0c\u5bf9\u5408\u6210\u8bed\u97f3\u7684\u8bf4\u8bdd\u98ce\u683c/\u8bed\u8c03\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u63d0\u51fa\u7684\u4fee\u6539\u6d89\u53ca\u5728TTS\u6a21\u578b\u7684\u8bed\u97f3\u4fa7\u6216\u6587\u672c\u4fa7\u6dfb\u52a0\u5177\u6709\u65f6\u95f4\u7ed3\u6784\u7684\u5d4c\u5165\u7f51\u7edc\u3002</p> <p>\u56e0\u6b64\uff0c\u5f97\u5230\u7684\u8bed\u8c03\u5d4c\u5165\u662f\u53ef\u53d8\u957f\u5ea6\u7684\uff0c\u5e76\u4e14\u6839\u636e\u5d4c\u5165\u7f51\u7edc\u7684\u4f4d\u7f6e\u7528\u4e8e\u6761\u4ef6\u8f93\u5165\u5230\u7f16\u7801\u5668\u6216\u89e3\u7801\u5668\u3002\u8bed\u97f3\u4fa7\u8bed\u8c03\u5d4c\u5165\u63d0\u4f9b\u5e27\u7ea7\u8bed\u8c03\u8c03\u6574\uff0c\u800c\u6587\u672c\u4fa7\u8bed\u8c03\u5d4c\u5165\u5141\u8bb8\u97f3\u7d20\u7ea7\u8bed\u8c03\u63a7\u5236\u3002</p> <p>\u5355\u7ea7\u8bed\u8c03\u5d4c\u5165\u53ef\u4ee5\u8f6c\u6362\u4e3a\u79bb\u6563\u5d4c\u5165\uff0c\u5982[70] [116]\u3002\u79bb\u6563\u8bed\u8c03\u8868\u793a\u66f4\u5bb9\u6613\u63a7\u5236\u548c\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u8bed\u8c03\u98ce\u683c\u7684\u66f4\u597d\u89e3\u91ca\u3002</p> <p>In [116], a word-level prosody embedding is proposed based on decision trees and a GMM. A word-level refer-ence encoder is first used to obtain word-level prosody embedding from reference audio. A binary decision tree is employed to cluster embeddings with their identities based on their phonetic information. Prosody embed-dings of words in each leaf node will differ only in their prosodies. Then prosody embeddings of each leaf can be clustered via a GMM model where clusters represent prosody tags. If the applied GMM consists of five com-ponents and a tree of ten leaf nodes, a set of 50 prosody tags is produced. At inference time, prosody tags can be selected manually or via a prosody predictor that is trained to select appropriate prosody tags based on input text. In [70], an audiobook speech synthesis model is pro-posed. The model uses a character-acting-style extrac-tion module based on ResCNN [165] to extract different character acting styles from the input speech. Discrete character-level styles are obtained via vector quantization(VQ) [149], which maps them to a codebook, limiting the number of styles. At inference, the discrete character-act-ing-styles are predicted via a style predictor. The charac-ter-level style predictor uses both character embeddings from Skip-Gram [166] and text-based features from RoB-ERTa [167] as input. \u5728[116]\u4e2d\uff0c\u57fa\u4e8e\u51b3\u7b56\u6811\u548cGMM\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bcd\u7ea7\u8bed\u8c03\u5d4c\u5165\u3002\u9996\u5148\u4f7f\u7528\u8bcd\u7ea7\u53c2\u8003\u7f16\u7801\u5668\u4ece\u53c2\u8003\u97f3\u9891\u4e2d\u83b7\u53d6\u8bcd\u7ea7\u8bed\u8c03\u5d4c\u5165\u3002\u4f7f\u7528\u4e8c\u53c9\u51b3\u7b56\u6811\u6839\u636e\u5176\u97f3\u7d20\u4fe1\u606f\u5bf9\u5d4c\u5165\u8fdb\u884c\u805a\u7c7b\u3002\u6bcf\u4e2a\u53f6\u8282\u70b9\u4e2d\u5355\u8bcd\u7684\u8bed\u8c03\u5d4c\u5165\u4ec5\u5728\u8bed\u8c03\u4e0a\u6709\u6240\u4e0d\u540c\u3002\u7136\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7GMM\u6a21\u578b\u5bf9\u6bcf\u4e2a\u53f6\u7684\u8bed\u8c03\u5d4c\u5165\u8fdb\u884c\u805a\u7c7b\uff0c\u5176\u4e2d\u805a\u7c7b\u8868\u793a\u8bed\u8c03\u6807\u7b7e\u3002\u5982\u679c\u5e94\u7528\u7684GMM\u5305\u542b\u4e94\u4e2a\u7ec4\u4ef6\u548c\u4e00\u4e2a\u5341\u4e2a\u53f6\u8282\u70b9\u7684\u6811\uff0c\u5219\u4f1a\u4ea7\u751f50\u4e2a\u8bed\u8c03\u6807\u7b7e\u3002\u5728\u63a8\u7406\u65f6\uff0c\u8bed\u8c03\u6807\u7b7e\u53ef\u4ee5\u624b\u52a8\u9009\u62e9\u6216\u901a\u8fc7\u8bad\u7ec3\u4ee5\u6839\u636e\u8f93\u5165\u6587\u672c\u9009\u62e9\u9002\u5f53\u8bed\u8c03\u6807\u7b7e\u7684\u8bed\u8c03\u9884\u6d4b\u5668\u9009\u62e9\u3002</p> <p>\u5728[70]\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u58f0\u8bfb\u7269\u8bed\u97f3\u5408\u6210\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u57fa\u4e8eResCNN [165]\u7684\u89d2\u8272\u626e\u6f14\u98ce\u683c\u63d0\u53d6\u6a21\u5757\u4ece\u8f93\u5165\u8bed\u97f3\u4e2d\u63d0\u53d6\u4e0d\u540c\u7684\u89d2\u8272\u626e\u6f14\u98ce\u683c\u3002\u901a\u8fc7\u77e2\u91cf\u91cf\u5316(VQ) [149]\u83b7\u5f97\u79bb\u6563\u89d2\u8272\u7ea7\u98ce\u683c\uff0c\u5c06\u5176\u6620\u5c04\u5230\u7801\u672c\uff0c\u9650\u5236\u98ce\u683c\u6570\u91cf\u3002\u5728\u63a8\u7406\u65f6\uff0c\u901a\u8fc7\u98ce\u683c\u9884\u6d4b\u5668\u9884\u6d4b\u79bb\u6563\u89d2\u8272\u626e\u6f14\u98ce\u683c\u3002\u89d2\u8272\u7ea7\u98ce\u683c\u9884\u6d4b\u5668\u4f7f\u7528\u6765\u81eaSkip-Gram [166]\u7684\u89d2\u8272\u5d4c\u5165\u548c\u6765\u81eaRoBERTa [167]\u7684\u6587\u672c\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\u3002</p> <p>Regarding multi-level prosody modeling, some stud-ies propose enhancing prosody control in the baseline models [74] [75] [77] by modifying their single-level pros-ody modeling to multiple levels. For instance, [133] pro-poses a hierarchical structure of [75] with multiple GST layers. Three GST layers are employed in the proposed model, each consisting of 10 tokens, which were found to yield better token interpretation. Tokens of the first and second layers were found to learn different speak-ers and styles, but these representations were not easily interpreted. Interestingly, the tokens in the third layer were able to generate higher quality samples with more distinct and interpretable styles. Specifically, third-layer styles exhibit clear differences in their features, includ-ing pitch, stress, speaking rate, start offset, rhythm, pause position, and duration. Model in [77] is further extended in [68] with three VAEs to generate three different levels (utterance, phrase,and word) of latent variables with varying time resolu-tions. Acoustic features and linguistic features are passed as input to the three VAEs. Initially, a conditional prior(CP) is applied to learn a distribution for sampling utter-ance-level latent variables based on linguistic features from the input text. The generated latent variables are passed to other levels via auto-regressive (AR) latent converters that convert latent variables from coarser-level to finer-level with input text condition. In fact, the utterance-level latent variables can be used to control the generated speech styles, regardless of latent variables of other levels, as they are predicted based on the utterance-level latent variables.</p> <p>The Controllable Expressive Speech Synthesis (ConEx)model in [79] proposes modeling prosody at two levels,utterance-level (global) and phone-level (local), using reference encoders [74]. However, the global prosody embedding is used to condition the local prosody embed-ding, resulting in an integrated prosody embedding. The local embeddings are 3D vectors that are converted into discrete local prosody embeddings (codes) via vec-tor quantization (VQ) [149]. At inference time, the integrated prosody embedding is predicted by an auto-regressive (AR) prior model trained to predict categori-cal distributions for each of the discrete codes utilizing global prosody embedding and the phoneme embed-ding as inputs. While global prosody embedding can be obtained from training samples or from an audio refer-ence, local prosody embeddings for a given global pros-ody embedding are achieved via the AR prior model. Fine-grained prosody control can be achieved by select-ing a specific phoneme to start adjusting prosody from. The AR prior model will first generate the top k pros-ody options for this phoneme. Then, the local prosody sequence will be generated autoregressively for each of the first top k options by the AR prior model.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#532-modelingspecific-prosody-features","title":"5.3.2 Modeling\u2011specific prosody features","text":"<p>This group of studies provides individual representations of prosody-related features. Control over prosody of the synthesized speech is provided via selecting or adjust-ing a specific representation of a specific prosody-related feature. Some studies in this direction model prosody features at the global or utterance-level [97] [128], while other studies propose modeling at fine-grained lev-els [48] [63] [71] [122] [138], such as phoneme, syllable, or word-level. The STYLER model [97], for example, employs multi-ple style encoders to factor speech style into several com-ponents, including duration, pitch, speaker, energy, and noise. This structure enables STYLER to generate con-trollable expressive speech by adjusting each of the indi-vidually modeled features. Furthermore, with the explicit noise encoding, other encoders can be constrained to exclude noise information as a style factor, and thus the model can generate clean speech even with noisy refer-ences. Adjusting the style factors, various styles of speech can be generated from STYLER.</p> <p>\u8fd9\u4e9b\u7814\u7a76\u63d0\u4f9b\u4e86\u4e0e\u97f5\u5f8b\u76f8\u5173\u7684\u7279\u5f81\u7684\u4e2a\u4f53\u8868\u793a\u3002\u901a\u8fc7\u9009\u62e9\u6216\u8c03\u6574\u7279\u5b9a\u97f5\u5f8b\u76f8\u5173\u7279\u5f81\u7684\u7279\u5b9a\u8868\u793a\u6765\u63a7\u5236\u5408\u6210\u7684\u8bed\u97f3\u7684\u97f5\u5f8b\u3002\u5728\u8fd9\u4e2a\u65b9\u5411\u4e0a\uff0c\u4e00\u4e9b\u7814\u7a76\u5728\u5168\u5c40\u6216\u53e5\u5b50\u7ea7\u522b\u5efa\u6a21\u97f5\u5f8b\u7279\u5f81[97] [128]\uff0c\u800c\u5176\u4ed6\u7814\u7a76\u5219\u63d0\u51fa\u5728\u7ec6\u7c92\u5ea6\u7ea7\u522b\u5efa\u6a21[48] [63] [71] [122] [138]\uff0c\u4f8b\u5982\u97f3\u7d20\u3001\u97f3\u8282\u6216\u5355\u8bcd\u7ea7\u522b\u3002</p> <p>\u4f8b\u5982\uff0cSTYLER\u6a21\u578b[97]\u4f7f\u7528\u591a\u4e2a\u98ce\u683c\u7f16\u7801\u5668\u5c06\u8bed\u97f3\u98ce\u683c\u5206\u89e3\u4e3a\u51e0\u4e2a\u7ec4\u6210\u90e8\u5206\uff0c\u5305\u62ec\u6301\u7eed\u65f6\u95f4\u3001\u97f3\u9ad8\u3001\u8bf4\u8bdd\u8005\u3001\u80fd\u91cf\u548c\u566a\u58f0\u3002\u8fd9\u79cd\u7ed3\u6784\u4f7fSTYLER\u80fd\u591f\u901a\u8fc7\u8c03\u6574\u6bcf\u4e2a\u5355\u72ec\u5efa\u6a21\u7684\u7279\u5f81\u6765\u751f\u6210\u53ef\u63a7\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u663e\u5f0f\u566a\u58f0\u7f16\u7801\uff0c\u5176\u4ed6\u7f16\u7801\u5668\u53ef\u4ee5\u88ab\u7ea6\u675f\u4ee5\u6392\u9664\u566a\u58f0\u4fe1\u606f\u4f5c\u4e3a\u98ce\u683c\u56e0\u7d20\uff0c\u56e0\u6b64\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u5e72\u51c0\u7684\u8bed\u97f3\uff0c\u5373\u4f7f\u4f7f\u7528\u5608\u6742\u7684\u53c2\u8003\u3002\u901a\u8fc7\u8c03\u6574\u98ce\u683c\u56e0\u7d20\uff0c\u53ef\u4ee5\u4eceSTYLER\u751f\u6210\u5404\u79cd\u98ce\u683c\u7684\u8bed\u97f3\u3002</p> <p>Adjusting several features at fine-grained levels can be a difficult task.  For example, FastSpeech2 provides fine-grained control over pitch range, duration,energy, which are modeled at the phone-level (phone-wise), and it is not easy to adjust these features to achieve a specific prosodic output.  Raitio and Seshadri [128] improves FastSpeech2 with an utterance-wise (coarse-grained) prosody model using an additional variance adaptor. That second variance adaptor is the same as the original one, but it models five features at the utterance-level: pitch, pitch range, duration, energy,and spectral tilt. These features are then concatenated with the corresponding output of the first variance adaptor. Such utterance-wise prosody model enables easier control of prosody while still allowing modification at the phone-level. To control high-level prosody,a bias is added to the corresponding utterance-wise prosody predictions. A phone-level prosody control is achieved by directly modifying the phone-wise features. Fine-grained control over a specific prosody-feature can also be required specially for strong speaking styles. To that end, in [71], a predictor is proposed to predict F0, energy, and duration features at the phoneme-level. During inference, the predicted features are generated based on the input text alone; however, they can also be provided externally and modified as desired. Furthermore, two prosody modeling levels are pro-posed in [63]: the local level (word-level) and global level (utterance-wise). The global prosody embedding is the emotion embedding obtained by a reference-based encoder. The local prosody embedding is obtained from a predictor of the F0 features at the word-level with global prosody embedding and the phoneme embed-ding as inputs. Both embeddings are then passed to a multi-style encoder to form the final multi-style pros-ody embedding. Therefore, modifying the predicted F0values can provide control of prosody at the utterance,word, and phoneme levels.</p> <p>\u5728\u7ec6\u7c92\u5ea6\u7ea7\u522b\u8c03\u6574\u591a\u4e2a\u7279\u5f81\u53ef\u80fd\u662f\u4e00\u9879\u56f0\u96be\u7684\u4efb\u52a1\u3002\u4f8b\u5982\uff0cFastSpeech2 \u5728\u97f3\u7d20\u7ea7\u522b\uff08\u97f3\u7d20\u7ea7\uff09\u63d0\u4f9b\u5bf9\u97f3\u9ad8\u8303\u56f4\u3001\u6301\u7eed\u65f6\u95f4\u548c\u80fd\u91cf\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u4e14\u4e0d\u5bb9\u6613\u8c03\u6574\u8fd9\u4e9b\u7279\u5f81\u4ee5\u5b9e\u73b0\u7279\u5b9a\u7684\u97f5\u5f8b\u8f93\u51fa\u3002Raitio\u548cSeshadri[128]\u901a\u8fc7\u4f7f\u7528\u989d\u5916\u7684\u53d8\u5f02\u9002\u914d\u5668\u6539\u8fdbFastSpeech2\uff0c\u8be5\u53d8\u5f02\u9002\u914d\u5668\u5728\u53e5\u5b50\u7ea7\u522b\uff08\u7c97\u7c92\u5ea6\uff09\u4f7f\u7528\u97f5\u5f8b\u6a21\u578b\u3002\u7b2c\u4e8c\u4e2a\u53d8\u5f02\u9002\u914d\u5668\u4e0e\u539f\u59cb\u53d8\u5f02\u9002\u914d\u5668\u76f8\u540c\uff0c\u4f46\u5728\u53e5\u5b50\u7ea7\u522b\u5efa\u6a21\u4e94\u4e2a\u7279\u5f81\uff1a\u97f3\u9ad8\u3001\u97f3\u9ad8\u8303\u56f4\u3001\u6301\u7eed\u65f6\u95f4\u3001\u80fd\u91cf\u548c\u9891\u8c31\u503e\u659c\u3002\u7136\u540e\uff0c\u5c06\u8fd9\u4e9b\u7279\u5f81\u4e0e\u7b2c\u4e00\u4e2a\u53d8\u5f02\u9002\u914d\u5668\u7684\u76f8\u5e94\u8f93\u51fa\u8fde\u63a5\u8d77\u6765\u3002\u8fd9\u79cd\u53e5\u5b50\u7ea7\u522b\u7684\u97f5\u5f8b\u6a21\u578b\u5141\u8bb8\u66f4\u5bb9\u6613\u5730\u63a7\u5236\u97f5\u5f8b\uff0c\u540c\u65f6\u4ecd\u7136\u5141\u8bb8\u5728\u97f3\u7d20\u7ea7\u522b\u8fdb\u884c\u4fee\u6539\u3002\u4e3a\u4e86\u63a7\u5236\u9ad8\u7ea7\u522b\u97f5\u5f8b\uff0c\u5411\u76f8\u5e94\u7684\u53e5\u5b50\u7ea7\u522b\u97f5\u5f8b\u9884\u6d4b\u6dfb\u52a0\u504f\u5dee\u3002\u901a\u8fc7\u76f4\u63a5\u4fee\u6539\u97f3\u7d20\u7ea7\u7279\u5f81\u6765\u5b9e\u73b0\u97f3\u7d20\u7ea7\u97f5\u5f8b\u63a7\u5236\u3002</p> <p>\u5bf9\u4e8e\u5f3a\u70c8\u7684\u8bf4\u8bdd\u98ce\u683c\uff0c\u53ef\u80fd\u9700\u8981\u5bf9\u7279\u5b9a\u97f5\u5f8b\u7279\u5f81\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u4e3a\u6b64\uff0c[71]\u4e2d\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u5668\u6765\u9884\u6d4b\u97f3\u7d20\u7ea7\u522b\u7684F0\u3001\u80fd\u91cf\u548c\u6301\u7eed\u65f6\u95f4\u7279\u5f81\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6839\u636e\u8f93\u5165\u6587\u672c\u751f\u6210\u9884\u6d4b\u7279\u5f81\uff1b\u4f46\u662f\uff0c\u5b83\u4eec\u4e5f\u53ef\u4ee5\u4ece\u5916\u90e8\u63d0\u4f9b\u5e76\u6839\u636e\u9700\u8981\u8fdb\u884c\u4fee\u6539\u3002\u6b64\u5916\uff0c[63]\u4e2d\u63d0\u51fa\u4e86\u4e24\u4e2a\u97f5\u5f8b\u5efa\u6a21\u7ea7\u522b\uff1a\u5c40\u90e8\u7ea7\u522b\uff08\u5355\u8bcd\u7ea7\u522b\uff09\u548c\u5168\u5c40\u7ea7\u522b\uff08\u53e5\u5b50\u7ea7\u522b\uff09\u3002\u5168\u5c40\u97f5\u5f8b\u5d4c\u5165\u662f\u4ece\u57fa\u4e8e\u53c2\u8003\u7684\u7f16\u7801\u5668\u83b7\u5f97\u7684\u60c5\u7eea\u5d4c\u5165\u3002\u5c40\u90e8\u97f5\u5f8b\u5d4c\u5165\u662f\u4ece\u5177\u6709\u5168\u5c40\u97f5\u5f8b\u5d4c\u5165\u548c\u97f3\u7d20\u5d4c\u5165\u4f5c\u4e3a\u8f93\u5165\u7684\u9884\u6d4b\u5668\u83b7\u5f97\u7684\u97f3\u7d20\u7ea7\u522b\u7684F0\u7279\u5f81\u3002\u7136\u540e\uff0c\u5c06\u8fd9\u4e24\u4e2a\u5d4c\u5165\u4f20\u9012\u5230\u4e00\u4e2a\u591a\u98ce\u683c\u7f16\u7801\u5668\uff0c\u4ee5\u5f62\u6210\u6700\u7ec8\u7684\u591a\u98ce\u683c\u97f5\u5f8b\u5d4c\u5165\u3002\u56e0\u6b64\uff0c\u4fee\u6539\u9884\u6d4b\u7684F0\u503c\u53ef\u4ee5\u63d0\u4f9b\u5bf9\u53e5\u5b50\u3001\u5355\u8bcd\u548c\u97f3\u7d20\u7ea7\u522b\u7684\u97f5\u5f8b\u7684\u63a7\u5236\u3002</p> <p>More flexibility in controlling the F0 feature is pro-vided in the controllable deep auto-regressive model(C-DAR) model [138] which allows for F0 contour adjustment by the user. To achieve this goal, three strat-egies are used: 1) context awareness by conditioning the model on the preceding and following speech dur-ing training, 2) conditioning the model on some ran-dom segments of ground truth F0, and 3) predicting F0 values in reverse order. Additionally, several text-based features are used as input to the model, includ-ing word embeddings derived from BERT, V/UV label,one-hot vector for the nearby punctuation, and pho-neme encodings. At inference, F0 values specified by the user are used as alternatives for the ground truth F0 segments, and the model predicts the rest of the utter-ance\u2019s F0 contour through context awareness. Discrete fine-grained representations for prosody features as in [48] [122] are also useful to limit the number of the obtained representations. Both studies [48] [122]utilize VQ [149] to map each prosody embedding to the closest discrete representation from a predefined code-book. In [48], a pitch predictor is used to predict charac-ter-level continuous pitch representation using character embeddings from the text encoder as input. Zhang et al.[122], however, produces syllable-level prosody embed-dings from a reference encoder that takes F0, intensity,and duration features from reference audio as input. The resulting prosody embeddings are then mapped to a pre-defined codebook to extractb discrete prosody codes.</p> <p>\u5728\u53ef\u63a7\u6df1\u5ea6\u81ea\u56de\u5f52\u6a21\u578b\uff08C-DAR\uff09\u6a21\u578b[138]\u4e2d\u63d0\u4f9b\u4e86\u5bf9F0\u7279\u5f81\u7684\u66f4\u591a\u63a7\u5236\u7075\u6d3b\u6027\uff0c\u8be5\u6a21\u578b\u5141\u8bb8\u7528\u6237\u8c03\u6574F0\u8f6e\u5ed3\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u4f7f\u7528\u4e86\u4e09\u79cd\u7b56\u7565\uff1a1\uff09\u901a\u8fc7\u5728\u8bad\u7ec3\u671f\u95f4\u5c06\u6a21\u578b\u6761\u4ef6\u5316\u5728\u5148\u524d\u548c\u968f\u540e\u7684\u8bed\u97f3\u4e0a\u6765\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c2\uff09\u5c06\u6a21\u578b\u6761\u4ef6\u5316\u5728\u5730\u9762\u771fF0\u7684\u4e00\u4e9b\u968f\u673a\u7247\u6bb5\u4e0a\uff0c\u4ee5\u53ca3\uff09\u4ee5\u76f8\u53cd\u7684\u987a\u5e8f\u9884\u6d4bF0\u503c\u3002\u6b64\u5916\uff0c\u5c06\u51e0\u4e2a\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\u7528\u4f5c\u6a21\u578b\u7684\u8f93\u5165\uff0c\u5305\u62ec\u4eceBERT\u6d3e\u751f\u7684\u8bcd\u5d4c\u5165\u3001V/UV\u6807\u7b7e\u3001\u9644\u8fd1\u6807\u70b9\u7b26\u53f7\u7684\u4e00\u70ed\u5411\u91cf\u548c\u97f3\u7d20\u7f16\u7801\u3002\u5728\u63a8\u7406\u65f6\uff0c\u7528\u6237\u6307\u5b9a\u7684F0\u503c\u88ab\u7528\u4f5c\u5730\u9762\u771fF0\u7247\u6bb5\u7684\u66ff\u4ee3\u54c1\uff0c\u5e76\u4e14\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u6d4b\u6574\u4e2a\u53e5\u5b50\u7684F0\u8f6e\u5ed3\u3002</p> <p>\u4e0e[48] [122]\u4e2d\u4e00\u6837\uff0c\u79bb\u6563\u7ec6\u7c92\u5ea6\u8868\u793a\u5bf9\u4e8e\u97f5\u5f8b\u7279\u5f81\u4e5f\u5f88\u6709\u7528\uff0c\u53ef\u4ee5\u9650\u5236\u83b7\u5f97\u7684\u8868\u793a\u7684\u6570\u91cf\u3002\u4e24\u9879\u7814\u7a76[48] [122]\u90fd\u4f7f\u7528VQ[149]\u5c06\u6bcf\u4e2a\u97f5\u5f8b\u5d4c\u5165\u6620\u5c04\u5230\u9884\u5b9a\u4e49\u7801\u672c\u4e2d\u6700\u8fd1\u7684\u79bb\u6563\u8868\u793a\u3002\u5728[48]\u4e2d\uff0c\u4f7f\u7528\u97f3\u9ad8\u9884\u6d4b\u5668\u6765\u9884\u6d4b\u4f7f\u7528\u6587\u672c\u7f16\u7801\u5668\u4f5c\u4e3a\u8f93\u5165\u7684\u5b57\u7b26\u5d4c\u5165\u7684\u5b57\u7b26\u7ea7\u8fde\u7eed\u97f3\u9ad8\u8868\u793a\u3002\u7136\u800c\uff0c\u5f20\u7b49\u4eba[122]\u4ece\u53c2\u8003\u7f16\u7801\u5668\u4ea7\u751f\u97f3\u8282\u7ea7\u97f5\u5f8b\u5d4c\u5165\uff0c\u8be5\u53c2\u8003\u7f16\u7801\u5668\u5c06\u53c2\u8003\u97f3\u9891\u7684F0\u3001\u5f3a\u5ea6\u548c\u6301\u7eed\u65f6\u95f4\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\u3002\u7136\u540e\uff0c\u5c06\u5f97\u5230\u7684\u97f5\u5f8b\u5d4c\u5165\u6620\u5c04\u5230\u4e00\u4e2a\u9884\u5b9a\u4e49\u7684\u7801\u672c\uff0c\u4ee5\u63d0\u53d6\u79bb\u6563\u7684\u97f5\u5f8b\u4ee3\u7801\u3002</p> <p>Resulting prosody codes in [48] represent the pitch and other suprasegmental information that can be adjusted via a specific bias value to generate speech with differ-ent pitch accents. The codes in [122], can be interpreted as representing some prosody features such as pitch and duration. The prosody variation at the syllable-level can be manually controlled by assigning each syllable the desired prosody code from the codebook. In [125], ToBI features, which involve a set of con-ventions used for transcribing and annotating speech prosody, are used. The applied ToBI features are four word-level tags: pitch accents, boundary tones, phrase accents, and break indices. The extracted ToBI tags are used as input to TTS model. Simultaneously, a ToBI pre-dictor is trained to predict these prosody tags based on grammatical and semantic information extracted from the input text using a self-supervised language model. The resulting model had the ability to control the stress,intonation, and pause of the generated speech to sound natural, utilizing only ToBI tags from the text-based predictor.</p> <p>\u5728[48]\u4e2d\uff0c\u5f97\u5230\u7684\u97f5\u5f8b\u4ee3\u7801\u8868\u793a\u97f3\u9ad8\u548c\u5176\u4ed6\u8d85\u97f3\u6bb5\u4fe1\u606f\uff0c\u53ef\u4ee5\u901a\u8fc7\u7279\u5b9a\u7684\u504f\u7f6e\u503c\u8fdb\u884c\u8c03\u6574\uff0c\u4ee5\u751f\u6210\u5177\u6709\u4e0d\u540c\u97f3\u9ad8\u91cd\u97f3\u7684\u8bed\u97f3\u3002\u5728[122]\u4e2d\uff0c\u4ee3\u7801\u53ef\u4ee5\u89e3\u91ca\u4e3a\u8868\u793a\u4e00\u4e9b\u97f5\u5f8b\u7279\u5f81\uff0c\u5982\u97f3\u9ad8\u548c\u6301\u7eed\u65f6\u95f4\u3002\u53ef\u4ee5\u901a\u8fc7\u5c06\u6bcf\u4e2a\u97f3\u8282\u5206\u914d\u4ece\u7801\u672c\u4e2d\u6240\u9700\u7684\u97f5\u5f8b\u4ee3\u7801\u6765\u624b\u52a8\u63a7\u5236\u97f3\u8282\u7ea7\u522b\u7684\u97f5\u5f8b\u53d8\u5316\u3002</p> <p>\u5728[125]\u4e2d\uff0c\u4f7f\u7528\u4e86ToBI\u7279\u5f81\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u8f6c\u5f55\u548c\u6ce8\u91ca\u8bed\u97f3\u97f5\u5f8b\u7684\u7ea6\u5b9a\u96c6\u3002\u5e94\u7528\u7684ToBI\u7279\u5f81\u662f\u56db\u4e2a\u5355\u8bcd\u7ea7\u522b\u7684\u6807\u7b7e\uff1a\u97f3\u9ad8\u91cd\u97f3\u3001\u8fb9\u754c\u97f3\u3001\u77ed\u8bed\u91cd\u97f3\u548c\u505c\u987f\u7d22\u5f15\u3002\u63d0\u53d6\u7684ToBI\u6807\u7b7e\u7528\u4f5cTTS\u6a21\u578b\u7684\u8f93\u5165\u3002\u540c\u65f6\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2aToBI\u9884\u6d4b\u5668\u6765\u9884\u6d4b\u8fd9\u4e9b\u97f5\u5f8b\u6807\u7b7e\uff0c\u57fa\u4e8e\u4f7f\u7528\u81ea\u76d1\u7763\u8bed\u8a00\u6a21\u578b\u4ece\u8f93\u5165\u6587\u672c\u4e2d\u63d0\u53d6\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\u3002</p> <p>\u6700\u7ec8\u6a21\u578b\u80fd\u591f\u4ec5\u4f7f\u7528\u6587\u672c\u9884\u6d4b\u5668\u4e2d\u7684ToBI\u6807\u7b7e\u6765\u63a7\u5236\u751f\u6210\u7684\u8bed\u97f3\u7684\u5f3a\u8c03\u3001\u8bed\u8c03\u548c\u505c\u987f\uff0c\u4f7f\u5176\u542c\u8d77\u6765\u81ea\u7136\u3002</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#533-modeling-prosody-strength","title":"5.3.3 Modeling prosody strength","text":"<p>This group of studies focus on regulating the strength of emotion or prosody. For instance, [61] utilizes the distance between emotion embeddings and the neutral emotion embeddings to identify scalar values for emotion intensity. It proposes a phoneme-level emotion embed-ding and a fine-grained emotion intensity. The emo-tion embedding is first obtained via a reference encoder. The emotion intensity is then generated by an intensity extractor that takes the emotion embedding as input. The intensity extractor produces intensity as a scalar value based on the distance between the emotion embedding and the centroid of a pre-defined cluster for neutral emo-tion embeddings. The resulting emotion intensity values are quantized into pseudo-labels that serve as the index for an intensity embedding table. Another method for learning emotion strength values in an unsupervised manner is by using ranking functions. Studies [27] [31] [33] [64] utilize a ranking function-based method named relative attributes [89] for this purpose. In[33], prosody is modeled at three levels: global-level rep-resentation by emotion embedding, utterance-level rep-resented by prosody embedding from a reference-based encoder, and the local-level represented by emotion strength. The study trains an emotion strength extractor at the syllable-level based on input speech utilizing the ranking function. Simultaneously, a predictor of emo-tion strength is trained based on features extracted from input text via BERT model. Besides changing emotion label and emotion reference audio, the model provides manual control of the emotion strength values in the syn-thesized speech. Alternatively, the reference encoder in [31] functions as a ranking function to learn a phoneme-level emotion strength (descriptor) sequence. The proposed ranking function [89] receives its input from fragments of target reference audio obtained via a forced alignment model to phoneme boundaries. The OpenSMILE [139] tool is then used to extract 384-dimensional emotion-related features from these reference speech fragments as input to the ranking function. Similarly, the proposed ranking function in [27] takes a set of acoustic features extracted from the input speech via OpenSMILE tool but at the utter-ance-level as input. The ranking function leverages the difference between neutral samples and samples associ-ated with each emotion class in the dataset. The training process is formulated as solving a max-margin optimiza-tion problem. The resulting emotion strength scalars can be manually adjusted or predicted based on text or refer-ence speech. In [64], both emotion class and emotion strength value are obtained via a joint emotion predictor based only on the input text. The input to the predictor is features extracted from input text via the Generative Pre-trained Transformer (GPT)-3 [88]. Emotion class and emotion strength are the two outputs of the predictor where the former is represented as a one-hot encoded vector and the latter is presented as a scalar value. Emotion labels and emotion strength values which are also obtained via[89], are used as ground truth for predictor training. Another ranking method is proposed in \"Cross-Speaker Emotion Transfer by Manipulating Speech Style Latents\" using the ranking support vector machine. The model generates style embedding and speaker embedding via two separate encoders. Both style and speaker embeddings at infer-ence time are represented by centroids of each single speaker and style embeddings. However, a linear SVM is trained with the model to provide the ability for style embedding adjustment. The proposed SVM model is trained to classify between neutral emotion and a specific emotion embedding, where the learned hyperplane is utilized to move(scale) the style vectors in a direction towards/opposite to the hyperplane. Another type of control that contributes to generat-ing speech with a better representation of local prosodic variation is introduced in [124]. The proposed model suggests an unsupervised approach to obtain word-level prominence and phrasal boundary strength features. For this purpose, continuous wavelet transform (CWT) [168]is utilized to extract continuous estimates of word promi-nence and boundary information from the audio signal. First, the three prosodic signals f0, energy, and duration are extracted and combined as input to the CWT. Then,the combined signal is decomposed via CWT into scales that represent prosodic hierarchy. Word and phrase-level prosody are then obtained by following ridges or valleys across certain scales. The continuous word prominence and boundary estimates are achieved via the integration of the resulting lines aligned with the textual informa-tion. With manually identified intervals, the continuous values of prominence and boundary strength are then discretized.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#534-prosody-clustering","title":"5.3.4 Prosody clustering","text":"<p>In this section, methods for selecting the appropri-ate prosody embedding for the referenced-based ETTS models are described. To begin with, clustering methods are utilized in [57] [58] to generate representative pros-ody embeddings for each emotion class when the GST-TTS model is trained with a labeled dataset. Initially,the resulting emotion embeddings are clustered in a 2d space. In [57], the centroid of each cluster is used as the weights of the GSTs to generate emotion embedding for each emotion class. In [58], the weight vector that repre-sents each emotion cluster is obtained by considering the inter and intra distances between emotion embedding clusters. Specifically, an algorithm is used for minimizing each embedding distance to the target emotion cluster and maximizing its distance to other emotion clusters. Similarly, clustering algorithms are applied in [112] [113] to achieve discrete prosody embeddings but for two specific prosody-related features. The two studies employ K-means algorithm to cluster F0 and duration features extracted for each phoneme. The centroids of the clus-ters are then used as discrete F0 and duration values/tokens for each phoneme. work [112] applies a balanced clustering method with duration features to overcome degradation in voice quality that appeared in [113] dur-ing duration control. Moreover, to keep phonetic and prosodic information separate during training, an atten-tion unit is introduced to map prosody tokens to decoder hidden states and generate prosody context vectors. The resulting discrete tokens for F0 and duration features provide a fine-grained level of control over prosody by changing the corresponding prosodic tokens for each phoneme. In [105], a cross-domain SER model with the GST-TTS model is proposed to obtain emotion embeddings for an unlabeled dataset. The cross-domain SER model is trained using two datasets including: 1) an SER data-set (source) labeled with emotions, and 2) a TTS data-set (target) that is not labeled. Simultaneously, the SER model trains an emotion classifier that generates soft labels for the unlabeled TTS dataset. These soft labels are then used to train an extended version of the baseline in[74] with an emotion predictor. In the training process,the weights of the style tokens layer are passed as input to the predictor, which employs the learned soft labels as ground truth values. At inference time, weights vectors for each emotion class are averaged to obtain the emo-tion class embedding. However, since the predicted labels for the TTS dataset are soft labels, and thus not entirely reliable, only the top K samples with the highest posterior probabilities are selected.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#54-speech-synthesis-for-unseen-speakers-and-unseen-styles","title":"5.4 Speech synthesis for unseen speakers and unseen styles","text":"<p>Building a speech synthesis model that supports mul-tiple speakers or styles can be achieved by training TTS model with a multi-speaker multi-style dataset. How-ever, generating speech for an unseen speaker or style is a challenging task for which several solutions have been proposed in the literature. A popular approach is to fine-tune the averaged TTS model with some samples from the unseen target speaker or style. The fine-tuning pro-cess may require a single sample from the unseen speaker or style (referred to as one-shot models) or a few samples(referred to as few-shot models). There are also models that do not require any fine-tuning steps, and these are known as zero-shot TTS models. For instance, the fine-tuning process proposed in [112]focused on sentences used in the process to ensure pho-netic coverage, meaning that each phoneme should appear at least once in these sentences. The proposed model requires about 5 minutes of recordings from the unseen target speaker to clone the voice and allow for manipulation of some voice features (such as F0 and duration) by the model at the phoneme-level. Another approach to address the problem of unseen data is to employ specific structures in the TTS model,as proposed in [52] [96] [97] [107]. As an example, in [107],a cycle consistency network is proposed with two Vari-ational Autoencoders (VAEs). The model incorporates two training paths: a paired path and an unpaired path. The unpaired path refers to training scenarios where the reference audio differs from the output (target) speech in terms of text, style, or speaker. Two separate style encod-ers are utilized in the model, with one dedicated to each path. This structure facilitates style transfer among intra-speaker, inter-speaker, and unseen speaker scenarios. In [52], the U-net structure proposed for the TTS model supports one-shot speech synthesis for unseen styles and speakers. The U-net structure is used between the style encoder and the mel decoder of the TTS model,with an opposite flow between them. Both the style encoder and decoder consist of multiple modules with the main building unit as ResCnn1D and instance nor-malization (IN) layers. The decoder receives phoneme embedding and produces the Mel-spectrogram as out-put. In parallel, the style encoder receives the reference audio and produces its linguistic content with guidance from the content (text) encoder. The style encoder mod-ules produce latent variables, i.e., mean, and standard deviation, for the hidden inputs in the IN layers. These latent variables are used to bias and scale the normal-ized hiddens of the corresponding module layers in the decoder. A separate encoder (reference encoder) has been used in [96] to extract speaker-related information besides the prosody encoder (extractor) that encodes prosody fea-tures into the prosody embedding. A prosody predictor is also trained to predict the prosody embedding based on the phoneme-embedding. While the instance nor-malization (IN) layer is utilized by the prosody extractor to remove global (speaker) information and to keep pros-ody-related information, the speaker encoder is designed with a special structure (Conv2D layers, residual blocks(GLU with fully connected layers), and a multi-head self-attention unit) for better extraction of speaker informa-tion. Moreover, instead of concatenation or summation with the decoder input, the speaker embedding is adap-tively affine transformed to the different FFT blocks of the decoder through a Speaker-Adaptive Linear Modu-lation (SALM) network that is inspired by Feature-wise Linear Modulation (FiLM) [141]. The speaker encoder and conditioning of decoder blocks with speaker embed-ding allow the model to generate natural speech for unseen speakers with only a single reference sample(zero-shot).The attention unit used in seq2seq TTS models aims at mapping the different length between text and audio pairs. However, it can get unstable when the input is not seen during training [97]. The STYLER model has addressed this issue by using a linear compression or expansion of the audio to match the text\u2019s length via a method named Mel Calibrator. With this simplification of the alignment process as a scaling method, the unseen data robustness issue is alleviated and all audio-related style factors become dependent only on the audio. Similarly, in [119], the Householder Normalizing Flow[169] is incorporated into the VAE-based baseline model[77]. The Householder normalizing flow applies a series of easily invertible affine transformations to align the VAE\u2019s latent vectors (style embeddings) with a full covari-ance Gaussian distribution. As a result, the correlation among the latent vectors is improved. Generally, this architecture enhances the disentanglement capability of the baseline model and enables it to generate embedding for unseen style with just a single (one-shot) utterance of around one second length. The Multi-SpectroGAN TTS model proposed in [98]is a multi-speaker model trained based on adversarial feedback. The model supports the generation of speech for unseen styles/speakers by introducing adversarial style combination (ASC) during the training process. Style combinations result from mixing/interpolating style embeddings from different source speakers. The model is then trained with adversarial feedback using mixed-style mel-spectrograms. Two mixing methods are employed:binary selection or manifold mix-up via linear combina-tion. This training strategy enables the model to generate more natural speech for unseen speakers.</p> <p>Lastly, recent TTS models based on in-context learning (NaturalSpeech2 (2022), VALL-E (2023), Voicebox) all share the capability to perform zero-shot speech synthesis, as explained in Section 4.4. In fact, the in-context training strategy underlies the ability of these models to synthesize speech given only a style prompt with the input text. Specifically, the synthesis process treats the provided prompt/reference as part of the desired output speech. Therefore, the model\u2019s goal is to predict the rest of this speech in the same style as the given part (prompt) and with the input text. In Table 5 we list papers addressing each challenge.</p>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#6","title":"6.\u6570\u636e\u96c6\u4e0e\u5f00\u6e90\u4ee3\u7801","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#7","title":"7.\u8bc4\u4ef7\u6307\u6807","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#8","title":"8.\u8ba8\u8bba","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#9","title":"9.\u7ed3\u8bba","text":"","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]},{"location":"TTS/Papers/Survey/DL-Based_Expressive_Speech_Synthesis/#r","title":"R.\u53c2\u8003\u6587\u732e","text":"<ul> <li>019 Cross-Speaker Emotion Transfer by Manipulating Speech Style Latents <li>032 Controllable Emotion Transfer for End-to-End Speech Synthesis</li> <li>034 Cross-Speaker Emotion Disentangling and Transfer for End-to-End Speech Synthesis</li> <li>047 Multi-Speaker Emotional Speech Synthesis with Fine-Grained Prosody Modeling</li> <li>074 Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron</li> <li>090 Domain-Adversarial Training of Neural Networks</li> <li>097 STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text To Speech</li> <li>102 Joint and Adversarial Training with ASR for Expressive Speech Synthesis</li> <li>111 Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement</li>","tags":["\u8bed\u97f3\u5408\u6210_TTS","#\u7efc\u8ff0_Survey","#\u8868\u73b0\u6027\u8bed\u97f3_Expressive_Speech"]}]}